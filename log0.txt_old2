start
Mon Dec 26 12:46:37 CET 2022
2022-12-26 12:46:38.975591: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-26 12:46:39.086925: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-12-26 12:46:39.113404: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-12-26 12:47:08,230 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fede71ef8b0> object, created with modnet version 0.1.12
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
2022-12-26 12:47:10.102661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-26 12:47:10.483771: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2022-12-26 12:47:10.483889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20637 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:0f:00.0, compute capability: 8.6
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense (Dense)               (None, 2528)              3197920   
                                                                 
 batch_normalization (BatchN  (None, 2528)             10112     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_2 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 2528)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
2022-12-26 12:47:12.763047: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
5969/5969 - 18s - loss: 0.0080 - val_loss: 0.0031 - 18s/epoch - 3ms/step
Epoch 2/100
5969/5969 - 17s - loss: 0.0035 - val_loss: 0.0020 - 17s/epoch - 3ms/step
Epoch 3/100
5969/5969 - 17s - loss: 0.0023 - val_loss: 0.0015 - 17s/epoch - 3ms/step
Epoch 4/100
5969/5969 - 17s - loss: 0.0017 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 5/100
5969/5969 - 17s - loss: 0.0013 - val_loss: 9.4761e-04 - 17s/epoch - 3ms/step
Epoch 6/100
5969/5969 - 17s - loss: 0.0011 - val_loss: 9.1764e-04 - 17s/epoch - 3ms/step
Epoch 7/100
5969/5969 - 17s - loss: 9.9807e-04 - val_loss: 8.9202e-04 - 17s/epoch - 3ms/step
Epoch 8/100
5969/5969 - 17s - loss: 9.1425e-04 - val_loss: 7.4634e-04 - 17s/epoch - 3ms/step
Epoch 9/100
5969/5969 - 17s - loss: 8.6282e-04 - val_loss: 7.0940e-04 - 17s/epoch - 3ms/step
Epoch 10/100
5969/5969 - 17s - loss: 7.9778e-04 - val_loss: 7.4811e-04 - 17s/epoch - 3ms/step
Epoch 11/100
5969/5969 - 17s - loss: 7.5184e-04 - val_loss: 6.7340e-04 - 17s/epoch - 3ms/step
Epoch 12/100
5969/5969 - 17s - loss: 7.2434e-04 - val_loss: 6.6350e-04 - 17s/epoch - 3ms/step
Epoch 13/100
5969/5969 - 17s - loss: 6.9301e-04 - val_loss: 6.2620e-04 - 17s/epoch - 3ms/step
Epoch 14/100
5969/5969 - 17s - loss: 6.7922e-04 - val_loss: 6.0964e-04 - 17s/epoch - 3ms/step
Epoch 15/100
5969/5969 - 17s - loss: 6.5032e-04 - val_loss: 5.9511e-04 - 17s/epoch - 3ms/step
Epoch 16/100
5969/5969 - 17s - loss: 6.1581e-04 - val_loss: 6.0884e-04 - 17s/epoch - 3ms/step
Epoch 17/100
5969/5969 - 17s - loss: 6.0684e-04 - val_loss: 6.7998e-04 - 17s/epoch - 3ms/step
Epoch 18/100
5969/5969 - 17s - loss: 5.9587e-04 - val_loss: 6.0606e-04 - 17s/epoch - 3ms/step
Epoch 19/100
5969/5969 - 17s - loss: 5.7452e-04 - val_loss: 6.3090e-04 - 17s/epoch - 3ms/step
Epoch 20/100
5969/5969 - 17s - loss: 5.6785e-04 - val_loss: 5.5661e-04 - 17s/epoch - 3ms/step
Epoch 21/100
5969/5969 - 17s - loss: 5.5534e-04 - val_loss: 5.9223e-04 - 17s/epoch - 3ms/step
Epoch 22/100
5969/5969 - 17s - loss: 5.5655e-04 - val_loss: 5.8067e-04 - 17s/epoch - 3ms/step
Epoch 23/100
5969/5969 - 17s - loss: 5.2525e-04 - val_loss: 6.0999e-04 - 17s/epoch - 3ms/step
Epoch 24/100
5969/5969 - 17s - loss: 5.1919e-04 - val_loss: 4.9800e-04 - 17s/epoch - 3ms/step
Epoch 25/100
5969/5969 - 17s - loss: 5.1044e-04 - val_loss: 5.5796e-04 - 17s/epoch - 3ms/step
Epoch 26/100
5969/5969 - 17s - loss: 5.0736e-04 - val_loss: 5.3725e-04 - 17s/epoch - 3ms/step
Epoch 27/100
5969/5969 - 17s - loss: 5.0307e-04 - val_loss: 5.0007e-04 - 17s/epoch - 3ms/step
Epoch 28/100
5969/5969 - 17s - loss: 4.8822e-04 - val_loss: 5.1902e-04 - 17s/epoch - 3ms/step
Epoch 29/100
5969/5969 - 17s - loss: 5.0084e-04 - val_loss: 5.4798e-04 - 17s/epoch - 3ms/step
Epoch 30/100
5969/5969 - 17s - loss: 4.7898e-04 - val_loss: 4.5093e-04 - 17s/epoch - 3ms/step
Epoch 31/100
5969/5969 - 17s - loss: 4.7502e-04 - val_loss: 5.2618e-04 - 17s/epoch - 3ms/step
Epoch 32/100
5969/5969 - 17s - loss: 4.6033e-04 - val_loss: 6.6443e-04 - 17s/epoch - 3ms/step
Epoch 33/100
5969/5969 - 17s - loss: 4.5744e-04 - val_loss: 4.3813e-04 - 17s/epoch - 3ms/step
Epoch 34/100
5969/5969 - 17s - loss: 4.5406e-04 - val_loss: 4.3895e-04 - 17s/epoch - 3ms/step
Epoch 35/100
5969/5969 - 17s - loss: 4.4817e-04 - val_loss: 6.6059e-04 - 17s/epoch - 3ms/step
Epoch 36/100
5969/5969 - 17s - loss: 4.4264e-04 - val_loss: 7.1311e-04 - 17s/epoch - 3ms/step
Epoch 37/100
5969/5969 - 17s - loss: 4.3955e-04 - val_loss: 8.2196e-04 - 17s/epoch - 3ms/step
Epoch 38/100
5969/5969 - 17s - loss: 4.3592e-04 - val_loss: 5.7332e-04 - 17s/epoch - 3ms/step
Epoch 39/100
5969/5969 - 17s - loss: 4.3191e-04 - val_loss: 4.1519e-04 - 17s/epoch - 3ms/step
Epoch 40/100
5969/5969 - 16s - loss: 4.2050e-04 - val_loss: 5.5995e-04 - 16s/epoch - 3ms/step
Epoch 41/100
5969/5969 - 16s - loss: 4.2335e-04 - val_loss: 5.0128e-04 - 16s/epoch - 3ms/step
Epoch 42/100
5969/5969 - 16s - loss: 4.1983e-04 - val_loss: 7.0966e-04 - 16s/epoch - 3ms/step
Epoch 43/100
5969/5969 - 16s - loss: 4.1089e-04 - val_loss: 5.6058e-04 - 16s/epoch - 3ms/step
Epoch 44/100
5969/5969 - 16s - loss: 4.1353e-04 - val_loss: 5.3831e-04 - 16s/epoch - 3ms/step
Epoch 45/100
5969/5969 - 16s - loss: 4.1175e-04 - val_loss: 6.2395e-04 - 16s/epoch - 3ms/step
Epoch 46/100
5969/5969 - 16s - loss: 4.1194e-04 - val_loss: 5.3374e-04 - 16s/epoch - 3ms/step
Epoch 47/100
5969/5969 - 16s - loss: 4.0641e-04 - val_loss: 6.1462e-04 - 16s/epoch - 3ms/step
Epoch 48/100
5969/5969 - 16s - loss: 3.9793e-04 - val_loss: 6.0235e-04 - 16s/epoch - 3ms/step
Epoch 49/100
5969/5969 - 16s - loss: 4.0338e-04 - val_loss: 5.5997e-04 - 16s/epoch - 3ms/step
Epoch 50/100
5969/5969 - 16s - loss: 3.8811e-04 - val_loss: 4.6480e-04 - 16s/epoch - 3ms/step
Epoch 51/100
5969/5969 - 16s - loss: 3.9915e-04 - val_loss: 3.8708e-04 - 16s/epoch - 3ms/step
Epoch 52/100
5969/5969 - 16s - loss: 3.8724e-04 - val_loss: 8.2339e-04 - 16s/epoch - 3ms/step
Epoch 53/100
5969/5969 - 16s - loss: 3.9574e-04 - val_loss: 5.4879e-04 - 16s/epoch - 3ms/step
Epoch 54/100
5969/5969 - 16s - loss: 3.8413e-04 - val_loss: 8.4387e-04 - 16s/epoch - 3ms/step
Epoch 55/100
5969/5969 - 16s - loss: 3.8085e-04 - val_loss: 4.0945e-04 - 16s/epoch - 3ms/step
Epoch 56/100
5969/5969 - 16s - loss: 3.8177e-04 - val_loss: 5.0002e-04 - 16s/epoch - 3ms/step
Epoch 57/100
5969/5969 - 16s - loss: 3.7609e-04 - val_loss: 5.4818e-04 - 16s/epoch - 3ms/step
Epoch 58/100
5969/5969 - 16s - loss: 3.7071e-04 - val_loss: 4.6168e-04 - 16s/epoch - 3ms/step
Epoch 59/100
5969/5969 - 16s - loss: 3.6735e-04 - val_loss: 4.9853e-04 - 16s/epoch - 3ms/step
Epoch 60/100
5969/5969 - 16s - loss: 3.7407e-04 - val_loss: 5.3523e-04 - 16s/epoch - 3ms/step
Epoch 61/100
5969/5969 - 16s - loss: 3.7101e-04 - val_loss: 6.6665e-04 - 16s/epoch - 3ms/step
Epoch 62/100
5969/5969 - 16s - loss: 3.6521e-04 - val_loss: 9.6282e-04 - 16s/epoch - 3ms/step
Epoch 63/100
5969/5969 - 16s - loss: 3.7052e-04 - val_loss: 5.1237e-04 - 16s/epoch - 3ms/step
Epoch 64/100
5969/5969 - 16s - loss: 3.6444e-04 - val_loss: 5.5179e-04 - 16s/epoch - 3ms/step
Epoch 65/100
5969/5969 - 16s - loss: 3.6108e-04 - val_loss: 5.0256e-04 - 16s/epoch - 3ms/step
Epoch 66/100
5969/5969 - 16s - loss: 3.6258e-04 - val_loss: 5.6407e-04 - 16s/epoch - 3ms/step
Epoch 67/100
5969/5969 - 16s - loss: 3.6191e-04 - val_loss: 5.3983e-04 - 16s/epoch - 3ms/step
Epoch 68/100
5969/5969 - 16s - loss: 3.5664e-04 - val_loss: 5.9705e-04 - 16s/epoch - 3ms/step
Epoch 69/100
5969/5969 - 16s - loss: 3.5181e-04 - val_loss: 6.6150e-04 - 16s/epoch - 3ms/step
Epoch 70/100
5969/5969 - 16s - loss: 3.5287e-04 - val_loss: 6.6501e-04 - 16s/epoch - 3ms/step
Epoch 71/100
5969/5969 - 16s - loss: 3.5075e-04 - val_loss: 5.4178e-04 - 16s/epoch - 3ms/step
Epoch 72/100
5969/5969 - 16s - loss: 3.5011e-04 - val_loss: 8.2912e-04 - 16s/epoch - 3ms/step
Epoch 73/100
5969/5969 - 16s - loss: 3.4907e-04 - val_loss: 6.4239e-04 - 16s/epoch - 3ms/step
Epoch 74/100
5969/5969 - 16s - loss: 3.4802e-04 - val_loss: 5.0406e-04 - 16s/epoch - 3ms/step
Epoch 75/100
5969/5969 - 16s - loss: 3.4818e-04 - val_loss: 5.8499e-04 - 16s/epoch - 3ms/step
Epoch 76/100
5969/5969 - 16s - loss: 3.3740e-04 - val_loss: 6.1780e-04 - 16s/epoch - 3ms/step
Epoch 77/100
5969/5969 - 16s - loss: 3.4559e-04 - val_loss: 4.7736e-04 - 16s/epoch - 3ms/step
Epoch 78/100
5969/5969 - 16s - loss: 3.3908e-04 - val_loss: 4.3781e-04 - 16s/epoch - 3ms/step
Epoch 79/100
5969/5969 - 16s - loss: 3.3994e-04 - val_loss: 4.5983e-04 - 16s/epoch - 3ms/step
Epoch 80/100
5969/5969 - 16s - loss: 3.4042e-04 - val_loss: 3.3012e-04 - 16s/epoch - 3ms/step
Epoch 81/100
5969/5969 - 16s - loss: 3.3723e-04 - val_loss: 6.6765e-04 - 16s/epoch - 3ms/step
Epoch 82/100
5969/5969 - 16s - loss: 3.4301e-04 - val_loss: 4.7288e-04 - 16s/epoch - 3ms/step
Epoch 83/100
5969/5969 - 16s - loss: 3.4033e-04 - val_loss: 7.6321e-04 - 16s/epoch - 3ms/step
Epoch 84/100
5969/5969 - 16s - loss: 3.3649e-04 - val_loss: 6.7891e-04 - 16s/epoch - 3ms/step
Epoch 85/100
5969/5969 - 16s - loss: 3.3728e-04 - val_loss: 5.5333e-04 - 16s/epoch - 3ms/step
Epoch 86/100
5969/5969 - 16s - loss: 3.3229e-04 - val_loss: 4.1161e-04 - 16s/epoch - 3ms/step
Epoch 87/100
5969/5969 - 16s - loss: 3.2976e-04 - val_loss: 5.7323e-04 - 16s/epoch - 3ms/step
Epoch 88/100
5969/5969 - 16s - loss: 3.3158e-04 - val_loss: 4.6810e-04 - 16s/epoch - 3ms/step
Epoch 89/100
5969/5969 - 16s - loss: 3.3010e-04 - val_loss: 4.1293e-04 - 16s/epoch - 3ms/step
Epoch 90/100
5969/5969 - 16s - loss: 3.2399e-04 - val_loss: 3.9085e-04 - 16s/epoch - 3ms/step
Epoch 91/100
5969/5969 - 16s - loss: 3.3509e-04 - val_loss: 3.9153e-04 - 16s/epoch - 3ms/step
Epoch 92/100
5969/5969 - 16s - loss: 3.2594e-04 - val_loss: 8.3894e-04 - 16s/epoch - 3ms/step
Epoch 93/100
5969/5969 - 16s - loss: 3.2563e-04 - val_loss: 5.8629e-04 - 16s/epoch - 3ms/step
Epoch 94/100
5969/5969 - 16s - loss: 3.2742e-04 - val_loss: 4.2101e-04 - 16s/epoch - 3ms/step
Epoch 95/100
5969/5969 - 16s - loss: 3.2496e-04 - val_loss: 6.7610e-04 - 16s/epoch - 3ms/step
Epoch 96/100
5969/5969 - 16s - loss: 3.5190e-04 - val_loss: 4.1891e-04 - 16s/epoch - 3ms/step
Epoch 97/100
5969/5969 - 16s - loss: 3.2966e-04 - val_loss: 5.2528e-04 - 16s/epoch - 3ms/step
Epoch 98/100
5969/5969 - 16s - loss: 3.2139e-04 - val_loss: 4.8953e-04 - 16s/epoch - 3ms/step
Epoch 99/100
5969/5969 - 16s - loss: 3.1767e-04 - val_loss: 5.1786e-04 - 16s/epoch - 3ms/step
Epoch 100/100
5969/5969 - 16s - loss: 3.1576e-04 - val_loss: 3.9587e-04 - 16s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.000395873561501503
  1/332 [..............................] - ETA: 34s 50/332 [===>..........................] - ETA: 0s 100/332 [========>.....................] - ETA: 0s150/332 [============>.................] - ETA: 0s201/332 [=================>............] - ETA: 0s250/332 [=====================>........] - ETA: 0s300/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.004318786794133695
cosine 0.0035355146756677236
MAE: 0.008807434
RMSE: 0.019896546
r2: 0.9743191706849785
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 2528)              3197920   
                                                                 
 batch_normalization (BatchN  (None, 2528)             10112     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_2 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 2528)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 2528)              3197920   
                                                                 
 batch_normalization (BatchN  (None, 2528)             10112     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_2 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 2528)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 16, 100, 0.0005, 0.5, 632, 0.00031576422043144703, 0.000395873561501503, 0.004318786794133695, 0.0035355146756677236, 0.008807433769106865, 0.019896546378731728, 0.9743191706849785, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_3 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_3 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_5 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2528)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
5969/5969 - 17s - loss: 0.0075 - val_loss: 0.0025 - 17s/epoch - 3ms/step
Epoch 2/100
5969/5969 - 17s - loss: 0.0027 - val_loss: 0.0017 - 17s/epoch - 3ms/step
Epoch 3/100
5969/5969 - 17s - loss: 0.0018 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 4/100
5969/5969 - 17s - loss: 0.0014 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 5/100
5969/5969 - 17s - loss: 0.0012 - val_loss: 9.5253e-04 - 17s/epoch - 3ms/step
Epoch 6/100
5969/5969 - 17s - loss: 0.0011 - val_loss: 8.5601e-04 - 17s/epoch - 3ms/step
Epoch 7/100
5969/5969 - 17s - loss: 0.0010 - val_loss: 8.2779e-04 - 17s/epoch - 3ms/step
Epoch 8/100
5969/5969 - 17s - loss: 9.4969e-04 - val_loss: 7.4109e-04 - 17s/epoch - 3ms/step
Epoch 9/100
5969/5969 - 17s - loss: 9.1438e-04 - val_loss: 7.0876e-04 - 17s/epoch - 3ms/step
Epoch 10/100
5969/5969 - 17s - loss: 8.4813e-04 - val_loss: 6.8500e-04 - 17s/epoch - 3ms/step
Epoch 11/100
5969/5969 - 17s - loss: 8.1243e-04 - val_loss: 6.4958e-04 - 17s/epoch - 3ms/step
Epoch 12/100
5969/5969 - 17s - loss: 7.8928e-04 - val_loss: 6.5395e-04 - 17s/epoch - 3ms/step
Epoch 13/100
5969/5969 - 17s - loss: 7.6725e-04 - val_loss: 5.9701e-04 - 17s/epoch - 3ms/step
Epoch 14/100
5969/5969 - 17s - loss: 7.3903e-04 - val_loss: 5.8095e-04 - 17s/epoch - 3ms/step
Epoch 15/100
5969/5969 - 17s - loss: 7.3875e-04 - val_loss: 5.8094e-04 - 17s/epoch - 3ms/step
Epoch 16/100
5969/5969 - 17s - loss: 7.0200e-04 - val_loss: 5.6945e-04 - 17s/epoch - 3ms/step
Epoch 17/100
5969/5969 - 17s - loss: 6.8355e-04 - val_loss: 5.7808e-04 - 17s/epoch - 3ms/step
Epoch 18/100
5969/5969 - 17s - loss: 6.8427e-04 - val_loss: 5.5555e-04 - 17s/epoch - 3ms/step
Epoch 19/100
5969/5969 - 17s - loss: 6.5254e-04 - val_loss: 5.4280e-04 - 17s/epoch - 3ms/step
Epoch 20/100
5969/5969 - 17s - loss: 6.4236e-04 - val_loss: 4.9917e-04 - 17s/epoch - 3ms/step
Epoch 21/100
5969/5969 - 17s - loss: 6.3657e-04 - val_loss: 4.9538e-04 - 17s/epoch - 3ms/step
Epoch 22/100
5969/5969 - 17s - loss: 6.1973e-04 - val_loss: 4.8512e-04 - 17s/epoch - 3ms/step
Epoch 23/100
5969/5969 - 17s - loss: 6.1711e-04 - val_loss: 5.0090e-04 - 17s/epoch - 3ms/step
Epoch 24/100
5969/5969 - 17s - loss: 6.0507e-04 - val_loss: 5.0860e-04 - 17s/epoch - 3ms/step
Epoch 25/100
5969/5969 - 17s - loss: 6.1381e-04 - val_loss: 4.7381e-04 - 17s/epoch - 3ms/step
Epoch 26/100
5969/5969 - 17s - loss: 5.8816e-04 - val_loss: 4.5280e-04 - 17s/epoch - 3ms/step
Epoch 27/100
5969/5969 - 17s - loss: 5.7768e-04 - val_loss: 4.5517e-04 - 17s/epoch - 3ms/step
Epoch 28/100
5969/5969 - 17s - loss: 5.7651e-04 - val_loss: 4.4699e-04 - 17s/epoch - 3ms/step
Epoch 29/100
5969/5969 - 17s - loss: 5.8119e-04 - val_loss: 4.6462e-04 - 17s/epoch - 3ms/step
Epoch 30/100
5969/5969 - 17s - loss: 5.5656e-04 - val_loss: 4.4144e-04 - 17s/epoch - 3ms/step
Epoch 31/100
5969/5969 - 17s - loss: 5.5920e-04 - val_loss: 4.1783e-04 - 17s/epoch - 3ms/step
Epoch 32/100
5969/5969 - 17s - loss: 5.9613e-04 - val_loss: 4.5514e-04 - 17s/epoch - 3ms/step
Epoch 33/100
5969/5969 - 17s - loss: 5.4463e-04 - val_loss: 4.2413e-04 - 17s/epoch - 3ms/step
Epoch 34/100
5969/5969 - 17s - loss: 5.5056e-04 - val_loss: 4.1161e-04 - 17s/epoch - 3ms/step
Epoch 35/100
5969/5969 - 17s - loss: 5.3256e-04 - val_loss: 4.0600e-04 - 17s/epoch - 3ms/step
Epoch 36/100
5969/5969 - 17s - loss: 5.2926e-04 - val_loss: 4.2964e-04 - 17s/epoch - 3ms/step
Epoch 37/100
5969/5969 - 17s - loss: 5.3453e-04 - val_loss: 4.3204e-04 - 17s/epoch - 3ms/step
Epoch 38/100
5969/5969 - 17s - loss: 5.1893e-04 - val_loss: 3.9974e-04 - 17s/epoch - 3ms/step
Epoch 39/100
5969/5969 - 17s - loss: 5.1627e-04 - val_loss: 4.0271e-04 - 17s/epoch - 3ms/step
Epoch 40/100
5969/5969 - 17s - loss: 5.0913e-04 - val_loss: 4.1245e-04 - 17s/epoch - 3ms/step
Epoch 41/100
5969/5969 - 17s - loss: 5.0750e-04 - val_loss: 3.9607e-04 - 17s/epoch - 3ms/step
Epoch 42/100
5969/5969 - 17s - loss: 5.0654e-04 - val_loss: 3.8913e-04 - 17s/epoch - 3ms/step
Epoch 43/100
5969/5969 - 17s - loss: 5.0401e-04 - val_loss: 4.0975e-04 - 17s/epoch - 3ms/step
Epoch 44/100
5969/5969 - 17s - loss: 4.9693e-04 - val_loss: 4.1269e-04 - 17s/epoch - 3ms/step
Epoch 45/100
5969/5969 - 17s - loss: 4.9592e-04 - val_loss: 3.8208e-04 - 17s/epoch - 3ms/step
Epoch 46/100
5969/5969 - 17s - loss: 4.8938e-04 - val_loss: 3.7960e-04 - 17s/epoch - 3ms/step
Epoch 47/100
5969/5969 - 17s - loss: 4.9414e-04 - val_loss: 3.9075e-04 - 17s/epoch - 3ms/step
Epoch 48/100
5969/5969 - 17s - loss: 4.8400e-04 - val_loss: 3.8154e-04 - 17s/epoch - 3ms/step
Epoch 49/100
5969/5969 - 17s - loss: 4.8988e-04 - val_loss: 3.7206e-04 - 17s/epoch - 3ms/step
Epoch 50/100
5969/5969 - 17s - loss: 4.7874e-04 - val_loss: 3.9231e-04 - 17s/epoch - 3ms/step
Epoch 51/100
5969/5969 - 17s - loss: 4.7282e-04 - val_loss: 3.7676e-04 - 17s/epoch - 3ms/step
Epoch 52/100
5969/5969 - 17s - loss: 4.6724e-04 - val_loss: 3.6029e-04 - 17s/epoch - 3ms/step
Epoch 53/100
5969/5969 - 17s - loss: 4.6985e-04 - val_loss: 3.6445e-04 - 17s/epoch - 3ms/step
Epoch 54/100
5969/5969 - 17s - loss: 4.6742e-04 - val_loss: 3.7345e-04 - 17s/epoch - 3ms/step
Epoch 55/100
5969/5969 - 17s - loss: 4.7152e-04 - val_loss: 3.5712e-04 - 17s/epoch - 3ms/step
Epoch 56/100
5969/5969 - 17s - loss: 4.5977e-04 - val_loss: 3.5222e-04 - 17s/epoch - 3ms/step
Epoch 57/100
5969/5969 - 17s - loss: 4.6302e-04 - val_loss: 3.5279e-04 - 17s/epoch - 3ms/step
Epoch 58/100
5969/5969 - 17s - loss: 4.6004e-04 - val_loss: 3.6322e-04 - 17s/epoch - 3ms/step
Epoch 59/100
5969/5969 - 17s - loss: 4.5573e-04 - val_loss: 3.4111e-04 - 17s/epoch - 3ms/step
Epoch 60/100
5969/5969 - 17s - loss: 4.5240e-04 - val_loss: 3.4781e-04 - 17s/epoch - 3ms/step
Epoch 61/100
5969/5969 - 17s - loss: 4.5005e-04 - val_loss: 3.6312e-04 - 17s/epoch - 3ms/step
Epoch 62/100
5969/5969 - 17s - loss: 4.4692e-04 - val_loss: 3.8021e-04 - 17s/epoch - 3ms/step
Epoch 63/100
5969/5969 - 17s - loss: 4.5228e-04 - val_loss: 3.5235e-04 - 17s/epoch - 3ms/step
Epoch 64/100
5969/5969 - 17s - loss: 4.4846e-04 - val_loss: 3.6356e-04 - 17s/epoch - 3ms/step
Epoch 65/100
5969/5969 - 17s - loss: 4.4259e-04 - val_loss: 3.5941e-04 - 17s/epoch - 3ms/step
Epoch 66/100
5969/5969 - 17s - loss: 4.3955e-04 - val_loss: 3.4645e-04 - 17s/epoch - 3ms/step
Epoch 67/100
5969/5969 - 17s - loss: 4.5367e-04 - val_loss: 3.8488e-04 - 17s/epoch - 3ms/step
Epoch 68/100
5969/5969 - 17s - loss: 4.3751e-04 - val_loss: 3.4592e-04 - 17s/epoch - 3ms/step
Epoch 69/100
5969/5969 - 17s - loss: 4.4601e-04 - val_loss: 3.5157e-04 - 17s/epoch - 3ms/step
Epoch 70/100
5969/5969 - 17s - loss: 4.3767e-04 - val_loss: 3.5113e-04 - 17s/epoch - 3ms/step
Epoch 71/100
5969/5969 - 17s - loss: 4.3973e-04 - val_loss: 3.5871e-04 - 17s/epoch - 3ms/step
Epoch 72/100
5969/5969 - 17s - loss: 4.3788e-04 - val_loss: 3.6482e-04 - 17s/epoch - 3ms/step
Epoch 73/100
5969/5969 - 17s - loss: 4.3114e-04 - val_loss: 3.3636e-04 - 17s/epoch - 3ms/step
Epoch 74/100
5969/5969 - 17s - loss: 4.4310e-04 - val_loss: 3.9547e-04 - 17s/epoch - 3ms/step
Epoch 75/100
5969/5969 - 17s - loss: 4.3858e-04 - val_loss: 3.7603e-04 - 17s/epoch - 3ms/step
Epoch 76/100
5969/5969 - 17s - loss: 4.2342e-04 - val_loss: 3.4019e-04 - 17s/epoch - 3ms/step
Epoch 77/100
5969/5969 - 17s - loss: 4.4236e-04 - val_loss: 3.4760e-04 - 17s/epoch - 3ms/step
Epoch 78/100
5969/5969 - 17s - loss: 4.2748e-04 - val_loss: 3.3537e-04 - 17s/epoch - 3ms/step
Epoch 79/100
5969/5969 - 17s - loss: 4.2147e-04 - val_loss: 3.3666e-04 - 17s/epoch - 3ms/step
Epoch 80/100
5969/5969 - 17s - loss: 4.3395e-04 - val_loss: 3.3843e-04 - 17s/epoch - 3ms/step
Epoch 81/100
5969/5969 - 17s - loss: 4.4139e-04 - val_loss: 3.5437e-04 - 17s/epoch - 3ms/step
Epoch 82/100
5969/5969 - 17s - loss: 4.1980e-04 - val_loss: 3.3217e-04 - 17s/epoch - 3ms/step
Epoch 83/100
5969/5969 - 17s - loss: 4.2647e-04 - val_loss: 3.5410e-04 - 17s/epoch - 3ms/step
Epoch 84/100
5969/5969 - 17s - loss: 4.1605e-04 - val_loss: 3.4118e-04 - 17s/epoch - 3ms/step
Epoch 85/100
5969/5969 - 17s - loss: 4.3610e-04 - val_loss: 3.5022e-04 - 17s/epoch - 3ms/step
Epoch 86/100
5969/5969 - 17s - loss: 4.1251e-04 - val_loss: 3.4120e-04 - 17s/epoch - 3ms/step
Epoch 87/100
5969/5969 - 17s - loss: 4.1852e-04 - val_loss: 3.3163e-04 - 17s/epoch - 3ms/step
Epoch 88/100
5969/5969 - 17s - loss: 4.1648e-04 - val_loss: 3.2010e-04 - 17s/epoch - 3ms/step
Epoch 89/100
5969/5969 - 17s - loss: 4.1802e-04 - val_loss: 3.6808e-04 - 17s/epoch - 3ms/step
Epoch 90/100
5969/5969 - 17s - loss: 4.1369e-04 - val_loss: 3.1754e-04 - 17s/epoch - 3ms/step
Epoch 91/100
5969/5969 - 17s - loss: 4.1940e-04 - val_loss: 3.3696e-04 - 17s/epoch - 3ms/step
Epoch 92/100
5969/5969 - 17s - loss: 4.0906e-04 - val_loss: 3.4349e-04 - 17s/epoch - 3ms/step
Epoch 93/100
5969/5969 - 17s - loss: 4.0968e-04 - val_loss: 3.4698e-04 - 17s/epoch - 3ms/step
Epoch 94/100
5969/5969 - 17s - loss: 4.0677e-04 - val_loss: 3.2052e-04 - 17s/epoch - 3ms/step
Epoch 95/100
5969/5969 - 17s - loss: 4.0836e-04 - val_loss: 3.2475e-04 - 17s/epoch - 3ms/step
Epoch 96/100
5969/5969 - 17s - loss: 4.0566e-04 - val_loss: 3.5081e-04 - 17s/epoch - 3ms/step
Epoch 97/100
5969/5969 - 17s - loss: 4.0658e-04 - val_loss: 3.2803e-04 - 17s/epoch - 3ms/step
Epoch 98/100
5969/5969 - 17s - loss: 3.9878e-04 - val_loss: 3.2393e-04 - 17s/epoch - 3ms/step
Epoch 99/100
5969/5969 - 17s - loss: 3.9683e-04 - val_loss: 3.1701e-04 - 17s/epoch - 3ms/step
Epoch 100/100
5969/5969 - 17s - loss: 4.1988e-04 - val_loss: 3.2478e-04 - 17s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0003247809363529086
  1/332 [..............................] - ETA: 28s 50/332 [===>..........................] - ETA: 0s  99/332 [=======>......................] - ETA: 0s148/332 [============>.................] - ETA: 0s197/332 [================>.............] - ETA: 0s245/332 [=====================>........] - ETA: 0s294/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0035578881429358783
cosine 0.0028446719943466072
MAE: 0.009452436
RMSE: 0.018021664
r2: 0.9789310965433028
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_3 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_5 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2528)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_3 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_5 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2528)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 16, 100, 0.001, 0.5, 632, 0.00041987589793279767, 0.0003247809363529086, 0.0035578881429358783, 0.0028446719943466072, 0.009452436119318008, 0.018021663650870323, 0.9789310965433028, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_6 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_6 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_8 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2528)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
5969/5969 - 17s - loss: 0.0076 - val_loss: 0.0025 - 17s/epoch - 3ms/step
Epoch 2/100
5969/5969 - 17s - loss: 0.0025 - val_loss: 0.0019 - 17s/epoch - 3ms/step
Epoch 3/100
5969/5969 - 17s - loss: 0.0018 - val_loss: 0.0015 - 17s/epoch - 3ms/step
Epoch 4/100
5969/5969 - 17s - loss: 0.0015 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 5/100
5969/5969 - 17s - loss: 0.0014 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 6/100
5969/5969 - 17s - loss: 0.0013 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 7/100
5969/5969 - 17s - loss: 0.0012 - val_loss: 9.3953e-04 - 17s/epoch - 3ms/step
Epoch 8/100
5969/5969 - 17s - loss: 0.0011 - val_loss: 8.8448e-04 - 17s/epoch - 3ms/step
Epoch 9/100
5969/5969 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 10/100
5969/5969 - 17s - loss: 0.0010 - val_loss: 8.2040e-04 - 17s/epoch - 3ms/step
Epoch 11/100
5969/5969 - 17s - loss: 9.8924e-04 - val_loss: 8.0435e-04 - 17s/epoch - 3ms/step
Epoch 12/100
5969/5969 - 17s - loss: 9.6432e-04 - val_loss: 8.0275e-04 - 17s/epoch - 3ms/step
Epoch 13/100
5969/5969 - 17s - loss: 9.3575e-04 - val_loss: 7.3354e-04 - 17s/epoch - 3ms/step
Epoch 14/100
5969/5969 - 17s - loss: 9.1753e-04 - val_loss: 7.4717e-04 - 17s/epoch - 3ms/step
Epoch 15/100
5969/5969 - 17s - loss: 9.0413e-04 - val_loss: 7.1083e-04 - 17s/epoch - 3ms/step
Epoch 16/100
5969/5969 - 17s - loss: 8.7384e-04 - val_loss: 7.4695e-04 - 17s/epoch - 3ms/step
Epoch 17/100
5969/5969 - 17s - loss: 8.6705e-04 - val_loss: 6.9482e-04 - 17s/epoch - 3ms/step
Epoch 18/100
5969/5969 - 17s - loss: 8.5504e-04 - val_loss: 6.7224e-04 - 17s/epoch - 3ms/step
Epoch 19/100
5969/5969 - 17s - loss: 8.3030e-04 - val_loss: 6.9856e-04 - 17s/epoch - 3ms/step
Epoch 20/100
5969/5969 - 17s - loss: 8.2681e-04 - val_loss: 6.7171e-04 - 17s/epoch - 3ms/step
Epoch 21/100
5969/5969 - 17s - loss: 8.1468e-04 - val_loss: 6.5967e-04 - 17s/epoch - 3ms/step
Epoch 22/100
5969/5969 - 17s - loss: 7.9488e-04 - val_loss: 6.3397e-04 - 17s/epoch - 3ms/step
Epoch 23/100
5969/5969 - 17s - loss: 7.8292e-04 - val_loss: 6.3185e-04 - 17s/epoch - 3ms/step
Epoch 24/100
5969/5969 - 17s - loss: 7.7121e-04 - val_loss: 6.3753e-04 - 17s/epoch - 3ms/step
Epoch 25/100
5969/5969 - 17s - loss: 7.8290e-04 - val_loss: 6.3575e-04 - 17s/epoch - 3ms/step
Epoch 26/100
5969/5969 - 17s - loss: 7.7631e-04 - val_loss: 6.0063e-04 - 17s/epoch - 3ms/step
Epoch 27/100
5969/5969 - 17s - loss: 7.4872e-04 - val_loss: 6.0090e-04 - 17s/epoch - 3ms/step
Epoch 28/100
5969/5969 - 17s - loss: 7.6702e-04 - val_loss: 5.9205e-04 - 17s/epoch - 3ms/step
Epoch 29/100
5969/5969 - 17s - loss: 7.4153e-04 - val_loss: 5.7471e-04 - 17s/epoch - 3ms/step
Epoch 30/100
5969/5969 - 17s - loss: 7.3641e-04 - val_loss: 5.9469e-04 - 17s/epoch - 3ms/step
Epoch 31/100
5969/5969 - 17s - loss: 7.2312e-04 - val_loss: 5.4339e-04 - 17s/epoch - 3ms/step
Epoch 32/100
5969/5969 - 17s - loss: 7.0846e-04 - val_loss: 6.0133e-04 - 17s/epoch - 3ms/step
Epoch 33/100
5969/5969 - 17s - loss: 7.7238e-04 - val_loss: 6.3194e-04 - 17s/epoch - 3ms/step
Epoch 34/100
5969/5969 - 17s - loss: 7.2263e-04 - val_loss: 5.4746e-04 - 17s/epoch - 3ms/step
Epoch 35/100
5969/5969 - 17s - loss: 7.1065e-04 - val_loss: 5.6716e-04 - 17s/epoch - 3ms/step
Epoch 36/100
5969/5969 - 17s - loss: 7.2736e-04 - val_loss: 5.6246e-04 - 17s/epoch - 3ms/step
Epoch 37/100
5969/5969 - 17s - loss: 7.0813e-04 - val_loss: 5.8983e-04 - 17s/epoch - 3ms/step
Epoch 38/100
5969/5969 - 17s - loss: 7.0409e-04 - val_loss: 5.5797e-04 - 17s/epoch - 3ms/step
Epoch 39/100
5969/5969 - 17s - loss: 6.9430e-04 - val_loss: 5.3605e-04 - 17s/epoch - 3ms/step
Epoch 40/100
5969/5969 - 17s - loss: 6.8301e-04 - val_loss: 5.2736e-04 - 17s/epoch - 3ms/step
Epoch 41/100
5969/5969 - 17s - loss: 6.8971e-04 - val_loss: 5.2322e-04 - 17s/epoch - 3ms/step
Epoch 42/100
5969/5969 - 17s - loss: 6.8180e-04 - val_loss: 5.3252e-04 - 17s/epoch - 3ms/step
Epoch 43/100
5969/5969 - 17s - loss: 6.7477e-04 - val_loss: 5.2920e-04 - 17s/epoch - 3ms/step
Epoch 44/100
5969/5969 - 17s - loss: 6.6422e-04 - val_loss: 5.6702e-04 - 17s/epoch - 3ms/step
Epoch 45/100
5969/5969 - 17s - loss: 6.7853e-04 - val_loss: 5.3031e-04 - 17s/epoch - 3ms/step
Epoch 46/100
5969/5969 - 17s - loss: 6.8887e-04 - val_loss: 6.0879e-04 - 17s/epoch - 3ms/step
Epoch 47/100
5969/5969 - 17s - loss: 6.6969e-04 - val_loss: 5.4699e-04 - 17s/epoch - 3ms/step
Epoch 48/100
5969/5969 - 17s - loss: 6.5415e-04 - val_loss: 5.1677e-04 - 17s/epoch - 3ms/step
Epoch 49/100
5969/5969 - 17s - loss: 7.5353e-04 - val_loss: 7.2425e-04 - 17s/epoch - 3ms/step
Epoch 50/100
5969/5969 - 17s - loss: 6.5829e-04 - val_loss: 5.8702e-04 - 17s/epoch - 3ms/step
Epoch 51/100
5969/5969 - 17s - loss: 6.7145e-04 - val_loss: 5.5335e-04 - 17s/epoch - 3ms/step
Epoch 52/100
5969/5969 - 17s - loss: 6.4493e-04 - val_loss: 5.3766e-04 - 17s/epoch - 3ms/step
Epoch 53/100
5969/5969 - 17s - loss: 6.4157e-04 - val_loss: 5.4023e-04 - 17s/epoch - 3ms/step
Epoch 54/100
5969/5969 - 17s - loss: 6.4138e-04 - val_loss: 5.2801e-04 - 17s/epoch - 3ms/step
Epoch 55/100
5969/5969 - 17s - loss: 6.4328e-04 - val_loss: 5.4057e-04 - 17s/epoch - 3ms/step
Epoch 56/100
5969/5969 - 17s - loss: 6.3018e-04 - val_loss: 5.8756e-04 - 17s/epoch - 3ms/step
Epoch 57/100
5969/5969 - 17s - loss: 6.3671e-04 - val_loss: 5.1943e-04 - 17s/epoch - 3ms/step
Epoch 58/100
5969/5969 - 17s - loss: 6.2478e-04 - val_loss: 5.1315e-04 - 17s/epoch - 3ms/step
Epoch 59/100
5969/5969 - 17s - loss: 6.3154e-04 - val_loss: 5.2713e-04 - 17s/epoch - 3ms/step
Epoch 60/100
5969/5969 - 17s - loss: 6.2309e-04 - val_loss: 4.9282e-04 - 17s/epoch - 3ms/step
Epoch 61/100
5969/5969 - 17s - loss: 6.3856e-04 - val_loss: 5.3887e-04 - 17s/epoch - 3ms/step
Epoch 62/100
5969/5969 - 17s - loss: 6.1810e-04 - val_loss: 6.4003e-04 - 17s/epoch - 3ms/step
Epoch 63/100
5969/5969 - 17s - loss: 6.2173e-04 - val_loss: 4.9864e-04 - 17s/epoch - 3ms/step
Epoch 64/100
5969/5969 - 17s - loss: 6.1376e-04 - val_loss: 4.9005e-04 - 17s/epoch - 3ms/step
Epoch 65/100
5969/5969 - 17s - loss: 6.1208e-04 - val_loss: 5.2451e-04 - 17s/epoch - 3ms/step
Epoch 66/100
5969/5969 - 17s - loss: 6.2645e-04 - val_loss: 4.9738e-04 - 17s/epoch - 3ms/step
Epoch 67/100
5969/5969 - 17s - loss: 6.1130e-04 - val_loss: 6.5332e-04 - 17s/epoch - 3ms/step
Epoch 68/100
5969/5969 - 17s - loss: 6.0544e-04 - val_loss: 5.7203e-04 - 17s/epoch - 3ms/step
Epoch 69/100
5969/5969 - 17s - loss: 6.3699e-04 - val_loss: 5.7477e-04 - 17s/epoch - 3ms/step
Epoch 70/100
5969/5969 - 17s - loss: 6.1310e-04 - val_loss: 5.2398e-04 - 17s/epoch - 3ms/step
Epoch 71/100
5969/5969 - 17s - loss: 6.0139e-04 - val_loss: 4.6540e-04 - 17s/epoch - 3ms/step
Epoch 72/100
5969/5969 - 17s - loss: 6.1817e-04 - val_loss: 4.8614e-04 - 17s/epoch - 3ms/step
Epoch 73/100
5969/5969 - 17s - loss: 5.9783e-04 - val_loss: 5.0066e-04 - 17s/epoch - 3ms/step
Epoch 74/100
5969/5969 - 17s - loss: 5.9821e-04 - val_loss: 5.3175e-04 - 17s/epoch - 3ms/step
Epoch 75/100
5969/5969 - 17s - loss: 5.9568e-04 - val_loss: 5.4316e-04 - 17s/epoch - 3ms/step
Epoch 76/100
5969/5969 - 17s - loss: 5.8890e-04 - val_loss: 5.9350e-04 - 17s/epoch - 3ms/step
Epoch 77/100
5969/5969 - 17s - loss: 6.4148e-04 - val_loss: 4.6396e-04 - 17s/epoch - 3ms/step
Epoch 78/100
5969/5969 - 17s - loss: 5.9847e-04 - val_loss: 4.6773e-04 - 17s/epoch - 3ms/step
Epoch 79/100
5969/5969 - 17s - loss: 6.0720e-04 - val_loss: 5.2073e-04 - 17s/epoch - 3ms/step
Epoch 80/100
5969/5969 - 17s - loss: 5.8585e-04 - val_loss: 4.8949e-04 - 17s/epoch - 3ms/step
Epoch 81/100
5969/5969 - 17s - loss: 5.8612e-04 - val_loss: 4.7525e-04 - 17s/epoch - 3ms/step
Epoch 82/100
5969/5969 - 17s - loss: 6.3603e-04 - val_loss: 4.9030e-04 - 17s/epoch - 3ms/step
Epoch 83/100
5969/5969 - 17s - loss: 6.0970e-04 - val_loss: 4.8978e-04 - 17s/epoch - 3ms/step
Epoch 84/100
5969/5969 - 17s - loss: 5.9497e-04 - val_loss: 4.7402e-04 - 17s/epoch - 3ms/step
Epoch 85/100
5969/5969 - 17s - loss: 5.9917e-04 - val_loss: 4.6143e-04 - 17s/epoch - 3ms/step
Epoch 86/100
5969/5969 - 17s - loss: 5.8496e-04 - val_loss: 4.8247e-04 - 17s/epoch - 3ms/step
Epoch 87/100
5969/5969 - 17s - loss: 5.9539e-04 - val_loss: 4.8376e-04 - 17s/epoch - 3ms/step
Epoch 88/100
5969/5969 - 17s - loss: 5.9120e-04 - val_loss: 4.4447e-04 - 17s/epoch - 3ms/step
Epoch 89/100
5969/5969 - 17s - loss: 5.8518e-04 - val_loss: 4.5185e-04 - 17s/epoch - 3ms/step
Epoch 90/100
5969/5969 - 17s - loss: 5.7989e-04 - val_loss: 4.3023e-04 - 17s/epoch - 3ms/step
Epoch 91/100
5969/5969 - 17s - loss: 6.0935e-04 - val_loss: 4.4147e-04 - 17s/epoch - 3ms/step
Epoch 92/100
5969/5969 - 17s - loss: 5.8033e-04 - val_loss: 4.6159e-04 - 17s/epoch - 3ms/step
Epoch 93/100
5969/5969 - 17s - loss: 5.8112e-04 - val_loss: 4.7950e-04 - 17s/epoch - 3ms/step
Epoch 94/100
5969/5969 - 17s - loss: 5.7289e-04 - val_loss: 4.4147e-04 - 17s/epoch - 3ms/step
Epoch 95/100
5969/5969 - 17s - loss: 5.7951e-04 - val_loss: 4.4847e-04 - 17s/epoch - 3ms/step
Epoch 96/100
5969/5969 - 17s - loss: 5.8558e-04 - val_loss: 4.7519e-04 - 17s/epoch - 3ms/step
Epoch 97/100
5969/5969 - 17s - loss: 5.7962e-04 - val_loss: 4.5358e-04 - 17s/epoch - 3ms/step
Epoch 98/100
5969/5969 - 17s - loss: 5.7126e-04 - val_loss: 4.7383e-04 - 17s/epoch - 3ms/step
Epoch 99/100
5969/5969 - 17s - loss: 5.7114e-04 - val_loss: 4.7171e-04 - 17s/epoch - 3ms/step
Epoch 100/100
5969/5969 - 17s - loss: 5.6523e-04 - val_loss: 4.4484e-04 - 17s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0004448443651199341
  1/332 [..............................] - ETA: 29s 49/332 [===>..........................] - ETA: 0s  98/332 [=======>......................] - ETA: 0s147/332 [============>.................] - ETA: 0s196/332 [================>.............] - ETA: 0s246/332 [=====================>........] - ETA: 0s295/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.004974899440722963
cosine 0.003990286151687918
MAE: 0.011420462
RMSE: 0.021091321
r2: 0.9711421910468354
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_6 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_8 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2528)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_6 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_8 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2528)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 16, 100, 0.002, 0.5, 632, 0.0005652257823385298, 0.0004448443651199341, 0.004974899440722963, 0.003990286151687918, 0.011420462280511856, 0.021091321483254433, 0.9711421910468354, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_9 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_9 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_11 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2528)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
5969/5969 - 17s - loss: 0.0081 - val_loss: 0.0029 - 17s/epoch - 3ms/step
Epoch 2/200
5969/5969 - 17s - loss: 0.0037 - val_loss: 0.0019 - 17s/epoch - 3ms/step
Epoch 3/200
5969/5969 - 17s - loss: 0.0022 - val_loss: 0.0016 - 17s/epoch - 3ms/step
Epoch 4/200
5969/5969 - 17s - loss: 0.0016 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 5/200
5969/5969 - 17s - loss: 0.0013 - val_loss: 9.5758e-04 - 17s/epoch - 3ms/step
Epoch 6/200
5969/5969 - 17s - loss: 0.0011 - val_loss: 8.4905e-04 - 17s/epoch - 3ms/step
Epoch 7/200
5969/5969 - 17s - loss: 9.9504e-04 - val_loss: 8.4540e-04 - 17s/epoch - 3ms/step
Epoch 8/200
5969/5969 - 17s - loss: 9.0020e-04 - val_loss: 7.1304e-04 - 17s/epoch - 3ms/step
Epoch 9/200
5969/5969 - 17s - loss: 8.5536e-04 - val_loss: 6.7328e-04 - 17s/epoch - 3ms/step
Epoch 10/200
5969/5969 - 17s - loss: 8.0041e-04 - val_loss: 6.5026e-04 - 17s/epoch - 3ms/step
Epoch 11/200
5969/5969 - 17s - loss: 7.5225e-04 - val_loss: 6.5005e-04 - 17s/epoch - 3ms/step
Epoch 12/200
5969/5969 - 17s - loss: 7.2428e-04 - val_loss: 6.1056e-04 - 17s/epoch - 3ms/step
Epoch 13/200
5969/5969 - 17s - loss: 7.0737e-04 - val_loss: 5.8064e-04 - 17s/epoch - 3ms/step
Epoch 14/200
5969/5969 - 17s - loss: 6.6774e-04 - val_loss: 5.5872e-04 - 17s/epoch - 3ms/step
Epoch 15/200
5969/5969 - 17s - loss: 6.4791e-04 - val_loss: 5.2428e-04 - 17s/epoch - 3ms/step
Epoch 16/200
5969/5969 - 17s - loss: 6.3217e-04 - val_loss: 5.6268e-04 - 17s/epoch - 3ms/step
Epoch 17/200
5969/5969 - 17s - loss: 6.0820e-04 - val_loss: 5.1686e-04 - 17s/epoch - 3ms/step
Epoch 18/200
5969/5969 - 17s - loss: 5.9285e-04 - val_loss: 5.2163e-04 - 17s/epoch - 3ms/step
Epoch 19/200
5969/5969 - 17s - loss: 5.7641e-04 - val_loss: 4.7604e-04 - 17s/epoch - 3ms/step
Epoch 20/200
5969/5969 - 17s - loss: 5.6478e-04 - val_loss: 4.3962e-04 - 17s/epoch - 3ms/step
Epoch 21/200
5969/5969 - 17s - loss: 5.5684e-04 - val_loss: 4.4984e-04 - 17s/epoch - 3ms/step
Epoch 22/200
5969/5969 - 17s - loss: 5.5395e-04 - val_loss: 4.8103e-04 - 17s/epoch - 3ms/step
Epoch 23/200
5969/5969 - 17s - loss: 5.3264e-04 - val_loss: 4.8840e-04 - 17s/epoch - 3ms/step
Epoch 24/200
5969/5969 - 17s - loss: 5.2846e-04 - val_loss: 4.5165e-04 - 17s/epoch - 3ms/step
Epoch 25/200
5969/5969 - 17s - loss: 5.1684e-04 - val_loss: 4.5228e-04 - 17s/epoch - 3ms/step
Epoch 26/200
5969/5969 - 17s - loss: 5.0639e-04 - val_loss: 4.4041e-04 - 17s/epoch - 3ms/step
Epoch 27/200
5969/5969 - 17s - loss: 4.9035e-04 - val_loss: 4.1975e-04 - 17s/epoch - 3ms/step
Epoch 28/200
5969/5969 - 17s - loss: 4.9073e-04 - val_loss: 4.1353e-04 - 17s/epoch - 3ms/step
Epoch 29/200
5969/5969 - 17s - loss: 4.9203e-04 - val_loss: 4.1967e-04 - 17s/epoch - 3ms/step
Epoch 30/200
5969/5969 - 17s - loss: 4.7408e-04 - val_loss: 3.9483e-04 - 17s/epoch - 3ms/step
Epoch 31/200
5969/5969 - 17s - loss: 4.9209e-04 - val_loss: 3.9415e-04 - 17s/epoch - 3ms/step
Epoch 32/200
5969/5969 - 17s - loss: 4.6731e-04 - val_loss: 4.7303e-04 - 17s/epoch - 3ms/step
Epoch 33/200
5969/5969 - 17s - loss: 4.6047e-04 - val_loss: 4.0024e-04 - 17s/epoch - 3ms/step
Epoch 34/200
5969/5969 - 17s - loss: 4.5022e-04 - val_loss: 3.7344e-04 - 17s/epoch - 3ms/step
Epoch 35/200
5969/5969 - 17s - loss: 4.7587e-04 - val_loss: 3.5801e-04 - 17s/epoch - 3ms/step
Epoch 36/200
5969/5969 - 17s - loss: 4.5078e-04 - val_loss: 4.2657e-04 - 17s/epoch - 3ms/step
Epoch 37/200
5969/5969 - 17s - loss: 4.4940e-04 - val_loss: 4.1320e-04 - 17s/epoch - 3ms/step
Epoch 38/200
5969/5969 - 17s - loss: 4.3976e-04 - val_loss: 4.2558e-04 - 17s/epoch - 3ms/step
Epoch 39/200
5969/5969 - 17s - loss: 4.3286e-04 - val_loss: 3.6918e-04 - 17s/epoch - 3ms/step
Epoch 40/200
5969/5969 - 17s - loss: 4.3172e-04 - val_loss: 3.8801e-04 - 17s/epoch - 3ms/step
Epoch 41/200
5969/5969 - 17s - loss: 4.2501e-04 - val_loss: 4.0044e-04 - 17s/epoch - 3ms/step
Epoch 42/200
5969/5969 - 17s - loss: 4.2389e-04 - val_loss: 3.8663e-04 - 17s/epoch - 3ms/step
Epoch 43/200
5969/5969 - 17s - loss: 4.1704e-04 - val_loss: 5.0063e-04 - 17s/epoch - 3ms/step
Epoch 44/200
5969/5969 - 17s - loss: 4.1555e-04 - val_loss: 4.1224e-04 - 17s/epoch - 3ms/step
Epoch 45/200
5969/5969 - 17s - loss: 4.1263e-04 - val_loss: 3.7722e-04 - 17s/epoch - 3ms/step
Epoch 46/200
5969/5969 - 17s - loss: 4.0934e-04 - val_loss: 3.9024e-04 - 17s/epoch - 3ms/step
Epoch 47/200
5969/5969 - 17s - loss: 4.0461e-04 - val_loss: 3.5662e-04 - 17s/epoch - 3ms/step
Epoch 48/200
5969/5969 - 17s - loss: 4.0027e-04 - val_loss: 4.0276e-04 - 17s/epoch - 3ms/step
Epoch 49/200
5969/5969 - 17s - loss: 4.0143e-04 - val_loss: 4.1939e-04 - 17s/epoch - 3ms/step
Epoch 50/200
5969/5969 - 17s - loss: 3.9434e-04 - val_loss: 3.9142e-04 - 17s/epoch - 3ms/step
Epoch 51/200
5969/5969 - 17s - loss: 3.9377e-04 - val_loss: 3.6419e-04 - 17s/epoch - 3ms/step
Epoch 52/200
5969/5969 - 17s - loss: 3.8889e-04 - val_loss: 4.3753e-04 - 17s/epoch - 3ms/step
Epoch 53/200
5969/5969 - 17s - loss: 3.8988e-04 - val_loss: 4.3241e-04 - 17s/epoch - 3ms/step
Epoch 54/200
5969/5969 - 17s - loss: 3.8865e-04 - val_loss: 4.4098e-04 - 17s/epoch - 3ms/step
Epoch 55/200
5969/5969 - 17s - loss: 3.8145e-04 - val_loss: 3.4326e-04 - 17s/epoch - 3ms/step
Epoch 56/200
5969/5969 - 17s - loss: 3.8403e-04 - val_loss: 5.1011e-04 - 17s/epoch - 3ms/step
Epoch 57/200
5969/5969 - 17s - loss: 3.7444e-04 - val_loss: 4.6653e-04 - 17s/epoch - 3ms/step
Epoch 58/200
5969/5969 - 17s - loss: 3.7751e-04 - val_loss: 4.0769e-04 - 17s/epoch - 3ms/step
Epoch 59/200
5969/5969 - 17s - loss: 3.7083e-04 - val_loss: 3.9852e-04 - 17s/epoch - 3ms/step
Epoch 60/200
5969/5969 - 17s - loss: 3.7352e-04 - val_loss: 4.4385e-04 - 17s/epoch - 3ms/step
Epoch 61/200
5969/5969 - 17s - loss: 3.7397e-04 - val_loss: 4.0956e-04 - 17s/epoch - 3ms/step
Epoch 62/200
5969/5969 - 17s - loss: 3.6966e-04 - val_loss: 5.8370e-04 - 17s/epoch - 3ms/step
Epoch 63/200
5969/5969 - 17s - loss: 3.6826e-04 - val_loss: 3.5544e-04 - 17s/epoch - 3ms/step
Epoch 64/200
5969/5969 - 17s - loss: 3.6298e-04 - val_loss: 4.7170e-04 - 17s/epoch - 3ms/step
Epoch 65/200
5969/5969 - 17s - loss: 3.6711e-04 - val_loss: 5.4772e-04 - 17s/epoch - 3ms/step
Epoch 66/200
5969/5969 - 17s - loss: 3.6300e-04 - val_loss: 4.5497e-04 - 17s/epoch - 3ms/step
Epoch 67/200
5969/5969 - 17s - loss: 3.6847e-04 - val_loss: 4.8965e-04 - 17s/epoch - 3ms/step
Epoch 68/200
5969/5969 - 17s - loss: 3.5874e-04 - val_loss: 5.2574e-04 - 17s/epoch - 3ms/step
Epoch 69/200
5969/5969 - 17s - loss: 3.6387e-04 - val_loss: 4.1517e-04 - 17s/epoch - 3ms/step
Epoch 70/200
5969/5969 - 17s - loss: 3.5337e-04 - val_loss: 5.3026e-04 - 17s/epoch - 3ms/step
Epoch 71/200
5969/5969 - 17s - loss: 3.5704e-04 - val_loss: 4.8177e-04 - 17s/epoch - 3ms/step
Epoch 72/200
5969/5969 - 17s - loss: 3.4997e-04 - val_loss: 6.5731e-04 - 17s/epoch - 3ms/step
Epoch 73/200
5969/5969 - 17s - loss: 3.5896e-04 - val_loss: 4.9761e-04 - 17s/epoch - 3ms/step
Epoch 74/200
5969/5969 - 17s - loss: 3.5231e-04 - val_loss: 3.8850e-04 - 17s/epoch - 3ms/step
Epoch 75/200
5969/5969 - 17s - loss: 3.4593e-04 - val_loss: 3.8911e-04 - 17s/epoch - 3ms/step
Epoch 76/200
5969/5969 - 17s - loss: 3.4664e-04 - val_loss: 4.4343e-04 - 17s/epoch - 3ms/step
Epoch 77/200
5969/5969 - 17s - loss: 3.4658e-04 - val_loss: 4.3445e-04 - 17s/epoch - 3ms/step
Epoch 78/200
5969/5969 - 17s - loss: 3.5506e-04 - val_loss: 4.6775e-04 - 17s/epoch - 3ms/step
Epoch 79/200
5969/5969 - 17s - loss: 3.4502e-04 - val_loss: 4.3447e-04 - 17s/epoch - 3ms/step
Epoch 80/200
5969/5969 - 17s - loss: 3.4617e-04 - val_loss: 3.7994e-04 - 17s/epoch - 3ms/step
Epoch 81/200
5969/5969 - 17s - loss: 3.4246e-04 - val_loss: 5.2581e-04 - 17s/epoch - 3ms/step
Epoch 82/200
5969/5969 - 17s - loss: 3.4328e-04 - val_loss: 3.5405e-04 - 17s/epoch - 3ms/step
Epoch 83/200
5969/5969 - 17s - loss: 3.4355e-04 - val_loss: 5.0960e-04 - 17s/epoch - 3ms/step
Epoch 84/200
5969/5969 - 17s - loss: 3.3923e-04 - val_loss: 5.1105e-04 - 17s/epoch - 3ms/step
Epoch 85/200
5969/5969 - 17s - loss: 3.4176e-04 - val_loss: 5.1363e-04 - 17s/epoch - 3ms/step
Epoch 86/200
5969/5969 - 17s - loss: 3.3484e-04 - val_loss: 5.0013e-04 - 17s/epoch - 3ms/step
Epoch 87/200
5969/5969 - 17s - loss: 3.3486e-04 - val_loss: 4.8860e-04 - 17s/epoch - 3ms/step
Epoch 88/200
5969/5969 - 17s - loss: 3.3231e-04 - val_loss: 3.1952e-04 - 17s/epoch - 3ms/step
Epoch 89/200
5969/5969 - 17s - loss: 3.3800e-04 - val_loss: 3.8650e-04 - 17s/epoch - 3ms/step
Epoch 90/200
5969/5969 - 17s - loss: 3.2923e-04 - val_loss: 3.5819e-04 - 17s/epoch - 3ms/step
Epoch 91/200
5969/5969 - 17s - loss: 3.3172e-04 - val_loss: 9.0797e-04 - 17s/epoch - 3ms/step
Epoch 92/200
5969/5969 - 17s - loss: 3.3656e-04 - val_loss: 5.3847e-04 - 17s/epoch - 3ms/step
Epoch 93/200
5969/5969 - 17s - loss: 3.3092e-04 - val_loss: 3.7450e-04 - 17s/epoch - 3ms/step
Epoch 94/200
5969/5969 - 17s - loss: 3.2684e-04 - val_loss: 4.4675e-04 - 17s/epoch - 3ms/step
Epoch 95/200
5969/5969 - 17s - loss: 3.2461e-04 - val_loss: 3.8396e-04 - 17s/epoch - 3ms/step
Epoch 96/200
5969/5969 - 17s - loss: 3.2875e-04 - val_loss: 4.6429e-04 - 17s/epoch - 3ms/step
Epoch 97/200
5969/5969 - 17s - loss: 3.2465e-04 - val_loss: 2.8176e-04 - 17s/epoch - 3ms/step
Epoch 98/200
5969/5969 - 17s - loss: 3.3087e-04 - val_loss: 4.4152e-04 - 17s/epoch - 3ms/step
Epoch 99/200
5969/5969 - 17s - loss: 3.2881e-04 - val_loss: 3.6641e-04 - 17s/epoch - 3ms/step
Epoch 100/200
5969/5969 - 17s - loss: 3.1827e-04 - val_loss: 2.9838e-04 - 17s/epoch - 3ms/step
Epoch 101/200
5969/5969 - 17s - loss: 3.2014e-04 - val_loss: 5.4959e-04 - 17s/epoch - 3ms/step
Epoch 102/200
5969/5969 - 17s - loss: 3.1946e-04 - val_loss: 4.3663e-04 - 17s/epoch - 3ms/step
Epoch 103/200
5969/5969 - 17s - loss: 3.1811e-04 - val_loss: 3.6782e-04 - 17s/epoch - 3ms/step
Epoch 104/200
5969/5969 - 17s - loss: 3.2165e-04 - val_loss: 2.8652e-04 - 17s/epoch - 3ms/step
Epoch 105/200
5969/5969 - 17s - loss: 3.2242e-04 - val_loss: 4.2178e-04 - 17s/epoch - 3ms/step
Epoch 106/200
5969/5969 - 17s - loss: 3.1554e-04 - val_loss: 3.5322e-04 - 17s/epoch - 3ms/step
Epoch 107/200
5969/5969 - 17s - loss: 3.3032e-04 - val_loss: 4.6094e-04 - 17s/epoch - 3ms/step
Epoch 108/200
5969/5969 - 17s - loss: 3.1441e-04 - val_loss: 4.0037e-04 - 17s/epoch - 3ms/step
Epoch 109/200
5969/5969 - 17s - loss: 3.1275e-04 - val_loss: 3.4108e-04 - 17s/epoch - 3ms/step
Epoch 110/200
5969/5969 - 17s - loss: 3.2140e-04 - val_loss: 2.8317e-04 - 17s/epoch - 3ms/step
Epoch 111/200
5969/5969 - 17s - loss: 3.1564e-04 - val_loss: 3.2484e-04 - 17s/epoch - 3ms/step
Epoch 112/200
5969/5969 - 17s - loss: 3.1371e-04 - val_loss: 3.1177e-04 - 17s/epoch - 3ms/step
Epoch 113/200
5969/5969 - 17s - loss: 3.1028e-04 - val_loss: 4.3083e-04 - 17s/epoch - 3ms/step
Epoch 114/200
5969/5969 - 17s - loss: 3.1018e-04 - val_loss: 3.6281e-04 - 17s/epoch - 3ms/step
Epoch 115/200
5969/5969 - 17s - loss: 3.0833e-04 - val_loss: 3.6366e-04 - 17s/epoch - 3ms/step
Epoch 116/200
5969/5969 - 17s - loss: 3.0861e-04 - val_loss: 3.6524e-04 - 17s/epoch - 3ms/step
Epoch 117/200
5969/5969 - 17s - loss: 3.0960e-04 - val_loss: 3.7085e-04 - 17s/epoch - 3ms/step
Epoch 118/200
5969/5969 - 17s - loss: 3.0795e-04 - val_loss: 2.9419e-04 - 17s/epoch - 3ms/step
Epoch 119/200
5969/5969 - 17s - loss: 3.0613e-04 - val_loss: 3.3104e-04 - 17s/epoch - 3ms/step
Epoch 120/200
5969/5969 - 17s - loss: 3.1346e-04 - val_loss: 4.4443e-04 - 17s/epoch - 3ms/step
Epoch 121/200
5969/5969 - 17s - loss: 3.0939e-04 - val_loss: 3.4820e-04 - 17s/epoch - 3ms/step
Epoch 122/200
5969/5969 - 17s - loss: 3.0789e-04 - val_loss: 2.7276e-04 - 17s/epoch - 3ms/step
Epoch 123/200
5969/5969 - 17s - loss: 3.0752e-04 - val_loss: 3.5355e-04 - 17s/epoch - 3ms/step
Epoch 124/200
5969/5969 - 17s - loss: 3.0804e-04 - val_loss: 2.8747e-04 - 17s/epoch - 3ms/step
Epoch 125/200
5969/5969 - 17s - loss: 3.0597e-04 - val_loss: 2.5667e-04 - 17s/epoch - 3ms/step
Epoch 126/200
5969/5969 - 17s - loss: 3.0372e-04 - val_loss: 2.7021e-04 - 17s/epoch - 3ms/step
Epoch 127/200
5969/5969 - 17s - loss: 3.0487e-04 - val_loss: 2.8441e-04 - 17s/epoch - 3ms/step
Epoch 128/200
5969/5969 - 17s - loss: 3.0452e-04 - val_loss: 3.1516e-04 - 17s/epoch - 3ms/step
Epoch 129/200
5969/5969 - 17s - loss: 3.0626e-04 - val_loss: 2.9241e-04 - 17s/epoch - 3ms/step
Epoch 130/200
5969/5969 - 17s - loss: 3.0372e-04 - val_loss: 4.0492e-04 - 17s/epoch - 3ms/step
Epoch 131/200
5969/5969 - 17s - loss: 2.9947e-04 - val_loss: 4.5250e-04 - 17s/epoch - 3ms/step
Epoch 132/200
5969/5969 - 17s - loss: 3.0273e-04 - val_loss: 3.0852e-04 - 17s/epoch - 3ms/step
Epoch 133/200
5969/5969 - 17s - loss: 2.9948e-04 - val_loss: 3.2967e-04 - 17s/epoch - 3ms/step
Epoch 134/200
5969/5969 - 17s - loss: 3.0150e-04 - val_loss: 3.3410e-04 - 17s/epoch - 3ms/step
Epoch 135/200
5969/5969 - 17s - loss: 2.9900e-04 - val_loss: 4.2673e-04 - 17s/epoch - 3ms/step
Epoch 136/200
5969/5969 - 17s - loss: 3.0589e-04 - val_loss: 2.6693e-04 - 17s/epoch - 3ms/step
Epoch 137/200
5969/5969 - 17s - loss: 3.0118e-04 - val_loss: 2.8456e-04 - 17s/epoch - 3ms/step
Epoch 138/200
5969/5969 - 17s - loss: 2.9768e-04 - val_loss: 4.0278e-04 - 17s/epoch - 3ms/step
Epoch 139/200
5969/5969 - 17s - loss: 3.0317e-04 - val_loss: 3.0224e-04 - 17s/epoch - 3ms/step
Epoch 140/200
5969/5969 - 17s - loss: 2.9820e-04 - val_loss: 3.1685e-04 - 17s/epoch - 3ms/step
Epoch 141/200
5969/5969 - 17s - loss: 2.9717e-04 - val_loss: 2.4703e-04 - 17s/epoch - 3ms/step
Epoch 142/200
5969/5969 - 17s - loss: 2.9658e-04 - val_loss: 3.1295e-04 - 17s/epoch - 3ms/step
Epoch 143/200
5969/5969 - 17s - loss: 3.0204e-04 - val_loss: 3.4033e-04 - 17s/epoch - 3ms/step
Epoch 144/200
5969/5969 - 17s - loss: 3.0086e-04 - val_loss: 3.1677e-04 - 17s/epoch - 3ms/step
Epoch 145/200
5969/5969 - 17s - loss: 2.9438e-04 - val_loss: 2.5964e-04 - 17s/epoch - 3ms/step
Epoch 146/200
5969/5969 - 17s - loss: 2.9618e-04 - val_loss: 3.3704e-04 - 17s/epoch - 3ms/step
Epoch 147/200
5969/5969 - 17s - loss: 2.9419e-04 - val_loss: 2.4897e-04 - 17s/epoch - 3ms/step
Epoch 148/200
5969/5969 - 17s - loss: 3.0613e-04 - val_loss: 3.5773e-04 - 17s/epoch - 3ms/step
Epoch 149/200
5969/5969 - 17s - loss: 2.9608e-04 - val_loss: 2.4790e-04 - 17s/epoch - 3ms/step
Epoch 150/200
5969/5969 - 17s - loss: 2.9115e-04 - val_loss: 2.7709e-04 - 17s/epoch - 3ms/step
Epoch 151/200
5969/5969 - 17s - loss: 2.9191e-04 - val_loss: 2.7938e-04 - 17s/epoch - 3ms/step
Epoch 152/200
5969/5969 - 17s - loss: 2.9547e-04 - val_loss: 2.6507e-04 - 17s/epoch - 3ms/step
Epoch 153/200
5969/5969 - 17s - loss: 2.9148e-04 - val_loss: 2.4969e-04 - 17s/epoch - 3ms/step
Epoch 154/200
5969/5969 - 17s - loss: 2.9123e-04 - val_loss: 2.9233e-04 - 17s/epoch - 3ms/step
Epoch 155/200
5969/5969 - 17s - loss: 2.9119e-04 - val_loss: 2.3582e-04 - 17s/epoch - 3ms/step
Epoch 156/200
5969/5969 - 17s - loss: 2.9178e-04 - val_loss: 2.8373e-04 - 17s/epoch - 3ms/step
Epoch 157/200
5969/5969 - 17s - loss: 2.9169e-04 - val_loss: 2.4478e-04 - 17s/epoch - 3ms/step
Epoch 158/200
5969/5969 - 17s - loss: 2.9104e-04 - val_loss: 2.4586e-04 - 17s/epoch - 3ms/step
Epoch 159/200
5969/5969 - 17s - loss: 3.1210e-04 - val_loss: 2.2664e-04 - 17s/epoch - 3ms/step
Epoch 160/200
5969/5969 - 17s - loss: 2.8837e-04 - val_loss: 2.4599e-04 - 17s/epoch - 3ms/step
Epoch 161/200
5969/5969 - 17s - loss: 2.9394e-04 - val_loss: 2.4667e-04 - 17s/epoch - 3ms/step
Epoch 162/200
5969/5969 - 17s - loss: 2.8874e-04 - val_loss: 2.7115e-04 - 17s/epoch - 3ms/step
Epoch 163/200
5969/5969 - 17s - loss: 2.9016e-04 - val_loss: 2.3713e-04 - 17s/epoch - 3ms/step
Epoch 164/200
5969/5969 - 17s - loss: 2.9653e-04 - val_loss: 2.5266e-04 - 17s/epoch - 3ms/step
Epoch 165/200
5969/5969 - 17s - loss: 2.9136e-04 - val_loss: 2.3507e-04 - 17s/epoch - 3ms/step
Epoch 166/200
5969/5969 - 17s - loss: 2.8440e-04 - val_loss: 2.6991e-04 - 17s/epoch - 3ms/step
Epoch 167/200
5969/5969 - 17s - loss: 2.8611e-04 - val_loss: 2.4007e-04 - 17s/epoch - 3ms/step
Epoch 168/200
5969/5969 - 17s - loss: 2.8403e-04 - val_loss: 2.4672e-04 - 17s/epoch - 3ms/step
Epoch 169/200
5969/5969 - 17s - loss: 2.8444e-04 - val_loss: 2.3536e-04 - 17s/epoch - 3ms/step
Epoch 170/200
5969/5969 - 17s - loss: 2.9721e-04 - val_loss: 2.3581e-04 - 17s/epoch - 3ms/step
Epoch 171/200
5969/5969 - 17s - loss: 2.8818e-04 - val_loss: 2.5145e-04 - 17s/epoch - 3ms/step
Epoch 172/200
5969/5969 - 17s - loss: 2.8298e-04 - val_loss: 2.5815e-04 - 17s/epoch - 3ms/step
Epoch 173/200
5969/5969 - 17s - loss: 2.8518e-04 - val_loss: 2.4376e-04 - 17s/epoch - 3ms/step
Epoch 174/200
5969/5969 - 17s - loss: 2.8289e-04 - val_loss: 2.4084e-04 - 17s/epoch - 3ms/step
Epoch 175/200
5969/5969 - 17s - loss: 2.8117e-04 - val_loss: 2.4360e-04 - 17s/epoch - 3ms/step
Epoch 176/200
5969/5969 - 17s - loss: 2.8134e-04 - val_loss: 2.3643e-04 - 17s/epoch - 3ms/step
Epoch 177/200
5969/5969 - 17s - loss: 2.8204e-04 - val_loss: 2.2775e-04 - 17s/epoch - 3ms/step
Epoch 178/200
5969/5969 - 17s - loss: 2.8505e-04 - val_loss: 2.2664e-04 - 17s/epoch - 3ms/step
Epoch 179/200
5969/5969 - 17s - loss: 2.8512e-04 - val_loss: 2.3003e-04 - 17s/epoch - 3ms/step
Epoch 180/200
5969/5969 - 17s - loss: 2.7957e-04 - val_loss: 2.3072e-04 - 17s/epoch - 3ms/step
Epoch 181/200
5969/5969 - 17s - loss: 2.8489e-04 - val_loss: 2.2973e-04 - 17s/epoch - 3ms/step
Epoch 182/200
5969/5969 - 17s - loss: 2.8240e-04 - val_loss: 2.4450e-04 - 17s/epoch - 3ms/step
Epoch 183/200
5969/5969 - 17s - loss: 2.7928e-04 - val_loss: 2.2565e-04 - 17s/epoch - 3ms/step
Epoch 184/200
5969/5969 - 17s - loss: 2.8078e-04 - val_loss: 2.9886e-04 - 17s/epoch - 3ms/step
Epoch 185/200
5969/5969 - 17s - loss: 2.7933e-04 - val_loss: 2.3513e-04 - 17s/epoch - 3ms/step
Epoch 186/200
5969/5969 - 17s - loss: 2.8378e-04 - val_loss: 2.3245e-04 - 17s/epoch - 3ms/step
Epoch 187/200
5969/5969 - 17s - loss: 2.8437e-04 - val_loss: 2.7739e-04 - 17s/epoch - 3ms/step
Epoch 188/200
5969/5969 - 17s - loss: 2.7745e-04 - val_loss: 2.4158e-04 - 17s/epoch - 3ms/step
Epoch 189/200
5969/5969 - 17s - loss: 2.7899e-04 - val_loss: 2.3411e-04 - 17s/epoch - 3ms/step
Epoch 190/200
5969/5969 - 17s - loss: 2.7990e-04 - val_loss: 2.5382e-04 - 17s/epoch - 3ms/step
Epoch 191/200
5969/5969 - 17s - loss: 2.7531e-04 - val_loss: 2.6770e-04 - 17s/epoch - 3ms/step
Epoch 192/200
5969/5969 - 17s - loss: 2.7612e-04 - val_loss: 2.2002e-04 - 17s/epoch - 3ms/step
Epoch 193/200
5969/5969 - 17s - loss: 2.7718e-04 - val_loss: 2.6008e-04 - 17s/epoch - 3ms/step
Epoch 194/200
5969/5969 - 17s - loss: 2.7583e-04 - val_loss: 2.3634e-04 - 17s/epoch - 3ms/step
Epoch 195/200
5969/5969 - 17s - loss: 2.8526e-04 - val_loss: 2.3694e-04 - 17s/epoch - 3ms/step
Epoch 196/200
5969/5969 - 17s - loss: 2.7467e-04 - val_loss: 2.3290e-04 - 17s/epoch - 3ms/step
Epoch 197/200
5969/5969 - 17s - loss: 2.8014e-04 - val_loss: 2.4353e-04 - 17s/epoch - 3ms/step
Epoch 198/200
5969/5969 - 17s - loss: 2.7769e-04 - val_loss: 2.5051e-04 - 17s/epoch - 3ms/step
Epoch 199/200
5969/5969 - 17s - loss: 2.7494e-04 - val_loss: 2.3416e-04 - 17s/epoch - 3ms/step
Epoch 200/200
5969/5969 - 17s - loss: 2.7796e-04 - val_loss: 2.3937e-04 - 17s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.000239371249335818
  1/332 [..............................] - ETA: 33s 47/332 [===>..........................] - ETA: 0s  95/332 [=======>......................] - ETA: 0s143/332 [===========>..................] - ETA: 0s191/332 [================>.............] - ETA: 0s239/332 [====================>.........] - ETA: 0s288/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0025823108944585537
cosine 0.0020913628533432
MAE: 0.007883532
RMSE: 0.015471611
r2: 0.9844723640627872
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_9 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_11 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2528)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_9 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_11 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2528)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 16, 200, 0.0005, 0.5, 632, 0.0002779562200885266, 0.000239371249335818, 0.0025823108944585537, 0.0020913628533432, 0.007883531972765923, 0.01547161117196083, 0.9844723640627872, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_12 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_12 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_14 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2528)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
5969/5969 - 18s - loss: 0.0075 - val_loss: 0.0027 - 18s/epoch - 3ms/step
Epoch 2/200
5969/5969 - 17s - loss: 0.0027 - val_loss: 0.0022 - 17s/epoch - 3ms/step
Epoch 3/200
5969/5969 - 17s - loss: 0.0019 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 4/200
5969/5969 - 17s - loss: 0.0014 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 5/200
5969/5969 - 17s - loss: 0.0012 - val_loss: 9.7207e-04 - 17s/epoch - 3ms/step
Epoch 6/200
5969/5969 - 17s - loss: 0.0011 - val_loss: 9.5865e-04 - 17s/epoch - 3ms/step
Epoch 7/200
5969/5969 - 17s - loss: 0.0010 - val_loss: 8.0301e-04 - 17s/epoch - 3ms/step
Epoch 8/200
5969/5969 - 17s - loss: 9.6497e-04 - val_loss: 7.6483e-04 - 17s/epoch - 3ms/step
Epoch 9/200
5969/5969 - 17s - loss: 9.2279e-04 - val_loss: 7.4284e-04 - 17s/epoch - 3ms/step
Epoch 10/200
5969/5969 - 17s - loss: 8.7486e-04 - val_loss: 7.1320e-04 - 17s/epoch - 3ms/step
Epoch 11/200
5969/5969 - 17s - loss: 8.3707e-04 - val_loss: 6.5373e-04 - 17s/epoch - 3ms/step
Epoch 12/200
5969/5969 - 17s - loss: 7.9885e-04 - val_loss: 6.6829e-04 - 17s/epoch - 3ms/step
Epoch 13/200
5969/5969 - 17s - loss: 7.7127e-04 - val_loss: 6.0392e-04 - 17s/epoch - 3ms/step
Epoch 14/200
5969/5969 - 17s - loss: 7.4896e-04 - val_loss: 5.9299e-04 - 17s/epoch - 3ms/step
Epoch 15/200
5969/5969 - 17s - loss: 7.4076e-04 - val_loss: 6.0495e-04 - 17s/epoch - 3ms/step
Epoch 16/200
5969/5969 - 17s - loss: 7.1968e-04 - val_loss: 6.2156e-04 - 17s/epoch - 3ms/step
Epoch 17/200
5969/5969 - 17s - loss: 7.0875e-04 - val_loss: 5.5917e-04 - 17s/epoch - 3ms/step
Epoch 18/200
5969/5969 - 17s - loss: 6.8643e-04 - val_loss: 5.5648e-04 - 17s/epoch - 3ms/step
Epoch 19/200
5969/5969 - 17s - loss: 6.6690e-04 - val_loss: 5.5142e-04 - 17s/epoch - 3ms/step
Epoch 20/200
5969/5969 - 17s - loss: 6.6551e-04 - val_loss: 5.5997e-04 - 17s/epoch - 3ms/step
Epoch 21/200
5969/5969 - 17s - loss: 6.5708e-04 - val_loss: 5.2696e-04 - 17s/epoch - 3ms/step
Epoch 22/200
5969/5969 - 17s - loss: 6.3543e-04 - val_loss: 5.0858e-04 - 17s/epoch - 3ms/step
Epoch 23/200
5969/5969 - 17s - loss: 6.2510e-04 - val_loss: 5.0271e-04 - 17s/epoch - 3ms/step
Epoch 24/200
5969/5969 - 17s - loss: 6.1362e-04 - val_loss: 5.0266e-04 - 17s/epoch - 3ms/step
Epoch 25/200
5969/5969 - 17s - loss: 6.0903e-04 - val_loss: 5.0697e-04 - 17s/epoch - 3ms/step
Epoch 26/200
5969/5969 - 17s - loss: 6.3992e-04 - val_loss: 4.8725e-04 - 17s/epoch - 3ms/step
Epoch 27/200
5969/5969 - 17s - loss: 5.9165e-04 - val_loss: 4.8298e-04 - 17s/epoch - 3ms/step
Epoch 28/200
5969/5969 - 17s - loss: 5.8620e-04 - val_loss: 4.7977e-04 - 17s/epoch - 3ms/step
Epoch 29/200
5969/5969 - 17s - loss: 5.8590e-04 - val_loss: 4.6972e-04 - 17s/epoch - 3ms/step
Epoch 30/200
5969/5969 - 17s - loss: 5.7308e-04 - val_loss: 4.4957e-04 - 17s/epoch - 3ms/step
Epoch 31/200
5969/5969 - 17s - loss: 5.8088e-04 - val_loss: 4.3069e-04 - 17s/epoch - 3ms/step
Epoch 32/200
5969/5969 - 17s - loss: 5.5622e-04 - val_loss: 4.6595e-04 - 17s/epoch - 3ms/step
Epoch 33/200
5969/5969 - 17s - loss: 5.5482e-04 - val_loss: 4.3530e-04 - 17s/epoch - 3ms/step
Epoch 34/200
5969/5969 - 17s - loss: 5.4668e-04 - val_loss: 4.2987e-04 - 17s/epoch - 3ms/step
Epoch 35/200
5969/5969 - 17s - loss: 5.4345e-04 - val_loss: 4.2127e-04 - 17s/epoch - 3ms/step
Epoch 36/200
5969/5969 - 17s - loss: 5.4004e-04 - val_loss: 4.4809e-04 - 17s/epoch - 3ms/step
Epoch 37/200
5969/5969 - 17s - loss: 5.3490e-04 - val_loss: 4.6020e-04 - 17s/epoch - 3ms/step
Epoch 38/200
5969/5969 - 17s - loss: 5.2861e-04 - val_loss: 4.3023e-04 - 17s/epoch - 3ms/step
Epoch 39/200
5969/5969 - 17s - loss: 5.3279e-04 - val_loss: 4.0734e-04 - 17s/epoch - 3ms/step
Epoch 40/200
5969/5969 - 17s - loss: 5.1768e-04 - val_loss: 4.1750e-04 - 17s/epoch - 3ms/step
Epoch 41/200
5969/5969 - 17s - loss: 5.2006e-04 - val_loss: 4.1992e-04 - 17s/epoch - 3ms/step
Epoch 42/200
5969/5969 - 17s - loss: 5.2700e-04 - val_loss: 4.3094e-04 - 17s/epoch - 3ms/step
Epoch 43/200
5969/5969 - 17s - loss: 5.1092e-04 - val_loss: 4.3246e-04 - 17s/epoch - 3ms/step
Epoch 44/200
5969/5969 - 17s - loss: 5.0839e-04 - val_loss: 4.2151e-04 - 17s/epoch - 3ms/step
Epoch 45/200
5969/5969 - 17s - loss: 5.0536e-04 - val_loss: 4.2303e-04 - 17s/epoch - 3ms/step
Epoch 46/200
5969/5969 - 17s - loss: 4.9966e-04 - val_loss: 3.9360e-04 - 17s/epoch - 3ms/step
Epoch 47/200
5969/5969 - 17s - loss: 4.9627e-04 - val_loss: 3.9968e-04 - 17s/epoch - 3ms/step
Epoch 48/200
5969/5969 - 17s - loss: 4.9719e-04 - val_loss: 4.0097e-04 - 17s/epoch - 3ms/step
Epoch 49/200
5969/5969 - 17s - loss: 4.8630e-04 - val_loss: 4.0911e-04 - 17s/epoch - 3ms/step
Epoch 50/200
5969/5969 - 17s - loss: 4.9679e-04 - val_loss: 4.1694e-04 - 17s/epoch - 3ms/step
Epoch 51/200
5969/5969 - 17s - loss: 4.8584e-04 - val_loss: 3.8188e-04 - 17s/epoch - 3ms/step
Epoch 52/200
5969/5969 - 17s - loss: 4.9956e-04 - val_loss: 3.9496e-04 - 17s/epoch - 3ms/step
Epoch 53/200
5969/5969 - 17s - loss: 4.9148e-04 - val_loss: 4.0661e-04 - 17s/epoch - 3ms/step
Epoch 54/200
5969/5969 - 17s - loss: 4.8566e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 55/200
5969/5969 - 17s - loss: 4.8361e-04 - val_loss: 3.9334e-04 - 17s/epoch - 3ms/step
Epoch 56/200
5969/5969 - 17s - loss: 4.7452e-04 - val_loss: 4.2575e-04 - 17s/epoch - 3ms/step
Epoch 57/200
5969/5969 - 17s - loss: 4.8298e-04 - val_loss: 3.9849e-04 - 17s/epoch - 3ms/step
Epoch 58/200
5969/5969 - 17s - loss: 4.6844e-04 - val_loss: 3.9772e-04 - 17s/epoch - 3ms/step
Epoch 59/200
5969/5969 - 17s - loss: 4.6226e-04 - val_loss: 3.8176e-04 - 17s/epoch - 3ms/step
Epoch 60/200
5969/5969 - 17s - loss: 4.6267e-04 - val_loss: 4.0196e-04 - 17s/epoch - 3ms/step
Epoch 61/200
5969/5969 - 17s - loss: 4.6116e-04 - val_loss: 4.0633e-04 - 17s/epoch - 3ms/step
Epoch 62/200
5969/5969 - 17s - loss: 4.8255e-04 - val_loss: 4.9663e-04 - 17s/epoch - 3ms/step
Epoch 63/200
5969/5969 - 17s - loss: 4.5698e-04 - val_loss: 4.1041e-04 - 17s/epoch - 3ms/step
Epoch 64/200
5969/5969 - 17s - loss: 4.5829e-04 - val_loss: 4.0177e-04 - 17s/epoch - 3ms/step
Epoch 65/200
5969/5969 - 17s - loss: 4.6208e-04 - val_loss: 4.1351e-04 - 17s/epoch - 3ms/step
Epoch 66/200
5969/5969 - 17s - loss: 4.6206e-04 - val_loss: 4.0261e-04 - 17s/epoch - 3ms/step
Epoch 67/200
5969/5969 - 17s - loss: 4.5662e-04 - val_loss: 4.0513e-04 - 17s/epoch - 3ms/step
Epoch 68/200
5969/5969 - 17s - loss: 4.5630e-04 - val_loss: 4.2391e-04 - 17s/epoch - 3ms/step
Epoch 69/200
5969/5969 - 17s - loss: 4.5424e-04 - val_loss: 4.1685e-04 - 17s/epoch - 3ms/step
Epoch 70/200
5969/5969 - 17s - loss: 4.5134e-04 - val_loss: 4.2998e-04 - 17s/epoch - 3ms/step
Epoch 71/200
5969/5969 - 17s - loss: 4.4734e-04 - val_loss: 3.9412e-04 - 17s/epoch - 3ms/step
Epoch 72/200
5969/5969 - 17s - loss: 4.4573e-04 - val_loss: 4.4807e-04 - 17s/epoch - 3ms/step
Epoch 73/200
5969/5969 - 17s - loss: 4.5006e-04 - val_loss: 4.5423e-04 - 17s/epoch - 3ms/step
Epoch 74/200
5969/5969 - 17s - loss: 4.4687e-04 - val_loss: 3.8891e-04 - 17s/epoch - 3ms/step
Epoch 75/200
5969/5969 - 17s - loss: 4.4484e-04 - val_loss: 4.0181e-04 - 17s/epoch - 3ms/step
Epoch 76/200
5969/5969 - 17s - loss: 4.3309e-04 - val_loss: 4.6057e-04 - 17s/epoch - 3ms/step
Epoch 77/200
5969/5969 - 17s - loss: 4.3940e-04 - val_loss: 4.3134e-04 - 17s/epoch - 3ms/step
Epoch 78/200
5969/5969 - 17s - loss: 4.3705e-04 - val_loss: 4.7041e-04 - 17s/epoch - 3ms/step
Epoch 79/200
5969/5969 - 17s - loss: 4.3447e-04 - val_loss: 4.1106e-04 - 17s/epoch - 3ms/step
Epoch 80/200
5969/5969 - 17s - loss: 4.4211e-04 - val_loss: 4.5744e-04 - 17s/epoch - 3ms/step
Epoch 81/200
5969/5969 - 17s - loss: 4.3090e-04 - val_loss: 4.5966e-04 - 17s/epoch - 3ms/step
Epoch 82/200
5969/5969 - 17s - loss: 4.3937e-04 - val_loss: 3.7572e-04 - 17s/epoch - 3ms/step
Epoch 83/200
5969/5969 - 17s - loss: 4.3181e-04 - val_loss: 4.4269e-04 - 17s/epoch - 3ms/step
Epoch 84/200
5969/5969 - 17s - loss: 4.3887e-04 - val_loss: 4.6745e-04 - 17s/epoch - 3ms/step
Epoch 85/200
5969/5969 - 17s - loss: 4.3442e-04 - val_loss: 4.4466e-04 - 17s/epoch - 3ms/step
Epoch 86/200
5969/5969 - 17s - loss: 4.2453e-04 - val_loss: 4.3544e-04 - 17s/epoch - 3ms/step
Epoch 87/200
5969/5969 - 17s - loss: 4.3885e-04 - val_loss: 5.0581e-04 - 17s/epoch - 3ms/step
Epoch 88/200
5969/5969 - 17s - loss: 4.4480e-04 - val_loss: 3.9445e-04 - 17s/epoch - 3ms/step
Epoch 89/200
5969/5969 - 17s - loss: 4.2306e-04 - val_loss: 3.9849e-04 - 17s/epoch - 3ms/step
Epoch 90/200
5969/5969 - 17s - loss: 4.2337e-04 - val_loss: 4.0195e-04 - 17s/epoch - 3ms/step
Epoch 91/200
5969/5969 - 17s - loss: 4.3494e-04 - val_loss: 4.4549e-04 - 17s/epoch - 3ms/step
Epoch 92/200
5969/5969 - 17s - loss: 4.2291e-04 - val_loss: 5.0583e-04 - 17s/epoch - 3ms/step
Epoch 93/200
5969/5969 - 17s - loss: 4.2200e-04 - val_loss: 4.1683e-04 - 17s/epoch - 3ms/step
Epoch 94/200
5969/5969 - 17s - loss: 4.1920e-04 - val_loss: 4.0696e-04 - 17s/epoch - 3ms/step
Epoch 95/200
5969/5969 - 17s - loss: 4.1557e-04 - val_loss: 4.5283e-04 - 17s/epoch - 3ms/step
Epoch 96/200
5969/5969 - 17s - loss: 4.1980e-04 - val_loss: 4.5298e-04 - 17s/epoch - 3ms/step
Epoch 97/200
5969/5969 - 17s - loss: 4.3146e-04 - val_loss: 4.1341e-04 - 17s/epoch - 3ms/step
Epoch 98/200
5969/5969 - 17s - loss: 4.1853e-04 - val_loss: 4.7549e-04 - 17s/epoch - 3ms/step
Epoch 99/200
5969/5969 - 17s - loss: 4.2137e-04 - val_loss: 4.6001e-04 - 17s/epoch - 3ms/step
Epoch 100/200
5969/5969 - 17s - loss: 4.1335e-04 - val_loss: 3.9057e-04 - 17s/epoch - 3ms/step
Epoch 101/200
5969/5969 - 17s - loss: 4.1594e-04 - val_loss: 4.3027e-04 - 17s/epoch - 3ms/step
Epoch 102/200
5969/5969 - 17s - loss: 4.3173e-04 - val_loss: 4.8969e-04 - 17s/epoch - 3ms/step
Epoch 103/200
5969/5969 - 17s - loss: 4.1384e-04 - val_loss: 4.2907e-04 - 17s/epoch - 3ms/step
Epoch 104/200
5969/5969 - 17s - loss: 4.2549e-04 - val_loss: 4.9191e-04 - 17s/epoch - 3ms/step
Epoch 105/200
5969/5969 - 17s - loss: 4.1236e-04 - val_loss: 4.7503e-04 - 17s/epoch - 3ms/step
Epoch 106/200
5969/5969 - 17s - loss: 4.1659e-04 - val_loss: 4.3340e-04 - 17s/epoch - 3ms/step
Epoch 107/200
5969/5969 - 17s - loss: 4.0574e-04 - val_loss: 5.8219e-04 - 17s/epoch - 3ms/step
Epoch 108/200
5969/5969 - 17s - loss: 4.1669e-04 - val_loss: 4.3622e-04 - 17s/epoch - 3ms/step
Epoch 109/200
5969/5969 - 17s - loss: 4.0353e-04 - val_loss: 4.2795e-04 - 17s/epoch - 3ms/step
Epoch 110/200
5969/5969 - 17s - loss: 4.2244e-04 - val_loss: 4.7605e-04 - 17s/epoch - 3ms/step
Epoch 111/200
5969/5969 - 17s - loss: 4.0714e-04 - val_loss: 5.9644e-04 - 17s/epoch - 3ms/step
Epoch 112/200
5969/5969 - 17s - loss: 4.0913e-04 - val_loss: 4.2582e-04 - 17s/epoch - 3ms/step
Epoch 113/200
5969/5969 - 17s - loss: 4.0234e-04 - val_loss: 4.3397e-04 - 17s/epoch - 3ms/step
Epoch 114/200
5969/5969 - 17s - loss: 4.0470e-04 - val_loss: 5.2100e-04 - 17s/epoch - 3ms/step
Epoch 115/200
5969/5969 - 17s - loss: 4.0480e-04 - val_loss: 3.7650e-04 - 17s/epoch - 3ms/step
Epoch 116/200
5969/5969 - 17s - loss: 3.9729e-04 - val_loss: 4.4157e-04 - 17s/epoch - 3ms/step
Epoch 117/200
5969/5969 - 17s - loss: 4.0383e-04 - val_loss: 4.0978e-04 - 17s/epoch - 3ms/step
Epoch 118/200
5969/5969 - 17s - loss: 4.0388e-04 - val_loss: 5.2192e-04 - 17s/epoch - 3ms/step
Epoch 119/200
5969/5969 - 17s - loss: 4.0515e-04 - val_loss: 4.6702e-04 - 17s/epoch - 3ms/step
Epoch 120/200
5969/5969 - 17s - loss: 4.2102e-04 - val_loss: 5.4263e-04 - 17s/epoch - 3ms/step
Epoch 121/200
5969/5969 - 17s - loss: 4.0237e-04 - val_loss: 4.6247e-04 - 17s/epoch - 3ms/step
Epoch 122/200
5969/5969 - 17s - loss: 4.0532e-04 - val_loss: 3.8197e-04 - 17s/epoch - 3ms/step
Epoch 123/200
5969/5969 - 17s - loss: 3.9762e-04 - val_loss: 4.7296e-04 - 17s/epoch - 3ms/step
Epoch 124/200
5969/5969 - 17s - loss: 3.9756e-04 - val_loss: 4.8141e-04 - 17s/epoch - 3ms/step
Epoch 125/200
5969/5969 - 17s - loss: 3.9664e-04 - val_loss: 4.7998e-04 - 17s/epoch - 3ms/step
Epoch 126/200
5969/5969 - 17s - loss: 3.9841e-04 - val_loss: 5.6049e-04 - 17s/epoch - 3ms/step
Epoch 127/200
5969/5969 - 17s - loss: 3.9584e-04 - val_loss: 5.2402e-04 - 17s/epoch - 3ms/step
Epoch 128/200
5969/5969 - 17s - loss: 3.9090e-04 - val_loss: 4.6783e-04 - 17s/epoch - 3ms/step
Epoch 129/200
5969/5969 - 17s - loss: 3.8742e-04 - val_loss: 4.9742e-04 - 17s/epoch - 3ms/step
Epoch 130/200
5969/5969 - 17s - loss: 3.9524e-04 - val_loss: 6.0824e-04 - 17s/epoch - 3ms/step
Epoch 131/200
5969/5969 - 17s - loss: 3.8530e-04 - val_loss: 5.4845e-04 - 17s/epoch - 3ms/step
Epoch 132/200
5969/5969 - 17s - loss: 4.0310e-04 - val_loss: 6.0034e-04 - 17s/epoch - 3ms/step
Epoch 133/200
5969/5969 - 17s - loss: 3.9410e-04 - val_loss: 5.1529e-04 - 17s/epoch - 3ms/step
Epoch 134/200
5969/5969 - 17s - loss: 3.9519e-04 - val_loss: 4.6599e-04 - 17s/epoch - 3ms/step
Epoch 135/200
5969/5969 - 17s - loss: 3.8805e-04 - val_loss: 4.7194e-04 - 17s/epoch - 3ms/step
Epoch 136/200
5969/5969 - 17s - loss: 3.8760e-04 - val_loss: 4.6603e-04 - 17s/epoch - 3ms/step
Epoch 137/200
5969/5969 - 17s - loss: 3.8729e-04 - val_loss: 4.4892e-04 - 17s/epoch - 3ms/step
Epoch 138/200
5969/5969 - 17s - loss: 3.8475e-04 - val_loss: 5.1273e-04 - 17s/epoch - 3ms/step
Epoch 139/200
5969/5969 - 17s - loss: 3.8807e-04 - val_loss: 5.0301e-04 - 17s/epoch - 3ms/step
Epoch 140/200
5969/5969 - 17s - loss: 3.8509e-04 - val_loss: 5.2500e-04 - 17s/epoch - 3ms/step
Epoch 141/200
5969/5969 - 17s - loss: 3.9715e-04 - val_loss: 4.0741e-04 - 17s/epoch - 3ms/step
Epoch 142/200
5969/5969 - 17s - loss: 3.9084e-04 - val_loss: 4.8641e-04 - 17s/epoch - 3ms/step
Epoch 143/200
5969/5969 - 17s - loss: 3.9562e-04 - val_loss: 5.9068e-04 - 17s/epoch - 3ms/step
Epoch 144/200
5969/5969 - 17s - loss: 3.8405e-04 - val_loss: 5.1631e-04 - 17s/epoch - 3ms/step
Epoch 145/200
5969/5969 - 17s - loss: 3.8805e-04 - val_loss: 4.7282e-04 - 17s/epoch - 3ms/step
Epoch 146/200
5969/5969 - 17s - loss: 3.8653e-04 - val_loss: 4.7456e-04 - 17s/epoch - 3ms/step
Epoch 147/200
5969/5969 - 17s - loss: 3.8058e-04 - val_loss: 5.9427e-04 - 17s/epoch - 3ms/step
Epoch 148/200
5969/5969 - 17s - loss: 3.7861e-04 - val_loss: 8.3889e-04 - 17s/epoch - 3ms/step
Epoch 149/200
5969/5969 - 17s - loss: 3.9825e-04 - val_loss: 7.3943e-04 - 17s/epoch - 3ms/step
Epoch 150/200
5969/5969 - 17s - loss: 3.8504e-04 - val_loss: 7.0217e-04 - 17s/epoch - 3ms/step
Epoch 151/200
5969/5969 - 17s - loss: 3.8541e-04 - val_loss: 4.9558e-04 - 17s/epoch - 3ms/step
Epoch 152/200
5969/5969 - 17s - loss: 3.9785e-04 - val_loss: 4.7922e-04 - 17s/epoch - 3ms/step
Epoch 153/200
5969/5969 - 17s - loss: 3.8573e-04 - val_loss: 5.8215e-04 - 17s/epoch - 3ms/step
Epoch 154/200
5969/5969 - 17s - loss: 3.7925e-04 - val_loss: 5.7935e-04 - 17s/epoch - 3ms/step
Epoch 155/200
5969/5969 - 17s - loss: 3.8441e-04 - val_loss: 4.6672e-04 - 17s/epoch - 3ms/step
Epoch 156/200
5969/5969 - 17s - loss: 3.7669e-04 - val_loss: 7.1940e-04 - 17s/epoch - 3ms/step
Epoch 157/200
5969/5969 - 17s - loss: 3.7885e-04 - val_loss: 4.8222e-04 - 17s/epoch - 3ms/step
Epoch 158/200
5969/5969 - 17s - loss: 3.8776e-04 - val_loss: 5.4706e-04 - 17s/epoch - 3ms/step
Epoch 159/200
5969/5969 - 17s - loss: 3.8240e-04 - val_loss: 4.8345e-04 - 17s/epoch - 3ms/step
Epoch 160/200
5969/5969 - 17s - loss: 3.8163e-04 - val_loss: 7.5235e-04 - 17s/epoch - 3ms/step
Epoch 161/200
5969/5969 - 17s - loss: 3.8003e-04 - val_loss: 5.9633e-04 - 17s/epoch - 3ms/step
Epoch 162/200
5969/5969 - 17s - loss: 3.7577e-04 - val_loss: 7.3561e-04 - 17s/epoch - 3ms/step
Epoch 163/200
5969/5969 - 17s - loss: 3.7422e-04 - val_loss: 7.0580e-04 - 17s/epoch - 3ms/step
Epoch 164/200
5969/5969 - 17s - loss: 3.7987e-04 - val_loss: 7.5935e-04 - 17s/epoch - 3ms/step
Epoch 165/200
5969/5969 - 17s - loss: 3.7718e-04 - val_loss: 8.3739e-04 - 17s/epoch - 3ms/step
Epoch 166/200
5969/5969 - 17s - loss: 3.7201e-04 - val_loss: 6.4192e-04 - 17s/epoch - 3ms/step
Epoch 167/200
5969/5969 - 17s - loss: 3.7681e-04 - val_loss: 4.8925e-04 - 17s/epoch - 3ms/step
Epoch 168/200
5969/5969 - 17s - loss: 3.7438e-04 - val_loss: 5.5192e-04 - 17s/epoch - 3ms/step
Epoch 169/200
5969/5969 - 17s - loss: 3.8837e-04 - val_loss: 5.9792e-04 - 17s/epoch - 3ms/step
Epoch 170/200
5969/5969 - 17s - loss: 3.7287e-04 - val_loss: 5.4674e-04 - 17s/epoch - 3ms/step
Epoch 171/200
5969/5969 - 17s - loss: 3.7051e-04 - val_loss: 5.9077e-04 - 17s/epoch - 3ms/step
Epoch 172/200
5969/5969 - 17s - loss: 3.9195e-04 - val_loss: 8.7210e-04 - 17s/epoch - 3ms/step
Epoch 173/200
5969/5969 - 17s - loss: 3.7451e-04 - val_loss: 4.8682e-04 - 17s/epoch - 3ms/step
Epoch 174/200
5969/5969 - 17s - loss: 3.7015e-04 - val_loss: 5.8130e-04 - 17s/epoch - 3ms/step
Epoch 175/200
5969/5969 - 17s - loss: 3.8182e-04 - val_loss: 5.6191e-04 - 17s/epoch - 3ms/step
Epoch 176/200
5969/5969 - 17s - loss: 3.7348e-04 - val_loss: 5.2691e-04 - 17s/epoch - 3ms/step
Epoch 177/200
5969/5969 - 17s - loss: 3.7444e-04 - val_loss: 7.0420e-04 - 17s/epoch - 3ms/step
Epoch 178/200
5969/5969 - 17s - loss: 3.7054e-04 - val_loss: 5.8306e-04 - 17s/epoch - 3ms/step
Epoch 179/200
5969/5969 - 17s - loss: 3.6918e-04 - val_loss: 8.8987e-04 - 17s/epoch - 3ms/step
Epoch 180/200
5969/5969 - 17s - loss: 3.8211e-04 - val_loss: 6.9064e-04 - 17s/epoch - 3ms/step
Epoch 181/200
5969/5969 - 17s - loss: 3.6862e-04 - val_loss: 7.2538e-04 - 17s/epoch - 3ms/step
Epoch 182/200
5969/5969 - 17s - loss: 3.7892e-04 - val_loss: 8.4931e-04 - 17s/epoch - 3ms/step
Epoch 183/200
5969/5969 - 17s - loss: 3.6535e-04 - val_loss: 6.7593e-04 - 17s/epoch - 3ms/step
Epoch 184/200
5969/5969 - 17s - loss: 3.7032e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 185/200
5969/5969 - 17s - loss: 3.6894e-04 - val_loss: 6.4615e-04 - 17s/epoch - 3ms/step
Epoch 186/200
5969/5969 - 17s - loss: 3.7061e-04 - val_loss: 8.3221e-04 - 17s/epoch - 3ms/step
Epoch 187/200
5969/5969 - 17s - loss: 3.7394e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 188/200
5969/5969 - 17s - loss: 3.7975e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 189/200
5969/5969 - 17s - loss: 3.7423e-04 - val_loss: 7.6099e-04 - 17s/epoch - 3ms/step
Epoch 190/200
5969/5969 - 17s - loss: 3.6875e-04 - val_loss: 9.8019e-04 - 17s/epoch - 3ms/step
Epoch 191/200
5969/5969 - 17s - loss: 3.6946e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 192/200
5969/5969 - 17s - loss: 3.6533e-04 - val_loss: 6.0188e-04 - 17s/epoch - 3ms/step
Epoch 193/200
5969/5969 - 17s - loss: 3.6400e-04 - val_loss: 0.0016 - 17s/epoch - 3ms/step
Epoch 194/200
5969/5969 - 17s - loss: 3.6097e-04 - val_loss: 0.0015 - 17s/epoch - 3ms/step
Epoch 195/200
5969/5969 - 17s - loss: 3.6334e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 196/200
5969/5969 - 17s - loss: 3.6286e-04 - val_loss: 0.0020 - 17s/epoch - 3ms/step
Epoch 197/200
5969/5969 - 17s - loss: 3.6144e-04 - val_loss: 0.0018 - 17s/epoch - 3ms/step
Epoch 198/200
5969/5969 - 17s - loss: 3.6335e-04 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 199/200
5969/5969 - 17s - loss: 3.6542e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 200/200
5969/5969 - 17s - loss: 3.9126e-04 - val_loss: 0.0029 - 17s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0029396163299679756
  1/332 [..............................] - ETA: 34s 48/332 [===>..........................] - ETA: 0s  96/332 [=======>......................] - ETA: 0s144/332 [============>.................] - ETA: 0s193/332 [================>.............] - ETA: 0s242/332 [====================>.........] - ETA: 0s291/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0122850960449839
cosine 0.010871257788834821
MAE: 0.013446868
RMSE: 0.05421806
r2: 0.809301641830483
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_12 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_14 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2528)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_13"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_14 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_12 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_14"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_15 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_14 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2528)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 16, 200, 0.001, 0.5, 632, 0.00039126258343458176, 0.0029396163299679756, 0.0122850960449839, 0.010871257788834821, 0.013446868397295475, 0.05421806126832962, 0.809301641830483, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
5969/5969 - 18s - loss: 0.0077 - val_loss: 0.0027 - 18s/epoch - 3ms/step
Epoch 2/200
5969/5969 - 17s - loss: 0.0026 - val_loss: 0.0018 - 17s/epoch - 3ms/step
Epoch 3/200
5969/5969 - 17s - loss: 0.0019 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 4/200
5969/5969 - 17s - loss: 0.0017 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 5/200
5969/5969 - 17s - loss: 0.0014 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 6/200
5969/5969 - 17s - loss: 0.0014 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 7/200
5969/5969 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 8/200
5969/5969 - 17s - loss: 0.0012 - val_loss: 9.3496e-04 - 17s/epoch - 3ms/step
Epoch 9/200
5969/5969 - 17s - loss: 0.0011 - val_loss: 9.2465e-04 - 17s/epoch - 3ms/step
Epoch 10/200
5969/5969 - 17s - loss: 0.0011 - val_loss: 8.6722e-04 - 17s/epoch - 3ms/step
Epoch 11/200
5969/5969 - 17s - loss: 0.0011 - val_loss: 8.9182e-04 - 17s/epoch - 3ms/step
Epoch 12/200
5969/5969 - 17s - loss: 0.0010 - val_loss: 9.1194e-04 - 17s/epoch - 3ms/step
Epoch 13/200
5969/5969 - 17s - loss: 0.0010 - val_loss: 9.2700e-04 - 17s/epoch - 3ms/step
Epoch 14/200
5969/5969 - 17s - loss: 9.8632e-04 - val_loss: 8.2761e-04 - 17s/epoch - 3ms/step
Epoch 15/200
5969/5969 - 17s - loss: 9.5254e-04 - val_loss: 8.2824e-04 - 17s/epoch - 3ms/step
Epoch 16/200
5969/5969 - 17s - loss: 9.3882e-04 - val_loss: 8.1550e-04 - 17s/epoch - 3ms/step
Epoch 17/200
5969/5969 - 17s - loss: 9.6570e-04 - val_loss: 7.4837e-04 - 17s/epoch - 3ms/step
Epoch 18/200
5969/5969 - 17s - loss: 9.8766e-04 - val_loss: 7.6056e-04 - 17s/epoch - 3ms/step
Epoch 19/200
5969/5969 - 17s - loss: 9.2410e-04 - val_loss: 7.9407e-04 - 17s/epoch - 3ms/step
Epoch 20/200
5969/5969 - 17s - loss: 9.0568e-04 - val_loss: 7.1144e-04 - 17s/epoch - 3ms/step
Epoch 21/200
5969/5969 - 17s - loss: 8.8756e-04 - val_loss: 9.5377e-04 - 17s/epoch - 3ms/step
Epoch 22/200
5969/5969 - 17s - loss: 8.6929e-04 - val_loss: 8.0790e-04 - 17s/epoch - 3ms/step
Epoch 23/200
5969/5969 - 17s - loss: 8.7043e-04 - val_loss: 7.4856e-04 - 17s/epoch - 3ms/step
Epoch 24/200
5969/5969 - 17s - loss: 8.7468e-04 - val_loss: 6.8549e-04 - 17s/epoch - 3ms/step
Epoch 25/200
5969/5969 - 17s - loss: 8.5061e-04 - val_loss: 6.8598e-04 - 17s/epoch - 3ms/step
Epoch 26/200
5969/5969 - 17s - loss: 8.3279e-04 - val_loss: 6.8042e-04 - 17s/epoch - 3ms/step
Epoch 27/200
5969/5969 - 17s - loss: 8.5494e-04 - val_loss: 7.6324e-04 - 17s/epoch - 3ms/step
Epoch 28/200
5969/5969 - 17s - loss: 8.3705e-04 - val_loss: 6.5591e-04 - 17s/epoch - 3ms/step
Epoch 29/200
5969/5969 - 17s - loss: 8.0472e-04 - val_loss: 7.0029e-04 - 17s/epoch - 3ms/step
Epoch 30/200
5969/5969 - 17s - loss: 7.9828e-04 - val_loss: 6.7799e-04 - 17s/epoch - 3ms/step
Epoch 31/200
5969/5969 - 17s - loss: 7.9419e-04 - val_loss: 6.4786e-04 - 17s/epoch - 3ms/step
Epoch 32/200
5969/5969 - 17s - loss: 7.8176e-04 - val_loss: 8.9171e-04 - 17s/epoch - 3ms/step
Epoch 33/200
5969/5969 - 17s - loss: 7.7646e-04 - val_loss: 7.5230e-04 - 17s/epoch - 3ms/step
Epoch 34/200
5969/5969 - 17s - loss: 7.7506e-04 - val_loss: 6.8915e-04 - 17s/epoch - 3ms/step
Epoch 35/200
5969/5969 - 17s - loss: 8.1958e-04 - val_loss: 6.2768e-04 - 17s/epoch - 3ms/step
Epoch 36/200
5969/5969 - 17s - loss: 7.6208e-04 - val_loss: 7.0757e-04 - 17s/epoch - 3ms/step
Epoch 37/200
5969/5969 - 17s - loss: 7.7418e-04 - val_loss: 7.6352e-04 - 17s/epoch - 3ms/step
Epoch 38/200
5969/5969 - 17s - loss: 7.5196e-04 - val_loss: 7.6756e-04 - 17s/epoch - 3ms/step
Epoch 39/200
5969/5969 - 17s - loss: 7.8353e-04 - val_loss: 8.3540e-04 - 17s/epoch - 3ms/step
Epoch 40/200
5969/5969 - 17s - loss: 7.6335e-04 - val_loss: 7.1894e-04 - 17s/epoch - 3ms/step
Epoch 41/200
5969/5969 - 17s - loss: 7.4317e-04 - val_loss: 8.0978e-04 - 17s/epoch - 3ms/step
Epoch 42/200
5969/5969 - 17s - loss: 7.4248e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 43/200
5969/5969 - 17s - loss: 7.5782e-04 - val_loss: 8.0667e-04 - 17s/epoch - 3ms/step
Epoch 44/200
5969/5969 - 17s - loss: 7.2801e-04 - val_loss: 8.7056e-04 - 17s/epoch - 3ms/step
Epoch 45/200
5969/5969 - 17s - loss: 7.4935e-04 - val_loss: 8.3151e-04 - 17s/epoch - 3ms/step
Epoch 46/200
5969/5969 - 17s - loss: 7.3235e-04 - val_loss: 8.5173e-04 - 17s/epoch - 3ms/step
Epoch 47/200
5969/5969 - 17s - loss: 7.1931e-04 - val_loss: 8.8504e-04 - 17s/epoch - 3ms/step
Epoch 48/200
5969/5969 - 17s - loss: 7.2524e-04 - val_loss: 8.2784e-04 - 17s/epoch - 3ms/step
Epoch 49/200
5969/5969 - 17s - loss: 7.2983e-04 - val_loss: 6.1278e-04 - 17s/epoch - 3ms/step
Epoch 50/200
5969/5969 - 17s - loss: 7.2022e-04 - val_loss: 0.0015 - 17s/epoch - 3ms/step
Epoch 51/200
5969/5969 - 17s - loss: 7.0385e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 52/200
5969/5969 - 17s - loss: 7.1170e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 53/200
5969/5969 - 17s - loss: 7.0737e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 54/200
5969/5969 - 17s - loss: 6.9733e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 55/200
5969/5969 - 17s - loss: 6.9461e-04 - val_loss: 8.6824e-04 - 17s/epoch - 3ms/step
Epoch 56/200
5969/5969 - 17s - loss: 7.0559e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 57/200
5969/5969 - 17s - loss: 6.7982e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 58/200
5969/5969 - 17s - loss: 6.8149e-04 - val_loss: 0.0018 - 17s/epoch - 3ms/step
Epoch 59/200
5969/5969 - 17s - loss: 6.9480e-04 - val_loss: 0.0024 - 17s/epoch - 3ms/step
Epoch 60/200
5969/5969 - 17s - loss: 6.9951e-04 - val_loss: 0.0024 - 17s/epoch - 3ms/step
Epoch 61/200
5969/5969 - 17s - loss: 6.8232e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 62/200
5969/5969 - 17s - loss: 6.7270e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 63/200
5969/5969 - 17s - loss: 7.2286e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 64/200
5969/5969 - 17s - loss: 6.7085e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 65/200
5969/5969 - 17s - loss: 6.9722e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 66/200
5969/5969 - 17s - loss: 6.8319e-04 - val_loss: 7.3348e-04 - 17s/epoch - 3ms/step
Epoch 67/200
5969/5969 - 17s - loss: 7.1355e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 68/200
5969/5969 - 17s - loss: 6.7807e-04 - val_loss: 8.0408e-04 - 17s/epoch - 3ms/step
Epoch 69/200
5969/5969 - 17s - loss: 6.9568e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 70/200
5969/5969 - 17s - loss: 6.6486e-04 - val_loss: 9.1033e-04 - 17s/epoch - 3ms/step
Epoch 71/200
5969/5969 - 17s - loss: 6.8001e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 72/200
5969/5969 - 17s - loss: 6.8512e-04 - val_loss: 0.0016 - 17s/epoch - 3ms/step
Epoch 73/200
5969/5969 - 17s - loss: 6.6637e-04 - val_loss: 9.4203e-04 - 17s/epoch - 3ms/step
Epoch 74/200
5969/5969 - 17s - loss: 6.7158e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 75/200
5969/5969 - 17s - loss: 6.8941e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 76/200
5969/5969 - 17s - loss: 6.5611e-04 - val_loss: 8.8563e-04 - 17s/epoch - 3ms/step
Epoch 77/200
5969/5969 - 17s - loss: 6.5410e-04 - val_loss: 7.5340e-04 - 17s/epoch - 3ms/step
Epoch 78/200
5969/5969 - 17s - loss: 6.5752e-04 - val_loss: 0.0015 - 17s/epoch - 3ms/step
Epoch 79/200
5969/5969 - 17s - loss: 6.4903e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 80/200
5969/5969 - 17s - loss: 6.7546e-04 - val_loss: 7.3916e-04 - 17s/epoch - 3ms/step
Epoch 81/200
5969/5969 - 17s - loss: 6.4850e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 82/200
5969/5969 - 17s - loss: 6.5771e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 83/200
5969/5969 - 17s - loss: 6.7481e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 84/200
5969/5969 - 17s - loss: 6.4428e-04 - val_loss: 0.0016 - 17s/epoch - 3ms/step
Epoch 85/200
5969/5969 - 17s - loss: 6.5943e-04 - val_loss: 0.0015 - 17s/epoch - 3ms/step
Epoch 86/200
5969/5969 - 17s - loss: 6.5723e-04 - val_loss: 0.0026 - 17s/epoch - 3ms/step
Epoch 87/200
5969/5969 - 17s - loss: 6.5187e-04 - val_loss: 0.0015 - 17s/epoch - 3ms/step
Epoch 88/200
5969/5969 - 17s - loss: 6.4538e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 89/200
5969/5969 - 17s - loss: 6.6649e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 90/200
5969/5969 - 17s - loss: 6.3634e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 91/200
5969/5969 - 17s - loss: 6.4731e-04 - val_loss: 6.5963e-04 - 17s/epoch - 3ms/step
Epoch 92/200
5969/5969 - 17s - loss: 6.2694e-04 - val_loss: 8.0640e-04 - 17s/epoch - 3ms/step
Epoch 93/200
5969/5969 - 17s - loss: 6.2931e-04 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 94/200
5969/5969 - 17s - loss: 6.2555e-04 - val_loss: 0.0021 - 17s/epoch - 3ms/step
Epoch 95/200
5969/5969 - 17s - loss: 6.5960e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 96/200
5969/5969 - 17s - loss: 6.5398e-04 - val_loss: 0.0027 - 17s/epoch - 3ms/step
Epoch 97/200
5969/5969 - 17s - loss: 6.3383e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 98/200
5969/5969 - 17s - loss: 6.3899e-04 - val_loss: 8.0581e-04 - 17s/epoch - 3ms/step
Epoch 99/200
5969/5969 - 17s - loss: 6.2659e-04 - val_loss: 0.0016 - 17s/epoch - 3ms/step
Epoch 100/200
5969/5969 - 17s - loss: 6.3129e-04 - val_loss: 9.5813e-04 - 17s/epoch - 3ms/step
Epoch 101/200
5969/5969 - 17s - loss: 6.3622e-04 - val_loss: 6.9257e-04 - 17s/epoch - 3ms/step
Epoch 102/200
5969/5969 - 17s - loss: 6.3136e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 103/200
5969/5969 - 17s - loss: 6.2038e-04 - val_loss: 8.1629e-04 - 17s/epoch - 3ms/step
Epoch 104/200
5969/5969 - 17s - loss: 6.3264e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 105/200
5969/5969 - 17s - loss: 6.2553e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 106/200
5969/5969 - 17s - loss: 6.1966e-04 - val_loss: 0.0021 - 17s/epoch - 3ms/step
Epoch 107/200
5969/5969 - 17s - loss: 6.2068e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 108/200
5969/5969 - 17s - loss: 6.6318e-04 - val_loss: 0.0028 - 17s/epoch - 3ms/step
Epoch 109/200
5969/5969 - 17s - loss: 6.2876e-04 - val_loss: 9.5075e-04 - 17s/epoch - 3ms/step
Epoch 110/200
5969/5969 - 17s - loss: 6.3554e-04 - val_loss: 8.4931e-04 - 17s/epoch - 3ms/step
Epoch 111/200
5969/5969 - 17s - loss: 6.2676e-04 - val_loss: 7.7760e-04 - 17s/epoch - 3ms/step
Epoch 112/200
5969/5969 - 17s - loss: 6.2223e-04 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 113/200
5969/5969 - 17s - loss: 6.1253e-04 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 114/200
5969/5969 - 17s - loss: 6.2039e-04 - val_loss: 0.0019 - 17s/epoch - 3ms/step
Epoch 115/200
5969/5969 - 17s - loss: 6.4933e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 116/200
5969/5969 - 17s - loss: 6.5534e-04 - val_loss: 0.0020 - 17s/epoch - 3ms/step
Epoch 117/200
5969/5969 - 17s - loss: 6.1767e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 118/200
5969/5969 - 17s - loss: 6.3061e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 119/200
5969/5969 - 17s - loss: 6.1504e-04 - val_loss: 0.0017 - 17s/epoch - 3ms/step
Epoch 120/200
5969/5969 - 17s - loss: 6.1250e-04 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 121/200
5969/5969 - 17s - loss: 6.2356e-04 - val_loss: 0.0016 - 17s/epoch - 3ms/step
Epoch 122/200
5969/5969 - 17s - loss: 6.2740e-04 - val_loss: 9.7487e-04 - 17s/epoch - 3ms/step
Epoch 123/200
5969/5969 - 17s - loss: 6.2405e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 124/200
5969/5969 - 17s - loss: 6.6262e-04 - val_loss: 7.8653e-04 - 17s/epoch - 3ms/step
Epoch 125/200
5969/5969 - 17s - loss: 6.1969e-04 - val_loss: 8.3611e-04 - 17s/epoch - 3ms/step
Epoch 126/200
5969/5969 - 17s - loss: 6.2148e-04 - val_loss: 0.0018 - 17s/epoch - 3ms/step
Epoch 127/200
5969/5969 - 17s - loss: 6.1159e-04 - val_loss: 0.0019 - 17s/epoch - 3ms/step
Epoch 128/200
5969/5969 - 17s - loss: 6.1490e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 129/200
5969/5969 - 17s - loss: 6.1066e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 130/200
5969/5969 - 17s - loss: 6.1152e-04 - val_loss: 0.0015 - 17s/epoch - 3ms/step
Epoch 131/200
5969/5969 - 17s - loss: 6.1840e-04 - val_loss: 0.0016 - 17s/epoch - 3ms/step
Epoch 132/200
5969/5969 - 17s - loss: 6.3217e-04 - val_loss: 0.0026 - 17s/epoch - 3ms/step
Epoch 133/200
5969/5969 - 17s - loss: 6.0852e-04 - val_loss: 0.0016 - 17s/epoch - 3ms/step
Epoch 134/200
5969/5969 - 17s - loss: 6.1282e-04 - val_loss: 0.0016 - 17s/epoch - 3ms/step
Epoch 135/200
5969/5969 - 17s - loss: 6.1498e-04 - val_loss: 0.0015 - 17s/epoch - 3ms/step
Epoch 136/200
5969/5969 - 17s - loss: 6.0435e-04 - val_loss: 0.0019 - 17s/epoch - 3ms/step
Epoch 137/200
5969/5969 - 17s - loss: 6.0307e-04 - val_loss: 0.0026 - 17s/epoch - 3ms/step
Epoch 138/200
5969/5969 - 17s - loss: 6.1438e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 139/200
5969/5969 - 17s - loss: 6.2261e-04 - val_loss: 0.0024 - 17s/epoch - 3ms/step
Epoch 140/200
5969/5969 - 17s - loss: 6.0704e-04 - val_loss: 0.0035 - 17s/epoch - 3ms/step
Epoch 141/200
5969/5969 - 17s - loss: 6.1926e-04 - val_loss: 0.0017 - 17s/epoch - 3ms/step
Epoch 142/200
5969/5969 - 17s - loss: 6.2387e-04 - val_loss: 0.0016 - 17s/epoch - 3ms/step
Epoch 143/200
5969/5969 - 17s - loss: 6.1265e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 144/200
5969/5969 - 17s - loss: 6.0317e-04 - val_loss: 0.0018 - 17s/epoch - 3ms/step
Epoch 145/200
5969/5969 - 17s - loss: 6.0864e-04 - val_loss: 0.0024 - 17s/epoch - 3ms/step
Epoch 146/200
5969/5969 - 17s - loss: 6.2447e-04 - val_loss: 0.0021 - 17s/epoch - 3ms/step
Epoch 147/200
5969/5969 - 17s - loss: 6.1604e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 148/200
5969/5969 - 17s - loss: 6.1023e-04 - val_loss: 9.7444e-04 - 17s/epoch - 3ms/step
Epoch 149/200
5969/5969 - 17s - loss: 5.9778e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 150/200
5969/5969 - 17s - loss: 6.1041e-04 - val_loss: 0.0021 - 17s/epoch - 3ms/step
Epoch 151/200
5969/5969 - 17s - loss: 5.9396e-04 - val_loss: 0.0015 - 17s/epoch - 3ms/step
Epoch 152/200
5969/5969 - 17s - loss: 5.8945e-04 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 153/200
5969/5969 - 17s - loss: 6.0193e-04 - val_loss: 0.0021 - 17s/epoch - 3ms/step
Epoch 154/200
5969/5969 - 17s - loss: 5.9525e-04 - val_loss: 0.0024 - 17s/epoch - 3ms/step
Epoch 155/200
5969/5969 - 17s - loss: 6.3414e-04 - val_loss: 0.0031 - 17s/epoch - 3ms/step
Epoch 156/200
5969/5969 - 17s - loss: 6.3440e-04 - val_loss: 9.7422e-04 - 17s/epoch - 3ms/step
Epoch 157/200
5969/5969 - 17s - loss: 6.0006e-04 - val_loss: 0.0017 - 17s/epoch - 3ms/step
Epoch 158/200
5969/5969 - 17s - loss: 5.9915e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 159/200
5969/5969 - 17s - loss: 6.0243e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 160/200
5969/5969 - 17s - loss: 5.9735e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 161/200
5969/5969 - 17s - loss: 5.8980e-04 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 162/200
5969/5969 - 17s - loss: 5.9340e-04 - val_loss: 0.0024 - 17s/epoch - 3ms/step
Epoch 163/200
5969/5969 - 17s - loss: 5.8871e-04 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 164/200
5969/5969 - 17s - loss: 6.0178e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 165/200
5969/5969 - 17s - loss: 5.9527e-04 - val_loss: 0.0018 - 17s/epoch - 3ms/step
Epoch 166/200
5969/5969 - 17s - loss: 6.2748e-04 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 167/200
5969/5969 - 17s - loss: 5.9134e-04 - val_loss: 0.0018 - 17s/epoch - 3ms/step
Epoch 168/200
5969/5969 - 17s - loss: 6.0498e-04 - val_loss: 0.0015 - 17s/epoch - 3ms/step
Epoch 169/200
5969/5969 - 17s - loss: 5.8508e-04 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 170/200
5969/5969 - 17s - loss: 6.0469e-04 - val_loss: 7.6764e-04 - 17s/epoch - 3ms/step
Epoch 171/200
5969/5969 - 17s - loss: 5.8681e-04 - val_loss: 0.0027 - 17s/epoch - 3ms/step
Epoch 172/200
5969/5969 - 17s - loss: 5.9662e-04 - val_loss: 0.0029 - 17s/epoch - 3ms/step
Epoch 173/200
5969/5969 - 17s - loss: 5.9964e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 174/200
5969/5969 - 17s - loss: 6.0313e-04 - val_loss: 0.0021 - 17s/epoch - 3ms/step
Epoch 175/200
5969/5969 - 17s - loss: 5.8534e-04 - val_loss: 0.0021 - 17s/epoch - 3ms/step
Epoch 176/200
5969/5969 - 17s - loss: 5.8592e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 177/200
5969/5969 - 17s - loss: 5.9768e-04 - val_loss: 9.2321e-04 - 17s/epoch - 3ms/step
Epoch 178/200
5969/5969 - 17s - loss: 5.9099e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 179/200
5969/5969 - 17s - loss: 5.9018e-04 - val_loss: 7.9144e-04 - 17s/epoch - 3ms/step
Epoch 180/200
5969/5969 - 17s - loss: 6.0130e-04 - val_loss: 0.0020 - 17s/epoch - 3ms/step
Epoch 181/200
5969/5969 - 17s - loss: 5.8444e-04 - val_loss: 0.0036 - 17s/epoch - 3ms/step
Epoch 182/200
5969/5969 - 17s - loss: 5.8981e-04 - val_loss: 0.0023 - 17s/epoch - 3ms/step
Epoch 183/200
5969/5969 - 17s - loss: 5.9314e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 184/200
5969/5969 - 17s - loss: 5.9100e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 185/200
5969/5969 - 17s - loss: 5.7852e-04 - val_loss: 0.0025 - 17s/epoch - 3ms/step
Epoch 186/200
5969/5969 - 17s - loss: 5.8945e-04 - val_loss: 0.0017 - 17s/epoch - 3ms/step
Epoch 187/200
5969/5969 - 17s - loss: 5.9644e-04 - val_loss: 0.0020 - 17s/epoch - 3ms/step
Epoch 188/200
5969/5969 - 17s - loss: 5.8151e-04 - val_loss: 0.0026 - 17s/epoch - 3ms/step
Epoch 189/200
5969/5969 - 17s - loss: 5.8793e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 190/200
5969/5969 - 17s - loss: 5.7838e-04 - val_loss: 0.0027 - 17s/epoch - 3ms/step
Epoch 191/200
5969/5969 - 17s - loss: 5.8101e-04 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 192/200
5969/5969 - 17s - loss: 5.8090e-04 - val_loss: 0.0017 - 17s/epoch - 3ms/step
Epoch 193/200
5969/5969 - 17s - loss: 5.8061e-04 - val_loss: 0.0028 - 17s/epoch - 3ms/step
Epoch 194/200
5969/5969 - 17s - loss: 5.8282e-04 - val_loss: 0.0046 - 17s/epoch - 3ms/step
Epoch 195/200
5969/5969 - 17s - loss: 5.6524e-04 - val_loss: 0.0058 - 17s/epoch - 3ms/step
Epoch 196/200
5969/5969 - 17s - loss: 5.8294e-04 - val_loss: 0.0019 - 17s/epoch - 3ms/step
Epoch 197/200
5969/5969 - 17s - loss: 5.8004e-04 - val_loss: 0.0019 - 17s/epoch - 3ms/step
Epoch 198/200
5969/5969 - 17s - loss: 5.6634e-04 - val_loss: 0.0026 - 17s/epoch - 3ms/step
Epoch 199/200
5969/5969 - 17s - loss: 5.6959e-04 - val_loss: 0.0035 - 17s/epoch - 3ms/step
Epoch 200/200
5969/5969 - 17s - loss: 5.7986e-04 - val_loss: 0.0025 - 17s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.002525154035538435
  1/332 [..............................] - ETA: 36s 47/332 [===>..........................] - ETA: 0s  95/332 [=======>......................] - ETA: 0s143/332 [===========>..................] - ETA: 0s192/332 [================>.............] - ETA: 0s240/332 [====================>.........] - ETA: 0s288/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.012082151671231939
cosine 0.010119017361021923
MAE: 0.013717567
RMSE: 0.0502508
r2: 0.8361876464047407
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_16"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_17 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_17"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_18 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 16, 200, 0.002, 0.5, 632, 0.0005798625061288476, 0.002525154035538435, 0.012082151671231939, 0.010119017361021923, 0.01371756661683321, 0.05025079846382141, 0.8361876464047407, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_18 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_18 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_20 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2528)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/300
5969/5969 - 18s - loss: 0.0080 - val_loss: 0.0032 - 18s/epoch - 3ms/step
Epoch 2/300
5969/5969 - 17s - loss: 0.0036 - val_loss: 0.0020 - 17s/epoch - 3ms/step
Epoch 3/300
5969/5969 - 17s - loss: 0.0022 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 4/300
5969/5969 - 17s - loss: 0.0016 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 5/300
5969/5969 - 17s - loss: 0.0013 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 6/300
5969/5969 - 17s - loss: 0.0011 - val_loss: 9.4617e-04 - 17s/epoch - 3ms/step
Epoch 7/300
5969/5969 - 17s - loss: 9.9548e-04 - val_loss: 8.6471e-04 - 17s/epoch - 3ms/step
Epoch 8/300
5969/5969 - 17s - loss: 9.1080e-04 - val_loss: 7.4436e-04 - 17s/epoch - 3ms/step
Epoch 9/300
5969/5969 - 17s - loss: 8.3749e-04 - val_loss: 7.0902e-04 - 17s/epoch - 3ms/step
Epoch 10/300
5969/5969 - 17s - loss: 8.0410e-04 - val_loss: 6.6641e-04 - 17s/epoch - 3ms/step
Epoch 11/300
5969/5969 - 17s - loss: 7.5240e-04 - val_loss: 6.6655e-04 - 17s/epoch - 3ms/step
Epoch 12/300
5969/5969 - 17s - loss: 7.1566e-04 - val_loss: 6.0794e-04 - 17s/epoch - 3ms/step
Epoch 13/300
5969/5969 - 17s - loss: 6.8887e-04 - val_loss: 5.9030e-04 - 17s/epoch - 3ms/step
Epoch 14/300
5969/5969 - 17s - loss: 6.6236e-04 - val_loss: 5.9663e-04 - 17s/epoch - 3ms/step
Epoch 15/300
5969/5969 - 17s - loss: 6.4165e-04 - val_loss: 6.0627e-04 - 17s/epoch - 3ms/step
Epoch 16/300
5969/5969 - 17s - loss: 6.1871e-04 - val_loss: 5.9384e-04 - 17s/epoch - 3ms/step
Epoch 17/300
5969/5969 - 17s - loss: 6.3449e-04 - val_loss: 5.1150e-04 - 17s/epoch - 3ms/step
Epoch 18/300
5969/5969 - 17s - loss: 5.9281e-04 - val_loss: 5.4039e-04 - 17s/epoch - 3ms/step
Epoch 19/300
5969/5969 - 17s - loss: 5.7719e-04 - val_loss: 5.1858e-04 - 17s/epoch - 3ms/step
Epoch 20/300
5969/5969 - 17s - loss: 5.6204e-04 - val_loss: 4.8146e-04 - 17s/epoch - 3ms/step
Epoch 21/300
5969/5969 - 17s - loss: 5.5283e-04 - val_loss: 4.8581e-04 - 17s/epoch - 3ms/step
Epoch 22/300
5969/5969 - 17s - loss: 5.6119e-04 - val_loss: 4.8605e-04 - 17s/epoch - 3ms/step
Epoch 23/300
5969/5969 - 17s - loss: 5.2977e-04 - val_loss: 4.6052e-04 - 17s/epoch - 3ms/step
Epoch 24/300
5969/5969 - 17s - loss: 5.1519e-04 - val_loss: 5.3148e-04 - 17s/epoch - 3ms/step
Epoch 25/300
5969/5969 - 17s - loss: 5.3437e-04 - val_loss: 4.7089e-04 - 17s/epoch - 3ms/step
Epoch 26/300
5969/5969 - 17s - loss: 5.0299e-04 - val_loss: 4.2361e-04 - 17s/epoch - 3ms/step
Epoch 27/300
5969/5969 - 17s - loss: 4.9406e-04 - val_loss: 4.3718e-04 - 17s/epoch - 3ms/step
Epoch 28/300
5969/5969 - 17s - loss: 4.9437e-04 - val_loss: 4.3234e-04 - 17s/epoch - 3ms/step
Epoch 29/300
5969/5969 - 17s - loss: 4.8309e-04 - val_loss: 4.6504e-04 - 17s/epoch - 3ms/step
Epoch 30/300
5969/5969 - 17s - loss: 4.6853e-04 - val_loss: 3.9811e-04 - 17s/epoch - 3ms/step
Epoch 31/300
5969/5969 - 17s - loss: 4.7103e-04 - val_loss: 3.8497e-04 - 17s/epoch - 3ms/step
Epoch 32/300
5969/5969 - 17s - loss: 4.5850e-04 - val_loss: 4.2377e-04 - 17s/epoch - 3ms/step
Epoch 33/300
5969/5969 - 17s - loss: 4.5279e-04 - val_loss: 3.9786e-04 - 17s/epoch - 3ms/step
Epoch 34/300
5969/5969 - 17s - loss: 4.5716e-04 - val_loss: 3.7690e-04 - 17s/epoch - 3ms/step
Epoch 35/300
5969/5969 - 17s - loss: 4.5098e-04 - val_loss: 3.9469e-04 - 17s/epoch - 3ms/step
Epoch 36/300
5969/5969 - 17s - loss: 4.4109e-04 - val_loss: 4.1202e-04 - 17s/epoch - 3ms/step
Epoch 37/300
5969/5969 - 17s - loss: 4.3737e-04 - val_loss: 4.0080e-04 - 17s/epoch - 3ms/step
Epoch 38/300
5969/5969 - 17s - loss: 4.3394e-04 - val_loss: 3.7075e-04 - 17s/epoch - 3ms/step
Epoch 39/300
5969/5969 - 17s - loss: 4.2777e-04 - val_loss: 3.9755e-04 - 17s/epoch - 3ms/step
Epoch 40/300
5969/5969 - 17s - loss: 4.2910e-04 - val_loss: 3.7019e-04 - 17s/epoch - 3ms/step
Epoch 41/300
5969/5969 - 17s - loss: 4.1959e-04 - val_loss: 3.5683e-04 - 17s/epoch - 3ms/step
Epoch 42/300
5969/5969 - 17s - loss: 4.1608e-04 - val_loss: 3.6177e-04 - 17s/epoch - 3ms/step
Epoch 43/300
5969/5969 - 17s - loss: 4.1535e-04 - val_loss: 3.6320e-04 - 17s/epoch - 3ms/step
Epoch 44/300
5969/5969 - 17s - loss: 4.1119e-04 - val_loss: 3.6099e-04 - 17s/epoch - 3ms/step
Epoch 45/300
5969/5969 - 17s - loss: 4.0286e-04 - val_loss: 3.6696e-04 - 17s/epoch - 3ms/step
Epoch 46/300
5969/5969 - 17s - loss: 4.0073e-04 - val_loss: 3.4693e-04 - 17s/epoch - 3ms/step
Epoch 47/300
5969/5969 - 17s - loss: 4.0680e-04 - val_loss: 3.6043e-04 - 17s/epoch - 3ms/step
Epoch 48/300
5969/5969 - 17s - loss: 4.0153e-04 - val_loss: 3.4546e-04 - 17s/epoch - 3ms/step
Epoch 49/300
5969/5969 - 17s - loss: 4.0112e-04 - val_loss: 3.5121e-04 - 17s/epoch - 3ms/step
Epoch 50/300
5969/5969 - 17s - loss: 3.9189e-04 - val_loss: 3.8476e-04 - 17s/epoch - 3ms/step
Epoch 51/300
5969/5969 - 17s - loss: 3.8984e-04 - val_loss: 3.4240e-04 - 17s/epoch - 3ms/step
Epoch 52/300
5969/5969 - 17s - loss: 3.8339e-04 - val_loss: 3.3507e-04 - 17s/epoch - 3ms/step
Epoch 53/300
5969/5969 - 17s - loss: 3.9090e-04 - val_loss: 3.4116e-04 - 17s/epoch - 3ms/step
Epoch 54/300
5969/5969 - 17s - loss: 3.7839e-04 - val_loss: 3.7186e-04 - 17s/epoch - 3ms/step
Epoch 55/300
5969/5969 - 17s - loss: 3.7526e-04 - val_loss: 3.3080e-04 - 17s/epoch - 3ms/step
Epoch 56/300
5969/5969 - 17s - loss: 3.7645e-04 - val_loss: 3.3628e-04 - 17s/epoch - 3ms/step
Epoch 57/300
5969/5969 - 17s - loss: 3.7684e-04 - val_loss: 3.3580e-04 - 17s/epoch - 3ms/step
Epoch 58/300
5969/5969 - 17s - loss: 3.6851e-04 - val_loss: 3.3830e-04 - 17s/epoch - 3ms/step
Epoch 59/300
5969/5969 - 17s - loss: 3.6812e-04 - val_loss: 3.4676e-04 - 17s/epoch - 3ms/step
Epoch 60/300
5969/5969 - 17s - loss: 3.6743e-04 - val_loss: 3.3058e-04 - 17s/epoch - 3ms/step
Epoch 61/300
5969/5969 - 17s - loss: 3.6850e-04 - val_loss: 3.4489e-04 - 17s/epoch - 3ms/step
Epoch 62/300
5969/5969 - 17s - loss: 3.6867e-04 - val_loss: 3.5503e-04 - 17s/epoch - 3ms/step
Epoch 63/300
5969/5969 - 17s - loss: 3.6107e-04 - val_loss: 3.4315e-04 - 17s/epoch - 3ms/step
Epoch 64/300
5969/5969 - 17s - loss: 3.6005e-04 - val_loss: 3.5293e-04 - 17s/epoch - 3ms/step
Epoch 65/300
5969/5969 - 17s - loss: 3.5817e-04 - val_loss: 3.4564e-04 - 17s/epoch - 3ms/step
Epoch 66/300
5969/5969 - 17s - loss: 3.5403e-04 - val_loss: 3.2186e-04 - 17s/epoch - 3ms/step
Epoch 67/300
5969/5969 - 17s - loss: 3.5624e-04 - val_loss: 3.4617e-04 - 17s/epoch - 3ms/step
Epoch 68/300
5969/5969 - 17s - loss: 3.5274e-04 - val_loss: 3.1767e-04 - 17s/epoch - 3ms/step
Epoch 69/300
5969/5969 - 17s - loss: 3.5415e-04 - val_loss: 3.3132e-04 - 17s/epoch - 3ms/step
Epoch 70/300
5969/5969 - 17s - loss: 3.4683e-04 - val_loss: 3.2876e-04 - 17s/epoch - 3ms/step
Epoch 71/300
5969/5969 - 17s - loss: 3.5315e-04 - val_loss: 3.5861e-04 - 17s/epoch - 3ms/step
Epoch 72/300
5969/5969 - 17s - loss: 3.5251e-04 - val_loss: 3.5438e-04 - 17s/epoch - 3ms/step
Epoch 73/300
5969/5969 - 17s - loss: 3.4501e-04 - val_loss: 3.3502e-04 - 17s/epoch - 3ms/step
Epoch 74/300
5969/5969 - 17s - loss: 3.5089e-04 - val_loss: 3.2527e-04 - 17s/epoch - 3ms/step
Epoch 75/300
5969/5969 - 17s - loss: 3.4858e-04 - val_loss: 3.2490e-04 - 17s/epoch - 3ms/step
Epoch 76/300
5969/5969 - 17s - loss: 3.3847e-04 - val_loss: 3.2435e-04 - 17s/epoch - 3ms/step
Epoch 77/300
5969/5969 - 17s - loss: 3.3524e-04 - val_loss: 3.0118e-04 - 17s/epoch - 3ms/step
Epoch 78/300
5969/5969 - 17s - loss: 3.4427e-04 - val_loss: 3.5325e-04 - 17s/epoch - 3ms/step
Epoch 79/300
5969/5969 - 17s - loss: 3.3569e-04 - val_loss: 3.1923e-04 - 17s/epoch - 3ms/step
Epoch 80/300
5969/5969 - 17s - loss: 3.3393e-04 - val_loss: 2.9379e-04 - 17s/epoch - 3ms/step
Epoch 81/300
5969/5969 - 17s - loss: 3.3949e-04 - val_loss: 3.1023e-04 - 17s/epoch - 3ms/step
Epoch 82/300
5969/5969 - 17s - loss: 3.4535e-04 - val_loss: 2.9381e-04 - 17s/epoch - 3ms/step
Epoch 83/300
5969/5969 - 17s - loss: 3.3829e-04 - val_loss: 3.2690e-04 - 17s/epoch - 3ms/step
Epoch 84/300
5969/5969 - 17s - loss: 3.3372e-04 - val_loss: 3.4164e-04 - 17s/epoch - 3ms/step
Epoch 85/300
5969/5969 - 17s - loss: 3.3084e-04 - val_loss: 3.5066e-04 - 17s/epoch - 3ms/step
Epoch 86/300
5969/5969 - 17s - loss: 3.2718e-04 - val_loss: 3.3252e-04 - 17s/epoch - 3ms/step
Epoch 87/300
5969/5969 - 17s - loss: 3.3165e-04 - val_loss: 3.2291e-04 - 17s/epoch - 3ms/step
Epoch 88/300
5969/5969 - 17s - loss: 3.2478e-04 - val_loss: 2.9727e-04 - 17s/epoch - 3ms/step
Epoch 89/300
5969/5969 - 17s - loss: 3.3239e-04 - val_loss: 3.0486e-04 - 17s/epoch - 3ms/step
Epoch 90/300
5969/5969 - 17s - loss: 3.2541e-04 - val_loss: 2.9320e-04 - 17s/epoch - 3ms/step
Epoch 91/300
5969/5969 - 17s - loss: 3.3590e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 92/300
5969/5969 - 17s - loss: 3.3653e-04 - val_loss: 3.2893e-04 - 17s/epoch - 3ms/step
Epoch 93/300
5969/5969 - 17s - loss: 3.2365e-04 - val_loss: 3.0207e-04 - 17s/epoch - 3ms/step
Epoch 94/300
5969/5969 - 17s - loss: 3.2162e-04 - val_loss: 3.1348e-04 - 17s/epoch - 3ms/step
Epoch 95/300
5969/5969 - 17s - loss: 3.2194e-04 - val_loss: 2.9546e-04 - 17s/epoch - 3ms/step
Epoch 96/300
5969/5969 - 17s - loss: 3.2691e-04 - val_loss: 3.4240e-04 - 17s/epoch - 3ms/step
Epoch 97/300
5969/5969 - 17s - loss: 3.2356e-04 - val_loss: 2.9357e-04 - 17s/epoch - 3ms/step
Epoch 98/300
5969/5969 - 17s - loss: 3.2240e-04 - val_loss: 2.8729e-04 - 17s/epoch - 3ms/step
Epoch 99/300
5969/5969 - 17s - loss: 3.1788e-04 - val_loss: 2.8569e-04 - 17s/epoch - 3ms/step
Epoch 100/300
5969/5969 - 17s - loss: 3.1575e-04 - val_loss: 3.0544e-04 - 17s/epoch - 3ms/step
Epoch 101/300
5969/5969 - 17s - loss: 3.0974e-04 - val_loss: 2.7759e-04 - 17s/epoch - 3ms/step
Epoch 102/300
5969/5969 - 17s - loss: 3.2648e-04 - val_loss: 3.0091e-04 - 17s/epoch - 3ms/step
Epoch 103/300
5969/5969 - 17s - loss: 3.1696e-04 - val_loss: 2.8412e-04 - 17s/epoch - 3ms/step
Epoch 104/300
5969/5969 - 17s - loss: 3.1310e-04 - val_loss: 3.0658e-04 - 17s/epoch - 3ms/step
Epoch 105/300
5969/5969 - 17s - loss: 3.1828e-04 - val_loss: 3.0204e-04 - 17s/epoch - 3ms/step
Epoch 106/300
5969/5969 - 17s - loss: 3.1462e-04 - val_loss: 3.1041e-04 - 17s/epoch - 3ms/step
Epoch 107/300
5969/5969 - 17s - loss: 3.1029e-04 - val_loss: 3.0848e-04 - 17s/epoch - 3ms/step
Epoch 108/300
5969/5969 - 17s - loss: 3.1260e-04 - val_loss: 2.9171e-04 - 17s/epoch - 3ms/step
Epoch 109/300
5969/5969 - 17s - loss: 3.0589e-04 - val_loss: 2.9864e-04 - 17s/epoch - 3ms/step
Epoch 110/300
5969/5969 - 17s - loss: 3.1501e-04 - val_loss: 2.9646e-04 - 17s/epoch - 3ms/step
Epoch 111/300
5969/5969 - 17s - loss: 3.0829e-04 - val_loss: 3.0379e-04 - 17s/epoch - 3ms/step
Epoch 112/300
5969/5969 - 17s - loss: 3.0728e-04 - val_loss: 2.9862e-04 - 17s/epoch - 3ms/step
Epoch 113/300
5969/5969 - 17s - loss: 3.0910e-04 - val_loss: 3.1437e-04 - 17s/epoch - 3ms/step
Epoch 114/300
5969/5969 - 17s - loss: 3.0278e-04 - val_loss: 2.8750e-04 - 17s/epoch - 3ms/step
Epoch 115/300
5969/5969 - 17s - loss: 3.0490e-04 - val_loss: 2.7735e-04 - 17s/epoch - 3ms/step
Epoch 116/300
5969/5969 - 17s - loss: 3.0651e-04 - val_loss: 3.0453e-04 - 17s/epoch - 3ms/step
Epoch 117/300
5969/5969 - 17s - loss: 3.0398e-04 - val_loss: 3.0491e-04 - 17s/epoch - 3ms/step
Epoch 118/300
5969/5969 - 17s - loss: 3.0170e-04 - val_loss: 2.9261e-04 - 17s/epoch - 3ms/step
Epoch 119/300
5969/5969 - 17s - loss: 3.0484e-04 - val_loss: 2.9315e-04 - 17s/epoch - 3ms/step
Epoch 120/300
5969/5969 - 17s - loss: 3.0213e-04 - val_loss: 2.9645e-04 - 17s/epoch - 3ms/step
Epoch 121/300
5969/5969 - 17s - loss: 3.1088e-04 - val_loss: 2.9994e-04 - 17s/epoch - 3ms/step
Epoch 122/300
5969/5969 - 17s - loss: 3.0186e-04 - val_loss: 2.8702e-04 - 17s/epoch - 3ms/step
Epoch 123/300
5969/5969 - 17s - loss: 3.1166e-04 - val_loss: 3.0667e-04 - 17s/epoch - 3ms/step
Epoch 124/300
5969/5969 - 17s - loss: 3.0044e-04 - val_loss: 2.8200e-04 - 17s/epoch - 3ms/step
Epoch 125/300
5969/5969 - 17s - loss: 2.9674e-04 - val_loss: 2.8748e-04 - 17s/epoch - 3ms/step
Epoch 126/300
5969/5969 - 17s - loss: 3.0034e-04 - val_loss: 3.0752e-04 - 17s/epoch - 3ms/step
Epoch 127/300
5969/5969 - 17s - loss: 2.9542e-04 - val_loss: 3.0953e-04 - 17s/epoch - 3ms/step
Epoch 128/300
5969/5969 - 17s - loss: 2.9794e-04 - val_loss: 2.9242e-04 - 17s/epoch - 3ms/step
Epoch 129/300
5969/5969 - 17s - loss: 2.9413e-04 - val_loss: 2.6939e-04 - 17s/epoch - 3ms/step
Epoch 130/300
5969/5969 - 17s - loss: 2.9345e-04 - val_loss: 2.9164e-04 - 17s/epoch - 3ms/step
Epoch 131/300
5969/5969 - 17s - loss: 2.9222e-04 - val_loss: 2.9051e-04 - 17s/epoch - 3ms/step
Epoch 132/300
5969/5969 - 17s - loss: 3.1649e-04 - val_loss: 3.0579e-04 - 17s/epoch - 3ms/step
Epoch 133/300
5969/5969 - 17s - loss: 2.9762e-04 - val_loss: 2.9142e-04 - 17s/epoch - 3ms/step
Epoch 134/300
5969/5969 - 17s - loss: 2.9598e-04 - val_loss: 2.9992e-04 - 17s/epoch - 3ms/step
Epoch 135/300
5969/5969 - 17s - loss: 2.9197e-04 - val_loss: 2.8169e-04 - 17s/epoch - 3ms/step
Epoch 136/300
5969/5969 - 17s - loss: 2.9452e-04 - val_loss: 2.8036e-04 - 17s/epoch - 3ms/step
Epoch 137/300
5969/5969 - 17s - loss: 2.9238e-04 - val_loss: 3.1652e-04 - 17s/epoch - 3ms/step
Epoch 138/300
5969/5969 - 17s - loss: 2.9005e-04 - val_loss: 3.0571e-04 - 17s/epoch - 3ms/step
Epoch 139/300
5969/5969 - 17s - loss: 2.9386e-04 - val_loss: 2.9264e-04 - 17s/epoch - 3ms/step
Epoch 140/300
5969/5969 - 17s - loss: 2.9039e-04 - val_loss: 3.2636e-04 - 17s/epoch - 3ms/step
Epoch 141/300
5969/5969 - 17s - loss: 2.9675e-04 - val_loss: 2.6444e-04 - 17s/epoch - 3ms/step
Epoch 142/300
5969/5969 - 17s - loss: 2.8700e-04 - val_loss: 3.0072e-04 - 17s/epoch - 3ms/step
Epoch 143/300
5969/5969 - 17s - loss: 2.9576e-04 - val_loss: 2.9054e-04 - 17s/epoch - 3ms/step
Epoch 144/300
5969/5969 - 17s - loss: 2.9245e-04 - val_loss: 2.9751e-04 - 17s/epoch - 3ms/step
Epoch 145/300
5969/5969 - 17s - loss: 2.8775e-04 - val_loss: 3.1335e-04 - 17s/epoch - 3ms/step
Epoch 146/300
5969/5969 - 17s - loss: 3.1208e-04 - val_loss: 3.3736e-04 - 17s/epoch - 3ms/step
Epoch 147/300
5969/5969 - 17s - loss: 2.9486e-04 - val_loss: 2.8072e-04 - 17s/epoch - 3ms/step
Epoch 148/300
5969/5969 - 17s - loss: 2.8882e-04 - val_loss: 3.0447e-04 - 17s/epoch - 3ms/step
Epoch 149/300
5969/5969 - 17s - loss: 2.8984e-04 - val_loss: 2.8524e-04 - 17s/epoch - 3ms/step
Epoch 150/300
5969/5969 - 17s - loss: 2.9163e-04 - val_loss: 3.0544e-04 - 17s/epoch - 3ms/step
Epoch 151/300
5969/5969 - 17s - loss: 2.8723e-04 - val_loss: 2.8712e-04 - 17s/epoch - 3ms/step
Epoch 152/300
5969/5969 - 17s - loss: 2.8647e-04 - val_loss: 2.7676e-04 - 17s/epoch - 3ms/step
Epoch 153/300
5969/5969 - 17s - loss: 2.9568e-04 - val_loss: 2.7512e-04 - 17s/epoch - 3ms/step
Epoch 154/300
5969/5969 - 17s - loss: 2.8829e-04 - val_loss: 3.0906e-04 - 17s/epoch - 3ms/step
Epoch 155/300
5969/5969 - 17s - loss: 2.9206e-04 - val_loss: 2.6924e-04 - 17s/epoch - 3ms/step
Epoch 156/300
5969/5969 - 17s - loss: 2.8718e-04 - val_loss: 2.6630e-04 - 17s/epoch - 3ms/step
Epoch 157/300
5969/5969 - 17s - loss: 2.8708e-04 - val_loss: 2.7695e-04 - 17s/epoch - 3ms/step
Epoch 158/300
5969/5969 - 17s - loss: 2.8259e-04 - val_loss: 2.8068e-04 - 17s/epoch - 3ms/step
Epoch 159/300
5969/5969 - 17s - loss: 2.8534e-04 - val_loss: 2.5818e-04 - 17s/epoch - 3ms/step
Epoch 160/300
5969/5969 - 17s - loss: 2.8910e-04 - val_loss: 2.7007e-04 - 17s/epoch - 3ms/step
Epoch 161/300
5969/5969 - 17s - loss: 2.8672e-04 - val_loss: 3.0508e-04 - 17s/epoch - 3ms/step
Epoch 162/300
5969/5969 - 17s - loss: 2.7942e-04 - val_loss: 2.9848e-04 - 17s/epoch - 3ms/step
Epoch 163/300
5969/5969 - 17s - loss: 2.9202e-04 - val_loss: 2.7733e-04 - 17s/epoch - 3ms/step
Epoch 164/300
5969/5969 - 17s - loss: 2.8828e-04 - val_loss: 2.8449e-04 - 17s/epoch - 3ms/step
Epoch 165/300
5969/5969 - 17s - loss: 2.8154e-04 - val_loss: 2.7881e-04 - 17s/epoch - 3ms/step
Epoch 166/300
5969/5969 - 17s - loss: 2.7878e-04 - val_loss: 3.0179e-04 - 17s/epoch - 3ms/step
Epoch 167/300
5969/5969 - 17s - loss: 2.7839e-04 - val_loss: 2.9789e-04 - 17s/epoch - 3ms/step
Epoch 168/300
5969/5969 - 17s - loss: 2.8645e-04 - val_loss: 2.6719e-04 - 17s/epoch - 3ms/step
Epoch 169/300
5969/5969 - 17s - loss: 2.8452e-04 - val_loss: 2.6916e-04 - 17s/epoch - 3ms/step
Epoch 170/300
5969/5969 - 17s - loss: 2.8642e-04 - val_loss: 2.5676e-04 - 17s/epoch - 3ms/step
Epoch 171/300
5969/5969 - 17s - loss: 2.7874e-04 - val_loss: 3.1186e-04 - 17s/epoch - 3ms/step
Epoch 172/300
5969/5969 - 17s - loss: 2.7990e-04 - val_loss: 2.8786e-04 - 17s/epoch - 3ms/step
Epoch 173/300
5969/5969 - 17s - loss: 2.7653e-04 - val_loss: 2.7616e-04 - 17s/epoch - 3ms/step
Epoch 174/300
5969/5969 - 17s - loss: 2.7518e-04 - val_loss: 2.6180e-04 - 17s/epoch - 3ms/step
Epoch 175/300
5969/5969 - 17s - loss: 2.7698e-04 - val_loss: 2.7576e-04 - 17s/epoch - 3ms/step
Epoch 176/300
5969/5969 - 17s - loss: 2.8204e-04 - val_loss: 2.7552e-04 - 17s/epoch - 3ms/step
Epoch 177/300
5969/5969 - 17s - loss: 2.8377e-04 - val_loss: 2.6260e-04 - 17s/epoch - 3ms/step
Epoch 178/300
5969/5969 - 17s - loss: 2.8491e-04 - val_loss: 2.6710e-04 - 17s/epoch - 3ms/step
Epoch 179/300
5969/5969 - 17s - loss: 2.7818e-04 - val_loss: 2.5827e-04 - 17s/epoch - 3ms/step
Epoch 180/300
5969/5969 - 17s - loss: 2.7476e-04 - val_loss: 2.7502e-04 - 17s/epoch - 3ms/step
Epoch 181/300
5969/5969 - 17s - loss: 2.7510e-04 - val_loss: 2.8227e-04 - 17s/epoch - 3ms/step
Epoch 182/300
5969/5969 - 17s - loss: 2.7215e-04 - val_loss: 2.9258e-04 - 17s/epoch - 3ms/step
Epoch 183/300
5969/5969 - 17s - loss: 2.7353e-04 - val_loss: 2.5784e-04 - 17s/epoch - 3ms/step
Epoch 184/300
5969/5969 - 17s - loss: 2.7264e-04 - val_loss: 2.7004e-04 - 17s/epoch - 3ms/step
Epoch 185/300
5969/5969 - 17s - loss: 2.7182e-04 - val_loss: 2.7691e-04 - 17s/epoch - 3ms/step
Epoch 186/300
5969/5969 - 17s - loss: 2.6958e-04 - val_loss: 2.6546e-04 - 17s/epoch - 3ms/step
Epoch 187/300
5969/5969 - 17s - loss: 2.7422e-04 - val_loss: 3.0948e-04 - 17s/epoch - 3ms/step
Epoch 188/300
5969/5969 - 17s - loss: 2.7255e-04 - val_loss: 2.8197e-04 - 17s/epoch - 3ms/step
Epoch 189/300
5969/5969 - 17s - loss: 2.6991e-04 - val_loss: 2.5931e-04 - 17s/epoch - 3ms/step
Epoch 190/300
5969/5969 - 17s - loss: 2.7350e-04 - val_loss: 2.8582e-04 - 17s/epoch - 3ms/step
Epoch 191/300
5969/5969 - 17s - loss: 2.7229e-04 - val_loss: 2.8869e-04 - 17s/epoch - 3ms/step
Epoch 192/300
5969/5969 - 17s - loss: 2.7151e-04 - val_loss: 2.7293e-04 - 17s/epoch - 3ms/step
Epoch 193/300
5969/5969 - 17s - loss: 2.7126e-04 - val_loss: 2.8745e-04 - 17s/epoch - 3ms/step
Epoch 194/300
5969/5969 - 17s - loss: 2.7031e-04 - val_loss: 2.9929e-04 - 17s/epoch - 3ms/step
Epoch 195/300
5969/5969 - 17s - loss: 2.6646e-04 - val_loss: 2.8264e-04 - 17s/epoch - 3ms/step
Epoch 196/300
5969/5969 - 17s - loss: 2.7456e-04 - val_loss: 2.5791e-04 - 17s/epoch - 3ms/step
Epoch 197/300
5969/5969 - 17s - loss: 2.7391e-04 - val_loss: 2.6432e-04 - 17s/epoch - 3ms/step
Epoch 198/300
5969/5969 - 17s - loss: 2.7045e-04 - val_loss: 2.7508e-04 - 17s/epoch - 3ms/step
Epoch 199/300
5969/5969 - 17s - loss: 2.7398e-04 - val_loss: 2.8532e-04 - 17s/epoch - 3ms/step
Epoch 200/300
5969/5969 - 17s - loss: 2.6832e-04 - val_loss: 2.8642e-04 - 17s/epoch - 3ms/step
Epoch 201/300
5969/5969 - 17s - loss: 2.6996e-04 - val_loss: 3.0775e-04 - 17s/epoch - 3ms/step
Epoch 202/300
5969/5969 - 17s - loss: 2.7224e-04 - val_loss: 2.6810e-04 - 17s/epoch - 3ms/step
Epoch 203/300
5969/5969 - 17s - loss: 2.7015e-04 - val_loss: 2.6020e-04 - 17s/epoch - 3ms/step
Epoch 204/300
5969/5969 - 17s - loss: 2.6563e-04 - val_loss: 2.8175e-04 - 17s/epoch - 3ms/step
Epoch 205/300
5969/5969 - 17s - loss: 2.7562e-04 - val_loss: 2.4265e-04 - 17s/epoch - 3ms/step
Epoch 206/300
5969/5969 - 17s - loss: 2.7081e-04 - val_loss: 2.6825e-04 - 17s/epoch - 3ms/step
Epoch 207/300
5969/5969 - 17s - loss: 2.6612e-04 - val_loss: 3.0992e-04 - 17s/epoch - 3ms/step
Epoch 208/300
5969/5969 - 17s - loss: 2.6460e-04 - val_loss: 2.4629e-04 - 17s/epoch - 3ms/step
Epoch 209/300
5969/5969 - 17s - loss: 2.6690e-04 - val_loss: 2.5962e-04 - 17s/epoch - 3ms/step
Epoch 210/300
5969/5969 - 17s - loss: 2.6933e-04 - val_loss: 2.5671e-04 - 17s/epoch - 3ms/step
Epoch 211/300
5969/5969 - 17s - loss: 2.7238e-04 - val_loss: 2.6709e-04 - 17s/epoch - 3ms/step
Epoch 212/300
5969/5969 - 17s - loss: 2.6643e-04 - val_loss: 2.5160e-04 - 17s/epoch - 3ms/step
Epoch 213/300
5969/5969 - 17s - loss: 2.6465e-04 - val_loss: 2.7371e-04 - 17s/epoch - 3ms/step
Epoch 214/300
5969/5969 - 17s - loss: 2.6452e-04 - val_loss: 2.8555e-04 - 17s/epoch - 3ms/step
Epoch 215/300
5969/5969 - 17s - loss: 2.6968e-04 - val_loss: 2.4863e-04 - 17s/epoch - 3ms/step
Epoch 216/300
5969/5969 - 17s - loss: 2.6934e-04 - val_loss: 2.7791e-04 - 17s/epoch - 3ms/step
Epoch 217/300
5969/5969 - 17s - loss: 2.6412e-04 - val_loss: 2.6918e-04 - 17s/epoch - 3ms/step
Epoch 218/300
5969/5969 - 17s - loss: 2.6227e-04 - val_loss: 2.8408e-04 - 17s/epoch - 3ms/step
Epoch 219/300
5969/5969 - 17s - loss: 2.6830e-04 - val_loss: 2.7837e-04 - 17s/epoch - 3ms/step
Epoch 220/300
5969/5969 - 17s - loss: 2.6314e-04 - val_loss: 2.9148e-04 - 17s/epoch - 3ms/step
Epoch 221/300
5969/5969 - 17s - loss: 2.6078e-04 - val_loss: 2.6558e-04 - 17s/epoch - 3ms/step
Epoch 222/300
5969/5969 - 17s - loss: 2.6233e-04 - val_loss: 2.5964e-04 - 17s/epoch - 3ms/step
Epoch 223/300
5969/5969 - 17s - loss: 2.6583e-04 - val_loss: 2.5636e-04 - 17s/epoch - 3ms/step
Epoch 224/300
5969/5969 - 17s - loss: 2.6171e-04 - val_loss: 2.7697e-04 - 17s/epoch - 3ms/step
Epoch 225/300
5969/5969 - 17s - loss: 2.7495e-04 - val_loss: 3.0460e-04 - 17s/epoch - 3ms/step
Epoch 226/300
5969/5969 - 17s - loss: 2.6949e-04 - val_loss: 3.0476e-04 - 17s/epoch - 3ms/step
Epoch 227/300
5969/5969 - 17s - loss: 2.6234e-04 - val_loss: 2.7246e-04 - 17s/epoch - 3ms/step
Epoch 228/300
5969/5969 - 17s - loss: 2.6167e-04 - val_loss: 2.5034e-04 - 17s/epoch - 3ms/step
Epoch 229/300
5969/5969 - 17s - loss: 2.6839e-04 - val_loss: 2.5654e-04 - 17s/epoch - 3ms/step
Epoch 230/300
5969/5969 - 17s - loss: 2.6287e-04 - val_loss: 2.7808e-04 - 17s/epoch - 3ms/step
Epoch 231/300
5969/5969 - 17s - loss: 2.6352e-04 - val_loss: 2.7434e-04 - 17s/epoch - 3ms/step
Epoch 232/300
5969/5969 - 17s - loss: 2.6167e-04 - val_loss: 2.6129e-04 - 17s/epoch - 3ms/step
Epoch 233/300
5969/5969 - 17s - loss: 2.6230e-04 - val_loss: 3.1552e-04 - 17s/epoch - 3ms/step
Epoch 234/300
5969/5969 - 17s - loss: 2.6728e-04 - val_loss: 2.8045e-04 - 17s/epoch - 3ms/step
Epoch 235/300
5969/5969 - 17s - loss: 2.6116e-04 - val_loss: 2.7204e-04 - 17s/epoch - 3ms/step
Epoch 236/300
5969/5969 - 17s - loss: 2.6408e-04 - val_loss: 3.0994e-04 - 17s/epoch - 3ms/step
Epoch 237/300
5969/5969 - 17s - loss: 2.6337e-04 - val_loss: 2.7699e-04 - 17s/epoch - 3ms/step
Epoch 238/300
5969/5969 - 17s - loss: 2.5758e-04 - val_loss: 2.6554e-04 - 17s/epoch - 3ms/step
Epoch 239/300
5969/5969 - 17s - loss: 2.5800e-04 - val_loss: 2.9318e-04 - 17s/epoch - 3ms/step
Epoch 240/300
5969/5969 - 17s - loss: 2.6383e-04 - val_loss: 2.6348e-04 - 17s/epoch - 3ms/step
Epoch 241/300
5969/5969 - 17s - loss: 2.5752e-04 - val_loss: 2.6236e-04 - 17s/epoch - 3ms/step
Epoch 242/300
5969/5969 - 17s - loss: 2.6791e-04 - val_loss: 2.5807e-04 - 17s/epoch - 3ms/step
Epoch 243/300
5969/5969 - 17s - loss: 2.6767e-04 - val_loss: 2.9014e-04 - 17s/epoch - 3ms/step
Epoch 244/300
5969/5969 - 17s - loss: 2.6459e-04 - val_loss: 2.6841e-04 - 17s/epoch - 3ms/step
Epoch 245/300
5969/5969 - 17s - loss: 2.6350e-04 - val_loss: 2.9147e-04 - 17s/epoch - 3ms/step
Epoch 246/300
5969/5969 - 17s - loss: 2.6389e-04 - val_loss: 2.5434e-04 - 17s/epoch - 3ms/step
Epoch 247/300
5969/5969 - 17s - loss: 2.5939e-04 - val_loss: 2.3818e-04 - 17s/epoch - 3ms/step
Epoch 248/300
5969/5969 - 17s - loss: 2.5972e-04 - val_loss: 2.9195e-04 - 17s/epoch - 3ms/step
Epoch 249/300
5969/5969 - 17s - loss: 2.6172e-04 - val_loss: 2.7569e-04 - 17s/epoch - 3ms/step
Epoch 250/300
5969/5969 - 17s - loss: 2.5777e-04 - val_loss: 2.7648e-04 - 17s/epoch - 3ms/step
Epoch 251/300
5969/5969 - 17s - loss: 2.5714e-04 - val_loss: 2.7489e-04 - 17s/epoch - 3ms/step
Epoch 252/300
5969/5969 - 17s - loss: 2.6515e-04 - val_loss: 3.5980e-04 - 17s/epoch - 3ms/step
Epoch 253/300
5969/5969 - 17s - loss: 2.5755e-04 - val_loss: 3.8008e-04 - 17s/epoch - 3ms/step
Epoch 254/300
5969/5969 - 17s - loss: 2.6297e-04 - val_loss: 3.4989e-04 - 17s/epoch - 3ms/step
Epoch 255/300
5969/5969 - 17s - loss: 2.5981e-04 - val_loss: 2.5689e-04 - 17s/epoch - 3ms/step
Epoch 256/300
5969/5969 - 17s - loss: 2.6974e-04 - val_loss: 2.5529e-04 - 17s/epoch - 3ms/step
Epoch 257/300
5969/5969 - 17s - loss: 2.5953e-04 - val_loss: 2.8339e-04 - 17s/epoch - 3ms/step
Epoch 258/300
5969/5969 - 17s - loss: 2.5480e-04 - val_loss: 2.6104e-04 - 17s/epoch - 3ms/step
Epoch 259/300
5969/5969 - 17s - loss: 2.5468e-04 - val_loss: 2.8238e-04 - 17s/epoch - 3ms/step
Epoch 260/300
5969/5969 - 17s - loss: 2.5983e-04 - val_loss: 2.5612e-04 - 17s/epoch - 3ms/step
Epoch 261/300
5969/5969 - 17s - loss: 2.5551e-04 - val_loss: 3.6483e-04 - 17s/epoch - 3ms/step
Epoch 262/300
5969/5969 - 17s - loss: 2.5570e-04 - val_loss: 3.0538e-04 - 17s/epoch - 3ms/step
Epoch 263/300
5969/5969 - 17s - loss: 2.5613e-04 - val_loss: 2.7256e-04 - 17s/epoch - 3ms/step
Epoch 264/300
5969/5969 - 17s - loss: 2.5684e-04 - val_loss: 2.7510e-04 - 17s/epoch - 3ms/step
Epoch 265/300
5969/5969 - 17s - loss: 2.6015e-04 - val_loss: 2.6935e-04 - 17s/epoch - 3ms/step
Epoch 266/300
5969/5969 - 17s - loss: 2.5966e-04 - val_loss: 2.5305e-04 - 17s/epoch - 3ms/step
Epoch 267/300
5969/5969 - 17s - loss: 2.5260e-04 - val_loss: 2.5423e-04 - 17s/epoch - 3ms/step
Epoch 268/300
5969/5969 - 17s - loss: 2.5108e-04 - val_loss: 2.7020e-04 - 17s/epoch - 3ms/step
Epoch 269/300
5969/5969 - 17s - loss: 2.5282e-04 - val_loss: 2.8633e-04 - 17s/epoch - 3ms/step
Epoch 270/300
5969/5969 - 17s - loss: 2.5582e-04 - val_loss: 2.6977e-04 - 17s/epoch - 3ms/step
Epoch 271/300
5969/5969 - 17s - loss: 2.5428e-04 - val_loss: 3.1480e-04 - 17s/epoch - 3ms/step
Epoch 272/300
5969/5969 - 17s - loss: 2.5791e-04 - val_loss: 2.6183e-04 - 17s/epoch - 3ms/step
Epoch 273/300
5969/5969 - 17s - loss: 2.5421e-04 - val_loss: 3.0882e-04 - 17s/epoch - 3ms/step
Epoch 274/300
5969/5969 - 17s - loss: 2.5391e-04 - val_loss: 2.8449e-04 - 17s/epoch - 3ms/step
Epoch 275/300
5969/5969 - 17s - loss: 2.5503e-04 - val_loss: 2.9528e-04 - 17s/epoch - 3ms/step
Epoch 276/300
5969/5969 - 17s - loss: 2.4982e-04 - val_loss: 2.5296e-04 - 17s/epoch - 3ms/step
Epoch 277/300
5969/5969 - 17s - loss: 2.5654e-04 - val_loss: 2.4677e-04 - 17s/epoch - 3ms/step
Epoch 278/300
5969/5969 - 17s - loss: 2.6001e-04 - val_loss: 2.6685e-04 - 17s/epoch - 3ms/step
Epoch 279/300
5969/5969 - 17s - loss: 2.5535e-04 - val_loss: 3.0385e-04 - 17s/epoch - 3ms/step
Epoch 280/300
5969/5969 - 17s - loss: 2.5158e-04 - val_loss: 2.7652e-04 - 17s/epoch - 3ms/step
Epoch 281/300
5969/5969 - 17s - loss: 2.6008e-04 - val_loss: 2.7672e-04 - 17s/epoch - 3ms/step
Epoch 282/300
5969/5969 - 17s - loss: 2.5397e-04 - val_loss: 2.8180e-04 - 17s/epoch - 3ms/step
Epoch 283/300
5969/5969 - 17s - loss: 2.5487e-04 - val_loss: 2.8708e-04 - 17s/epoch - 3ms/step
Epoch 284/300
5969/5969 - 17s - loss: 2.5561e-04 - val_loss: 2.5919e-04 - 17s/epoch - 3ms/step
Epoch 285/300
5969/5969 - 17s - loss: 2.5385e-04 - val_loss: 3.1462e-04 - 17s/epoch - 3ms/step
Epoch 286/300
5969/5969 - 17s - loss: 2.5668e-04 - val_loss: 4.6829e-04 - 17s/epoch - 3ms/step
Epoch 287/300
5969/5969 - 17s - loss: 2.5034e-04 - val_loss: 4.3225e-04 - 17s/epoch - 3ms/step
Epoch 288/300
5969/5969 - 17s - loss: 2.5652e-04 - val_loss: 2.7078e-04 - 17s/epoch - 3ms/step
Epoch 289/300
5969/5969 - 17s - loss: 2.5011e-04 - val_loss: 2.4151e-04 - 17s/epoch - 3ms/step
Epoch 290/300
5969/5969 - 17s - loss: 2.5191e-04 - val_loss: 3.0641e-04 - 17s/epoch - 3ms/step
Epoch 291/300
5969/5969 - 17s - loss: 2.5011e-04 - val_loss: 3.1484e-04 - 17s/epoch - 3ms/step
Epoch 292/300
5969/5969 - 17s - loss: 2.5774e-04 - val_loss: 2.5633e-04 - 17s/epoch - 3ms/step
Epoch 293/300
5969/5969 - 17s - loss: 2.5461e-04 - val_loss: 2.9808e-04 - 17s/epoch - 3ms/step
Epoch 294/300
5969/5969 - 17s - loss: 2.5918e-04 - val_loss: 2.6902e-04 - 17s/epoch - 3ms/step
Epoch 295/300
5969/5969 - 17s - loss: 2.4948e-04 - val_loss: 2.7191e-04 - 17s/epoch - 3ms/step
Epoch 296/300
5969/5969 - 17s - loss: 2.4994e-04 - val_loss: 2.8230e-04 - 17s/epoch - 3ms/step
Epoch 297/300
5969/5969 - 17s - loss: 2.4951e-04 - val_loss: 2.8970e-04 - 17s/epoch - 3ms/step
Epoch 298/300
5969/5969 - 17s - loss: 2.5482e-04 - val_loss: 2.8089e-04 - 17s/epoch - 3ms/step
Epoch 299/300
5969/5969 - 17s - loss: 2.5348e-04 - val_loss: 3.1890e-04 - 17s/epoch - 3ms/step
Epoch 300/300
5969/5969 - 17s - loss: 2.5248e-04 - val_loss: 2.5278e-04 - 17s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0002527761389501393
  1/332 [..............................] - ETA: 34s 48/332 [===>..........................] - ETA: 0s  96/332 [=======>......................] - ETA: 0s145/332 [============>.................] - ETA: 0s194/332 [================>.............] - ETA: 0s240/332 [====================>.........] - ETA: 0s289/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0027787464600978955
cosine 0.0022325866792141783
MAE: 0.0074958084
RMSE: 0.01589892
r2: 0.983602617484615
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_18 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_20 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2528)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_19"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_20 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_18 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_20"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_21 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_20 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2528)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 16, 300, 0.0005, 0.5, 632, 0.0002524780866224319, 0.0002527761389501393, 0.0027787464600978955, 0.0022325866792141783, 0.00749580841511488, 0.015898920595645905, 0.983602617484615, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_21 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_21 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_22 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 632)               0         
                                                                 
 dense_22 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_23 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2528)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/300
5969/5969 - 18s - loss: 0.0075 - val_loss: 0.0024 - 18s/epoch - 3ms/step
Epoch 2/300
5969/5969 - 17s - loss: 0.0027 - val_loss: 0.0018 - 17s/epoch - 3ms/step
Epoch 3/300
5969/5969 - 17s - loss: 0.0017 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 4/300
5969/5969 - 17s - loss: 0.0014 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 5/300
5969/5969 - 17s - loss: 0.0012 - val_loss: 9.8091e-04 - 17s/epoch - 3ms/step
Epoch 6/300
5969/5969 - 17s - loss: 0.0011 - val_loss: 8.8908e-04 - 17s/epoch - 3ms/step
Epoch 7/300
5969/5969 - 17s - loss: 0.0010 - val_loss: 8.1970e-04 - 17s/epoch - 3ms/step
Epoch 8/300
5969/5969 - 17s - loss: 9.3714e-04 - val_loss: 7.4793e-04 - 17s/epoch - 3ms/step
Epoch 9/300
5969/5969 - 17s - loss: 9.0636e-04 - val_loss: 7.0081e-04 - 17s/epoch - 3ms/step
Epoch 10/300
5969/5969 - 17s - loss: 8.5897e-04 - val_loss: 7.0323e-04 - 17s/epoch - 3ms/step
Epoch 11/300
5969/5969 - 17s - loss: 8.1943e-04 - val_loss: 6.6928e-04 - 17s/epoch - 3ms/step
Epoch 12/300
5969/5969 - 17s - loss: 7.7816e-04 - val_loss: 6.4805e-04 - 17s/epoch - 3ms/step
Epoch 13/300
5969/5969 - 17s - loss: 7.5629e-04 - val_loss: 5.9016e-04 - 17s/epoch - 3ms/step
Epoch 14/300
5969/5969 - 17s - loss: 7.4210e-04 - val_loss: 5.7928e-04 - 17s/epoch - 3ms/step
Epoch 15/300
5969/5969 - 17s - loss: 7.1887e-04 - val_loss: 5.7830e-04 - 17s/epoch - 3ms/step
Epoch 16/300
5969/5969 - 17s - loss: 7.0350e-04 - val_loss: 5.8818e-04 - 17s/epoch - 3ms/step
Epoch 17/300
5969/5969 - 17s - loss: 6.9096e-04 - val_loss: 5.5515e-04 - 17s/epoch - 3ms/step
Epoch 18/300
5969/5969 - 17s - loss: 6.9094e-04 - val_loss: 5.6055e-04 - 17s/epoch - 3ms/step
Epoch 19/300
5969/5969 - 17s - loss: 6.5541e-04 - val_loss: 5.5788e-04 - 17s/epoch - 3ms/step
Epoch 20/300
5969/5969 - 17s - loss: 6.4490e-04 - val_loss: 5.2348e-04 - 17s/epoch - 3ms/step
Epoch 21/300
5969/5969 - 17s - loss: 6.3327e-04 - val_loss: 4.8192e-04 - 17s/epoch - 3ms/step
Epoch 22/300
5969/5969 - 17s - loss: 6.2168e-04 - val_loss: 5.0523e-04 - 17s/epoch - 3ms/step
Epoch 23/300
5969/5969 - 17s - loss: 6.0594e-04 - val_loss: 4.8841e-04 - 17s/epoch - 3ms/step
Epoch 24/300
5969/5969 - 17s - loss: 5.9989e-04 - val_loss: 4.8406e-04 - 17s/epoch - 3ms/step
Epoch 25/300
5969/5969 - 17s - loss: 6.2174e-04 - val_loss: 4.7289e-04 - 17s/epoch - 3ms/step
Epoch 26/300
5969/5969 - 17s - loss: 5.8905e-04 - val_loss: 4.5711e-04 - 17s/epoch - 3ms/step
Epoch 27/300
5969/5969 - 17s - loss: 5.7219e-04 - val_loss: 4.5024e-04 - 17s/epoch - 3ms/step
Epoch 28/300
5969/5969 - 17s - loss: 5.7711e-04 - val_loss: 4.7290e-04 - 17s/epoch - 3ms/step
Epoch 29/300
5969/5969 - 17s - loss: 5.9020e-04 - val_loss: 4.8514e-04 - 17s/epoch - 3ms/step
Epoch 30/300
5969/5969 - 17s - loss: 5.5988e-04 - val_loss: 4.4253e-04 - 17s/epoch - 3ms/step
Epoch 31/300
5969/5969 - 17s - loss: 5.5865e-04 - val_loss: 4.2831e-04 - 17s/epoch - 3ms/step
Epoch 32/300
5969/5969 - 17s - loss: 5.4320e-04 - val_loss: 4.5249e-04 - 17s/epoch - 3ms/step
Epoch 33/300
5969/5969 - 17s - loss: 5.4623e-04 - val_loss: 4.3388e-04 - 17s/epoch - 3ms/step
Epoch 34/300
5969/5969 - 17s - loss: 5.3560e-04 - val_loss: 4.4725e-04 - 17s/epoch - 3ms/step
Epoch 35/300
5969/5969 - 17s - loss: 5.3380e-04 - val_loss: 5.4844e-04 - 17s/epoch - 3ms/step
Epoch 36/300
5969/5969 - 17s - loss: 5.2815e-04 - val_loss: 4.1802e-04 - 17s/epoch - 3ms/step
Epoch 37/300
5969/5969 - 17s - loss: 5.2643e-04 - val_loss: 4.3667e-04 - 17s/epoch - 3ms/step
Epoch 38/300
5969/5969 - 17s - loss: 5.2852e-04 - val_loss: 4.2650e-04 - 17s/epoch - 3ms/step
Epoch 39/300
5969/5969 - 17s - loss: 5.1543e-04 - val_loss: 4.0857e-04 - 17s/epoch - 3ms/step
Epoch 40/300
5969/5969 - 17s - loss: 5.0895e-04 - val_loss: 4.2271e-04 - 17s/epoch - 3ms/step
Epoch 41/300
5969/5969 - 17s - loss: 5.0797e-04 - val_loss: 4.1482e-04 - 17s/epoch - 3ms/step
Epoch 42/300
5969/5969 - 17s - loss: 5.0754e-04 - val_loss: 3.9494e-04 - 17s/epoch - 3ms/step
Epoch 43/300
5969/5969 - 17s - loss: 5.0040e-04 - val_loss: 4.0200e-04 - 17s/epoch - 3ms/step
Epoch 44/300
5969/5969 - 17s - loss: 4.9630e-04 - val_loss: 4.0510e-04 - 17s/epoch - 3ms/step
Epoch 45/300
5969/5969 - 17s - loss: 4.9016e-04 - val_loss: 3.8619e-04 - 17s/epoch - 3ms/step
Epoch 46/300
5969/5969 - 17s - loss: 4.8333e-04 - val_loss: 3.8111e-04 - 17s/epoch - 3ms/step
Epoch 47/300
5969/5969 - 17s - loss: 4.8188e-04 - val_loss: 3.7649e-04 - 17s/epoch - 3ms/step
Epoch 48/300
5969/5969 - 17s - loss: 4.9460e-04 - val_loss: 3.8978e-04 - 17s/epoch - 3ms/step
Epoch 49/300
5969/5969 - 17s - loss: 4.7927e-04 - val_loss: 3.9130e-04 - 17s/epoch - 3ms/step
Epoch 50/300
5969/5969 - 17s - loss: 4.7359e-04 - val_loss: 3.7480e-04 - 17s/epoch - 3ms/step
Epoch 51/300
5969/5969 - 17s - loss: 4.7311e-04 - val_loss: 3.7557e-04 - 17s/epoch - 3ms/step
Epoch 52/300
5969/5969 - 17s - loss: 4.6442e-04 - val_loss: 3.7320e-04 - 17s/epoch - 3ms/step
Epoch 53/300
5969/5969 - 17s - loss: 4.6706e-04 - val_loss: 3.7613e-04 - 17s/epoch - 3ms/step
Epoch 54/300
5969/5969 - 17s - loss: 4.6154e-04 - val_loss: 3.8652e-04 - 17s/epoch - 3ms/step
Epoch 55/300
5969/5969 - 17s - loss: 4.6458e-04 - val_loss: 3.5593e-04 - 17s/epoch - 3ms/step
Epoch 56/300
5969/5969 - 17s - loss: 4.6354e-04 - val_loss: 3.6765e-04 - 17s/epoch - 3ms/step
Epoch 57/300
5969/5969 - 17s - loss: 4.6215e-04 - val_loss: 3.6683e-04 - 17s/epoch - 3ms/step
Epoch 58/300
5969/5969 - 17s - loss: 4.5324e-04 - val_loss: 3.6418e-04 - 17s/epoch - 3ms/step
Epoch 59/300
5969/5969 - 17s - loss: 4.4846e-04 - val_loss: 3.6668e-04 - 17s/epoch - 3ms/step
Epoch 60/300
5969/5969 - 17s - loss: 4.5883e-04 - val_loss: 3.6148e-04 - 17s/epoch - 3ms/step
Epoch 61/300
5969/5969 - 17s - loss: 4.5149e-04 - val_loss: 3.6978e-04 - 17s/epoch - 3ms/step
Epoch 62/300
5969/5969 - 17s - loss: 4.4531e-04 - val_loss: 3.8258e-04 - 17s/epoch - 3ms/step
Epoch 63/300
5969/5969 - 17s - loss: 4.4778e-04 - val_loss: 3.9423e-04 - 17s/epoch - 3ms/step
Epoch 64/300
5969/5969 - 17s - loss: 4.4110e-04 - val_loss: 3.6997e-04 - 17s/epoch - 3ms/step
Epoch 65/300
5969/5969 - 17s - loss: 4.4198e-04 - val_loss: 3.8682e-04 - 17s/epoch - 3ms/step
Epoch 66/300
5969/5969 - 17s - loss: 4.3849e-04 - val_loss: 3.4425e-04 - 17s/epoch - 3ms/step
Epoch 67/300
5969/5969 - 17s - loss: 4.3741e-04 - val_loss: 4.0667e-04 - 17s/epoch - 3ms/step
Epoch 68/300
5969/5969 - 17s - loss: 4.3647e-04 - val_loss: 3.8318e-04 - 17s/epoch - 3ms/step
Epoch 69/300
5969/5969 - 17s - loss: 4.5078e-04 - val_loss: 3.7868e-04 - 17s/epoch - 3ms/step
Epoch 70/300
5969/5969 - 17s - loss: 4.3929e-04 - val_loss: 4.1112e-04 - 17s/epoch - 3ms/step
Epoch 71/300
5969/5969 - 17s - loss: 4.3468e-04 - val_loss: 3.5700e-04 - 17s/epoch - 3ms/step
Epoch 72/300
5969/5969 - 17s - loss: 4.3756e-04 - val_loss: 3.7443e-04 - 17s/epoch - 3ms/step
Epoch 73/300
5969/5969 - 17s - loss: 4.3227e-04 - val_loss: 3.5992e-04 - 17s/epoch - 3ms/step
Epoch 74/300
5969/5969 - 17s - loss: 4.2526e-04 - val_loss: 3.4117e-04 - 17s/epoch - 3ms/step
Epoch 75/300
5969/5969 - 17s - loss: 4.3213e-04 - val_loss: 3.8132e-04 - 17s/epoch - 3ms/step
Epoch 76/300
5969/5969 - 17s - loss: 4.1958e-04 - val_loss: 3.4143e-04 - 17s/epoch - 3ms/step
Epoch 77/300
5969/5969 - 17s - loss: 4.2468e-04 - val_loss: 3.8137e-04 - 17s/epoch - 3ms/step
Epoch 78/300
5969/5969 - 17s - loss: 4.2986e-04 - val_loss: 3.5232e-04 - 17s/epoch - 3ms/step
Epoch 79/300
5969/5969 - 17s - loss: 4.2557e-04 - val_loss: 3.4262e-04 - 17s/epoch - 3ms/step
Epoch 80/300
5969/5969 - 17s - loss: 4.1658e-04 - val_loss: 3.6812e-04 - 17s/epoch - 3ms/step
Epoch 81/300
5969/5969 - 17s - loss: 4.1635e-04 - val_loss: 3.5229e-04 - 17s/epoch - 3ms/step
Epoch 82/300
5969/5969 - 17s - loss: 4.4093e-04 - val_loss: 3.4019e-04 - 17s/epoch - 3ms/step
Epoch 83/300
5969/5969 - 17s - loss: 4.2373e-04 - val_loss: 3.6043e-04 - 17s/epoch - 3ms/step
Epoch 84/300
5969/5969 - 17s - loss: 4.1872e-04 - val_loss: 3.5610e-04 - 17s/epoch - 3ms/step
Epoch 85/300
5969/5969 - 17s - loss: 4.1876e-04 - val_loss: 3.6586e-04 - 17s/epoch - 3ms/step
Epoch 86/300
5969/5969 - 17s - loss: 4.1686e-04 - val_loss: 3.5157e-04 - 17s/epoch - 3ms/step
Epoch 87/300
5969/5969 - 17s - loss: 4.2351e-04 - val_loss: 3.7085e-04 - 17s/epoch - 3ms/step
Epoch 88/300
5969/5969 - 17s - loss: 4.1897e-04 - val_loss: 3.2738e-04 - 17s/epoch - 3ms/step
Epoch 89/300
5969/5969 - 17s - loss: 4.1720e-04 - val_loss: 3.4356e-04 - 17s/epoch - 3ms/step
Epoch 90/300
5969/5969 - 17s - loss: 4.0683e-04 - val_loss: 3.3341e-04 - 17s/epoch - 3ms/step
Epoch 91/300
5969/5969 - 17s - loss: 4.1414e-04 - val_loss: 3.6225e-04 - 17s/epoch - 3ms/step
Epoch 92/300
5969/5969 - 17s - loss: 4.0224e-04 - val_loss: 3.5973e-04 - 17s/epoch - 3ms/step
Epoch 93/300
5969/5969 - 17s - loss: 4.0562e-04 - val_loss: 3.4132e-04 - 17s/epoch - 3ms/step
Epoch 94/300
5969/5969 - 17s - loss: 4.0692e-04 - val_loss: 3.2506e-04 - 17s/epoch - 3ms/step
Epoch 95/300
5969/5969 - 17s - loss: 4.0991e-04 - val_loss: 3.3592e-04 - 17s/epoch - 3ms/step
Epoch 96/300
5969/5969 - 17s - loss: 4.1165e-04 - val_loss: 3.4507e-04 - 17s/epoch - 3ms/step
Epoch 97/300
5969/5969 - 17s - loss: 4.0198e-04 - val_loss: 3.2673e-04 - 17s/epoch - 3ms/step
Epoch 98/300
5969/5969 - 17s - loss: 4.0711e-04 - val_loss: 3.3500e-04 - 17s/epoch - 3ms/step
Epoch 99/300
5969/5969 - 17s - loss: 3.9938e-04 - val_loss: 3.3459e-04 - 17s/epoch - 3ms/step
Epoch 100/300
5969/5969 - 17s - loss: 3.9911e-04 - val_loss: 3.2314e-04 - 17s/epoch - 3ms/step
Epoch 101/300
5969/5969 - 17s - loss: 3.9525e-04 - val_loss: 3.2925e-04 - 17s/epoch - 3ms/step
Epoch 102/300
5969/5969 - 17s - loss: 3.9848e-04 - val_loss: 3.3963e-04 - 17s/epoch - 3ms/step
Epoch 103/300
5969/5969 - 17s - loss: 4.0171e-04 - val_loss: 3.3525e-04 - 17s/epoch - 3ms/step
Epoch 104/300
5969/5969 - 17s - loss: 4.0698e-04 - val_loss: 3.2865e-04 - 17s/epoch - 3ms/step
Epoch 105/300
5969/5969 - 17s - loss: 4.0171e-04 - val_loss: 3.5433e-04 - 17s/epoch - 3ms/step
Epoch 106/300
5969/5969 - 17s - loss: 3.9056e-04 - val_loss: 3.2557e-04 - 17s/epoch - 3ms/step
Epoch 107/300
5969/5969 - 17s - loss: 3.8871e-04 - val_loss: 3.5620e-04 - 17s/epoch - 3ms/step
Epoch 108/300
5969/5969 - 17s - loss: 3.9348e-04 - val_loss: 3.2993e-04 - 17s/epoch - 3ms/step
Epoch 109/300
5969/5969 - 17s - loss: 3.8994e-04 - val_loss: 3.2653e-04 - 17s/epoch - 3ms/step
Epoch 110/300
5969/5969 - 17s - loss: 3.9866e-04 - val_loss: 3.3574e-04 - 17s/epoch - 3ms/step
Epoch 111/300
5969/5969 - 17s - loss: 3.9005e-04 - val_loss: 3.3494e-04 - 17s/epoch - 3ms/step
Epoch 112/300
5969/5969 - 17s - loss: 3.8881e-04 - val_loss: 3.3969e-04 - 17s/epoch - 3ms/step
Epoch 113/300
5969/5969 - 17s - loss: 3.8786e-04 - val_loss: 3.4960e-04 - 17s/epoch - 3ms/step
Epoch 114/300
5969/5969 - 17s - loss: 3.8448e-04 - val_loss: 3.1556e-04 - 17s/epoch - 3ms/step
Epoch 115/300
5969/5969 - 17s - loss: 3.8518e-04 - val_loss: 3.2228e-04 - 17s/epoch - 3ms/step
Epoch 116/300
5969/5969 - 17s - loss: 3.8992e-04 - val_loss: 6.2946e-04 - 17s/epoch - 3ms/step
Epoch 117/300
5969/5969 - 17s - loss: 3.8757e-04 - val_loss: 3.1808e-04 - 17s/epoch - 3ms/step
Epoch 118/300
5969/5969 - 17s - loss: 3.9370e-04 - val_loss: 3.2733e-04 - 17s/epoch - 3ms/step
Epoch 119/300
5969/5969 - 17s - loss: 3.8241e-04 - val_loss: 3.2096e-04 - 17s/epoch - 3ms/step
Epoch 120/300
5969/5969 - 17s - loss: 3.7995e-04 - val_loss: 3.2316e-04 - 17s/epoch - 3ms/step
Epoch 121/300
5969/5969 - 17s - loss: 3.8411e-04 - val_loss: 3.3265e-04 - 17s/epoch - 3ms/step
Epoch 122/300
5969/5969 - 17s - loss: 3.8218e-04 - val_loss: 3.0914e-04 - 17s/epoch - 3ms/step
Epoch 123/300
5969/5969 - 17s - loss: 3.7968e-04 - val_loss: 3.2593e-04 - 17s/epoch - 3ms/step
Epoch 124/300
5969/5969 - 17s - loss: 3.9182e-04 - val_loss: 3.3039e-04 - 17s/epoch - 3ms/step
Epoch 125/300
5969/5969 - 17s - loss: 3.9542e-04 - val_loss: 3.1336e-04 - 17s/epoch - 3ms/step
Epoch 126/300
5969/5969 - 17s - loss: 3.8207e-04 - val_loss: 3.4013e-04 - 17s/epoch - 3ms/step
Epoch 127/300
5969/5969 - 17s - loss: 3.7981e-04 - val_loss: 3.2122e-04 - 17s/epoch - 3ms/step
Epoch 128/300
5969/5969 - 17s - loss: 3.8343e-04 - val_loss: 3.4782e-04 - 17s/epoch - 3ms/step
Epoch 129/300
5969/5969 - 17s - loss: 4.0623e-04 - val_loss: 3.8844e-04 - 17s/epoch - 3ms/step
Epoch 130/300
5969/5969 - 17s - loss: 3.9006e-04 - val_loss: 3.2785e-04 - 17s/epoch - 3ms/step
Epoch 131/300
5969/5969 - 17s - loss: 3.7255e-04 - val_loss: 3.3649e-04 - 17s/epoch - 3ms/step
Epoch 132/300
5969/5969 - 17s - loss: 3.8602e-04 - val_loss: 3.3531e-04 - 17s/epoch - 3ms/step
Epoch 133/300
5969/5969 - 17s - loss: 3.7232e-04 - val_loss: 3.0925e-04 - 17s/epoch - 3ms/step
Epoch 134/300
5969/5969 - 17s - loss: 3.7588e-04 - val_loss: 3.2661e-04 - 17s/epoch - 3ms/step
Epoch 135/300
5969/5969 - 17s - loss: 3.7011e-04 - val_loss: 3.1327e-04 - 17s/epoch - 3ms/step
Epoch 136/300
5969/5969 - 17s - loss: 3.7293e-04 - val_loss: 3.1320e-04 - 17s/epoch - 3ms/step
Epoch 137/300
5969/5969 - 17s - loss: 3.7530e-04 - val_loss: 3.0905e-04 - 17s/epoch - 3ms/step
Epoch 138/300
5969/5969 - 17s - loss: 3.7109e-04 - val_loss: 3.2504e-04 - 17s/epoch - 3ms/step
Epoch 139/300
5969/5969 - 17s - loss: 3.7278e-04 - val_loss: 3.2415e-04 - 17s/epoch - 3ms/step
Epoch 140/300
5969/5969 - 17s - loss: 3.6856e-04 - val_loss: 3.1850e-04 - 17s/epoch - 3ms/step
Epoch 141/300
5969/5969 - 17s - loss: 3.8067e-04 - val_loss: 3.1429e-04 - 17s/epoch - 3ms/step
Epoch 142/300
5969/5969 - 17s - loss: 3.7346e-04 - val_loss: 3.2367e-04 - 17s/epoch - 3ms/step
Epoch 143/300
5969/5969 - 17s - loss: 3.8882e-04 - val_loss: 3.2946e-04 - 17s/epoch - 3ms/step
Epoch 144/300
5969/5969 - 17s - loss: 3.7762e-04 - val_loss: 2.9252e-04 - 17s/epoch - 3ms/step
Epoch 145/300
5969/5969 - 17s - loss: 3.6924e-04 - val_loss: 3.0367e-04 - 17s/epoch - 3ms/step
Epoch 146/300
5969/5969 - 17s - loss: 3.7614e-04 - val_loss: 2.9869e-04 - 17s/epoch - 3ms/step
Epoch 147/300
5969/5969 - 17s - loss: 3.6756e-04 - val_loss: 3.2827e-04 - 17s/epoch - 3ms/step
Epoch 148/300
5969/5969 - 17s - loss: 3.8975e-04 - val_loss: 3.9956e-04 - 17s/epoch - 3ms/step
Epoch 149/300
5969/5969 - 17s - loss: 3.7906e-04 - val_loss: 3.3171e-04 - 17s/epoch - 3ms/step
Epoch 150/300
5969/5969 - 17s - loss: 3.7537e-04 - val_loss: 3.4143e-04 - 17s/epoch - 3ms/step
Epoch 151/300
5969/5969 - 17s - loss: 3.6706e-04 - val_loss: 3.3866e-04 - 17s/epoch - 3ms/step
Epoch 152/300
5969/5969 - 17s - loss: 3.8124e-04 - val_loss: 2.9997e-04 - 17s/epoch - 3ms/step
Epoch 153/300
5969/5969 - 17s - loss: 3.6990e-04 - val_loss: 3.2592e-04 - 17s/epoch - 3ms/step
Epoch 154/300
5969/5969 - 17s - loss: 3.6415e-04 - val_loss: 3.1921e-04 - 17s/epoch - 3ms/step
Epoch 155/300
5969/5969 - 17s - loss: 3.6188e-04 - val_loss: 3.0313e-04 - 17s/epoch - 3ms/step
Epoch 156/300
5969/5969 - 17s - loss: 3.7551e-04 - val_loss: 3.8293e-04 - 17s/epoch - 3ms/step
Epoch 157/300
5969/5969 - 17s - loss: 3.6086e-04 - val_loss: 3.1309e-04 - 17s/epoch - 3ms/step
Epoch 158/300
5969/5969 - 17s - loss: 3.6642e-04 - val_loss: 3.0441e-04 - 17s/epoch - 3ms/step
Epoch 159/300
5969/5969 - 17s - loss: 3.7663e-04 - val_loss: 3.6347e-04 - 17s/epoch - 3ms/step
Epoch 160/300
5969/5969 - 17s - loss: 3.6042e-04 - val_loss: 4.3918e-04 - 17s/epoch - 3ms/step
Epoch 161/300
5969/5969 - 17s - loss: 3.6655e-04 - val_loss: 3.1752e-04 - 17s/epoch - 3ms/step
Epoch 162/300
5969/5969 - 17s - loss: 3.6116e-04 - val_loss: 3.7843e-04 - 17s/epoch - 3ms/step
Epoch 163/300
5969/5969 - 17s - loss: 3.6024e-04 - val_loss: 3.3300e-04 - 17s/epoch - 3ms/step
Epoch 164/300
5969/5969 - 17s - loss: 3.7084e-04 - val_loss: 3.8810e-04 - 17s/epoch - 3ms/step
Epoch 165/300
5969/5969 - 17s - loss: 3.5874e-04 - val_loss: 3.5450e-04 - 17s/epoch - 3ms/step
Epoch 166/300
5969/5969 - 17s - loss: 3.6207e-04 - val_loss: 3.6674e-04 - 17s/epoch - 3ms/step
Epoch 167/300
5969/5969 - 17s - loss: 3.5840e-04 - val_loss: 3.1948e-04 - 17s/epoch - 3ms/step
Epoch 168/300
5969/5969 - 17s - loss: 3.8482e-04 - val_loss: 3.1810e-04 - 17s/epoch - 3ms/step
Epoch 169/300
5969/5969 - 17s - loss: 3.6345e-04 - val_loss: 6.9205e-04 - 17s/epoch - 3ms/step
Epoch 170/300
5969/5969 - 17s - loss: 3.5403e-04 - val_loss: 3.0167e-04 - 17s/epoch - 3ms/step
Epoch 171/300
5969/5969 - 17s - loss: 3.7128e-04 - val_loss: 3.0157e-04 - 17s/epoch - 3ms/step
Epoch 172/300
5969/5969 - 17s - loss: 3.5897e-04 - val_loss: 3.2000e-04 - 17s/epoch - 3ms/step
Epoch 173/300
5969/5969 - 17s - loss: 3.5715e-04 - val_loss: 3.1074e-04 - 17s/epoch - 3ms/step
Epoch 174/300
5969/5969 - 17s - loss: 3.5424e-04 - val_loss: 3.0848e-04 - 17s/epoch - 3ms/step
Epoch 175/300
5969/5969 - 17s - loss: 3.5414e-04 - val_loss: 3.1292e-04 - 17s/epoch - 3ms/step
Epoch 176/300
5969/5969 - 17s - loss: 3.5566e-04 - val_loss: 3.1975e-04 - 17s/epoch - 3ms/step
Epoch 177/300
5969/5969 - 17s - loss: 3.7201e-04 - val_loss: 3.4552e-04 - 17s/epoch - 3ms/step
Epoch 178/300
5969/5969 - 17s - loss: 3.6079e-04 - val_loss: 3.2605e-04 - 17s/epoch - 3ms/step
Epoch 179/300
5969/5969 - 17s - loss: 3.5269e-04 - val_loss: 3.2188e-04 - 17s/epoch - 3ms/step
Epoch 180/300
5969/5969 - 17s - loss: 3.5354e-04 - val_loss: 3.0933e-04 - 17s/epoch - 3ms/step
Epoch 181/300
5969/5969 - 17s - loss: 3.5740e-04 - val_loss: 3.3565e-04 - 17s/epoch - 3ms/step
Epoch 182/300
5969/5969 - 17s - loss: 3.5452e-04 - val_loss: 3.4144e-04 - 17s/epoch - 3ms/step
Epoch 183/300
5969/5969 - 17s - loss: 3.5503e-04 - val_loss: 3.1051e-04 - 17s/epoch - 3ms/step
Epoch 184/300
5969/5969 - 17s - loss: 3.7552e-04 - val_loss: 3.7527e-04 - 17s/epoch - 3ms/step
Epoch 185/300
5969/5969 - 17s - loss: 3.5055e-04 - val_loss: 3.2955e-04 - 17s/epoch - 3ms/step
Epoch 186/300
5969/5969 - 17s - loss: 3.5967e-04 - val_loss: 3.1200e-04 - 17s/epoch - 3ms/step
Epoch 187/300
5969/5969 - 17s - loss: 3.5201e-04 - val_loss: 3.7135e-04 - 17s/epoch - 3ms/step
Epoch 188/300
5969/5969 - 17s - loss: 3.4858e-04 - val_loss: 3.7695e-04 - 17s/epoch - 3ms/step
Epoch 189/300
5969/5969 - 17s - loss: 3.4841e-04 - val_loss: 3.1234e-04 - 17s/epoch - 3ms/step
Epoch 190/300
5969/5969 - 17s - loss: 3.4807e-04 - val_loss: 3.5277e-04 - 17s/epoch - 3ms/step
Epoch 191/300
5969/5969 - 17s - loss: 3.5040e-04 - val_loss: 3.3310e-04 - 17s/epoch - 3ms/step
Epoch 192/300
5969/5969 - 17s - loss: 3.4604e-04 - val_loss: 2.9592e-04 - 17s/epoch - 3ms/step
Epoch 193/300
5969/5969 - 17s - loss: 3.5314e-04 - val_loss: 3.6733e-04 - 17s/epoch - 3ms/step
Epoch 194/300
5969/5969 - 17s - loss: 3.4625e-04 - val_loss: 3.5314e-04 - 17s/epoch - 3ms/step
Epoch 195/300
5969/5969 - 17s - loss: 3.5015e-04 - val_loss: 3.5161e-04 - 17s/epoch - 3ms/step
Epoch 196/300
5969/5969 - 17s - loss: 3.5071e-04 - val_loss: 3.9233e-04 - 17s/epoch - 3ms/step
Epoch 197/300
5969/5969 - 17s - loss: 3.5258e-04 - val_loss: 3.6931e-04 - 17s/epoch - 3ms/step
Epoch 198/300
5969/5969 - 17s - loss: 3.4700e-04 - val_loss: 3.2299e-04 - 17s/epoch - 3ms/step
Epoch 199/300
5969/5969 - 17s - loss: 3.5681e-04 - val_loss: 3.4447e-04 - 17s/epoch - 3ms/step
Epoch 200/300
5969/5969 - 17s - loss: 3.4703e-04 - val_loss: 3.7060e-04 - 17s/epoch - 3ms/step
Epoch 201/300
5969/5969 - 17s - loss: 3.4141e-04 - val_loss: 3.5015e-04 - 17s/epoch - 3ms/step
Epoch 202/300
5969/5969 - 17s - loss: 3.4311e-04 - val_loss: 3.3773e-04 - 17s/epoch - 3ms/step
Epoch 203/300
5969/5969 - 17s - loss: 3.4393e-04 - val_loss: 3.0215e-04 - 17s/epoch - 3ms/step
Epoch 204/300
5969/5969 - 17s - loss: 3.4275e-04 - val_loss: 3.5708e-04 - 17s/epoch - 3ms/step
Epoch 205/300
5969/5969 - 17s - loss: 3.6303e-04 - val_loss: 3.8687e-04 - 17s/epoch - 3ms/step
Epoch 206/300
5969/5969 - 17s - loss: 3.6592e-04 - val_loss: 4.1952e-04 - 17s/epoch - 3ms/step
Epoch 207/300
5969/5969 - 17s - loss: 3.5535e-04 - val_loss: 3.7793e-04 - 17s/epoch - 3ms/step
Epoch 208/300
5969/5969 - 17s - loss: 3.5034e-04 - val_loss: 3.0987e-04 - 17s/epoch - 3ms/step
Epoch 209/300
5969/5969 - 17s - loss: 3.5072e-04 - val_loss: 3.4720e-04 - 17s/epoch - 3ms/step
Epoch 210/300
5969/5969 - 17s - loss: 3.4756e-04 - val_loss: 3.3021e-04 - 17s/epoch - 3ms/step
Epoch 211/300
5969/5969 - 17s - loss: 3.4332e-04 - val_loss: 3.2650e-04 - 17s/epoch - 3ms/step
Epoch 212/300
5969/5969 - 17s - loss: 3.6306e-04 - val_loss: 3.6087e-04 - 17s/epoch - 3ms/step
Epoch 213/300
5969/5969 - 17s - loss: 3.4571e-04 - val_loss: 3.7936e-04 - 17s/epoch - 3ms/step
Epoch 214/300
5969/5969 - 17s - loss: 3.4228e-04 - val_loss: 3.6375e-04 - 17s/epoch - 3ms/step
Epoch 215/300
5969/5969 - 17s - loss: 3.4987e-04 - val_loss: 3.0298e-04 - 17s/epoch - 3ms/step
Epoch 216/300
5969/5969 - 17s - loss: 3.4839e-04 - val_loss: 3.4282e-04 - 17s/epoch - 3ms/step
Epoch 217/300
5969/5969 - 17s - loss: 3.4872e-04 - val_loss: 3.5352e-04 - 17s/epoch - 3ms/step
Epoch 218/300
5969/5969 - 17s - loss: 3.4580e-04 - val_loss: 4.4294e-04 - 17s/epoch - 3ms/step
Epoch 219/300
5969/5969 - 17s - loss: 3.4307e-04 - val_loss: 3.7821e-04 - 17s/epoch - 3ms/step
Epoch 220/300
5969/5969 - 17s - loss: 3.3955e-04 - val_loss: 3.7895e-04 - 17s/epoch - 3ms/step
Epoch 221/300
5969/5969 - 17s - loss: 3.3907e-04 - val_loss: 3.3907e-04 - 17s/epoch - 3ms/step
Epoch 222/300
5969/5969 - 17s - loss: 3.4294e-04 - val_loss: 3.2435e-04 - 17s/epoch - 3ms/step
Epoch 223/300
5969/5969 - 17s - loss: 3.4304e-04 - val_loss: 3.2729e-04 - 17s/epoch - 3ms/step
Epoch 224/300
5969/5969 - 17s - loss: 3.3683e-04 - val_loss: 3.5499e-04 - 17s/epoch - 3ms/step
Epoch 225/300
5969/5969 - 17s - loss: 3.4507e-04 - val_loss: 4.2834e-04 - 17s/epoch - 3ms/step
Epoch 226/300
5969/5969 - 17s - loss: 3.4649e-04 - val_loss: 4.2395e-04 - 17s/epoch - 3ms/step
Epoch 227/300
5969/5969 - 17s - loss: 3.4747e-04 - val_loss: 3.7373e-04 - 17s/epoch - 3ms/step
Epoch 228/300
5969/5969 - 17s - loss: 3.3803e-04 - val_loss: 4.2427e-04 - 17s/epoch - 3ms/step
Epoch 229/300
5969/5969 - 17s - loss: 3.5187e-04 - val_loss: 4.2165e-04 - 17s/epoch - 3ms/step
Epoch 230/300
5969/5969 - 17s - loss: 3.3962e-04 - val_loss: 3.3989e-04 - 17s/epoch - 3ms/step
Epoch 231/300
5969/5969 - 17s - loss: 3.3798e-04 - val_loss: 3.6901e-04 - 17s/epoch - 3ms/step
Epoch 232/300
5969/5969 - 17s - loss: 3.3762e-04 - val_loss: 3.3747e-04 - 17s/epoch - 3ms/step
Epoch 233/300
5969/5969 - 17s - loss: 3.6153e-04 - val_loss: 3.8763e-04 - 17s/epoch - 3ms/step
Epoch 234/300
5969/5969 - 17s - loss: 3.4721e-04 - val_loss: 3.9956e-04 - 17s/epoch - 3ms/step
Epoch 235/300
5969/5969 - 17s - loss: 3.3707e-04 - val_loss: 3.7018e-04 - 17s/epoch - 3ms/step
Epoch 236/300
5969/5969 - 17s - loss: 3.3491e-04 - val_loss: 4.1115e-04 - 17s/epoch - 3ms/step
Epoch 237/300
5969/5969 - 17s - loss: 3.4387e-04 - val_loss: 4.3745e-04 - 17s/epoch - 3ms/step
Epoch 238/300
5969/5969 - 17s - loss: 3.3395e-04 - val_loss: 3.6655e-04 - 17s/epoch - 3ms/step
Epoch 239/300
5969/5969 - 17s - loss: 3.4316e-04 - val_loss: 4.2215e-04 - 17s/epoch - 3ms/step
Epoch 240/300
5969/5969 - 17s - loss: 3.3525e-04 - val_loss: 3.2155e-04 - 17s/epoch - 3ms/step
Epoch 241/300
5969/5969 - 17s - loss: 3.3786e-04 - val_loss: 4.4061e-04 - 17s/epoch - 3ms/step
Epoch 242/300
5969/5969 - 17s - loss: 3.3987e-04 - val_loss: 4.8303e-04 - 17s/epoch - 3ms/step
Epoch 243/300
5969/5969 - 17s - loss: 3.3875e-04 - val_loss: 4.8088e-04 - 17s/epoch - 3ms/step
Epoch 244/300
5969/5969 - 17s - loss: 3.3811e-04 - val_loss: 4.5726e-04 - 17s/epoch - 3ms/step
Epoch 245/300
5969/5969 - 17s - loss: 3.3432e-04 - val_loss: 6.6704e-04 - 17s/epoch - 3ms/step
Epoch 246/300
5969/5969 - 17s - loss: 3.4874e-04 - val_loss: 0.0017 - 17s/epoch - 3ms/step
Epoch 247/300
5969/5969 - 17s - loss: 3.4018e-04 - val_loss: 3.6959e-04 - 17s/epoch - 3ms/step
Epoch 248/300
5969/5969 - 17s - loss: 3.4642e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 249/300
5969/5969 - 17s - loss: 3.3802e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 250/300
5969/5969 - 17s - loss: 3.5260e-04 - val_loss: 4.2673e-04 - 17s/epoch - 3ms/step
Epoch 251/300
5969/5969 - 17s - loss: 3.3216e-04 - val_loss: 4.4805e-04 - 17s/epoch - 3ms/step
Epoch 252/300
5969/5969 - 17s - loss: 3.3802e-04 - val_loss: 8.0711e-04 - 17s/epoch - 3ms/step
Epoch 253/300
5969/5969 - 17s - loss: 3.3773e-04 - val_loss: 8.0681e-04 - 17s/epoch - 3ms/step
Epoch 254/300
5969/5969 - 17s - loss: 3.3517e-04 - val_loss: 5.2497e-04 - 17s/epoch - 3ms/step
Epoch 255/300
5969/5969 - 17s - loss: 3.5244e-04 - val_loss: 5.9793e-04 - 17s/epoch - 3ms/step
Epoch 256/300
5969/5969 - 17s - loss: 3.3271e-04 - val_loss: 5.1003e-04 - 17s/epoch - 3ms/step
Epoch 257/300
5969/5969 - 17s - loss: 3.3573e-04 - val_loss: 3.9363e-04 - 17s/epoch - 3ms/step
Epoch 258/300
5969/5969 - 17s - loss: 3.4794e-04 - val_loss: 4.8192e-04 - 17s/epoch - 3ms/step
Epoch 259/300
5969/5969 - 17s - loss: 3.3481e-04 - val_loss: 5.8524e-04 - 17s/epoch - 3ms/step
Epoch 260/300
5969/5969 - 17s - loss: 3.2992e-04 - val_loss: 4.8135e-04 - 17s/epoch - 3ms/step
Epoch 261/300
5969/5969 - 17s - loss: 3.2896e-04 - val_loss: 7.8008e-04 - 17s/epoch - 3ms/step
Epoch 262/300
5969/5969 - 17s - loss: 3.3894e-04 - val_loss: 7.1950e-04 - 17s/epoch - 3ms/step
Epoch 263/300
5969/5969 - 17s - loss: 3.3033e-04 - val_loss: 5.2961e-04 - 17s/epoch - 3ms/step
Epoch 264/300
5969/5969 - 17s - loss: 3.5666e-04 - val_loss: 6.7755e-04 - 17s/epoch - 3ms/step
Epoch 265/300
5969/5969 - 17s - loss: 3.4057e-04 - val_loss: 4.1502e-04 - 17s/epoch - 3ms/step
Epoch 266/300
5969/5969 - 17s - loss: 3.2924e-04 - val_loss: 4.9615e-04 - 17s/epoch - 3ms/step
Epoch 267/300
5969/5969 - 17s - loss: 3.3096e-04 - val_loss: 4.8611e-04 - 17s/epoch - 3ms/step
Epoch 268/300
5969/5969 - 17s - loss: 3.4496e-04 - val_loss: 6.2252e-04 - 17s/epoch - 3ms/step
Epoch 269/300
5969/5969 - 17s - loss: 3.3628e-04 - val_loss: 4.2889e-04 - 17s/epoch - 3ms/step
Epoch 270/300
5969/5969 - 17s - loss: 3.4824e-04 - val_loss: 5.1942e-04 - 17s/epoch - 3ms/step
Epoch 271/300
5969/5969 - 17s - loss: 3.4225e-04 - val_loss: 5.9661e-04 - 17s/epoch - 3ms/step
Epoch 272/300
5969/5969 - 17s - loss: 3.3326e-04 - val_loss: 4.6979e-04 - 17s/epoch - 3ms/step
Epoch 273/300
5969/5969 - 17s - loss: 3.3279e-04 - val_loss: 5.0634e-04 - 17s/epoch - 3ms/step
Epoch 274/300
5969/5969 - 17s - loss: 3.3243e-04 - val_loss: 5.1712e-04 - 17s/epoch - 3ms/step
Epoch 275/300
5969/5969 - 17s - loss: 3.3035e-04 - val_loss: 3.8263e-04 - 17s/epoch - 3ms/step
Epoch 276/300
5969/5969 - 17s - loss: 3.2343e-04 - val_loss: 4.1135e-04 - 17s/epoch - 3ms/step
Epoch 277/300
5969/5969 - 17s - loss: 3.2856e-04 - val_loss: 4.7722e-04 - 17s/epoch - 3ms/step
Epoch 278/300
5969/5969 - 17s - loss: 3.2526e-04 - val_loss: 6.6263e-04 - 17s/epoch - 3ms/step
Epoch 279/300
5969/5969 - 17s - loss: 3.3498e-04 - val_loss: 9.2969e-04 - 17s/epoch - 3ms/step
Epoch 280/300
5969/5969 - 17s - loss: 3.2943e-04 - val_loss: 8.0109e-04 - 17s/epoch - 3ms/step
Epoch 281/300
5969/5969 - 17s - loss: 3.4270e-04 - val_loss: 5.4929e-04 - 17s/epoch - 3ms/step
Epoch 282/300
5969/5969 - 17s - loss: 3.4310e-04 - val_loss: 6.6506e-04 - 17s/epoch - 3ms/step
Epoch 283/300
5969/5969 - 17s - loss: 3.4597e-04 - val_loss: 7.4274e-04 - 17s/epoch - 3ms/step
Epoch 284/300
5969/5969 - 17s - loss: 3.3381e-04 - val_loss: 4.9456e-04 - 17s/epoch - 3ms/step
Epoch 285/300
5969/5969 - 17s - loss: 3.3614e-04 - val_loss: 7.6213e-04 - 17s/epoch - 3ms/step
Epoch 286/300
5969/5969 - 17s - loss: 3.2698e-04 - val_loss: 5.5800e-04 - 17s/epoch - 3ms/step
Epoch 287/300
5969/5969 - 17s - loss: 3.2630e-04 - val_loss: 7.4927e-04 - 17s/epoch - 3ms/step
Epoch 288/300
5969/5969 - 17s - loss: 3.3553e-04 - val_loss: 5.5290e-04 - 17s/epoch - 3ms/step
Epoch 289/300
5969/5969 - 17s - loss: 3.2403e-04 - val_loss: 6.1064e-04 - 17s/epoch - 3ms/step
Epoch 290/300
5969/5969 - 17s - loss: 3.4862e-04 - val_loss: 6.2331e-04 - 17s/epoch - 3ms/step
Epoch 291/300
5969/5969 - 17s - loss: 3.2357e-04 - val_loss: 4.7717e-04 - 17s/epoch - 3ms/step
Epoch 292/300
5969/5969 - 17s - loss: 3.4534e-04 - val_loss: 7.0097e-04 - 17s/epoch - 3ms/step
Epoch 293/300
5969/5969 - 17s - loss: 3.2586e-04 - val_loss: 8.3477e-04 - 17s/epoch - 3ms/step
Epoch 294/300
5969/5969 - 17s - loss: 3.3168e-04 - val_loss: 6.6694e-04 - 17s/epoch - 3ms/step
Epoch 295/300
5969/5969 - 17s - loss: 3.4005e-04 - val_loss: 5.5848e-04 - 17s/epoch - 3ms/step
Epoch 296/300
5969/5969 - 17s - loss: 3.3134e-04 - val_loss: 6.9577e-04 - 17s/epoch - 3ms/step
Epoch 297/300
5969/5969 - 17s - loss: 3.2627e-04 - val_loss: 7.7041e-04 - 17s/epoch - 3ms/step
Epoch 298/300
5969/5969 - 17s - loss: 3.2567e-04 - val_loss: 6.7683e-04 - 17s/epoch - 3ms/step
Epoch 299/300
5969/5969 - 17s - loss: 3.1988e-04 - val_loss: 5.2365e-04 - 17s/epoch - 3ms/step
Epoch 300/300
5969/5969 - 17s - loss: 3.2428e-04 - val_loss: 3.9591e-04 - 17s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0003959147725254297
  1/332 [..............................] - ETA: 30s 48/332 [===>..........................] - ETA: 0s  97/332 [=======>......................] - ETA: 0s145/332 [============>.................] - ETA: 0s194/332 [================>.............] - ETA: 0s243/332 [====================>.........] - ETA: 0s292/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.004161798033991086
cosine 0.0034596764654920903
MAE: 0.008777303
RMSE: 0.019897584
r2: 0.9743166277304343
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       multiple                  0         
                                                                 
 dense_21 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_21 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_22 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 632)               0         
                                                                 
 dense_22 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_23 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2528)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_22"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_23 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_22 (InputLayer)       multiple                  0         
                                                                 
 dense_21 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_21 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_23"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_24 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_22 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 632)               0         
                                                                 
 dense_22 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_23 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2528)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 16, 300, 0.001, 0.5, 632, 0.00032428346457891166, 0.0003959147725254297, 0.004161798033991086, 0.0034596764654920903, 0.00877730268985033, 0.01989758387207985, 0.9743166277304343, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_24"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_25 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_24 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_24 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_25 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 632)               0         
                                                                 
 dense_25 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_26 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2528)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/300
5969/5969 - 18s - loss: 0.0077 - val_loss: 0.0025 - 18s/epoch - 3ms/step
Epoch 2/300
5969/5969 - 17s - loss: 0.0025 - val_loss: 0.0018 - 17s/epoch - 3ms/step
Epoch 3/300
5969/5969 - 17s - loss: 0.0018 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 4/300
5969/5969 - 17s - loss: 0.0015 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 5/300
5969/5969 - 17s - loss: 0.0013 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 6/300
5969/5969 - 17s - loss: 0.0012 - val_loss: 9.5145e-04 - 17s/epoch - 3ms/step
Epoch 7/300
5969/5969 - 17s - loss: 0.0012 - val_loss: 9.6721e-04 - 17s/epoch - 3ms/step
Epoch 8/300
5969/5969 - 17s - loss: 0.0011 - val_loss: 8.3960e-04 - 17s/epoch - 3ms/step
Epoch 9/300
5969/5969 - 17s - loss: 0.0011 - val_loss: 8.9927e-04 - 17s/epoch - 3ms/step
Epoch 10/300
5969/5969 - 17s - loss: 0.0010 - val_loss: 8.7575e-04 - 17s/epoch - 3ms/step
Epoch 11/300
5969/5969 - 17s - loss: 9.8934e-04 - val_loss: 8.2024e-04 - 17s/epoch - 3ms/step
Epoch 12/300
5969/5969 - 17s - loss: 9.4913e-04 - val_loss: 9.0122e-04 - 17s/epoch - 3ms/step
Epoch 13/300
5969/5969 - 17s - loss: 9.4044e-04 - val_loss: 7.3493e-04 - 17s/epoch - 3ms/step
Epoch 14/300
5969/5969 - 17s - loss: 9.0198e-04 - val_loss: 7.3391e-04 - 17s/epoch - 3ms/step
Epoch 15/300
5969/5969 - 17s - loss: 8.9853e-04 - val_loss: 7.2244e-04 - 17s/epoch - 3ms/step
Epoch 16/300
5969/5969 - 17s - loss: 8.7181e-04 - val_loss: 7.2875e-04 - 17s/epoch - 3ms/step
Epoch 17/300
5969/5969 - 17s - loss: 8.4563e-04 - val_loss: 6.8120e-04 - 17s/epoch - 3ms/step
Epoch 18/300
5969/5969 - 17s - loss: 8.6080e-04 - val_loss: 7.0951e-04 - 17s/epoch - 3ms/step
Epoch 19/300
5969/5969 - 17s - loss: 8.2925e-04 - val_loss: 6.8331e-04 - 17s/epoch - 3ms/step
Epoch 20/300
5969/5969 - 17s - loss: 8.1497e-04 - val_loss: 6.5872e-04 - 17s/epoch - 3ms/step
Epoch 21/300
5969/5969 - 17s - loss: 8.2382e-04 - val_loss: 6.6884e-04 - 17s/epoch - 3ms/step
Epoch 22/300
5969/5969 - 17s - loss: 8.1020e-04 - val_loss: 7.0218e-04 - 17s/epoch - 3ms/step
Epoch 23/300
5969/5969 - 17s - loss: 7.8222e-04 - val_loss: 6.3664e-04 - 17s/epoch - 3ms/step
Epoch 24/300
5969/5969 - 17s - loss: 7.7006e-04 - val_loss: 6.3569e-04 - 17s/epoch - 3ms/step
Epoch 25/300
5969/5969 - 17s - loss: 7.6820e-04 - val_loss: 6.3398e-04 - 17s/epoch - 3ms/step
Epoch 26/300
5969/5969 - 17s - loss: 7.9583e-04 - val_loss: 6.2383e-04 - 17s/epoch - 3ms/step
Epoch 27/300
5969/5969 - 17s - loss: 7.4079e-04 - val_loss: 5.6824e-04 - 17s/epoch - 3ms/step
Epoch 28/300
5969/5969 - 17s - loss: 7.6806e-04 - val_loss: 5.7524e-04 - 17s/epoch - 3ms/step
Epoch 29/300
5969/5969 - 17s - loss: 7.4595e-04 - val_loss: 5.9501e-04 - 17s/epoch - 3ms/step
Epoch 30/300
5969/5969 - 17s - loss: 7.1839e-04 - val_loss: 5.5429e-04 - 17s/epoch - 3ms/step
Epoch 31/300
5969/5969 - 17s - loss: 7.2396e-04 - val_loss: 5.5121e-04 - 17s/epoch - 3ms/step
Epoch 32/300
5969/5969 - 17s - loss: 7.0675e-04 - val_loss: 6.1729e-04 - 17s/epoch - 3ms/step
Epoch 33/300
5969/5969 - 17s - loss: 7.0187e-04 - val_loss: 5.4467e-04 - 17s/epoch - 3ms/step
Epoch 34/300
5969/5969 - 17s - loss: 6.9995e-04 - val_loss: 5.2120e-04 - 17s/epoch - 3ms/step
Epoch 35/300
5969/5969 - 17s - loss: 6.8562e-04 - val_loss: 5.6032e-04 - 17s/epoch - 3ms/step
Epoch 36/300
5969/5969 - 17s - loss: 6.8382e-04 - val_loss: 6.0836e-04 - 17s/epoch - 3ms/step
Epoch 37/300
5969/5969 - 17s - loss: 6.8568e-04 - val_loss: 5.8073e-04 - 17s/epoch - 3ms/step
Epoch 38/300
5969/5969 - 17s - loss: 6.8085e-04 - val_loss: 5.4183e-04 - 17s/epoch - 3ms/step
Epoch 39/300
5969/5969 - 17s - loss: 6.7439e-04 - val_loss: 6.6287e-04 - 17s/epoch - 3ms/step
Epoch 40/300
5969/5969 - 17s - loss: 6.6641e-04 - val_loss: 5.4537e-04 - 17s/epoch - 3ms/step
Epoch 41/300
5969/5969 - 17s - loss: 6.8516e-04 - val_loss: 5.8727e-04 - 17s/epoch - 3ms/step
Epoch 42/300
5969/5969 - 17s - loss: 6.6732e-04 - val_loss: 5.3245e-04 - 17s/epoch - 3ms/step
Epoch 43/300
5969/5969 - 17s - loss: 6.7577e-04 - val_loss: 5.5932e-04 - 17s/epoch - 3ms/step
Epoch 44/300
5969/5969 - 17s - loss: 6.5975e-04 - val_loss: 5.8382e-04 - 17s/epoch - 3ms/step
Epoch 45/300
5969/5969 - 17s - loss: 6.5831e-04 - val_loss: 5.2056e-04 - 17s/epoch - 3ms/step
Epoch 46/300
5969/5969 - 17s - loss: 6.4333e-04 - val_loss: 5.2588e-04 - 17s/epoch - 3ms/step
Epoch 47/300
5969/5969 - 17s - loss: 6.4372e-04 - val_loss: 5.1967e-04 - 17s/epoch - 3ms/step
Epoch 48/300
5969/5969 - 17s - loss: 6.5126e-04 - val_loss: 5.1233e-04 - 17s/epoch - 3ms/step
Epoch 49/300
5969/5969 - 17s - loss: 6.5165e-04 - val_loss: 5.1189e-04 - 17s/epoch - 3ms/step
Epoch 50/300
5969/5969 - 17s - loss: 6.3189e-04 - val_loss: 5.0875e-04 - 17s/epoch - 3ms/step
Epoch 51/300
5969/5969 - 17s - loss: 6.4935e-04 - val_loss: 5.1878e-04 - 17s/epoch - 3ms/step
Epoch 52/300
5969/5969 - 17s - loss: 6.3088e-04 - val_loss: 5.1485e-04 - 17s/epoch - 3ms/step
Epoch 53/300
5969/5969 - 17s - loss: 6.3064e-04 - val_loss: 4.9710e-04 - 17s/epoch - 3ms/step
Epoch 54/300
5969/5969 - 17s - loss: 6.2603e-04 - val_loss: 6.3844e-04 - 17s/epoch - 3ms/step
Epoch 55/300
5969/5969 - 17s - loss: 6.2639e-04 - val_loss: 4.7341e-04 - 17s/epoch - 3ms/step
Epoch 56/300
5969/5969 - 17s - loss: 6.4898e-04 - val_loss: 5.9059e-04 - 17s/epoch - 3ms/step
Epoch 57/300
5969/5969 - 17s - loss: 6.1848e-04 - val_loss: 5.1425e-04 - 17s/epoch - 3ms/step
Epoch 58/300
5969/5969 - 17s - loss: 6.0851e-04 - val_loss: 5.0930e-04 - 17s/epoch - 3ms/step
Epoch 59/300
5969/5969 - 17s - loss: 6.0726e-04 - val_loss: 4.8997e-04 - 17s/epoch - 3ms/step
Epoch 60/300
5969/5969 - 17s - loss: 6.1487e-04 - val_loss: 5.1001e-04 - 17s/epoch - 3ms/step
Epoch 61/300
5969/5969 - 17s - loss: 6.0969e-04 - val_loss: 5.6347e-04 - 17s/epoch - 3ms/step
Epoch 62/300
5969/5969 - 17s - loss: 6.1201e-04 - val_loss: 6.8507e-04 - 17s/epoch - 3ms/step
Epoch 63/300
5969/5969 - 17s - loss: 6.0016e-04 - val_loss: 5.9929e-04 - 17s/epoch - 3ms/step
Epoch 64/300
5969/5969 - 17s - loss: 6.2194e-04 - val_loss: 4.7078e-04 - 17s/epoch - 3ms/step
Epoch 65/300
5969/5969 - 17s - loss: 6.1287e-04 - val_loss: 5.4961e-04 - 17s/epoch - 3ms/step
Epoch 66/300
5969/5969 - 17s - loss: 6.1820e-04 - val_loss: 4.6478e-04 - 17s/epoch - 3ms/step
Epoch 67/300
5969/5969 - 17s - loss: 6.0365e-04 - val_loss: 5.1801e-04 - 17s/epoch - 3ms/step
Epoch 68/300
5969/5969 - 17s - loss: 5.9426e-04 - val_loss: 5.0122e-04 - 17s/epoch - 3ms/step
Epoch 69/300
5969/5969 - 17s - loss: 6.0622e-04 - val_loss: 4.6435e-04 - 17s/epoch - 3ms/step
Epoch 70/300
5969/5969 - 17s - loss: 6.0038e-04 - val_loss: 5.1557e-04 - 17s/epoch - 3ms/step
Epoch 71/300
5969/5969 - 17s - loss: 5.8895e-04 - val_loss: 4.6293e-04 - 17s/epoch - 3ms/step
Epoch 72/300
5969/5969 - 17s - loss: 6.2001e-04 - val_loss: 4.9916e-04 - 17s/epoch - 3ms/step
Epoch 73/300
5969/5969 - 17s - loss: 5.8533e-04 - val_loss: 4.7031e-04 - 17s/epoch - 3ms/step
Epoch 74/300
5969/5969 - 17s - loss: 5.9120e-04 - val_loss: 5.1898e-04 - 17s/epoch - 3ms/step
Epoch 75/300
5969/5969 - 17s - loss: 6.2571e-04 - val_loss: 4.9089e-04 - 17s/epoch - 3ms/step
Epoch 76/300
5969/5969 - 17s - loss: 5.8209e-04 - val_loss: 5.0005e-04 - 17s/epoch - 3ms/step
Epoch 77/300
5969/5969 - 17s - loss: 5.8461e-04 - val_loss: 4.4380e-04 - 17s/epoch - 3ms/step
Epoch 78/300
5969/5969 - 17s - loss: 5.8127e-04 - val_loss: 4.8828e-04 - 17s/epoch - 3ms/step
Epoch 79/300
5969/5969 - 17s - loss: 5.8189e-04 - val_loss: 4.7441e-04 - 17s/epoch - 3ms/step
Epoch 80/300
5969/5969 - 17s - loss: 5.8877e-04 - val_loss: 4.5793e-04 - 17s/epoch - 3ms/step
Epoch 81/300
5969/5969 - 17s - loss: 5.7778e-04 - val_loss: 5.3314e-04 - 17s/epoch - 3ms/step
Epoch 82/300
5969/5969 - 17s - loss: 5.7648e-04 - val_loss: 4.3580e-04 - 17s/epoch - 3ms/step
Epoch 83/300
5969/5969 - 17s - loss: 5.7240e-04 - val_loss: 4.8838e-04 - 17s/epoch - 3ms/step
Epoch 84/300
5969/5969 - 17s - loss: 5.9883e-04 - val_loss: 4.8132e-04 - 17s/epoch - 3ms/step
Epoch 85/300
5969/5969 - 17s - loss: 5.8028e-04 - val_loss: 4.8529e-04 - 17s/epoch - 3ms/step
Epoch 86/300
5969/5969 - 17s - loss: 5.6801e-04 - val_loss: 4.9375e-04 - 17s/epoch - 3ms/step
Epoch 87/300
5969/5969 - 17s - loss: 5.7145e-04 - val_loss: 4.9002e-04 - 17s/epoch - 3ms/step
Epoch 88/300
5969/5969 - 17s - loss: 5.8504e-04 - val_loss: 4.7071e-04 - 17s/epoch - 3ms/step
Epoch 89/300
5969/5969 - 17s - loss: 5.7772e-04 - val_loss: 4.4971e-04 - 17s/epoch - 3ms/step
Epoch 90/300
5969/5969 - 17s - loss: 5.6223e-04 - val_loss: 4.6049e-04 - 17s/epoch - 3ms/step
Epoch 91/300
5969/5969 - 17s - loss: 6.3630e-04 - val_loss: 4.7323e-04 - 17s/epoch - 3ms/step
Epoch 92/300
5969/5969 - 17s - loss: 5.7063e-04 - val_loss: 4.8559e-04 - 17s/epoch - 3ms/step
Epoch 93/300
5969/5969 - 17s - loss: 6.1589e-04 - val_loss: 4.8649e-04 - 17s/epoch - 3ms/step
Epoch 94/300
5969/5969 - 17s - loss: 5.9420e-04 - val_loss: 4.6928e-04 - 17s/epoch - 3ms/step
Epoch 95/300
5969/5969 - 17s - loss: 5.9080e-04 - val_loss: 4.6454e-04 - 17s/epoch - 3ms/step
Epoch 96/300
5969/5969 - 17s - loss: 5.8160e-04 - val_loss: 4.7969e-04 - 17s/epoch - 3ms/step
Epoch 97/300
5969/5969 - 17s - loss: 5.7071e-04 - val_loss: 4.4979e-04 - 17s/epoch - 3ms/step
Epoch 98/300
5969/5969 - 17s - loss: 5.6540e-04 - val_loss: 4.4773e-04 - 17s/epoch - 3ms/step
Epoch 99/300
5969/5969 - 17s - loss: 5.6214e-04 - val_loss: 4.5492e-04 - 17s/epoch - 3ms/step
Epoch 100/300
5969/5969 - 17s - loss: 6.4799e-04 - val_loss: 4.6670e-04 - 17s/epoch - 3ms/step
Epoch 101/300
5969/5969 - 17s - loss: 5.9289e-04 - val_loss: 4.8070e-04 - 17s/epoch - 3ms/step
Epoch 102/300
5969/5969 - 17s - loss: 5.8660e-04 - val_loss: 4.7436e-04 - 17s/epoch - 3ms/step
Epoch 103/300
5969/5969 - 17s - loss: 6.2451e-04 - val_loss: 4.7714e-04 - 17s/epoch - 3ms/step
Epoch 104/300
5969/5969 - 17s - loss: 5.9393e-04 - val_loss: 5.6693e-04 - 17s/epoch - 3ms/step
Epoch 105/300
5969/5969 - 17s - loss: 6.1817e-04 - val_loss: 4.8182e-04 - 17s/epoch - 3ms/step
Epoch 106/300
5969/5969 - 17s - loss: 5.7415e-04 - val_loss: 4.7195e-04 - 17s/epoch - 3ms/step
Epoch 107/300
5969/5969 - 17s - loss: 5.8392e-04 - val_loss: 5.3845e-04 - 17s/epoch - 3ms/step
Epoch 108/300
5969/5969 - 17s - loss: 5.6612e-04 - val_loss: 4.5058e-04 - 17s/epoch - 3ms/step
Epoch 109/300
5969/5969 - 17s - loss: 5.8526e-04 - val_loss: 5.6832e-04 - 17s/epoch - 3ms/step
Epoch 110/300
5969/5969 - 17s - loss: 5.9111e-04 - val_loss: 4.6937e-04 - 17s/epoch - 3ms/step
Epoch 111/300
5969/5969 - 17s - loss: 5.6790e-04 - val_loss: 5.0894e-04 - 17s/epoch - 3ms/step
Epoch 112/300
5969/5969 - 17s - loss: 5.9511e-04 - val_loss: 5.1730e-04 - 17s/epoch - 3ms/step
Epoch 113/300
5969/5969 - 17s - loss: 5.7394e-04 - val_loss: 4.7966e-04 - 17s/epoch - 3ms/step
Epoch 114/300
5969/5969 - 17s - loss: 5.6607e-04 - val_loss: 4.8215e-04 - 17s/epoch - 3ms/step
Epoch 115/300
5969/5969 - 17s - loss: 5.6580e-04 - val_loss: 4.5081e-04 - 17s/epoch - 3ms/step
Epoch 116/300
5969/5969 - 17s - loss: 5.7364e-04 - val_loss: 4.3743e-04 - 17s/epoch - 3ms/step
Epoch 117/300
5969/5969 - 17s - loss: 5.7277e-04 - val_loss: 4.2828e-04 - 17s/epoch - 3ms/step
Epoch 118/300
5969/5969 - 17s - loss: 5.6874e-04 - val_loss: 5.1212e-04 - 17s/epoch - 3ms/step
Epoch 119/300
5969/5969 - 17s - loss: 5.5965e-04 - val_loss: 4.6559e-04 - 17s/epoch - 3ms/step
Epoch 120/300
5969/5969 - 17s - loss: 5.7164e-04 - val_loss: 4.7239e-04 - 17s/epoch - 3ms/step
Epoch 121/300
5969/5969 - 17s - loss: 5.9569e-04 - val_loss: 0.0031 - 17s/epoch - 3ms/step
Epoch 122/300
5969/5969 - 17s - loss: 5.8629e-04 - val_loss: 5.8156e-04 - 17s/epoch - 3ms/step
Epoch 123/300
5969/5969 - 17s - loss: 5.5463e-04 - val_loss: 7.0060e-04 - 17s/epoch - 3ms/step
Epoch 124/300
5969/5969 - 17s - loss: 5.6200e-04 - val_loss: 5.5807e-04 - 17s/epoch - 3ms/step
Epoch 125/300
5969/5969 - 17s - loss: 5.6760e-04 - val_loss: 5.0542e-04 - 17s/epoch - 3ms/step
Epoch 126/300
5969/5969 - 17s - loss: 5.5682e-04 - val_loss: 5.7710e-04 - 17s/epoch - 3ms/step
Epoch 127/300
5969/5969 - 17s - loss: 5.5024e-04 - val_loss: 5.2941e-04 - 17s/epoch - 3ms/step
Epoch 128/300
5969/5969 - 17s - loss: 5.4889e-04 - val_loss: 5.2263e-04 - 17s/epoch - 3ms/step
Epoch 129/300
5969/5969 - 17s - loss: 5.5767e-04 - val_loss: 5.3659e-04 - 17s/epoch - 3ms/step
Epoch 130/300
5969/5969 - 17s - loss: 5.6789e-04 - val_loss: 6.4787e-04 - 17s/epoch - 3ms/step
Epoch 131/300
5969/5969 - 17s - loss: 5.4997e-04 - val_loss: 5.5774e-04 - 17s/epoch - 3ms/step
Epoch 132/300
5969/5969 - 17s - loss: 6.0340e-04 - val_loss: 5.5246e-04 - 17s/epoch - 3ms/step
Epoch 133/300
5969/5969 - 17s - loss: 5.7465e-04 - val_loss: 5.3576e-04 - 17s/epoch - 3ms/step
Epoch 134/300
5969/5969 - 17s - loss: 5.5970e-04 - val_loss: 5.1234e-04 - 17s/epoch - 3ms/step
Epoch 135/300
5969/5969 - 17s - loss: 5.5215e-04 - val_loss: 5.2424e-04 - 17s/epoch - 3ms/step
Epoch 136/300
5969/5969 - 17s - loss: 5.7323e-04 - val_loss: 4.6350e-04 - 17s/epoch - 3ms/step
Epoch 137/300
5969/5969 - 17s - loss: 5.4532e-04 - val_loss: 4.6389e-04 - 17s/epoch - 3ms/step
Epoch 138/300
5969/5969 - 17s - loss: 5.5961e-04 - val_loss: 4.8502e-04 - 17s/epoch - 3ms/step
Epoch 139/300
5969/5969 - 17s - loss: 5.4606e-04 - val_loss: 4.9600e-04 - 17s/epoch - 3ms/step
Epoch 140/300
5969/5969 - 17s - loss: 5.4398e-04 - val_loss: 4.9891e-04 - 17s/epoch - 3ms/step
Epoch 141/300
5969/5969 - 17s - loss: 5.7369e-04 - val_loss: 4.6747e-04 - 17s/epoch - 3ms/step
Epoch 142/300
5969/5969 - 17s - loss: 5.6257e-04 - val_loss: 6.1945e-04 - 17s/epoch - 3ms/step
Epoch 143/300
5969/5969 - 17s - loss: 5.7116e-04 - val_loss: 5.7335e-04 - 17s/epoch - 3ms/step
Epoch 144/300
5969/5969 - 17s - loss: 6.0507e-04 - val_loss: 6.4214e-04 - 17s/epoch - 3ms/step
Epoch 145/300
5969/5969 - 17s - loss: 5.6549e-04 - val_loss: 5.9654e-04 - 17s/epoch - 3ms/step
Epoch 146/300
5969/5969 - 17s - loss: 5.8824e-04 - val_loss: 5.0440e-04 - 17s/epoch - 3ms/step
Epoch 147/300
5969/5969 - 17s - loss: 5.6146e-04 - val_loss: 6.0311e-04 - 17s/epoch - 3ms/step
Epoch 148/300
5969/5969 - 17s - loss: 5.7003e-04 - val_loss: 5.7818e-04 - 17s/epoch - 3ms/step
Epoch 149/300
5969/5969 - 17s - loss: 5.4584e-04 - val_loss: 5.0661e-04 - 17s/epoch - 3ms/step
Epoch 150/300
5969/5969 - 17s - loss: 5.5072e-04 - val_loss: 5.5569e-04 - 17s/epoch - 3ms/step
Epoch 151/300
5969/5969 - 17s - loss: 5.3614e-04 - val_loss: 4.8023e-04 - 17s/epoch - 3ms/step
Epoch 152/300
5969/5969 - 17s - loss: 5.4767e-04 - val_loss: 4.4811e-04 - 17s/epoch - 3ms/step
Epoch 153/300
5969/5969 - 17s - loss: 5.6807e-04 - val_loss: 7.6828e-04 - 17s/epoch - 3ms/step
Epoch 154/300
5969/5969 - 17s - loss: 5.4265e-04 - val_loss: 5.0965e-04 - 17s/epoch - 3ms/step
Epoch 155/300
5969/5969 - 17s - loss: 5.6285e-04 - val_loss: 5.2482e-04 - 17s/epoch - 3ms/step
Epoch 156/300
5969/5969 - 17s - loss: 5.3095e-04 - val_loss: 6.8133e-04 - 17s/epoch - 3ms/step
Epoch 157/300
5969/5969 - 17s - loss: 5.3435e-04 - val_loss: 5.5181e-04 - 17s/epoch - 3ms/step
Epoch 158/300
5969/5969 - 17s - loss: 5.2924e-04 - val_loss: 5.6798e-04 - 17s/epoch - 3ms/step
Epoch 159/300
5969/5969 - 17s - loss: 5.4663e-04 - val_loss: 5.2106e-04 - 17s/epoch - 3ms/step
Epoch 160/300
5969/5969 - 17s - loss: 5.3314e-04 - val_loss: 6.2590e-04 - 17s/epoch - 3ms/step
Epoch 161/300
5969/5969 - 17s - loss: 5.3102e-04 - val_loss: 5.6917e-04 - 17s/epoch - 3ms/step
Epoch 162/300
5969/5969 - 17s - loss: 5.6599e-04 - val_loss: 7.5803e-04 - 17s/epoch - 3ms/step
Epoch 163/300
5969/5969 - 17s - loss: 5.4024e-04 - val_loss: 5.3983e-04 - 17s/epoch - 3ms/step
Epoch 164/300
5969/5969 - 17s - loss: 5.3759e-04 - val_loss: 6.0130e-04 - 17s/epoch - 3ms/step
Epoch 165/300
5969/5969 - 17s - loss: 5.3418e-04 - val_loss: 6.1577e-04 - 17s/epoch - 3ms/step
Epoch 166/300
5969/5969 - 17s - loss: 5.2276e-04 - val_loss: 6.3161e-04 - 17s/epoch - 3ms/step
Epoch 167/300
5969/5969 - 17s - loss: 5.4088e-04 - val_loss: 5.7097e-04 - 17s/epoch - 3ms/step
Epoch 168/300
5969/5969 - 17s - loss: 5.2641e-04 - val_loss: 5.7398e-04 - 17s/epoch - 3ms/step
Epoch 169/300
5969/5969 - 17s - loss: 5.7121e-04 - val_loss: 5.3537e-04 - 17s/epoch - 3ms/step
Epoch 170/300
5969/5969 - 17s - loss: 5.3799e-04 - val_loss: 4.8429e-04 - 17s/epoch - 3ms/step
Epoch 171/300
5969/5969 - 17s - loss: 5.3211e-04 - val_loss: 5.9296e-04 - 17s/epoch - 3ms/step
Epoch 172/300
5969/5969 - 17s - loss: 5.2277e-04 - val_loss: 5.9531e-04 - 17s/epoch - 3ms/step
Epoch 173/300
5969/5969 - 17s - loss: 5.2570e-04 - val_loss: 5.0020e-04 - 17s/epoch - 3ms/step
Epoch 174/300
5969/5969 - 17s - loss: 5.2920e-04 - val_loss: 5.0598e-04 - 17s/epoch - 3ms/step
Epoch 175/300
5969/5969 - 17s - loss: 5.3012e-04 - val_loss: 5.7930e-04 - 17s/epoch - 3ms/step
Epoch 176/300
5969/5969 - 17s - loss: 5.2987e-04 - val_loss: 4.7045e-04 - 17s/epoch - 3ms/step
Epoch 177/300
5969/5969 - 17s - loss: 5.2491e-04 - val_loss: 5.3208e-04 - 17s/epoch - 3ms/step
Epoch 178/300
5969/5969 - 17s - loss: 5.2906e-04 - val_loss: 5.5065e-04 - 17s/epoch - 3ms/step
Epoch 179/300
5969/5969 - 17s - loss: 5.2892e-04 - val_loss: 5.7992e-04 - 17s/epoch - 3ms/step
Epoch 180/300
5969/5969 - 17s - loss: 5.1939e-04 - val_loss: 5.3478e-04 - 17s/epoch - 3ms/step
Epoch 181/300
5969/5969 - 17s - loss: 5.2700e-04 - val_loss: 6.2853e-04 - 17s/epoch - 3ms/step
Epoch 182/300
5969/5969 - 17s - loss: 5.5835e-04 - val_loss: 5.8128e-04 - 17s/epoch - 3ms/step
Epoch 183/300
5969/5969 - 17s - loss: 5.3106e-04 - val_loss: 5.2459e-04 - 17s/epoch - 3ms/step
Epoch 184/300
5969/5969 - 17s - loss: 5.1826e-04 - val_loss: 6.2401e-04 - 17s/epoch - 3ms/step
Epoch 185/300
5969/5969 - 17s - loss: 5.1514e-04 - val_loss: 5.2200e-04 - 17s/epoch - 3ms/step
Epoch 186/300
5969/5969 - 17s - loss: 5.5309e-04 - val_loss: 6.2832e-04 - 17s/epoch - 3ms/step
Epoch 187/300
5969/5969 - 17s - loss: 5.4555e-04 - val_loss: 7.0117e-04 - 17s/epoch - 3ms/step
Epoch 188/300
5969/5969 - 17s - loss: 5.2381e-04 - val_loss: 5.7747e-04 - 17s/epoch - 3ms/step
Epoch 189/300
5969/5969 - 17s - loss: 5.8532e-04 - val_loss: 4.6978e-04 - 17s/epoch - 3ms/step
Epoch 190/300
5969/5969 - 17s - loss: 5.3344e-04 - val_loss: 5.4593e-04 - 17s/epoch - 3ms/step
Epoch 191/300
5969/5969 - 17s - loss: 5.3246e-04 - val_loss: 6.3164e-04 - 17s/epoch - 3ms/step
Epoch 192/300
5969/5969 - 17s - loss: 5.2634e-04 - val_loss: 5.5216e-04 - 17s/epoch - 3ms/step
Epoch 193/300
5969/5969 - 17s - loss: 5.2931e-04 - val_loss: 8.2442e-04 - 17s/epoch - 3ms/step
Epoch 194/300
5969/5969 - 17s - loss: 5.6322e-04 - val_loss: 7.5764e-04 - 17s/epoch - 3ms/step
Epoch 195/300
5969/5969 - 17s - loss: 5.3376e-04 - val_loss: 5.8026e-04 - 17s/epoch - 3ms/step
Epoch 196/300
5969/5969 - 17s - loss: 5.3083e-04 - val_loss: 7.2449e-04 - 17s/epoch - 3ms/step
Epoch 197/300
5969/5969 - 17s - loss: 5.3921e-04 - val_loss: 6.2624e-04 - 17s/epoch - 3ms/step
Epoch 198/300
5969/5969 - 17s - loss: 5.2751e-04 - val_loss: 6.1704e-04 - 17s/epoch - 3ms/step
Epoch 199/300
5969/5969 - 17s - loss: 5.1997e-04 - val_loss: 6.1511e-04 - 17s/epoch - 3ms/step
Epoch 200/300
5969/5969 - 17s - loss: 5.3113e-04 - val_loss: 7.4908e-04 - 17s/epoch - 3ms/step
Epoch 201/300
5969/5969 - 17s - loss: 5.3277e-04 - val_loss: 6.5797e-04 - 17s/epoch - 3ms/step
Epoch 202/300
5969/5969 - 17s - loss: 5.2243e-04 - val_loss: 5.9437e-04 - 17s/epoch - 3ms/step
Epoch 203/300
5969/5969 - 17s - loss: 5.3405e-04 - val_loss: 5.4672e-04 - 17s/epoch - 3ms/step
Epoch 204/300
5969/5969 - 17s - loss: 5.5360e-04 - val_loss: 6.5714e-04 - 17s/epoch - 3ms/step
Epoch 205/300
5969/5969 - 17s - loss: 5.2889e-04 - val_loss: 5.3136e-04 - 17s/epoch - 3ms/step
Epoch 206/300
5969/5969 - 17s - loss: 5.1895e-04 - val_loss: 6.6854e-04 - 17s/epoch - 3ms/step
Epoch 207/300
5969/5969 - 17s - loss: 5.2126e-04 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 208/300
5969/5969 - 17s - loss: 5.2598e-04 - val_loss: 7.8056e-04 - 17s/epoch - 3ms/step
Epoch 209/300
5969/5969 - 17s - loss: 5.2502e-04 - val_loss: 6.1098e-04 - 17s/epoch - 3ms/step
Epoch 210/300
5969/5969 - 17s - loss: 5.2367e-04 - val_loss: 5.6173e-04 - 17s/epoch - 3ms/step
Epoch 211/300
5969/5969 - 17s - loss: 5.4594e-04 - val_loss: 5.6724e-04 - 17s/epoch - 3ms/step
Epoch 212/300
5969/5969 - 17s - loss: 5.2791e-04 - val_loss: 5.9998e-04 - 17s/epoch - 3ms/step
Epoch 213/300
5969/5969 - 17s - loss: 5.2807e-04 - val_loss: 7.3047e-04 - 17s/epoch - 3ms/step
Epoch 214/300
5969/5969 - 17s - loss: 5.2093e-04 - val_loss: 8.2011e-04 - 17s/epoch - 3ms/step
Epoch 215/300
5969/5969 - 17s - loss: 5.3751e-04 - val_loss: 6.0707e-04 - 17s/epoch - 3ms/step
Epoch 216/300
5969/5969 - 17s - loss: 5.2047e-04 - val_loss: 7.3808e-04 - 17s/epoch - 3ms/step
Epoch 217/300
5969/5969 - 17s - loss: 5.3104e-04 - val_loss: 6.9873e-04 - 17s/epoch - 3ms/step
Epoch 218/300
5969/5969 - 17s - loss: 5.2823e-04 - val_loss: 6.9947e-04 - 17s/epoch - 3ms/step
Epoch 219/300
5969/5969 - 17s - loss: 5.4554e-04 - val_loss: 7.6220e-04 - 17s/epoch - 3ms/step
Epoch 220/300
5969/5969 - 17s - loss: 5.1835e-04 - val_loss: 7.3332e-04 - 17s/epoch - 3ms/step
Epoch 221/300
5969/5969 - 17s - loss: 5.0735e-04 - val_loss: 6.4899e-04 - 17s/epoch - 3ms/step
Epoch 222/300
5969/5969 - 17s - loss: 5.1824e-04 - val_loss: 6.0771e-04 - 17s/epoch - 3ms/step
Epoch 223/300
5969/5969 - 17s - loss: 5.0841e-04 - val_loss: 6.2265e-04 - 17s/epoch - 3ms/step
Epoch 224/300
5969/5969 - 17s - loss: 5.2949e-04 - val_loss: 7.6722e-04 - 17s/epoch - 3ms/step
Epoch 225/300
5969/5969 - 17s - loss: 5.1945e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 226/300
slurmstepd: error: *** JOB 35380336 ON mb-icg101 CANCELLED AT 2022-12-26T20:53:30 ***
