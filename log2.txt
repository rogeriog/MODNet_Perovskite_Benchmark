start
Sun Jan  1 23:59:18 CET 2023
2023-01-01 23:59:27.900105: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-01 23:59:30.421798: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-01-02 00:00:36,706 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f6b3d61e760> object, created with modnet version 0.1.12
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_3n_b_cr0.5_bs16_ep100_loss_mse_lr0.0005_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_3n_b_cr0.5_bs16_ep100_loss_mse_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_3n_b_cr0.5_bs16_ep100_loss_mse_lr0.002_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_3n_b_cr0.5_bs16_ep200_loss_mse_lr0.0005_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_3n_b_cr0.5_bs16_ep200_loss_mse_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_3n_b_cr0.5_bs16_ep200_loss_mse_lr0.002_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_3n_b_cr0.5_bs16_ep300_loss_mse_lr0.0005_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_3n_b_cr0.5_bs16_ep300_loss_mse_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
2023-01-02 00:00:46.764904: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense (Dense)               (None, 3792)              4796880   
                                                                 
 batch_normalization (BatchN  (None, 3792)             15168     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_2 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 3792)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Epoch 1/300
5969/5969 - 296s - loss: 0.0103 - val_loss: 0.0026 - 296s/epoch - 50ms/step
Epoch 2/300
5969/5969 - 294s - loss: 0.0027 - val_loss: 0.0021 - 294s/epoch - 49ms/step
Epoch 3/300
5969/5969 - 295s - loss: 0.0019 - val_loss: 0.0016 - 295s/epoch - 49ms/step
Epoch 4/300
5969/5969 - 295s - loss: 0.0016 - val_loss: 0.0012 - 295s/epoch - 49ms/step
Epoch 5/300
5969/5969 - 295s - loss: 0.0014 - val_loss: 0.0011 - 295s/epoch - 49ms/step
Epoch 6/300
5969/5969 - 295s - loss: 0.0013 - val_loss: 0.0010 - 295s/epoch - 49ms/step
Epoch 7/300
5969/5969 - 295s - loss: 0.0012 - val_loss: 9.7234e-04 - 295s/epoch - 49ms/step
Epoch 8/300
5969/5969 - 295s - loss: 0.0011 - val_loss: 8.5565e-04 - 295s/epoch - 49ms/step
Epoch 9/300
5969/5969 - 295s - loss: 0.0011 - val_loss: 8.4427e-04 - 295s/epoch - 49ms/step
Epoch 10/300
5969/5969 - 295s - loss: 0.0011 - val_loss: 0.0010 - 295s/epoch - 49ms/step
Epoch 11/300
5969/5969 - 295s - loss: 0.0010 - val_loss: 8.7463e-04 - 295s/epoch - 49ms/step
Epoch 12/300
5969/5969 - 295s - loss: 9.6345e-04 - val_loss: 0.0012 - 295s/epoch - 49ms/step
Epoch 13/300
5969/5969 - 295s - loss: 9.4251e-04 - val_loss: 8.6622e-04 - 295s/epoch - 49ms/step
Epoch 14/300
5969/5969 - 295s - loss: 9.1672e-04 - val_loss: 8.2426e-04 - 295s/epoch - 49ms/step
Epoch 15/300
5969/5969 - 295s - loss: 8.9383e-04 - val_loss: 8.1726e-04 - 295s/epoch - 49ms/step
Epoch 16/300
5969/5969 - 295s - loss: 8.7930e-04 - val_loss: 8.1996e-04 - 295s/epoch - 49ms/step
Epoch 17/300
5969/5969 - 295s - loss: 8.6284e-04 - val_loss: 7.1136e-04 - 295s/epoch - 49ms/step
Epoch 18/300
5969/5969 - 295s - loss: 8.6179e-04 - val_loss: 7.2230e-04 - 295s/epoch - 49ms/step
Epoch 19/300
5969/5969 - 295s - loss: 8.3586e-04 - val_loss: 7.4963e-04 - 295s/epoch - 49ms/step
Epoch 20/300
5969/5969 - 295s - loss: 8.2327e-04 - val_loss: 6.7016e-04 - 295s/epoch - 49ms/step
Epoch 21/300
5969/5969 - 295s - loss: 8.1258e-04 - val_loss: 6.6116e-04 - 295s/epoch - 49ms/step
Epoch 22/300
5969/5969 - 295s - loss: 8.0273e-04 - val_loss: 6.2884e-04 - 295s/epoch - 49ms/step
Epoch 23/300
5969/5969 - 295s - loss: 7.8530e-04 - val_loss: 6.4835e-04 - 295s/epoch - 49ms/step
Epoch 24/300
5969/5969 - 295s - loss: 7.8250e-04 - val_loss: 6.4784e-04 - 295s/epoch - 49ms/step
Epoch 25/300
5969/5969 - 295s - loss: 8.0869e-04 - val_loss: 6.1212e-04 - 295s/epoch - 49ms/step
Epoch 26/300
5969/5969 - 295s - loss: 7.8429e-04 - val_loss: 6.3919e-04 - 295s/epoch - 49ms/step
Epoch 27/300
5969/5969 - 295s - loss: 7.5125e-04 - val_loss: 5.8058e-04 - 295s/epoch - 49ms/step
Epoch 28/300
5969/5969 - 295s - loss: 7.4363e-04 - val_loss: 6.1578e-04 - 295s/epoch - 49ms/step
Epoch 29/300
5969/5969 - 295s - loss: 7.9742e-04 - val_loss: 6.2022e-04 - 295s/epoch - 49ms/step
Epoch 30/300
5969/5969 - 295s - loss: 7.3700e-04 - val_loss: 5.6736e-04 - 295s/epoch - 49ms/step
Epoch 31/300
5969/5969 - 295s - loss: 7.4864e-04 - val_loss: 5.5058e-04 - 295s/epoch - 49ms/step
Epoch 32/300
5969/5969 - 295s - loss: 7.2243e-04 - val_loss: 6.2249e-04 - 295s/epoch - 49ms/step
Epoch 33/300
5969/5969 - 295s - loss: 7.2130e-04 - val_loss: 6.4032e-04 - 295s/epoch - 49ms/step
Epoch 34/300
5969/5969 - 295s - loss: 7.1626e-04 - val_loss: 5.9646e-04 - 295s/epoch - 49ms/step
Epoch 35/300
5969/5969 - 295s - loss: 7.1921e-04 - val_loss: 5.5436e-04 - 295s/epoch - 49ms/step
Epoch 36/300
5969/5969 - 295s - loss: 7.1042e-04 - val_loss: 6.4186e-04 - 295s/epoch - 49ms/step
Epoch 37/300
5969/5969 - 295s - loss: 7.2058e-04 - val_loss: 6.4434e-04 - 295s/epoch - 49ms/step
Epoch 38/300
5969/5969 - 295s - loss: 7.0862e-04 - val_loss: 6.0633e-04 - 295s/epoch - 50ms/step
Epoch 39/300
5969/5969 - 295s - loss: 7.1573e-04 - val_loss: 7.8497e-04 - 295s/epoch - 49ms/step
Epoch 40/300
5969/5969 - 296s - loss: 6.9292e-04 - val_loss: 5.7229e-04 - 296s/epoch - 50ms/step
Epoch 41/300
5969/5969 - 297s - loss: 6.8498e-04 - val_loss: 5.4543e-04 - 297s/epoch - 50ms/step
Epoch 42/300
5969/5969 - 297s - loss: 6.9127e-04 - val_loss: 5.5034e-04 - 297s/epoch - 50ms/step
Epoch 43/300
5969/5969 - 297s - loss: 6.6609e-04 - val_loss: 6.0038e-04 - 297s/epoch - 50ms/step
Epoch 44/300
5969/5969 - 297s - loss: 6.7133e-04 - val_loss: 5.7679e-04 - 297s/epoch - 50ms/step
Epoch 45/300
5969/5969 - 297s - loss: 6.9302e-04 - val_loss: 5.9835e-04 - 297s/epoch - 50ms/step
Epoch 46/300
5969/5969 - 296s - loss: 6.9798e-04 - val_loss: 5.6402e-04 - 296s/epoch - 50ms/step
Epoch 47/300
5969/5969 - 295s - loss: 6.7090e-04 - val_loss: 5.5277e-04 - 295s/epoch - 49ms/step
Epoch 48/300
5969/5969 - 295s - loss: 6.7713e-04 - val_loss: 5.2890e-04 - 295s/epoch - 49ms/step
Epoch 49/300
5969/5969 - 295s - loss: 6.7047e-04 - val_loss: 5.3703e-04 - 295s/epoch - 49ms/step
Epoch 50/300
5969/5969 - 295s - loss: 6.5609e-04 - val_loss: 5.5250e-04 - 295s/epoch - 49ms/step
Epoch 51/300
5969/5969 - 295s - loss: 6.5772e-04 - val_loss: 5.2680e-04 - 295s/epoch - 49ms/step
Epoch 52/300
5969/5969 - 295s - loss: 6.9193e-04 - val_loss: 5.7807e-04 - 295s/epoch - 49ms/step
Epoch 53/300
5969/5969 - 295s - loss: 6.4804e-04 - val_loss: 5.2556e-04 - 295s/epoch - 49ms/step
Epoch 54/300
5969/5969 - 295s - loss: 7.0328e-04 - val_loss: 6.4971e-04 - 295s/epoch - 49ms/step
Epoch 55/300
5969/5969 - 295s - loss: 6.4899e-04 - val_loss: 5.1841e-04 - 295s/epoch - 49ms/step
Epoch 56/300
5969/5969 - 295s - loss: 6.5683e-04 - val_loss: 5.0225e-04 - 295s/epoch - 49ms/step
Epoch 57/300
5969/5969 - 295s - loss: 6.4564e-04 - val_loss: 5.0855e-04 - 295s/epoch - 49ms/step
Epoch 58/300
5969/5969 - 295s - loss: 6.6091e-04 - val_loss: 5.2856e-04 - 295s/epoch - 49ms/step
Epoch 59/300
5969/5969 - 295s - loss: 6.3391e-04 - val_loss: 5.3831e-04 - 295s/epoch - 49ms/step
Epoch 60/300
5969/5969 - 295s - loss: 6.4966e-04 - val_loss: 5.1070e-04 - 295s/epoch - 49ms/step
Epoch 61/300
5969/5969 - 296s - loss: 6.2919e-04 - val_loss: 5.1424e-04 - 296s/epoch - 50ms/step
Epoch 62/300
5969/5969 - 296s - loss: 6.6355e-04 - val_loss: 5.6483e-04 - 296s/epoch - 50ms/step
Epoch 63/300
5969/5969 - 296s - loss: 6.6300e-04 - val_loss: 6.6754e-04 - 296s/epoch - 50ms/step
Epoch 64/300
5969/5969 - 296s - loss: 6.5059e-04 - val_loss: 5.3216e-04 - 296s/epoch - 50ms/step
Epoch 65/300
5969/5969 - 296s - loss: 6.4332e-04 - val_loss: 5.9179e-04 - 296s/epoch - 50ms/step
Epoch 66/300
5969/5969 - 298s - loss: 6.7545e-04 - val_loss: 5.7500e-04 - 298s/epoch - 50ms/step
Epoch 67/300
5969/5969 - 298s - loss: 6.5147e-04 - val_loss: 6.1944e-04 - 298s/epoch - 50ms/step
Epoch 68/300
5969/5969 - 297s - loss: 6.3158e-04 - val_loss: 5.6219e-04 - 297s/epoch - 50ms/step
Epoch 69/300
5969/5969 - 297s - loss: 6.2700e-04 - val_loss: 5.5826e-04 - 297s/epoch - 50ms/step
Epoch 70/300
5969/5969 - 297s - loss: 6.1572e-04 - val_loss: 5.5345e-04 - 297s/epoch - 50ms/step
Epoch 71/300
5969/5969 - 297s - loss: 6.2340e-04 - val_loss: 5.4751e-04 - 297s/epoch - 50ms/step
Epoch 72/300
5969/5969 - 297s - loss: 6.2677e-04 - val_loss: 5.7297e-04 - 297s/epoch - 50ms/step
Epoch 73/300
5969/5969 - 297s - loss: 6.3465e-04 - val_loss: 6.2875e-04 - 297s/epoch - 50ms/step
Epoch 74/300
5969/5969 - 296s - loss: 6.1763e-04 - val_loss: 5.7442e-04 - 296s/epoch - 50ms/step
Epoch 75/300
5969/5969 - 297s - loss: 6.1684e-04 - val_loss: 6.2748e-04 - 297s/epoch - 50ms/step
Epoch 76/300
5969/5969 - 297s - loss: 6.1651e-04 - val_loss: 5.9055e-04 - 297s/epoch - 50ms/step
Epoch 77/300
5969/5969 - 297s - loss: 6.0690e-04 - val_loss: 5.5881e-04 - 297s/epoch - 50ms/step
Epoch 78/300
5969/5969 - 297s - loss: 6.1545e-04 - val_loss: 5.8726e-04 - 297s/epoch - 50ms/step
Epoch 79/300
5969/5969 - 297s - loss: 6.0697e-04 - val_loss: 5.4749e-04 - 297s/epoch - 50ms/step
Epoch 80/300
5969/5969 - 296s - loss: 6.6946e-04 - val_loss: 5.4809e-04 - 296s/epoch - 50ms/step
Epoch 81/300
5969/5969 - 296s - loss: 6.0553e-04 - val_loss: 6.2915e-04 - 296s/epoch - 50ms/step
Epoch 82/300
5969/5969 - 296s - loss: 6.0841e-04 - val_loss: 5.1251e-04 - 296s/epoch - 50ms/step
Epoch 83/300
5969/5969 - 297s - loss: 6.2831e-04 - val_loss: 7.0746e-04 - 297s/epoch - 50ms/step
Epoch 84/300
5969/5969 - 297s - loss: 6.3383e-04 - val_loss: 5.8032e-04 - 297s/epoch - 50ms/step
Epoch 85/300
5969/5969 - 297s - loss: 6.1563e-04 - val_loss: 5.6541e-04 - 297s/epoch - 50ms/step
Epoch 86/300
5969/5969 - 296s - loss: 6.0488e-04 - val_loss: 6.3284e-04 - 296s/epoch - 50ms/step
Epoch 87/300
5969/5969 - 296s - loss: 6.2744e-04 - val_loss: 6.3402e-04 - 296s/epoch - 50ms/step
Epoch 88/300
5969/5969 - 296s - loss: 5.9157e-04 - val_loss: 5.2861e-04 - 296s/epoch - 50ms/step
Epoch 89/300
5969/5969 - 296s - loss: 6.0313e-04 - val_loss: 5.7777e-04 - 296s/epoch - 50ms/step
Epoch 90/300
5969/5969 - 297s - loss: 5.9287e-04 - val_loss: 5.3222e-04 - 297s/epoch - 50ms/step
Epoch 91/300
5969/5969 - 296s - loss: 6.6810e-04 - val_loss: 5.2733e-04 - 296s/epoch - 50ms/step
Epoch 92/300
5969/5969 - 297s - loss: 6.0734e-04 - val_loss: 8.1767e-04 - 297s/epoch - 50ms/step
Epoch 93/300
5969/5969 - 296s - loss: 6.1316e-04 - val_loss: 9.2200e-04 - 296s/epoch - 50ms/step
Epoch 94/300
5969/5969 - 297s - loss: 6.1148e-04 - val_loss: 5.1923e-04 - 297s/epoch - 50ms/step
Epoch 95/300
5969/5969 - 297s - loss: 6.1418e-04 - val_loss: 5.7354e-04 - 297s/epoch - 50ms/step
Epoch 96/300
5969/5969 - 297s - loss: 6.0347e-04 - val_loss: 7.4151e-04 - 297s/epoch - 50ms/step
Epoch 97/300
5969/5969 - 297s - loss: 5.9291e-04 - val_loss: 5.4399e-04 - 297s/epoch - 50ms/step
Epoch 98/300
5969/5969 - 297s - loss: 5.9802e-04 - val_loss: 6.2920e-04 - 297s/epoch - 50ms/step
Epoch 99/300
5969/5969 - 297s - loss: 6.2489e-04 - val_loss: 5.5060e-04 - 297s/epoch - 50ms/step
Epoch 100/300
5969/5969 - 296s - loss: 6.0018e-04 - val_loss: 5.0935e-04 - 296s/epoch - 50ms/step
Epoch 101/300
5969/5969 - 297s - loss: 5.8760e-04 - val_loss: 5.8330e-04 - 297s/epoch - 50ms/step
Epoch 102/300
5969/5969 - 296s - loss: 6.0758e-04 - val_loss: 5.9520e-04 - 296s/epoch - 50ms/step
Epoch 103/300
5969/5969 - 296s - loss: 5.8517e-04 - val_loss: 5.9407e-04 - 296s/epoch - 50ms/step
Epoch 104/300
5969/5969 - 296s - loss: 5.9418e-04 - val_loss: 6.7042e-04 - 296s/epoch - 50ms/step
Epoch 105/300
5969/5969 - 296s - loss: 5.9695e-04 - val_loss: 5.7005e-04 - 296s/epoch - 50ms/step
Epoch 106/300
5969/5969 - 296s - loss: 6.0367e-04 - val_loss: 5.4308e-04 - 296s/epoch - 50ms/step
Epoch 107/300
5969/5969 - 296s - loss: 5.8501e-04 - val_loss: 9.7581e-04 - 296s/epoch - 50ms/step
Epoch 108/300
5969/5969 - 296s - loss: 6.1488e-04 - val_loss: 5.1609e-04 - 296s/epoch - 50ms/step
Epoch 109/300
5969/5969 - 296s - loss: 5.8313e-04 - val_loss: 7.4546e-04 - 296s/epoch - 50ms/step
Epoch 110/300
5969/5969 - 296s - loss: 6.2625e-04 - val_loss: 6.1515e-04 - 296s/epoch - 50ms/step
Epoch 111/300
5969/5969 - 296s - loss: 6.1094e-04 - val_loss: 7.1247e-04 - 296s/epoch - 50ms/step
Epoch 112/300
5969/5969 - 296s - loss: 5.8783e-04 - val_loss: 5.6495e-04 - 296s/epoch - 50ms/step
Epoch 113/300
5969/5969 - 296s - loss: 5.9241e-04 - val_loss: 5.2356e-04 - 296s/epoch - 50ms/step
Epoch 114/300
5969/5969 - 296s - loss: 6.0216e-04 - val_loss: 6.0900e-04 - 296s/epoch - 50ms/step
Epoch 115/300
5969/5969 - 296s - loss: 5.7801e-04 - val_loss: 4.9535e-04 - 296s/epoch - 50ms/step
Epoch 116/300
5969/5969 - 296s - loss: 5.8861e-04 - val_loss: 5.2532e-04 - 296s/epoch - 50ms/step
Epoch 117/300
5969/5969 - 296s - loss: 5.8227e-04 - val_loss: 5.2822e-04 - 296s/epoch - 50ms/step
Epoch 118/300
5969/5969 - 296s - loss: 5.7796e-04 - val_loss: 6.4294e-04 - 296s/epoch - 50ms/step
Epoch 119/300
5969/5969 - 296s - loss: 5.7486e-04 - val_loss: 6.7263e-04 - 296s/epoch - 50ms/step
Epoch 120/300
5969/5969 - 296s - loss: 6.0758e-04 - val_loss: 7.4604e-04 - 296s/epoch - 50ms/step
Epoch 121/300
5969/5969 - 296s - loss: 5.8123e-04 - val_loss: 6.5848e-04 - 296s/epoch - 50ms/step
Epoch 122/300
5969/5969 - 296s - loss: 5.7204e-04 - val_loss: 5.7199e-04 - 296s/epoch - 50ms/step
Epoch 123/300
5969/5969 - 296s - loss: 6.0462e-04 - val_loss: 6.7146e-04 - 296s/epoch - 50ms/step
Epoch 124/300
5969/5969 - 296s - loss: 5.7808e-04 - val_loss: 6.6547e-04 - 296s/epoch - 50ms/step
Epoch 125/300
5969/5969 - 296s - loss: 5.8771e-04 - val_loss: 7.0106e-04 - 296s/epoch - 50ms/step
Epoch 126/300
5969/5969 - 296s - loss: 5.7300e-04 - val_loss: 0.0010 - 296s/epoch - 50ms/step
Epoch 127/300
5969/5969 - 296s - loss: 5.6515e-04 - val_loss: 7.6660e-04 - 296s/epoch - 50ms/step
Epoch 128/300
5969/5969 - 296s - loss: 5.8896e-04 - val_loss: 6.1084e-04 - 296s/epoch - 50ms/step
Epoch 129/300
5969/5969 - 296s - loss: 5.7826e-04 - val_loss: 6.8794e-04 - 296s/epoch - 50ms/step
Epoch 130/300
5969/5969 - 296s - loss: 5.6560e-04 - val_loss: 7.3585e-04 - 296s/epoch - 50ms/step
Epoch 131/300
5969/5969 - 296s - loss: 5.7715e-04 - val_loss: 7.9361e-04 - 296s/epoch - 50ms/step
Epoch 132/300
5969/5969 - 296s - loss: 5.7206e-04 - val_loss: 6.4859e-04 - 296s/epoch - 50ms/step
Epoch 133/300
5969/5969 - 296s - loss: 5.6530e-04 - val_loss: 5.4671e-04 - 296s/epoch - 50ms/step
Epoch 134/300
5969/5969 - 296s - loss: 5.6466e-04 - val_loss: 5.3962e-04 - 296s/epoch - 50ms/step
Epoch 135/300
5969/5969 - 296s - loss: 5.6241e-04 - val_loss: 7.4261e-04 - 296s/epoch - 50ms/step
Epoch 136/300
5969/5969 - 296s - loss: 5.7509e-04 - val_loss: 5.5817e-04 - 296s/epoch - 50ms/step
Epoch 137/300
5969/5969 - 296s - loss: 6.0077e-04 - val_loss: 6.0918e-04 - 296s/epoch - 50ms/step
Epoch 138/300
5969/5969 - 296s - loss: 5.7814e-04 - val_loss: 6.0994e-04 - 296s/epoch - 50ms/step
Epoch 139/300
5969/5969 - 296s - loss: 5.7542e-04 - val_loss: 6.8505e-04 - 296s/epoch - 50ms/step
Epoch 140/300
5969/5969 - 295s - loss: 5.6261e-04 - val_loss: 6.1460e-04 - 295s/epoch - 49ms/step
Epoch 141/300
5969/5969 - 295s - loss: 5.6203e-04 - val_loss: 4.9585e-04 - 295s/epoch - 49ms/step
Epoch 142/300
5969/5969 - 295s - loss: 5.6110e-04 - val_loss: 5.8134e-04 - 295s/epoch - 49ms/step
Epoch 143/300
5969/5969 - 295s - loss: 5.7877e-04 - val_loss: 0.0010 - 295s/epoch - 49ms/step
Epoch 144/300
5969/5969 - 296s - loss: 5.6547e-04 - val_loss: 6.9745e-04 - 296s/epoch - 50ms/step
Epoch 145/300
5969/5969 - 296s - loss: 5.6494e-04 - val_loss: 6.1577e-04 - 296s/epoch - 50ms/step
Epoch 146/300
5969/5969 - 296s - loss: 5.5794e-04 - val_loss: 7.1551e-04 - 296s/epoch - 50ms/step
Epoch 147/300
5969/5969 - 295s - loss: 5.5254e-04 - val_loss: 6.8313e-04 - 295s/epoch - 49ms/step
Epoch 148/300
5969/5969 - 295s - loss: 5.6032e-04 - val_loss: 8.4476e-04 - 295s/epoch - 49ms/step
Epoch 149/300
5969/5969 - 295s - loss: 5.6284e-04 - val_loss: 0.0012 - 295s/epoch - 49ms/step
Epoch 150/300
5969/5969 - 295s - loss: 5.7030e-04 - val_loss: 8.6538e-04 - 295s/epoch - 49ms/step
Epoch 151/300
5969/5969 - 295s - loss: 5.5275e-04 - val_loss: 7.3928e-04 - 295s/epoch - 49ms/step
Epoch 152/300
5969/5969 - 295s - loss: 5.6728e-04 - val_loss: 5.6146e-04 - 295s/epoch - 49ms/step
Epoch 153/300
5969/5969 - 295s - loss: 5.5396e-04 - val_loss: 7.9820e-04 - 295s/epoch - 49ms/step
Epoch 154/300
5969/5969 - 295s - loss: 5.6042e-04 - val_loss: 8.2463e-04 - 295s/epoch - 49ms/step
Epoch 155/300
5969/5969 - 295s - loss: 5.5824e-04 - val_loss: 6.4894e-04 - 295s/epoch - 49ms/step
Epoch 156/300
5969/5969 - 296s - loss: 5.6136e-04 - val_loss: 0.0010 - 296s/epoch - 50ms/step
Epoch 157/300
5969/5969 - 295s - loss: 5.5964e-04 - val_loss: 6.4045e-04 - 295s/epoch - 49ms/step
Epoch 158/300
5969/5969 - 295s - loss: 5.5006e-04 - val_loss: 8.7965e-04 - 295s/epoch - 49ms/step
Epoch 159/300
5969/5969 - 295s - loss: 5.8082e-04 - val_loss: 5.5269e-04 - 295s/epoch - 49ms/step
Epoch 160/300
5969/5969 - 295s - loss: 5.5661e-04 - val_loss: 0.0010 - 295s/epoch - 49ms/step
Epoch 161/300
5969/5969 - 298s - loss: 5.4487e-04 - val_loss: 8.6610e-04 - 298s/epoch - 50ms/step
Epoch 162/300
5969/5969 - 298s - loss: 5.4666e-04 - val_loss: 0.0011 - 298s/epoch - 50ms/step
Epoch 163/300
5969/5969 - 298s - loss: 5.6196e-04 - val_loss: 7.0163e-04 - 298s/epoch - 50ms/step
Epoch 164/300
5969/5969 - 298s - loss: 5.5039e-04 - val_loss: 0.0012 - 298s/epoch - 50ms/step
Epoch 165/300
5969/5969 - 298s - loss: 5.4507e-04 - val_loss: 0.0012 - 298s/epoch - 50ms/step
Epoch 166/300
5969/5969 - 298s - loss: 5.3723e-04 - val_loss: 0.0011 - 298s/epoch - 50ms/step
Epoch 167/300
5969/5969 - 298s - loss: 5.5858e-04 - val_loss: 6.1494e-04 - 298s/epoch - 50ms/step
Epoch 168/300
5969/5969 - 298s - loss: 5.4893e-04 - val_loss: 6.0226e-04 - 298s/epoch - 50ms/step
Epoch 169/300
5969/5969 - 298s - loss: 5.8249e-04 - val_loss: 0.0022 - 298s/epoch - 50ms/step
Epoch 170/300
5969/5969 - 298s - loss: 5.5386e-04 - val_loss: 7.0895e-04 - 298s/epoch - 50ms/step
Epoch 171/300
5969/5969 - 298s - loss: 5.5267e-04 - val_loss: 0.0011 - 298s/epoch - 50ms/step
Epoch 172/300
5969/5969 - 298s - loss: 5.4024e-04 - val_loss: 0.0017 - 298s/epoch - 50ms/step
Epoch 173/300
5969/5969 - 298s - loss: 5.4974e-04 - val_loss: 9.1039e-04 - 298s/epoch - 50ms/step
Epoch 174/300
5969/5969 - 298s - loss: 5.5554e-04 - val_loss: 8.4988e-04 - 298s/epoch - 50ms/step
Epoch 175/300
5969/5969 - 298s - loss: 5.4052e-04 - val_loss: 9.5755e-04 - 298s/epoch - 50ms/step
Epoch 176/300
5969/5969 - 298s - loss: 5.3624e-04 - val_loss: 6.8221e-04 - 298s/epoch - 50ms/step
Epoch 177/300
5969/5969 - 298s - loss: 5.7104e-04 - val_loss: 9.6507e-04 - 298s/epoch - 50ms/step
Epoch 178/300
5969/5969 - 298s - loss: 5.4975e-04 - val_loss: 6.5276e-04 - 298s/epoch - 50ms/step
Epoch 179/300
5969/5969 - 298s - loss: 5.4998e-04 - val_loss: 0.0011 - 298s/epoch - 50ms/step
Epoch 180/300
5969/5969 - 298s - loss: 5.4209e-04 - val_loss: 8.6365e-04 - 298s/epoch - 50ms/step
Epoch 181/300
5969/5969 - 298s - loss: 5.7873e-04 - val_loss: 9.9341e-04 - 298s/epoch - 50ms/step
Epoch 182/300
5969/5969 - 298s - loss: 5.4209e-04 - val_loss: 9.2575e-04 - 298s/epoch - 50ms/step
Epoch 183/300
5969/5969 - 298s - loss: 5.3501e-04 - val_loss: 5.8045e-04 - 298s/epoch - 50ms/step
Epoch 184/300
5969/5969 - 298s - loss: 5.3445e-04 - val_loss: 0.0012 - 298s/epoch - 50ms/step
Epoch 185/300
5969/5969 - 298s - loss: 5.3482e-04 - val_loss: 7.3593e-04 - 298s/epoch - 50ms/step
Epoch 186/300
5969/5969 - 298s - loss: 5.3420e-04 - val_loss: 7.8169e-04 - 298s/epoch - 50ms/step
Epoch 187/300
5969/5969 - 298s - loss: 5.3821e-04 - val_loss: 0.0014 - 298s/epoch - 50ms/step
Epoch 188/300
5969/5969 - 298s - loss: 5.3830e-04 - val_loss: 0.0010 - 298s/epoch - 50ms/step
Epoch 189/300
5969/5969 - 298s - loss: 5.3663e-04 - val_loss: 7.6210e-04 - 298s/epoch - 50ms/step
Epoch 190/300
5969/5969 - 298s - loss: 5.3293e-04 - val_loss: 9.5274e-04 - 298s/epoch - 50ms/step
Epoch 191/300
5969/5969 - 298s - loss: 5.3729e-04 - val_loss: 0.0011 - 298s/epoch - 50ms/step
Epoch 192/300
5969/5969 - 298s - loss: 5.5201e-04 - val_loss: 5.6780e-04 - 298s/epoch - 50ms/step
Epoch 193/300
5969/5969 - 298s - loss: 5.4962e-04 - val_loss: 0.0018 - 298s/epoch - 50ms/step
Epoch 194/300
5969/5969 - 298s - loss: 5.3657e-04 - val_loss: 9.1666e-04 - 298s/epoch - 50ms/step
Epoch 195/300
5969/5969 - 298s - loss: 5.4853e-04 - val_loss: 8.0360e-04 - 298s/epoch - 50ms/step
Epoch 196/300
5969/5969 - 298s - loss: 5.2877e-04 - val_loss: 0.0013 - 298s/epoch - 50ms/step
Epoch 197/300
5969/5969 - 298s - loss: 5.4038e-04 - val_loss: 0.0011 - 298s/epoch - 50ms/step
Epoch 198/300
5969/5969 - 298s - loss: 5.2591e-04 - val_loss: 8.8626e-04 - 298s/epoch - 50ms/step
Epoch 199/300
5969/5969 - 298s - loss: 5.2663e-04 - val_loss: 0.0012 - 298s/epoch - 50ms/step
Epoch 200/300
5969/5969 - 298s - loss: 5.3215e-04 - val_loss: 0.0015 - 298s/epoch - 50ms/step
Epoch 201/300
5969/5969 - 298s - loss: 5.4339e-04 - val_loss: 8.8381e-04 - 298s/epoch - 50ms/step
Epoch 202/300
5969/5969 - 298s - loss: 5.2932e-04 - val_loss: 8.1218e-04 - 298s/epoch - 50ms/step
Epoch 203/300
5969/5969 - 298s - loss: 5.4190e-04 - val_loss: 6.9289e-04 - 298s/epoch - 50ms/step
Epoch 204/300
5969/5969 - 298s - loss: 5.3801e-04 - val_loss: 0.0012 - 298s/epoch - 50ms/step
Epoch 205/300
5969/5969 - 298s - loss: 5.3956e-04 - val_loss: 0.0019 - 298s/epoch - 50ms/step
Epoch 206/300
5969/5969 - 298s - loss: 5.7079e-04 - val_loss: 0.0018 - 298s/epoch - 50ms/step
Epoch 207/300
5969/5969 - 298s - loss: 5.5199e-04 - val_loss: 9.6863e-04 - 298s/epoch - 50ms/step
Epoch 208/300
5969/5969 - 298s - loss: 5.2911e-04 - val_loss: 5.3453e-04 - 298s/epoch - 50ms/step
Epoch 209/300
5969/5969 - 298s - loss: 5.3423e-04 - val_loss: 0.0011 - 298s/epoch - 50ms/step
Epoch 210/300
5969/5969 - 298s - loss: 5.4626e-04 - val_loss: 8.4800e-04 - 298s/epoch - 50ms/step
Epoch 211/300
5969/5969 - 298s - loss: 5.3703e-04 - val_loss: 6.6862e-04 - 298s/epoch - 50ms/step
Epoch 212/300
5969/5969 - 298s - loss: 5.2327e-04 - val_loss: 8.3269e-04 - 298s/epoch - 50ms/step
Epoch 213/300
5969/5969 - 298s - loss: 5.3339e-04 - val_loss: 8.8516e-04 - 298s/epoch - 50ms/step
Epoch 214/300
5969/5969 - 298s - loss: 5.2182e-04 - val_loss: 0.0016 - 298s/epoch - 50ms/step
Epoch 215/300
5969/5969 - 298s - loss: 5.3740e-04 - val_loss: 6.4045e-04 - 298s/epoch - 50ms/step
Epoch 216/300
5969/5969 - 298s - loss: 5.4133e-04 - val_loss: 0.0014 - 298s/epoch - 50ms/step
Epoch 217/300
5969/5969 - 298s - loss: 5.2420e-04 - val_loss: 0.0010 - 298s/epoch - 50ms/step
Epoch 218/300
5969/5969 - 298s - loss: 5.2797e-04 - val_loss: 0.0010 - 298s/epoch - 50ms/step
Epoch 219/300
5969/5969 - 298s - loss: 5.2954e-04 - val_loss: 0.0014 - 298s/epoch - 50ms/step
Epoch 220/300
5969/5969 - 298s - loss: 5.3276e-04 - val_loss: 0.0016 - 298s/epoch - 50ms/step
Epoch 221/300
5969/5969 - 298s - loss: 5.2403e-04 - val_loss: 6.8214e-04 - 298s/epoch - 50ms/step
Epoch 222/300
5969/5969 - 298s - loss: 5.3140e-04 - val_loss: 7.1588e-04 - 298s/epoch - 50ms/step
Epoch 223/300
5969/5969 - 298s - loss: 5.3979e-04 - val_loss: 0.0010 - 298s/epoch - 50ms/step
Epoch 224/300
5969/5969 - 298s - loss: 5.2683e-04 - val_loss: 0.0011 - 298s/epoch - 50ms/step
Epoch 225/300
5969/5969 - 298s - loss: 5.2689e-04 - val_loss: 0.0029 - 298s/epoch - 50ms/step
Epoch 226/300
5969/5969 - 298s - loss: 5.3176e-04 - val_loss: 0.0015 - 298s/epoch - 50ms/step
Epoch 227/300
5969/5969 - 298s - loss: 5.2802e-04 - val_loss: 0.0016 - 298s/epoch - 50ms/step
Epoch 228/300
5969/5969 - 298s - loss: 5.2934e-04 - val_loss: 0.0021 - 298s/epoch - 50ms/step
Epoch 229/300
5969/5969 - 298s - loss: 5.2276e-04 - val_loss: 9.3887e-04 - 298s/epoch - 50ms/step
Epoch 230/300
5969/5969 - 298s - loss: 5.1413e-04 - val_loss: 9.9183e-04 - 298s/epoch - 50ms/step
Epoch 231/300
5969/5969 - 298s - loss: 5.3516e-04 - val_loss: 0.0033 - 298s/epoch - 50ms/step
Epoch 232/300
5969/5969 - 298s - loss: 5.2600e-04 - val_loss: 0.0012 - 298s/epoch - 50ms/step
Epoch 233/300
5969/5969 - 298s - loss: 5.2927e-04 - val_loss: 0.0018 - 298s/epoch - 50ms/step
Epoch 234/300
5969/5969 - 298s - loss: 5.2334e-04 - val_loss: 0.0017 - 298s/epoch - 50ms/step
Epoch 235/300
5969/5969 - 298s - loss: 5.1430e-04 - val_loss: 0.0013 - 298s/epoch - 50ms/step
Epoch 236/300
5969/5969 - 298s - loss: 5.1578e-04 - val_loss: 0.0016 - 298s/epoch - 50ms/step
Epoch 237/300
5969/5969 - 298s - loss: 5.1477e-04 - val_loss: 0.0020 - 298s/epoch - 50ms/step
Epoch 238/300
5969/5969 - 298s - loss: 5.2336e-04 - val_loss: 0.0010 - 298s/epoch - 50ms/step
Epoch 239/300
5969/5969 - 298s - loss: 5.1830e-04 - val_loss: 0.0013 - 298s/epoch - 50ms/step
Epoch 240/300
5969/5969 - 298s - loss: 5.2661e-04 - val_loss: 0.0010 - 298s/epoch - 50ms/step
Epoch 241/300
5969/5969 - 298s - loss: 5.1877e-04 - val_loss: 0.0017 - 298s/epoch - 50ms/step
Epoch 242/300
5969/5969 - 298s - loss: 5.5283e-04 - val_loss: 0.0019 - 298s/epoch - 50ms/step
Epoch 243/300
5969/5969 - 298s - loss: 5.1377e-04 - val_loss: 0.0015 - 298s/epoch - 50ms/step
Epoch 244/300
5969/5969 - 298s - loss: 5.2538e-04 - val_loss: 0.0014 - 298s/epoch - 50ms/step
Epoch 245/300
5969/5969 - 298s - loss: 5.0989e-04 - val_loss: 0.0033 - 298s/epoch - 50ms/step
Epoch 246/300
5969/5969 - 298s - loss: 5.4468e-04 - val_loss: 0.0018 - 298s/epoch - 50ms/step
Epoch 247/300
5969/5969 - 298s - loss: 5.1685e-04 - val_loss: 7.6065e-04 - 298s/epoch - 50ms/step
Epoch 248/300
5969/5969 - 298s - loss: 5.1265e-04 - val_loss: 0.0023 - 298s/epoch - 50ms/step
Epoch 249/300
5969/5969 - 298s - loss: 5.1863e-04 - val_loss: 0.0025 - 298s/epoch - 50ms/step
Epoch 250/300
5969/5969 - 298s - loss: 5.1517e-04 - val_loss: 0.0025 - 298s/epoch - 50ms/step
Epoch 251/300
5969/5969 - 298s - loss: 5.0699e-04 - val_loss: 0.0015 - 298s/epoch - 50ms/step
Epoch 252/300
5969/5969 - 298s - loss: 5.1310e-04 - val_loss: 0.0023 - 298s/epoch - 50ms/step
Epoch 253/300
5969/5969 - 298s - loss: 5.4075e-04 - val_loss: 0.0020 - 298s/epoch - 50ms/step
Epoch 254/300
5969/5969 - 298s - loss: 5.0909e-04 - val_loss: 9.3298e-04 - 298s/epoch - 50ms/step
Epoch 255/300
5969/5969 - 298s - loss: 5.3233e-04 - val_loss: 0.0014 - 298s/epoch - 50ms/step
Epoch 256/300
5969/5969 - 298s - loss: 5.1106e-04 - val_loss: 0.0014 - 298s/epoch - 50ms/step
Epoch 257/300
5969/5969 - 298s - loss: 5.3313e-04 - val_loss: 0.0015 - 298s/epoch - 50ms/step
Epoch 258/300
5969/5969 - 298s - loss: 5.1296e-04 - val_loss: 0.0013 - 298s/epoch - 50ms/step
Epoch 259/300
5969/5969 - 298s - loss: 5.0677e-04 - val_loss: 0.0017 - 298s/epoch - 50ms/step
Epoch 260/300
5969/5969 - 298s - loss: 5.1204e-04 - val_loss: 0.0011 - 298s/epoch - 50ms/step
Epoch 261/300
5969/5969 - 298s - loss: 5.1462e-04 - val_loss: 0.0023 - 298s/epoch - 50ms/step
Epoch 262/300
5969/5969 - 298s - loss: 5.1515e-04 - val_loss: 0.0025 - 298s/epoch - 50ms/step
Epoch 263/300
5969/5969 - 298s - loss: 5.1129e-04 - val_loss: 0.0014 - 298s/epoch - 50ms/step
Epoch 264/300
5969/5969 - 298s - loss: 5.1013e-04 - val_loss: 0.0024 - 298s/epoch - 50ms/step
Epoch 265/300
5969/5969 - 298s - loss: 5.0959e-04 - val_loss: 0.0018 - 298s/epoch - 50ms/step
Epoch 266/300
5969/5969 - 298s - loss: 5.0864e-04 - val_loss: 0.0028 - 298s/epoch - 50ms/step
Epoch 267/300
5969/5969 - 298s - loss: 5.0872e-04 - val_loss: 0.0019 - 298s/epoch - 50ms/step
Epoch 268/300
5969/5969 - 297s - loss: 5.1849e-04 - val_loss: 0.0045 - 297s/epoch - 50ms/step
Epoch 269/300
5969/5969 - 296s - loss: 5.1517e-04 - val_loss: 0.0019 - 296s/epoch - 50ms/step
Epoch 270/300
5969/5969 - 296s - loss: 5.0892e-04 - val_loss: 0.0020 - 296s/epoch - 50ms/step
Epoch 271/300
5969/5969 - 296s - loss: 5.0462e-04 - val_loss: 0.0020 - 296s/epoch - 50ms/step
Epoch 272/300
5969/5969 - 296s - loss: 5.1097e-04 - val_loss: 0.0017 - 296s/epoch - 50ms/step
Epoch 273/300
5969/5969 - 296s - loss: 5.1016e-04 - val_loss: 0.0018 - 296s/epoch - 50ms/step
Epoch 274/300
5969/5969 - 297s - loss: 5.3604e-04 - val_loss: 0.0023 - 297s/epoch - 50ms/step
Epoch 275/300
5969/5969 - 296s - loss: 5.2132e-04 - val_loss: 0.0015 - 296s/epoch - 50ms/step
Epoch 276/300
5969/5969 - 296s - loss: 4.9830e-04 - val_loss: 0.0013 - 296s/epoch - 50ms/step
Epoch 277/300
5969/5969 - 296s - loss: 5.1152e-04 - val_loss: 0.0015 - 296s/epoch - 50ms/step
Epoch 278/300
5969/5969 - 295s - loss: 5.0162e-04 - val_loss: 0.0030 - 295s/epoch - 49ms/step
Epoch 279/300
5969/5969 - 295s - loss: 5.2222e-04 - val_loss: 0.0039 - 295s/epoch - 49ms/step
Epoch 280/300
5969/5969 - 295s - loss: 5.0622e-04 - val_loss: 0.0066 - 295s/epoch - 49ms/step
Epoch 281/300
5969/5969 - 295s - loss: 5.0246e-04 - val_loss: 0.0019 - 295s/epoch - 49ms/step
Epoch 282/300
5969/5969 - 295s - loss: 5.1613e-04 - val_loss: 0.0029 - 295s/epoch - 49ms/step
Epoch 283/300
5969/5969 - 295s - loss: 5.0990e-04 - val_loss: 0.0039 - 295s/epoch - 49ms/step
Epoch 284/300
5969/5969 - 295s - loss: 5.0496e-04 - val_loss: 0.0014 - 295s/epoch - 49ms/step
Epoch 285/300
5969/5969 - 295s - loss: 4.9888e-04 - val_loss: 0.0032 - 295s/epoch - 49ms/step
Epoch 286/300
5969/5969 - 295s - loss: 5.0818e-04 - val_loss: 0.0028 - 295s/epoch - 49ms/step
Epoch 287/300
5969/5969 - 295s - loss: 5.1849e-04 - val_loss: 0.0032 - 295s/epoch - 49ms/step
Epoch 288/300
5969/5969 - 295s - loss: 5.1932e-04 - val_loss: 0.0021 - 295s/epoch - 49ms/step
Epoch 289/300
5969/5969 - 295s - loss: 5.0647e-04 - val_loss: 0.0022 - 295s/epoch - 49ms/step
Epoch 290/300
5969/5969 - 295s - loss: 5.2446e-04 - val_loss: 0.0025 - 295s/epoch - 49ms/step
Epoch 291/300
5969/5969 - 295s - loss: 5.1600e-04 - val_loss: 0.0020 - 295s/epoch - 49ms/step
Epoch 292/300
5969/5969 - 295s - loss: 5.0577e-04 - val_loss: 0.0031 - 295s/epoch - 49ms/step
Epoch 293/300
5969/5969 - 295s - loss: 5.0127e-04 - val_loss: 0.0042 - 295s/epoch - 49ms/step
Epoch 294/300
5969/5969 - 295s - loss: 5.0333e-04 - val_loss: 0.0026 - 295s/epoch - 49ms/step
Epoch 295/300
5969/5969 - 295s - loss: 5.0703e-04 - val_loss: 0.0016 - 295s/epoch - 49ms/step
Epoch 296/300
5969/5969 - 295s - loss: 5.1822e-04 - val_loss: 0.0022 - 295s/epoch - 49ms/step
Epoch 297/300
5969/5969 - 295s - loss: 5.1260e-04 - val_loss: 0.0035 - 295s/epoch - 49ms/step
Epoch 298/300
5969/5969 - 295s - loss: 5.0034e-04 - val_loss: 0.0024 - 295s/epoch - 49ms/step
Epoch 299/300
5969/5969 - 295s - loss: 5.1977e-04 - val_loss: 0.0023 - 295s/epoch - 49ms/step
Epoch 300/300
5969/5969 - 295s - loss: 5.1854e-04 - val_loss: 0.0017 - 295s/epoch - 49ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0016863084165379405
  1/332 [..............................] - ETA: 32s  7/332 [..............................] - ETA: 2s  14/332 [>.............................] - ETA: 2s 21/332 [>.............................] - ETA: 2s 28/332 [=>............................] - ETA: 2s 35/332 [==>...........................] - ETA: 2s 42/332 [==>...........................] - ETA: 2s 49/332 [===>..........................] - ETA: 2s 56/332 [====>.........................] - ETA: 2s 63/332 [====>.........................] - ETA: 2s 70/332 [=====>........................] - ETA: 2s 77/332 [=====>........................] - ETA: 2s 84/332 [======>.......................] - ETA: 2s 91/332 [=======>......................] - ETA: 1s 98/332 [=======>......................] - ETA: 1s105/332 [========>.....................] - ETA: 1s112/332 [=========>....................] - ETA: 1s119/332 [=========>....................] - ETA: 1s126/332 [==========>...................] - ETA: 1s133/332 [===========>..................] - ETA: 1s140/332 [===========>..................] - ETA: 1s147/332 [============>.................] - ETA: 1s154/332 [============>.................] - ETA: 1s161/332 [=============>................] - ETA: 1s168/332 [==============>...............] - ETA: 1s175/332 [==============>...............] - ETA: 1s182/332 [===============>..............] - ETA: 1s189/332 [================>.............] - ETA: 1s196/332 [================>.............] - ETA: 1s203/332 [=================>............] - ETA: 1s210/332 [=================>............] - ETA: 0s217/332 [==================>...........] - ETA: 0s224/332 [===================>..........] - ETA: 0s231/332 [===================>..........] - ETA: 0s238/332 [====================>.........] - ETA: 0s245/332 [=====================>........] - ETA: 0s252/332 [=====================>........] - ETA: 0s259/332 [======================>.......] - ETA: 0s266/332 [=======================>......] - ETA: 0s273/332 [=======================>......] - ETA: 0s280/332 [========================>.....] - ETA: 0s287/332 [========================>.....] - ETA: 0s294/332 [=========================>....] - ETA: 0s301/332 [==========================>...] - ETA: 0s308/332 [==========================>...] - ETA: 0s315/332 [===========================>..] - ETA: 0s322/332 [============================>.] - ETA: 0s329/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 8ms/step
correlation 0.009827901507789937
cosine 0.008097297853561693
MAE: 0.012980965
RMSE: 0.041064594
r2: 0.8906064254444094
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 3792)              4796880   
                                                                 
 batch_normalization (BatchN  (None, 3792)             15168     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_2 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 3792)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Encoder
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 3792)              4796880   
                                                                 
 batch_normalization (BatchN  (None, 3792)             15168     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
=================================================================
Total params: 7,209,224
Trainable params: 7,201,640
Non-trainable params: 7,584
_________________________________________________________________
Decoder
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_2 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 3792)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 7,212,384
Trainable params: 7,203,536
Non-trainable params: 8,848
_________________________________________________________________
['3n_b', 'mse', 16, 300, 0.002, 0.5, 632, 0.0005185416084714234, 0.0016863084165379405, 0.009827901507789937, 0.008097297853561693, 0.012980964966118336, 0.04106459394097328, 0.8906064254444094, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_3 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_3 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_5 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 3792)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Epoch 1/100
2985/2985 - 168s - loss: 0.0085 - val_loss: 0.0036 - 168s/epoch - 56ms/step
Epoch 2/100
2985/2985 - 167s - loss: 0.0033 - val_loss: 0.0017 - 167s/epoch - 56ms/step
Epoch 3/100
2985/2985 - 167s - loss: 0.0029 - val_loss: 0.0018 - 167s/epoch - 56ms/step
Epoch 4/100
2985/2985 - 167s - loss: 0.0020 - val_loss: 0.0012 - 167s/epoch - 56ms/step
Epoch 5/100
2985/2985 - 167s - loss: 0.0014 - val_loss: 9.1471e-04 - 167s/epoch - 56ms/step
Epoch 6/100
2985/2985 - 167s - loss: 0.0011 - val_loss: 9.3879e-04 - 167s/epoch - 56ms/step
Epoch 7/100
2985/2985 - 167s - loss: 9.1009e-04 - val_loss: 6.5685e-04 - 167s/epoch - 56ms/step
Epoch 8/100
2985/2985 - 167s - loss: 7.8236e-04 - val_loss: 6.1096e-04 - 167s/epoch - 56ms/step
Epoch 9/100
2985/2985 - 167s - loss: 6.8438e-04 - val_loss: 5.0568e-04 - 167s/epoch - 56ms/step
Epoch 10/100
2985/2985 - 167s - loss: 6.1828e-04 - val_loss: 4.7451e-04 - 167s/epoch - 56ms/step
Epoch 11/100
2985/2985 - 167s - loss: 5.7282e-04 - val_loss: 4.7224e-04 - 167s/epoch - 56ms/step
Epoch 12/100
2985/2985 - 167s - loss: 5.1807e-04 - val_loss: 0.0010 - 167s/epoch - 56ms/step
Epoch 13/100
2985/2985 - 167s - loss: 5.5081e-04 - val_loss: 4.2358e-04 - 167s/epoch - 56ms/step
Epoch 14/100
2985/2985 - 167s - loss: 4.7294e-04 - val_loss: 4.1119e-04 - 167s/epoch - 56ms/step
Epoch 15/100
2985/2985 - 167s - loss: 4.4416e-04 - val_loss: 3.8679e-04 - 167s/epoch - 56ms/step
Epoch 16/100
2985/2985 - 167s - loss: 4.1888e-04 - val_loss: 3.8335e-04 - 167s/epoch - 56ms/step
Epoch 17/100
2985/2985 - 167s - loss: 4.0259e-04 - val_loss: 3.3990e-04 - 167s/epoch - 56ms/step
Epoch 18/100
2985/2985 - 167s - loss: 3.8688e-04 - val_loss: 4.6951e-04 - 167s/epoch - 56ms/step
Epoch 19/100
2985/2985 - 168s - loss: 3.6897e-04 - val_loss: 3.4055e-04 - 168s/epoch - 56ms/step
Epoch 20/100
2985/2985 - 168s - loss: 3.9507e-04 - val_loss: 2.9940e-04 - 168s/epoch - 56ms/step
Epoch 21/100
2985/2985 - 167s - loss: 3.4371e-04 - val_loss: 2.9259e-04 - 167s/epoch - 56ms/step
Epoch 22/100
2985/2985 - 167s - loss: 3.4117e-04 - val_loss: 3.1848e-04 - 167s/epoch - 56ms/step
Epoch 23/100
2985/2985 - 167s - loss: 3.3125e-04 - val_loss: 3.3883e-04 - 167s/epoch - 56ms/step
Epoch 24/100
2985/2985 - 167s - loss: 3.2835e-04 - val_loss: 2.8164e-04 - 167s/epoch - 56ms/step
Epoch 25/100
2985/2985 - 167s - loss: 3.0782e-04 - val_loss: 2.6641e-04 - 167s/epoch - 56ms/step
Epoch 26/100
2985/2985 - 167s - loss: 2.9946e-04 - val_loss: 2.4909e-04 - 167s/epoch - 56ms/step
Epoch 27/100
2985/2985 - 167s - loss: 3.2103e-04 - val_loss: 2.5076e-04 - 167s/epoch - 56ms/step
Epoch 28/100
2985/2985 - 168s - loss: 2.8833e-04 - val_loss: 2.4069e-04 - 168s/epoch - 56ms/step
Epoch 29/100
2985/2985 - 167s - loss: 2.7987e-04 - val_loss: 2.4667e-04 - 167s/epoch - 56ms/step
Epoch 30/100
2985/2985 - 167s - loss: 2.8680e-04 - val_loss: 2.4982e-04 - 167s/epoch - 56ms/step
Epoch 31/100
2985/2985 - 167s - loss: 2.7013e-04 - val_loss: 2.4380e-04 - 167s/epoch - 56ms/step
Epoch 32/100
2985/2985 - 167s - loss: 2.7126e-04 - val_loss: 2.2007e-04 - 167s/epoch - 56ms/step
Epoch 33/100
2985/2985 - 167s - loss: 2.6182e-04 - val_loss: 2.3214e-04 - 167s/epoch - 56ms/step
Epoch 34/100
2985/2985 - 167s - loss: 2.7925e-04 - val_loss: 2.1550e-04 - 167s/epoch - 56ms/step
Epoch 35/100
2985/2985 - 167s - loss: 2.5582e-04 - val_loss: 2.1725e-04 - 167s/epoch - 56ms/step
Epoch 36/100
2985/2985 - 167s - loss: 2.4842e-04 - val_loss: 2.1595e-04 - 167s/epoch - 56ms/step
Epoch 37/100
2985/2985 - 167s - loss: 2.5533e-04 - val_loss: 2.1947e-04 - 167s/epoch - 56ms/step
Epoch 38/100
2985/2985 - 167s - loss: 2.4734e-04 - val_loss: 2.0606e-04 - 167s/epoch - 56ms/step
Epoch 39/100
2985/2985 - 167s - loss: 2.4313e-04 - val_loss: 2.0784e-04 - 167s/epoch - 56ms/step
Epoch 40/100
2985/2985 - 167s - loss: 2.3510e-04 - val_loss: 2.1542e-04 - 167s/epoch - 56ms/step
Epoch 41/100
2985/2985 - 167s - loss: 2.4746e-04 - val_loss: 2.2003e-04 - 167s/epoch - 56ms/step
Epoch 42/100
2985/2985 - 167s - loss: 2.3230e-04 - val_loss: 1.9478e-04 - 167s/epoch - 56ms/step
Epoch 43/100
2985/2985 - 167s - loss: 2.2425e-04 - val_loss: 2.1015e-04 - 167s/epoch - 56ms/step
Epoch 44/100
2985/2985 - 167s - loss: 2.2525e-04 - val_loss: 3.3014e-04 - 167s/epoch - 56ms/step
Epoch 45/100
2985/2985 - 167s - loss: 2.5268e-04 - val_loss: 1.9753e-04 - 167s/epoch - 56ms/step
Epoch 46/100
2985/2985 - 167s - loss: 2.2100e-04 - val_loss: 1.7925e-04 - 167s/epoch - 56ms/step
Epoch 47/100
2985/2985 - 167s - loss: 2.2043e-04 - val_loss: 1.9692e-04 - 167s/epoch - 56ms/step
Epoch 48/100
2985/2985 - 167s - loss: 2.1694e-04 - val_loss: 1.8479e-04 - 167s/epoch - 56ms/step
Epoch 49/100
2985/2985 - 167s - loss: 2.1722e-04 - val_loss: 2.0445e-04 - 167s/epoch - 56ms/step
Epoch 50/100
2985/2985 - 167s - loss: 2.1595e-04 - val_loss: 2.4128e-04 - 167s/epoch - 56ms/step
Epoch 51/100
2985/2985 - 167s - loss: 2.2138e-04 - val_loss: 1.7617e-04 - 167s/epoch - 56ms/step
Epoch 52/100
2985/2985 - 167s - loss: 2.0664e-04 - val_loss: 1.7391e-04 - 167s/epoch - 56ms/step
Epoch 53/100
2985/2985 - 167s - loss: 2.0476e-04 - val_loss: 1.7942e-04 - 167s/epoch - 56ms/step
Epoch 54/100
2985/2985 - 167s - loss: 2.0169e-04 - val_loss: 1.7282e-04 - 167s/epoch - 56ms/step
Epoch 55/100
2985/2985 - 167s - loss: 1.9870e-04 - val_loss: 1.7586e-04 - 167s/epoch - 56ms/step
Epoch 56/100
2985/2985 - 167s - loss: 2.0180e-04 - val_loss: 1.6390e-04 - 167s/epoch - 56ms/step
Epoch 57/100
2985/2985 - 167s - loss: 1.9548e-04 - val_loss: 1.7494e-04 - 167s/epoch - 56ms/step
Epoch 58/100
2985/2985 - 167s - loss: 1.9562e-04 - val_loss: 1.7939e-04 - 167s/epoch - 56ms/step
Epoch 59/100
2985/2985 - 167s - loss: 1.9156e-04 - val_loss: 1.7131e-04 - 167s/epoch - 56ms/step
Epoch 60/100
2985/2985 - 167s - loss: 2.2159e-04 - val_loss: 1.6093e-04 - 167s/epoch - 56ms/step
Epoch 61/100
2985/2985 - 167s - loss: 1.9199e-04 - val_loss: 1.6138e-04 - 167s/epoch - 56ms/step
Epoch 62/100
2985/2985 - 167s - loss: 1.9137e-04 - val_loss: 1.7719e-04 - 167s/epoch - 56ms/step
Epoch 63/100
2985/2985 - 167s - loss: 1.9097e-04 - val_loss: 5.2472e-04 - 167s/epoch - 56ms/step
Epoch 64/100
2985/2985 - 167s - loss: 1.9173e-04 - val_loss: 1.5923e-04 - 167s/epoch - 56ms/step
Epoch 65/100
2985/2985 - 167s - loss: 1.8650e-04 - val_loss: 1.6665e-04 - 167s/epoch - 56ms/step
Epoch 66/100
2985/2985 - 167s - loss: 1.8300e-04 - val_loss: 1.6642e-04 - 167s/epoch - 56ms/step
Epoch 67/100
2985/2985 - 167s - loss: 1.8317e-04 - val_loss: 1.5401e-04 - 167s/epoch - 56ms/step
Epoch 68/100
2985/2985 - 167s - loss: 1.8389e-04 - val_loss: 1.6900e-04 - 167s/epoch - 56ms/step
Epoch 69/100
2985/2985 - 167s - loss: 1.8637e-04 - val_loss: 1.6720e-04 - 167s/epoch - 56ms/step
Epoch 70/100
2985/2985 - 167s - loss: 1.7960e-04 - val_loss: 1.8108e-04 - 167s/epoch - 56ms/step
Epoch 71/100
2985/2985 - 167s - loss: 1.8303e-04 - val_loss: 1.5778e-04 - 167s/epoch - 56ms/step
Epoch 72/100
2985/2985 - 167s - loss: 1.7862e-04 - val_loss: 1.5355e-04 - 167s/epoch - 56ms/step
Epoch 73/100
2985/2985 - 167s - loss: 1.7831e-04 - val_loss: 1.6040e-04 - 167s/epoch - 56ms/step
Epoch 74/100
2985/2985 - 167s - loss: 1.7395e-04 - val_loss: 1.5300e-04 - 167s/epoch - 56ms/step
Epoch 75/100
2985/2985 - 167s - loss: 1.8044e-04 - val_loss: 1.5311e-04 - 167s/epoch - 56ms/step
Epoch 76/100
2985/2985 - 167s - loss: 1.7275e-04 - val_loss: 1.5776e-04 - 167s/epoch - 56ms/step
Epoch 77/100
2985/2985 - 167s - loss: 1.9235e-04 - val_loss: 2.8846e-04 - 167s/epoch - 56ms/step
Epoch 78/100
2985/2985 - 167s - loss: 2.0950e-04 - val_loss: 1.5322e-04 - 167s/epoch - 56ms/step
Epoch 79/100
2985/2985 - 167s - loss: 1.7631e-04 - val_loss: 1.6866e-04 - 167s/epoch - 56ms/step
Epoch 80/100
2985/2985 - 167s - loss: 1.7754e-04 - val_loss: 1.6233e-04 - 167s/epoch - 56ms/step
Epoch 81/100
2985/2985 - 167s - loss: 1.7264e-04 - val_loss: 1.7236e-04 - 167s/epoch - 56ms/step
Epoch 82/100
2985/2985 - 167s - loss: 1.7954e-04 - val_loss: 1.4925e-04 - 167s/epoch - 56ms/step
Epoch 83/100
2985/2985 - 167s - loss: 1.7033e-04 - val_loss: 1.5210e-04 - 167s/epoch - 56ms/step
Epoch 84/100
2985/2985 - 167s - loss: 1.6839e-04 - val_loss: 1.4280e-04 - 167s/epoch - 56ms/step
Epoch 85/100
2985/2985 - 167s - loss: 1.7691e-04 - val_loss: 1.4561e-04 - 167s/epoch - 56ms/step
Epoch 86/100
2985/2985 - 167s - loss: 1.6716e-04 - val_loss: 1.5298e-04 - 167s/epoch - 56ms/step
Epoch 87/100
2985/2985 - 167s - loss: 1.6665e-04 - val_loss: 1.6096e-04 - 167s/epoch - 56ms/step
Epoch 88/100
2985/2985 - 167s - loss: 1.6516e-04 - val_loss: 1.8955e-04 - 167s/epoch - 56ms/step
Epoch 89/100
2985/2985 - 167s - loss: 2.0080e-04 - val_loss: 1.4815e-04 - 167s/epoch - 56ms/step
Epoch 90/100
2985/2985 - 167s - loss: 1.6684e-04 - val_loss: 1.5743e-04 - 167s/epoch - 56ms/step
Epoch 91/100
2985/2985 - 167s - loss: 1.6200e-04 - val_loss: 1.5858e-04 - 167s/epoch - 56ms/step
Epoch 92/100
2985/2985 - 167s - loss: 1.6482e-04 - val_loss: 1.7312e-04 - 167s/epoch - 56ms/step
Epoch 93/100
2985/2985 - 167s - loss: 1.6405e-04 - val_loss: 1.5489e-04 - 167s/epoch - 56ms/step
Epoch 94/100
2985/2985 - 167s - loss: 1.5945e-04 - val_loss: 1.3734e-04 - 167s/epoch - 56ms/step
Epoch 95/100
2985/2985 - 167s - loss: 1.6142e-04 - val_loss: 1.5153e-04 - 167s/epoch - 56ms/step
Epoch 96/100
2985/2985 - 167s - loss: 1.6167e-04 - val_loss: 1.5270e-04 - 167s/epoch - 56ms/step
Epoch 97/100
2985/2985 - 167s - loss: 1.7963e-04 - val_loss: 1.4798e-04 - 167s/epoch - 56ms/step
Epoch 98/100
2985/2985 - 167s - loss: 1.5927e-04 - val_loss: 1.4014e-04 - 167s/epoch - 56ms/step
Epoch 99/100
2985/2985 - 167s - loss: 1.5786e-04 - val_loss: 1.4364e-04 - 167s/epoch - 56ms/step
Epoch 100/100
2985/2985 - 167s - loss: 1.6611e-04 - val_loss: 1.3829e-04 - 167s/epoch - 56ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00013828511873725802
  1/332 [..............................] - ETA: 31s  7/332 [..............................] - ETA: 2s  14/332 [>.............................] - ETA: 2s 21/332 [>.............................] - ETA: 2s 28/332 [=>............................] - ETA: 2s 35/332 [==>...........................] - ETA: 2s 42/332 [==>...........................] - ETA: 2s 49/332 [===>..........................] - ETA: 2s 56/332 [====>.........................] - ETA: 2s 63/332 [====>.........................] - ETA: 2s 70/332 [=====>........................] - ETA: 2s 77/332 [=====>........................] - ETA: 2s 84/332 [======>.......................] - ETA: 2s 91/332 [=======>......................] - ETA: 1s 98/332 [=======>......................] - ETA: 1s105/332 [========>.....................] - ETA: 1s112/332 [=========>....................] - ETA: 1s119/332 [=========>....................] - ETA: 1s126/332 [==========>...................] - ETA: 1s133/332 [===========>..................] - ETA: 1s140/332 [===========>..................] - ETA: 1s147/332 [============>.................] - ETA: 1s154/332 [============>.................] - ETA: 1s161/332 [=============>................] - ETA: 1s168/332 [==============>...............] - ETA: 1s175/332 [==============>...............] - ETA: 1s182/332 [===============>..............] - ETA: 1s189/332 [================>.............] - ETA: 1s196/332 [================>.............] - ETA: 1s203/332 [=================>............] - ETA: 1s210/332 [=================>............] - ETA: 0s217/332 [==================>...........] - ETA: 0s224/332 [===================>..........] - ETA: 0s231/332 [===================>..........] - ETA: 0s238/332 [====================>.........] - ETA: 0s245/332 [=====================>........] - ETA: 0s252/332 [=====================>........] - ETA: 0s259/332 [======================>.......] - ETA: 0s266/332 [=======================>......] - ETA: 0s273/332 [=======================>......] - ETA: 0s280/332 [========================>.....] - ETA: 0s287/332 [========================>.....] - ETA: 0s294/332 [=========================>....] - ETA: 0s301/332 [==========================>...] - ETA: 0s308/332 [==========================>...] - ETA: 0s315/332 [===========================>..] - ETA: 0s322/332 [============================>.] - ETA: 0s329/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 8ms/step
correlation 0.0015575723730300087
cosine 0.0012353313303964345
MAE: 0.006282252
RMSE: 0.011759461
r2: 0.9910298135553982
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_3 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_5 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 3792)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Encoder
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_3 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
=================================================================
Total params: 7,209,224
Trainable params: 7,201,640
Non-trainable params: 7,584
_________________________________________________________________
Decoder
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_5 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 3792)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 7,212,384
Trainable params: 7,203,536
Non-trainable params: 8,848
_________________________________________________________________
['3n_b', 'mse', 32, 100, 0.0005, 0.5, 632, 0.0001661124697420746, 0.00013828511873725802, 0.0015575723730300087, 0.0012353313303964345, 0.0062822517938911915, 0.011759460903704166, 0.9910298135553982, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_6 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_6 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_8 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 3792)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Epoch 1/100
2985/2985 - 165s - loss: 0.0090 - val_loss: 0.0023 - 165s/epoch - 55ms/step
Epoch 2/100
2985/2985 - 164s - loss: 0.0030 - val_loss: 0.0019 - 164s/epoch - 55ms/step
Epoch 3/100
2985/2985 - 164s - loss: 0.0020 - val_loss: 0.0013 - 164s/epoch - 55ms/step
Epoch 4/100
2985/2985 - 164s - loss: 0.0014 - val_loss: 0.0011 - 164s/epoch - 55ms/step
Epoch 5/100
2985/2985 - 164s - loss: 0.0011 - val_loss: 7.9330e-04 - 164s/epoch - 55ms/step
Epoch 6/100
2985/2985 - 164s - loss: 8.6474e-04 - val_loss: 7.0181e-04 - 164s/epoch - 55ms/step
Epoch 7/100
2985/2985 - 164s - loss: 7.4834e-04 - val_loss: 5.9491e-04 - 164s/epoch - 55ms/step
Epoch 8/100
2985/2985 - 164s - loss: 6.6602e-04 - val_loss: 5.4780e-04 - 164s/epoch - 55ms/step
Epoch 9/100
2985/2985 - 164s - loss: 6.1910e-04 - val_loss: 4.9688e-04 - 164s/epoch - 55ms/step
Epoch 10/100
2985/2985 - 164s - loss: 5.7159e-04 - val_loss: 4.6086e-04 - 164s/epoch - 55ms/step
Epoch 11/100
2985/2985 - 164s - loss: 5.3497e-04 - val_loss: 4.2984e-04 - 164s/epoch - 55ms/step
Epoch 12/100
2985/2985 - 164s - loss: 5.2386e-04 - val_loss: 4.7210e-04 - 164s/epoch - 55ms/step
Epoch 13/100
2985/2985 - 164s - loss: 4.9779e-04 - val_loss: 4.0458e-04 - 164s/epoch - 55ms/step
Epoch 14/100
2985/2985 - 164s - loss: 4.6532e-04 - val_loss: 3.7136e-04 - 164s/epoch - 55ms/step
Epoch 15/100
2985/2985 - 164s - loss: 4.4390e-04 - val_loss: 3.7190e-04 - 164s/epoch - 55ms/step
Epoch 16/100
2985/2985 - 164s - loss: 4.2701e-04 - val_loss: 3.7459e-04 - 164s/epoch - 55ms/step
Epoch 17/100
2985/2985 - 164s - loss: 4.2270e-04 - val_loss: 3.4102e-04 - 164s/epoch - 55ms/step
Epoch 18/100
2985/2985 - 164s - loss: 4.0324e-04 - val_loss: 3.4559e-04 - 164s/epoch - 55ms/step
Epoch 19/100
2985/2985 - 164s - loss: 4.4225e-04 - val_loss: 3.5390e-04 - 164s/epoch - 55ms/step
Epoch 20/100
2985/2985 - 164s - loss: 3.9125e-04 - val_loss: 3.1256e-04 - 164s/epoch - 55ms/step
Epoch 21/100
2985/2985 - 164s - loss: 3.7627e-04 - val_loss: 3.0072e-04 - 164s/epoch - 55ms/step
Epoch 22/100
2985/2985 - 164s - loss: 3.6433e-04 - val_loss: 4.9837e-04 - 164s/epoch - 55ms/step
Epoch 23/100
2985/2985 - 164s - loss: 4.1926e-04 - val_loss: 2.8145e-04 - 164s/epoch - 55ms/step
Epoch 24/100
2985/2985 - 164s - loss: 3.5811e-04 - val_loss: 2.9075e-04 - 164s/epoch - 55ms/step
Epoch 25/100
2985/2985 - 164s - loss: 3.4365e-04 - val_loss: 2.8544e-04 - 164s/epoch - 55ms/step
Epoch 26/100
2985/2985 - 164s - loss: 3.6287e-04 - val_loss: 2.7198e-04 - 164s/epoch - 55ms/step
Epoch 27/100
2985/2985 - 164s - loss: 3.2742e-04 - val_loss: 2.6333e-04 - 164s/epoch - 55ms/step
Epoch 28/100
2985/2985 - 164s - loss: 3.3877e-04 - val_loss: 2.5778e-04 - 164s/epoch - 55ms/step
Epoch 29/100
2985/2985 - 164s - loss: 3.1794e-04 - val_loss: 2.6137e-04 - 164s/epoch - 55ms/step
Epoch 30/100
2985/2985 - 164s - loss: 3.2207e-04 - val_loss: 2.7442e-04 - 164s/epoch - 55ms/step
Epoch 31/100
2985/2985 - 164s - loss: 3.1111e-04 - val_loss: 2.5553e-04 - 164s/epoch - 55ms/step
Epoch 32/100
2985/2985 - 164s - loss: 3.1591e-04 - val_loss: 2.3488e-04 - 164s/epoch - 55ms/step
Epoch 33/100
2985/2985 - 164s - loss: 3.0153e-04 - val_loss: 2.4197e-04 - 164s/epoch - 55ms/step
Epoch 34/100
2985/2985 - 164s - loss: 2.9542e-04 - val_loss: 2.3265e-04 - 164s/epoch - 55ms/step
Epoch 35/100
2985/2985 - 164s - loss: 2.9648e-04 - val_loss: 2.4625e-04 - 164s/epoch - 55ms/step
Epoch 36/100
2985/2985 - 164s - loss: 2.8814e-04 - val_loss: 2.3359e-04 - 164s/epoch - 55ms/step
Epoch 37/100
2985/2985 - 164s - loss: 2.9445e-04 - val_loss: 2.3956e-04 - 164s/epoch - 55ms/step
Epoch 38/100
2985/2985 - 164s - loss: 2.8398e-04 - val_loss: 2.2617e-04 - 164s/epoch - 55ms/step
Epoch 39/100
2985/2985 - 164s - loss: 2.7892e-04 - val_loss: 2.2960e-04 - 164s/epoch - 55ms/step
Epoch 40/100
2985/2985 - 164s - loss: 2.7360e-04 - val_loss: 2.4984e-04 - 164s/epoch - 55ms/step
Epoch 41/100
2985/2985 - 164s - loss: 2.7183e-04 - val_loss: 2.1455e-04 - 164s/epoch - 55ms/step
Epoch 42/100
2985/2985 - 164s - loss: 2.6947e-04 - val_loss: 2.2019e-04 - 164s/epoch - 55ms/step
Epoch 43/100
2985/2985 - 164s - loss: 2.8327e-04 - val_loss: 2.2659e-04 - 164s/epoch - 55ms/step
Epoch 44/100
2985/2985 - 164s - loss: 2.6649e-04 - val_loss: 2.4533e-04 - 164s/epoch - 55ms/step
Epoch 45/100
2985/2985 - 164s - loss: 2.7434e-04 - val_loss: 2.1659e-04 - 164s/epoch - 55ms/step
Epoch 46/100
2985/2985 - 164s - loss: 2.6033e-04 - val_loss: 2.1430e-04 - 164s/epoch - 55ms/step
Epoch 47/100
2985/2985 - 164s - loss: 2.5944e-04 - val_loss: 2.0622e-04 - 164s/epoch - 55ms/step
Epoch 48/100
2985/2985 - 164s - loss: 2.6230e-04 - val_loss: 2.1707e-04 - 164s/epoch - 55ms/step
Epoch 49/100
2985/2985 - 164s - loss: 2.5540e-04 - val_loss: 2.1821e-04 - 164s/epoch - 55ms/step
Epoch 50/100
2985/2985 - 164s - loss: 2.5532e-04 - val_loss: 2.9366e-04 - 164s/epoch - 55ms/step
Epoch 51/100
2985/2985 - 164s - loss: 2.7122e-04 - val_loss: 1.9753e-04 - 164s/epoch - 55ms/step
Epoch 52/100
2985/2985 - 164s - loss: 2.5883e-04 - val_loss: 2.0026e-04 - 164s/epoch - 55ms/step
Epoch 53/100
2985/2985 - 164s - loss: 2.5377e-04 - val_loss: 2.0561e-04 - 164s/epoch - 55ms/step
Epoch 54/100
2985/2985 - 164s - loss: 2.4636e-04 - val_loss: 2.0998e-04 - 164s/epoch - 55ms/step
Epoch 55/100
2985/2985 - 164s - loss: 2.4456e-04 - val_loss: 1.9102e-04 - 164s/epoch - 55ms/step
Epoch 56/100
2985/2985 - 164s - loss: 2.4385e-04 - val_loss: 1.9047e-04 - 164s/epoch - 55ms/step
Epoch 57/100
2985/2985 - 164s - loss: 2.4327e-04 - val_loss: 1.9279e-04 - 164s/epoch - 55ms/step
Epoch 58/100
2985/2985 - 164s - loss: 2.4801e-04 - val_loss: 1.9701e-04 - 164s/epoch - 55ms/step
Epoch 59/100
2985/2985 - 164s - loss: 2.3698e-04 - val_loss: 1.9114e-04 - 164s/epoch - 55ms/step
Epoch 60/100
2985/2985 - 164s - loss: 2.4905e-04 - val_loss: 1.9266e-04 - 164s/epoch - 55ms/step
Epoch 61/100
2985/2985 - 164s - loss: 2.4050e-04 - val_loss: 1.9105e-04 - 164s/epoch - 55ms/step
Epoch 62/100
2985/2985 - 164s - loss: 2.3337e-04 - val_loss: 2.0378e-04 - 164s/epoch - 55ms/step
Epoch 63/100
2985/2985 - 164s - loss: 2.3277e-04 - val_loss: 1.8650e-04 - 164s/epoch - 55ms/step
Epoch 64/100
2985/2985 - 164s - loss: 2.3053e-04 - val_loss: 1.7805e-04 - 164s/epoch - 55ms/step
Epoch 65/100
2985/2985 - 164s - loss: 2.3058e-04 - val_loss: 1.9228e-04 - 164s/epoch - 55ms/step
Epoch 66/100
2985/2985 - 164s - loss: 2.2617e-04 - val_loss: 1.9031e-04 - 164s/epoch - 55ms/step
Epoch 67/100
2985/2985 - 164s - loss: 2.2893e-04 - val_loss: 1.7807e-04 - 164s/epoch - 55ms/step
Epoch 68/100
2985/2985 - 164s - loss: 2.3044e-04 - val_loss: 1.9305e-04 - 164s/epoch - 55ms/step
Epoch 69/100
2985/2985 - 164s - loss: 2.7561e-04 - val_loss: 1.8540e-04 - 164s/epoch - 55ms/step
Epoch 70/100
2985/2985 - 164s - loss: 2.3485e-04 - val_loss: 2.7432e-04 - 164s/epoch - 55ms/step
Epoch 71/100
2985/2985 - 164s - loss: 2.3432e-04 - val_loss: 1.8722e-04 - 164s/epoch - 55ms/step
Epoch 72/100
2985/2985 - 164s - loss: 2.2517e-04 - val_loss: 1.7912e-04 - 164s/epoch - 55ms/step
Epoch 73/100
2985/2985 - 164s - loss: 2.3014e-04 - val_loss: 1.7333e-04 - 164s/epoch - 55ms/step
Epoch 74/100
2985/2985 - 164s - loss: 2.2231e-04 - val_loss: 1.8183e-04 - 164s/epoch - 55ms/step
Epoch 75/100
2985/2985 - 164s - loss: 2.2227e-04 - val_loss: 1.7840e-04 - 164s/epoch - 55ms/step
Epoch 76/100
2985/2985 - 164s - loss: 2.1843e-04 - val_loss: 1.7584e-04 - 164s/epoch - 55ms/step
Epoch 77/100
2985/2985 - 164s - loss: 2.6090e-04 - val_loss: 4.1204e-04 - 164s/epoch - 55ms/step
Epoch 78/100
2985/2985 - 164s - loss: 3.2619e-04 - val_loss: 1.8858e-04 - 164s/epoch - 55ms/step
Epoch 79/100
2985/2985 - 164s - loss: 2.3702e-04 - val_loss: 1.9322e-04 - 164s/epoch - 55ms/step
Epoch 80/100
2985/2985 - 164s - loss: 2.3191e-04 - val_loss: 1.8418e-04 - 164s/epoch - 55ms/step
Epoch 81/100
2985/2985 - 164s - loss: 2.2218e-04 - val_loss: 2.0850e-04 - 164s/epoch - 55ms/step
Epoch 82/100
2985/2985 - 164s - loss: 2.2008e-04 - val_loss: 1.8612e-04 - 164s/epoch - 55ms/step
Epoch 83/100
2985/2985 - 164s - loss: 2.1963e-04 - val_loss: 1.7532e-04 - 164s/epoch - 55ms/step
Epoch 84/100
2985/2985 - 164s - loss: 2.1767e-04 - val_loss: 1.7718e-04 - 164s/epoch - 55ms/step
Epoch 85/100
2985/2985 - 164s - loss: 2.2158e-04 - val_loss: 5.0312e-04 - 164s/epoch - 55ms/step
Epoch 86/100
2985/2985 - 164s - loss: 2.3472e-04 - val_loss: 1.7584e-04 - 164s/epoch - 55ms/step
Epoch 87/100
2985/2985 - 164s - loss: 2.2010e-04 - val_loss: 1.8481e-04 - 164s/epoch - 55ms/step
Epoch 88/100
2985/2985 - 164s - loss: 2.1383e-04 - val_loss: 1.8079e-04 - 164s/epoch - 55ms/step
Epoch 89/100
2985/2985 - 164s - loss: 2.1380e-04 - val_loss: 1.7922e-04 - 164s/epoch - 55ms/step
Epoch 90/100
2985/2985 - 164s - loss: 2.3964e-04 - val_loss: 1.7476e-04 - 164s/epoch - 55ms/step
Epoch 91/100
2985/2985 - 164s - loss: 2.1443e-04 - val_loss: 1.7678e-04 - 164s/epoch - 55ms/step
Epoch 92/100
2985/2985 - 164s - loss: 2.1100e-04 - val_loss: 1.8705e-04 - 164s/epoch - 55ms/step
Epoch 93/100
2985/2985 - 164s - loss: 2.0899e-04 - val_loss: 1.8132e-04 - 164s/epoch - 55ms/step
Epoch 94/100
2985/2985 - 164s - loss: 2.0528e-04 - val_loss: 1.5852e-04 - 164s/epoch - 55ms/step
Epoch 95/100
2985/2985 - 164s - loss: 2.0742e-04 - val_loss: 1.6124e-04 - 164s/epoch - 55ms/step
Epoch 96/100
2985/2985 - 164s - loss: 2.0848e-04 - val_loss: 1.8812e-04 - 164s/epoch - 55ms/step
Epoch 97/100
2985/2985 - 164s - loss: 2.0587e-04 - val_loss: 1.6725e-04 - 164s/epoch - 55ms/step
Epoch 98/100
2985/2985 - 164s - loss: 2.1414e-04 - val_loss: 1.7766e-04 - 164s/epoch - 55ms/step
Epoch 99/100
2985/2985 - 164s - loss: 2.1095e-04 - val_loss: 1.7775e-04 - 164s/epoch - 55ms/step
Epoch 100/100
2985/2985 - 164s - loss: 2.0403e-04 - val_loss: 1.7253e-04 - 164s/epoch - 55ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00017252832185477018
  1/332 [..............................] - ETA: 27s  7/332 [..............................] - ETA: 2s  14/332 [>.............................] - ETA: 2s 21/332 [>.............................] - ETA: 2s 28/332 [=>............................] - ETA: 2s 35/332 [==>...........................] - ETA: 2s 42/332 [==>...........................] - ETA: 2s 49/332 [===>..........................] - ETA: 2s 56/332 [====>.........................] - ETA: 2s 63/332 [====>.........................] - ETA: 2s 70/332 [=====>........................] - ETA: 2s 77/332 [=====>........................] - ETA: 2s 84/332 [======>.......................] - ETA: 2s 91/332 [=======>......................] - ETA: 1s 98/332 [=======>......................] - ETA: 1s105/332 [========>.....................] - ETA: 1s112/332 [=========>....................] - ETA: 1s119/332 [=========>....................] - ETA: 1s126/332 [==========>...................] - ETA: 1s133/332 [===========>..................] - ETA: 1s140/332 [===========>..................] - ETA: 1s147/332 [============>.................] - ETA: 1s154/332 [============>.................] - ETA: 1s161/332 [=============>................] - ETA: 1s168/332 [==============>...............] - ETA: 1s175/332 [==============>...............] - ETA: 1s182/332 [===============>..............] - ETA: 1s189/332 [================>.............] - ETA: 1s196/332 [================>.............] - ETA: 1s203/332 [=================>............] - ETA: 1s210/332 [=================>............] - ETA: 0s217/332 [==================>...........] - ETA: 0s224/332 [===================>..........] - ETA: 0s231/332 [===================>..........] - ETA: 0s238/332 [====================>.........] - ETA: 0s245/332 [=====================>........] - ETA: 0s252/332 [=====================>........] - ETA: 0s259/332 [======================>.......] - ETA: 0s266/332 [=======================>......] - ETA: 0s273/332 [=======================>......] - ETA: 0s280/332 [========================>.....] - ETA: 0s287/332 [========================>.....] - ETA: 0s294/332 [=========================>....] - ETA: 0s301/332 [==========================>...] - ETA: 0s308/332 [==========================>...] - ETA: 0s315/332 [===========================>..] - ETA: 0s322/332 [============================>.] - ETA: 0s329/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 8ms/step
correlation 0.001954410689936644
cosine 0.0015515449179193904
MAE: 0.007102121
RMSE: 0.0131349955
r2: 0.9888095878951292
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_6 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_8 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 3792)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Encoder
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_6 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
=================================================================
Total params: 7,209,224
Trainable params: 7,201,640
Non-trainable params: 7,584
_________________________________________________________________
Decoder
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_8 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 3792)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 7,212,384
Trainable params: 7,203,536
Non-trainable params: 8,848
_________________________________________________________________
['3n_b', 'mse', 32, 100, 0.001, 0.5, 632, 0.00020403353846631944, 0.00017252832185477018, 0.001954410689936644, 0.0015515449179193904, 0.0071021211333572865, 0.013134995475411415, 0.9888095878951292, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_9 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_9 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 3792)              2400336   
                                                                 
 batch_normalization_11 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 3792)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Epoch 1/100
2985/2985 - 168s - loss: 0.0161 - val_loss: 0.0069 - 168s/epoch - 56ms/step
Epoch 2/100
2985/2985 - 168s - loss: 0.0049 - val_loss: 0.0031 - 168s/epoch - 56ms/step
Epoch 3/100
2985/2985 - 168s - loss: 0.0030 - val_loss: 0.0020 - 168s/epoch - 56ms/step
Epoch 4/100
2985/2985 - 168s - loss: 0.0021 - val_loss: 0.0016 - 168s/epoch - 56ms/step
Epoch 5/100
2985/2985 - 168s - loss: 0.0017 - val_loss: 0.0013 - 168s/epoch - 56ms/step
Epoch 6/100
2985/2985 - 168s - loss: 0.0015 - val_loss: 0.0012 - 168s/epoch - 56ms/step
Epoch 7/100
2985/2985 - 168s - loss: 0.0013 - val_loss: 0.0010 - 168s/epoch - 56ms/step
Epoch 8/100
2985/2985 - 168s - loss: 0.0012 - val_loss: 9.5191e-04 - 168s/epoch - 56ms/step
Epoch 9/100
2985/2985 - 168s - loss: 0.0011 - val_loss: 8.5549e-04 - 168s/epoch - 56ms/step
Epoch 10/100
2985/2985 - 168s - loss: 0.0011 - val_loss: 8.4289e-04 - 168s/epoch - 56ms/step
Epoch 11/100
2985/2985 - 168s - loss: 0.0010 - val_loss: 8.1340e-04 - 168s/epoch - 56ms/step
Epoch 12/100
2985/2985 - 168s - loss: 0.0011 - val_loss: 7.7240e-04 - 168s/epoch - 56ms/step
Epoch 13/100
2985/2985 - 168s - loss: 9.5586e-04 - val_loss: 8.7135e-04 - 168s/epoch - 56ms/step
Epoch 14/100
2985/2985 - 168s - loss: 9.3688e-04 - val_loss: 7.2790e-04 - 168s/epoch - 56ms/step
Epoch 15/100
2985/2985 - 168s - loss: 9.0583e-04 - val_loss: 7.6399e-04 - 168s/epoch - 56ms/step
Epoch 16/100
2985/2985 - 168s - loss: 8.9672e-04 - val_loss: 0.0011 - 168s/epoch - 56ms/step
Epoch 17/100
2985/2985 - 168s - loss: 8.6995e-04 - val_loss: 8.0532e-04 - 168s/epoch - 56ms/step
Epoch 18/100
2985/2985 - 168s - loss: 8.5203e-04 - val_loss: 8.4285e-04 - 168s/epoch - 56ms/step
Epoch 19/100
2985/2985 - 168s - loss: 8.3709e-04 - val_loss: 6.9871e-04 - 168s/epoch - 56ms/step
Epoch 20/100
2985/2985 - 168s - loss: 8.0781e-04 - val_loss: 6.2810e-04 - 168s/epoch - 56ms/step
Epoch 21/100
2985/2985 - 168s - loss: 7.9980e-04 - val_loss: 6.5456e-04 - 168s/epoch - 56ms/step
Epoch 22/100
2985/2985 - 168s - loss: 7.8256e-04 - val_loss: 6.2540e-04 - 168s/epoch - 56ms/step
Epoch 23/100
2985/2985 - 168s - loss: 7.7718e-04 - val_loss: 5.8351e-04 - 168s/epoch - 56ms/step
Epoch 24/100
2985/2985 - 168s - loss: 7.5115e-04 - val_loss: 5.6906e-04 - 168s/epoch - 56ms/step
Epoch 25/100
2985/2985 - 168s - loss: 7.6464e-04 - val_loss: 5.6068e-04 - 168s/epoch - 56ms/step
Epoch 26/100
2985/2985 - 170s - loss: 7.4415e-04 - val_loss: 5.4850e-04 - 170s/epoch - 57ms/step
Epoch 27/100
2985/2985 - 178s - loss: 7.1607e-04 - val_loss: 5.2755e-04 - 178s/epoch - 60ms/step
Epoch 28/100
2985/2985 - 177s - loss: 0.0019 - val_loss: 8.9077e-04 - 177s/epoch - 59ms/step
Epoch 29/100
2985/2985 - 178s - loss: 0.0010 - val_loss: 7.6923e-04 - 178s/epoch - 60ms/step
Epoch 30/100
2985/2985 - 179s - loss: 9.3102e-04 - val_loss: 7.4832e-04 - 179s/epoch - 60ms/step
Epoch 31/100
2985/2985 - 178s - loss: 8.8557e-04 - val_loss: 6.5315e-04 - 178s/epoch - 60ms/step
Epoch 32/100
2985/2985 - 177s - loss: 8.4517e-04 - val_loss: 6.6797e-04 - 177s/epoch - 59ms/step
Epoch 33/100
2985/2985 - 180s - loss: 8.3444e-04 - val_loss: 6.6561e-04 - 180s/epoch - 60ms/step
Epoch 34/100
2985/2985 - 180s - loss: 8.2674e-04 - val_loss: 6.0227e-04 - 180s/epoch - 60ms/step
Epoch 35/100
2985/2985 - 178s - loss: 7.9405e-04 - val_loss: 7.1915e-04 - 178s/epoch - 60ms/step
Epoch 36/100
2985/2985 - 179s - loss: 8.5517e-04 - val_loss: 6.1424e-04 - 179s/epoch - 60ms/step
Epoch 37/100
2985/2985 - 178s - loss: 7.8198e-04 - val_loss: 6.4340e-04 - 178s/epoch - 60ms/step
Epoch 38/100
2985/2985 - 180s - loss: 7.6749e-04 - val_loss: 5.9394e-04 - 180s/epoch - 60ms/step
Epoch 39/100
2985/2985 - 179s - loss: 7.6817e-04 - val_loss: 5.7872e-04 - 179s/epoch - 60ms/step
Epoch 40/100
2985/2985 - 180s - loss: 7.5198e-04 - val_loss: 6.1028e-04 - 180s/epoch - 60ms/step
Epoch 41/100
2985/2985 - 178s - loss: 8.0063e-04 - val_loss: 5.8022e-04 - 178s/epoch - 60ms/step
Epoch 42/100
2985/2985 - 179s - loss: 7.3774e-04 - val_loss: 5.9419e-04 - 179s/epoch - 60ms/step
Epoch 43/100
2985/2985 - 176s - loss: 7.4644e-04 - val_loss: 5.7350e-04 - 176s/epoch - 59ms/step
Epoch 44/100
2985/2985 - 168s - loss: 7.2984e-04 - val_loss: 6.4847e-04 - 168s/epoch - 56ms/step
Epoch 45/100
2985/2985 - 168s - loss: 7.4022e-04 - val_loss: 5.8629e-04 - 168s/epoch - 56ms/step
Epoch 46/100
2985/2985 - 168s - loss: 7.2417e-04 - val_loss: 5.6147e-04 - 168s/epoch - 56ms/step
Epoch 47/100
2985/2985 - 168s - loss: 7.3506e-04 - val_loss: 5.6345e-04 - 168s/epoch - 56ms/step
Epoch 48/100
2985/2985 - 168s - loss: 7.2086e-04 - val_loss: 5.4628e-04 - 168s/epoch - 56ms/step
Epoch 49/100
2985/2985 - 168s - loss: 7.1102e-04 - val_loss: 6.0258e-04 - 168s/epoch - 56ms/step
Epoch 50/100
2985/2985 - 168s - loss: 7.1153e-04 - val_loss: 7.8403e-04 - 168s/epoch - 56ms/step
Epoch 51/100
2985/2985 - 168s - loss: 7.7983e-04 - val_loss: 5.4277e-04 - 168s/epoch - 56ms/step
Epoch 52/100
2985/2985 - 168s - loss: 6.9204e-04 - val_loss: 5.4996e-04 - 168s/epoch - 56ms/step
Epoch 53/100
2985/2985 - 168s - loss: 6.8173e-04 - val_loss: 5.6556e-04 - 168s/epoch - 56ms/step
Epoch 54/100
2985/2985 - 168s - loss: 7.4546e-04 - val_loss: 5.4579e-04 - 168s/epoch - 56ms/step
Epoch 55/100
2985/2985 - 168s - loss: 6.8420e-04 - val_loss: 5.3609e-04 - 168s/epoch - 56ms/step
Epoch 56/100
2985/2985 - 168s - loss: 6.9080e-04 - val_loss: 5.2256e-04 - 168s/epoch - 56ms/step
Epoch 57/100
2985/2985 - 168s - loss: 6.8039e-04 - val_loss: 5.6133e-04 - 168s/epoch - 56ms/step
Epoch 58/100
2985/2985 - 168s - loss: 6.7803e-04 - val_loss: 4.9612e-04 - 168s/epoch - 56ms/step
Epoch 59/100
2985/2985 - 168s - loss: 6.6320e-04 - val_loss: 5.1908e-04 - 168s/epoch - 56ms/step
Epoch 60/100
2985/2985 - 168s - loss: 6.6879e-04 - val_loss: 5.1410e-04 - 168s/epoch - 56ms/step
Epoch 61/100
2985/2985 - 168s - loss: 6.5893e-04 - val_loss: 5.1671e-04 - 168s/epoch - 56ms/step
Epoch 62/100
2985/2985 - 168s - loss: 6.5882e-04 - val_loss: 5.3455e-04 - 168s/epoch - 56ms/step
Epoch 63/100
2985/2985 - 168s - loss: 6.4725e-04 - val_loss: 5.1914e-04 - 168s/epoch - 56ms/step
Epoch 64/100
2985/2985 - 168s - loss: 6.5845e-04 - val_loss: 5.1688e-04 - 168s/epoch - 56ms/step
Epoch 65/100
2985/2985 - 168s - loss: 6.4593e-04 - val_loss: 7.1015e-04 - 168s/epoch - 56ms/step
Epoch 66/100
2985/2985 - 168s - loss: 6.9340e-04 - val_loss: 5.0485e-04 - 168s/epoch - 56ms/step
Epoch 67/100
2985/2985 - 168s - loss: 6.4018e-04 - val_loss: 5.0684e-04 - 168s/epoch - 56ms/step
Epoch 68/100
2985/2985 - 168s - loss: 6.3417e-04 - val_loss: 4.9493e-04 - 168s/epoch - 56ms/step
Epoch 69/100
2985/2985 - 168s - loss: 6.3568e-04 - val_loss: 5.0125e-04 - 168s/epoch - 56ms/step
Epoch 70/100
2985/2985 - 168s - loss: 6.2857e-04 - val_loss: 5.1771e-04 - 168s/epoch - 56ms/step
Epoch 71/100
2985/2985 - 168s - loss: 6.1832e-04 - val_loss: 4.8139e-04 - 168s/epoch - 56ms/step
Epoch 72/100
2985/2985 - 168s - loss: 6.2801e-04 - val_loss: 5.2587e-04 - 168s/epoch - 56ms/step
Epoch 73/100
2985/2985 - 168s - loss: 6.3043e-04 - val_loss: 5.1648e-04 - 168s/epoch - 56ms/step
Epoch 74/100
2985/2985 - 168s - loss: 6.2471e-04 - val_loss: 4.9883e-04 - 168s/epoch - 56ms/step
Epoch 75/100
2985/2985 - 168s - loss: 6.3580e-04 - val_loss: 4.9254e-04 - 168s/epoch - 56ms/step
Epoch 76/100
2985/2985 - 168s - loss: 6.1415e-04 - val_loss: 4.8740e-04 - 168s/epoch - 56ms/step
Epoch 77/100
2985/2985 - 168s - loss: 6.3203e-04 - val_loss: 5.0539e-04 - 168s/epoch - 56ms/step
Epoch 78/100
2985/2985 - 168s - loss: 6.3358e-04 - val_loss: 4.9698e-04 - 168s/epoch - 56ms/step
Epoch 79/100
2985/2985 - 168s - loss: 6.1527e-04 - val_loss: 4.8738e-04 - 168s/epoch - 56ms/step
Epoch 80/100
2985/2985 - 168s - loss: 6.1580e-04 - val_loss: 5.0097e-04 - 168s/epoch - 56ms/step
Epoch 81/100
2985/2985 - 168s - loss: 6.2395e-04 - val_loss: 5.6219e-04 - 168s/epoch - 56ms/step
Epoch 82/100
2985/2985 - 169s - loss: 6.0749e-04 - val_loss: 4.7723e-04 - 169s/epoch - 57ms/step
Epoch 83/100
2985/2985 - 172s - loss: 6.4301e-04 - val_loss: 5.2009e-04 - 172s/epoch - 58ms/step
Epoch 84/100
2985/2985 - 172s - loss: 6.1260e-04 - val_loss: 5.2970e-04 - 172s/epoch - 58ms/step
Epoch 85/100
2985/2985 - 172s - loss: 6.0750e-04 - val_loss: 5.4199e-04 - 172s/epoch - 58ms/step
Epoch 86/100
2985/2985 - 172s - loss: 6.1684e-04 - val_loss: 4.6260e-04 - 172s/epoch - 58ms/step
Epoch 87/100
2985/2985 - 173s - loss: 6.0197e-04 - val_loss: 5.0786e-04 - 173s/epoch - 58ms/step
Epoch 88/100
2985/2985 - 172s - loss: 5.9883e-04 - val_loss: 4.8897e-04 - 172s/epoch - 58ms/step
Epoch 89/100
2985/2985 - 172s - loss: 6.4796e-04 - val_loss: 5.0016e-04 - 172s/epoch - 58ms/step
Epoch 90/100
2985/2985 - 172s - loss: 6.0695e-04 - val_loss: 4.6079e-04 - 172s/epoch - 58ms/step
Epoch 91/100
2985/2985 - 173s - loss: 6.0128e-04 - val_loss: 5.0636e-04 - 173s/epoch - 58ms/step
Epoch 92/100
2985/2985 - 172s - loss: 6.1496e-04 - val_loss: 7.0109e-04 - 172s/epoch - 58ms/step
Epoch 93/100
2985/2985 - 172s - loss: 6.4004e-04 - val_loss: 5.4910e-04 - 172s/epoch - 57ms/step
Epoch 94/100
2985/2985 - 171s - loss: 6.0312e-04 - val_loss: 4.6197e-04 - 171s/epoch - 57ms/step
Epoch 95/100
2985/2985 - 172s - loss: 5.9353e-04 - val_loss: 4.6586e-04 - 172s/epoch - 58ms/step
Epoch 96/100
2985/2985 - 171s - loss: 5.9597e-04 - val_loss: 4.7762e-04 - 171s/epoch - 57ms/step
Epoch 97/100
2985/2985 - 171s - loss: 5.8614e-04 - val_loss: 4.7394e-04 - 171s/epoch - 57ms/step
Epoch 98/100
2985/2985 - 171s - loss: 5.9913e-04 - val_loss: 4.6388e-04 - 171s/epoch - 57ms/step
Epoch 99/100
2985/2985 - 172s - loss: 5.9305e-04 - val_loss: 4.7323e-04 - 172s/epoch - 58ms/step
Epoch 100/100
2985/2985 - 171s - loss: 5.9642e-04 - val_loss: 4.7768e-04 - 171s/epoch - 57ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0004776802961714566
  1/332 [..............................] - ETA: 48s  7/332 [..............................] - ETA: 2s  13/332 [>.............................] - ETA: 2s 19/332 [>.............................] - ETA: 2s 25/332 [=>............................] - ETA: 2s 31/332 [=>............................] - ETA: 2s 38/332 [==>...........................] - ETA: 2s 44/332 [==>...........................] - ETA: 2s 50/332 [===>..........................] - ETA: 2s 56/332 [====>.........................] - ETA: 2s 62/332 [====>.........................] - ETA: 2s 68/332 [=====>........................] - ETA: 2s 75/332 [=====>........................] - ETA: 2s 81/332 [======>.......................] - ETA: 2s 87/332 [======>.......................] - ETA: 2s 93/332 [=======>......................] - ETA: 2s 99/332 [=======>......................] - ETA: 2s105/332 [========>.....................] - ETA: 1s111/332 [=========>....................] - ETA: 1s117/332 [=========>....................] - ETA: 1s123/332 [==========>...................] - ETA: 1s129/332 [==========>...................] - ETA: 1s135/332 [===========>..................] - ETA: 1s142/332 [===========>..................] - ETA: 1s148/332 [============>.................] - ETA: 1s154/332 [============>.................] - ETA: 1s160/332 [=============>................] - ETA: 1s166/332 [==============>...............] - ETA: 1s172/332 [==============>...............] - ETA: 1s178/332 [===============>..............] - ETA: 1s184/332 [===============>..............] - ETA: 1s190/332 [================>.............] - ETA: 1s196/332 [================>.............] - ETA: 1s202/332 [=================>............] - ETA: 1s208/332 [=================>............] - ETA: 1s214/332 [==================>...........] - ETA: 1s220/332 [==================>...........] - ETA: 0s226/332 [===================>..........] - ETA: 0s233/332 [====================>.........] - ETA: 0s239/332 [====================>.........] - ETA: 0s245/332 [=====================>........] - ETA: 0s251/332 [=====================>........] - ETA: 0s257/332 [======================>.......] - ETA: 0s263/332 [======================>.......] - ETA: 0s270/332 [=======================>......] - ETA: 0s276/332 [=======================>......] - ETA: 0s282/332 [========================>.....] - ETA: 0s288/332 [=========================>....] - ETA: 0s294/332 [=========================>....] - ETA: 0s300/332 [==========================>...] - ETA: 0s306/332 [==========================>...] - ETA: 0s312/332 [===========================>..] - ETA: 0s318/332 [===========================>..] - ETA: 0s324/332 [============================>.] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 9ms/step
correlation 0.005594444418690405
cosine 0.004482914513567462
MAE: 0.011492695
RMSE: 0.021855889
r2: 0.9690119091530973
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_9 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 3792)              2400336   
                                                                 
 batch_normalization_11 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 3792)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Encoder
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_9 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
=================================================================
Total params: 7,209,224
Trainable params: 7,201,640
Non-trainable params: 7,584
_________________________________________________________________
Decoder
Model: "model_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 3792)              2400336   
                                                                 
 batch_normalization_11 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 3792)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 7,212,384
Trainable params: 7,203,536
Non-trainable params: 8,848
_________________________________________________________________
['3n_b', 'mse', 32, 100, 0.002, 0.5, 632, 0.0005964243318885565, 0.0004776802961714566, 0.005594444418690405, 0.004482914513567462, 0.011492694728076458, 0.02185588888823986, 0.9690119091530973, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_12 (Dense)            (None, 3792)              4796880   
                                                                 
 batch_normalization_12 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 3792)              2400336   
                                                                 
 batch_normalization_14 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 3792)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Epoch 1/200
2985/2985 - 171s - loss: 0.0085 - val_loss: 0.0032 - 171s/epoch - 57ms/step
Epoch 2/200
2985/2985 - 171s - loss: 0.0033 - val_loss: 0.0019 - 171s/epoch - 57ms/step
Epoch 3/200
2985/2985 - 171s - loss: 0.0033 - val_loss: 0.0015 - 171s/epoch - 57ms/step
Epoch 4/200
2985/2985 - 171s - loss: 0.0019 - val_loss: 0.0013 - 171s/epoch - 57ms/step
Epoch 5/200
2985/2985 - 170s - loss: 0.0015 - val_loss: 9.4395e-04 - 170s/epoch - 57ms/step
Epoch 6/200
2985/2985 - 171s - loss: 0.0011 - val_loss: 0.0019 - 171s/epoch - 57ms/step
Epoch 7/200
2985/2985 - 171s - loss: 0.0011 - val_loss: 6.7265e-04 - 171s/epoch - 57ms/step
Epoch 8/200
2985/2985 - 171s - loss: 8.1191e-04 - val_loss: 6.1982e-04 - 171s/epoch - 57ms/step
Epoch 9/200
2985/2985 - 171s - loss: 7.3500e-04 - val_loss: 5.4112e-04 - 171s/epoch - 57ms/step
Epoch 10/200
2985/2985 - 171s - loss: 6.4858e-04 - val_loss: 5.0544e-04 - 171s/epoch - 57ms/step
Epoch 11/200
2985/2985 - 171s - loss: 5.7978e-04 - val_loss: 4.8003e-04 - 171s/epoch - 57ms/step
Epoch 12/200
2985/2985 - 171s - loss: 5.3558e-04 - val_loss: 0.0011 - 171s/epoch - 57ms/step
Epoch 13/200
2985/2985 - 171s - loss: 5.7132e-04 - val_loss: 4.4436e-04 - 171s/epoch - 57ms/step
Epoch 14/200
2985/2985 - 170s - loss: 4.8290e-04 - val_loss: 4.1850e-04 - 170s/epoch - 57ms/step
Epoch 15/200
2985/2985 - 171s - loss: 4.6771e-04 - val_loss: 3.6299e-04 - 171s/epoch - 57ms/step
Epoch 16/200
2985/2985 - 171s - loss: 4.2942e-04 - val_loss: 3.9678e-04 - 171s/epoch - 57ms/step
Epoch 17/200
2985/2985 - 171s - loss: 4.1036e-04 - val_loss: 3.4912e-04 - 171s/epoch - 57ms/step
Epoch 18/200
2985/2985 - 171s - loss: 4.0052e-04 - val_loss: 3.5420e-04 - 171s/epoch - 57ms/step
Epoch 19/200
2985/2985 - 171s - loss: 3.7762e-04 - val_loss: 3.2750e-04 - 171s/epoch - 57ms/step
Epoch 20/200
2985/2985 - 171s - loss: 3.7805e-04 - val_loss: 3.1299e-04 - 171s/epoch - 57ms/step
Epoch 21/200
2985/2985 - 171s - loss: 3.5134e-04 - val_loss: 3.0264e-04 - 171s/epoch - 57ms/step
Epoch 22/200
2985/2985 - 171s - loss: 3.4208e-04 - val_loss: 3.2062e-04 - 171s/epoch - 57ms/step
Epoch 23/200
2985/2985 - 170s - loss: 3.3797e-04 - val_loss: 2.8675e-04 - 170s/epoch - 57ms/step
Epoch 24/200
2985/2985 - 172s - loss: 3.3493e-04 - val_loss: 3.0927e-04 - 172s/epoch - 58ms/step
Epoch 25/200
2985/2985 - 171s - loss: 3.2337e-04 - val_loss: 2.8982e-04 - 171s/epoch - 57ms/step
Epoch 26/200
2985/2985 - 172s - loss: 3.1071e-04 - val_loss: 2.4970e-04 - 172s/epoch - 58ms/step
Epoch 27/200
2985/2985 - 172s - loss: 3.0180e-04 - val_loss: 2.6045e-04 - 172s/epoch - 58ms/step
Epoch 28/200
2985/2985 - 172s - loss: 3.0168e-04 - val_loss: 2.5157e-04 - 172s/epoch - 57ms/step
Epoch 29/200
2985/2985 - 172s - loss: 2.9849e-04 - val_loss: 2.7008e-04 - 172s/epoch - 58ms/step
Epoch 30/200
2985/2985 - 171s - loss: 2.8798e-04 - val_loss: 2.5189e-04 - 171s/epoch - 57ms/step
Epoch 31/200
2985/2985 - 171s - loss: 2.8293e-04 - val_loss: 2.3832e-04 - 171s/epoch - 57ms/step
Epoch 32/200
2985/2985 - 171s - loss: 2.8155e-04 - val_loss: 2.3444e-04 - 171s/epoch - 57ms/step
Epoch 33/200
2985/2985 - 171s - loss: 2.7085e-04 - val_loss: 2.3338e-04 - 171s/epoch - 57ms/step
Epoch 34/200
2985/2985 - 171s - loss: 2.7584e-04 - val_loss: 2.3827e-04 - 171s/epoch - 57ms/step
Epoch 35/200
2985/2985 - 171s - loss: 2.7372e-04 - val_loss: 2.1998e-04 - 171s/epoch - 57ms/step
Epoch 36/200
2985/2985 - 170s - loss: 2.6070e-04 - val_loss: 2.1222e-04 - 170s/epoch - 57ms/step
Epoch 37/200
2985/2985 - 171s - loss: 2.5672e-04 - val_loss: 2.2526e-04 - 171s/epoch - 57ms/step
Epoch 38/200
2985/2985 - 170s - loss: 2.4964e-04 - val_loss: 2.0250e-04 - 170s/epoch - 57ms/step
Epoch 39/200
2985/2985 - 171s - loss: 2.4587e-04 - val_loss: 2.0391e-04 - 171s/epoch - 57ms/step
Epoch 40/200
2985/2985 - 171s - loss: 2.3912e-04 - val_loss: 2.0818e-04 - 171s/epoch - 57ms/step
Epoch 41/200
2985/2985 - 172s - loss: 2.4197e-04 - val_loss: 2.1454e-04 - 172s/epoch - 58ms/step
Epoch 42/200
2985/2985 - 171s - loss: 2.4643e-04 - val_loss: 2.0224e-04 - 171s/epoch - 57ms/step
Epoch 43/200
2985/2985 - 172s - loss: 2.4155e-04 - val_loss: 1.9418e-04 - 172s/epoch - 58ms/step
Epoch 44/200
2985/2985 - 172s - loss: 2.3115e-04 - val_loss: 3.0457e-04 - 172s/epoch - 58ms/step
Epoch 45/200
2985/2985 - 171s - loss: 2.4568e-04 - val_loss: 1.9283e-04 - 171s/epoch - 57ms/step
Epoch 46/200
2985/2985 - 173s - loss: 2.2921e-04 - val_loss: 2.0302e-04 - 173s/epoch - 58ms/step
Epoch 47/200
2985/2985 - 169s - loss: 2.2420e-04 - val_loss: 1.9203e-04 - 169s/epoch - 57ms/step
Epoch 48/200
2985/2985 - 169s - loss: 2.2144e-04 - val_loss: 1.8375e-04 - 169s/epoch - 57ms/step
Epoch 49/200
2985/2985 - 168s - loss: 2.2470e-04 - val_loss: 1.9374e-04 - 168s/epoch - 56ms/step
Epoch 50/200
2985/2985 - 168s - loss: 2.1572e-04 - val_loss: 2.1637e-04 - 168s/epoch - 56ms/step
Epoch 51/200
2985/2985 - 168s - loss: 2.2687e-04 - val_loss: 1.8452e-04 - 168s/epoch - 56ms/step
Epoch 52/200
2985/2985 - 168s - loss: 2.0979e-04 - val_loss: 1.7855e-04 - 168s/epoch - 56ms/step
Epoch 53/200
2985/2985 - 168s - loss: 2.1008e-04 - val_loss: 1.8431e-04 - 168s/epoch - 56ms/step
Epoch 54/200
2985/2985 - 168s - loss: 2.0708e-04 - val_loss: 2.0464e-04 - 168s/epoch - 56ms/step
Epoch 55/200
2985/2985 - 168s - loss: 2.0438e-04 - val_loss: 1.7344e-04 - 168s/epoch - 56ms/step
Epoch 56/200
2985/2985 - 168s - loss: 2.0682e-04 - val_loss: 1.7625e-04 - 168s/epoch - 56ms/step
Epoch 57/200
2985/2985 - 168s - loss: 2.0064e-04 - val_loss: 1.7365e-04 - 168s/epoch - 56ms/step
Epoch 58/200
2985/2985 - 167s - loss: 2.0090e-04 - val_loss: 1.7905e-04 - 167s/epoch - 56ms/step
Epoch 59/200
2985/2985 - 168s - loss: 1.9736e-04 - val_loss: 1.7007e-04 - 168s/epoch - 56ms/step
Epoch 60/200
2985/2985 - 168s - loss: 2.0480e-04 - val_loss: 1.6945e-04 - 168s/epoch - 56ms/step
Epoch 61/200
2985/2985 - 167s - loss: 1.9704e-04 - val_loss: 1.5849e-04 - 167s/epoch - 56ms/step
Epoch 62/200
2985/2985 - 168s - loss: 1.9439e-04 - val_loss: 1.8114e-04 - 168s/epoch - 56ms/step
Epoch 63/200
2985/2985 - 168s - loss: 1.9912e-04 - val_loss: 7.0926e-04 - 168s/epoch - 56ms/step
Epoch 64/200
2985/2985 - 168s - loss: 2.1169e-04 - val_loss: 1.5765e-04 - 168s/epoch - 56ms/step
Epoch 65/200
2985/2985 - 168s - loss: 1.9427e-04 - val_loss: 2.0575e-04 - 168s/epoch - 56ms/step
Epoch 66/200
2985/2985 - 168s - loss: 1.9366e-04 - val_loss: 1.6138e-04 - 168s/epoch - 56ms/step
Epoch 67/200
2985/2985 - 168s - loss: 1.8885e-04 - val_loss: 1.6270e-04 - 168s/epoch - 56ms/step
Epoch 68/200
2985/2985 - 168s - loss: 1.8894e-04 - val_loss: 1.6451e-04 - 168s/epoch - 56ms/step
Epoch 69/200
2985/2985 - 169s - loss: 1.8859e-04 - val_loss: 1.6390e-04 - 169s/epoch - 56ms/step
Epoch 70/200
2985/2985 - 168s - loss: 1.9070e-04 - val_loss: 4.5552e-04 - 168s/epoch - 56ms/step
Epoch 71/200
2985/2985 - 168s - loss: 2.2556e-04 - val_loss: 1.7773e-04 - 168s/epoch - 56ms/step
Epoch 72/200
2985/2985 - 168s - loss: 1.9410e-04 - val_loss: 1.6935e-04 - 168s/epoch - 56ms/step
Epoch 73/200
2985/2985 - 168s - loss: 1.8847e-04 - val_loss: 1.6287e-04 - 168s/epoch - 56ms/step
Epoch 74/200
2985/2985 - 168s - loss: 1.8297e-04 - val_loss: 1.5673e-04 - 168s/epoch - 56ms/step
Epoch 75/200
2985/2985 - 168s - loss: 1.9048e-04 - val_loss: 1.5407e-04 - 168s/epoch - 56ms/step
Epoch 76/200
2985/2985 - 168s - loss: 1.8105e-04 - val_loss: 1.6013e-04 - 168s/epoch - 56ms/step
Epoch 77/200
2985/2985 - 168s - loss: 1.8354e-04 - val_loss: 4.9544e-04 - 168s/epoch - 56ms/step
Epoch 78/200
2985/2985 - 169s - loss: 2.2432e-04 - val_loss: 1.5567e-04 - 169s/epoch - 57ms/step
Epoch 79/200
2985/2985 - 169s - loss: 1.8286e-04 - val_loss: 1.6121e-04 - 169s/epoch - 57ms/step
Epoch 80/200
2985/2985 - 168s - loss: 1.7957e-04 - val_loss: 1.6498e-04 - 168s/epoch - 56ms/step
Epoch 81/200
2985/2985 - 168s - loss: 1.7567e-04 - val_loss: 1.6779e-04 - 168s/epoch - 56ms/step
Epoch 82/200
2985/2985 - 167s - loss: 1.7401e-04 - val_loss: 1.5739e-04 - 167s/epoch - 56ms/step
Epoch 83/200
2985/2985 - 167s - loss: 1.7414e-04 - val_loss: 1.5776e-04 - 167s/epoch - 56ms/step
Epoch 84/200
2985/2985 - 167s - loss: 1.7178e-04 - val_loss: 1.5032e-04 - 167s/epoch - 56ms/step
Epoch 85/200
2985/2985 - 167s - loss: 1.7235e-04 - val_loss: 1.3932e-04 - 167s/epoch - 56ms/step
Epoch 86/200
2985/2985 - 167s - loss: 1.7193e-04 - val_loss: 1.4651e-04 - 167s/epoch - 56ms/step
Epoch 87/200
2985/2985 - 167s - loss: 1.6989e-04 - val_loss: 1.5794e-04 - 167s/epoch - 56ms/step
Epoch 88/200
2985/2985 - 167s - loss: 1.6746e-04 - val_loss: 1.4801e-04 - 167s/epoch - 56ms/step
Epoch 89/200
2985/2985 - 167s - loss: 1.7932e-04 - val_loss: 1.5389e-04 - 167s/epoch - 56ms/step
Epoch 90/200
2985/2985 - 168s - loss: 1.6824e-04 - val_loss: 1.5269e-04 - 168s/epoch - 56ms/step
Epoch 91/200
2985/2985 - 168s - loss: 1.6527e-04 - val_loss: 1.4954e-04 - 168s/epoch - 56ms/step
Epoch 92/200
2985/2985 - 168s - loss: 1.6724e-04 - val_loss: 1.6403e-04 - 168s/epoch - 56ms/step
Epoch 93/200
2985/2985 - 168s - loss: 1.6744e-04 - val_loss: 1.4535e-04 - 168s/epoch - 56ms/step
Epoch 94/200
2985/2985 - 168s - loss: 1.6267e-04 - val_loss: 1.4318e-04 - 168s/epoch - 56ms/step
Epoch 95/200
2985/2985 - 168s - loss: 1.7238e-04 - val_loss: 2.0153e-04 - 168s/epoch - 56ms/step
Epoch 96/200
2985/2985 - 168s - loss: 1.7126e-04 - val_loss: 1.4874e-04 - 168s/epoch - 56ms/step
Epoch 97/200
2985/2985 - 168s - loss: 1.6264e-04 - val_loss: 1.4316e-04 - 168s/epoch - 56ms/step
Epoch 98/200
2985/2985 - 168s - loss: 1.5980e-04 - val_loss: 1.4073e-04 - 168s/epoch - 56ms/step
Epoch 99/200
2985/2985 - 170s - loss: 1.5994e-04 - val_loss: 1.4131e-04 - 170s/epoch - 57ms/step
Epoch 100/200
2985/2985 - 169s - loss: 1.6191e-04 - val_loss: 1.3854e-04 - 169s/epoch - 56ms/step
Epoch 101/200
2985/2985 - 168s - loss: 1.5920e-04 - val_loss: 1.4537e-04 - 168s/epoch - 56ms/step
Epoch 102/200
2985/2985 - 168s - loss: 1.5955e-04 - val_loss: 1.6402e-04 - 168s/epoch - 56ms/step
Epoch 103/200
2985/2985 - 168s - loss: 1.6654e-04 - val_loss: 1.4375e-04 - 168s/epoch - 56ms/step
Epoch 104/200
2985/2985 - 168s - loss: 1.5792e-04 - val_loss: 1.4426e-04 - 168s/epoch - 56ms/step
Epoch 105/200
2985/2985 - 168s - loss: 1.5664e-04 - val_loss: 1.5705e-04 - 168s/epoch - 56ms/step
Epoch 106/200
2985/2985 - 168s - loss: 1.5739e-04 - val_loss: 1.5412e-04 - 168s/epoch - 56ms/step
Epoch 107/200
2985/2985 - 168s - loss: 1.5479e-04 - val_loss: 1.3760e-04 - 168s/epoch - 56ms/step
Epoch 108/200
2985/2985 - 168s - loss: 1.5570e-04 - val_loss: 1.4428e-04 - 168s/epoch - 56ms/step
Epoch 109/200
2985/2985 - 168s - loss: 1.5355e-04 - val_loss: 1.4099e-04 - 168s/epoch - 56ms/step
Epoch 110/200
2985/2985 - 168s - loss: 1.5395e-04 - val_loss: 1.4642e-04 - 168s/epoch - 56ms/step
Epoch 111/200
2985/2985 - 168s - loss: 1.6369e-04 - val_loss: 1.3742e-04 - 168s/epoch - 56ms/step
Epoch 112/200
2985/2985 - 168s - loss: 1.5472e-04 - val_loss: 1.3895e-04 - 168s/epoch - 56ms/step
Epoch 113/200
2985/2985 - 168s - loss: 1.5375e-04 - val_loss: 1.9284e-04 - 168s/epoch - 56ms/step
Epoch 114/200
2985/2985 - 168s - loss: 1.8075e-04 - val_loss: 1.3357e-04 - 168s/epoch - 56ms/step
Epoch 115/200
2985/2985 - 168s - loss: 1.5418e-04 - val_loss: 1.3652e-04 - 168s/epoch - 56ms/step
Epoch 116/200
2985/2985 - 168s - loss: 1.5256e-04 - val_loss: 1.3549e-04 - 168s/epoch - 56ms/step
Epoch 117/200
2985/2985 - 169s - loss: 1.5110e-04 - val_loss: 1.3507e-04 - 169s/epoch - 56ms/step
Epoch 118/200
2985/2985 - 168s - loss: 1.5088e-04 - val_loss: 1.3534e-04 - 168s/epoch - 56ms/step
Epoch 119/200
2985/2985 - 168s - loss: 1.4975e-04 - val_loss: 1.3820e-04 - 168s/epoch - 56ms/step
Epoch 120/200
2985/2985 - 168s - loss: 1.4960e-04 - val_loss: 1.4042e-04 - 168s/epoch - 56ms/step
Epoch 121/200
2985/2985 - 168s - loss: 1.4978e-04 - val_loss: 1.3633e-04 - 168s/epoch - 56ms/step
Epoch 122/200
2985/2985 - 168s - loss: 1.4946e-04 - val_loss: 1.3340e-04 - 168s/epoch - 56ms/step
Epoch 123/200
2985/2985 - 168s - loss: 1.4864e-04 - val_loss: 1.3504e-04 - 168s/epoch - 56ms/step
Epoch 124/200
2985/2985 - 168s - loss: 1.5395e-04 - val_loss: 2.1443e-04 - 168s/epoch - 56ms/step
Epoch 125/200
2985/2985 - 168s - loss: 1.5452e-04 - val_loss: 1.2723e-04 - 168s/epoch - 56ms/step
Epoch 126/200
2985/2985 - 169s - loss: 1.4776e-04 - val_loss: 1.4268e-04 - 169s/epoch - 57ms/step
Epoch 127/200
2985/2985 - 168s - loss: 1.4734e-04 - val_loss: 1.3056e-04 - 168s/epoch - 56ms/step
Epoch 128/200
2985/2985 - 168s - loss: 1.4637e-04 - val_loss: 1.2559e-04 - 168s/epoch - 56ms/step
Epoch 129/200
2985/2985 - 168s - loss: 1.5733e-04 - val_loss: 1.4274e-04 - 168s/epoch - 56ms/step
Epoch 130/200
2985/2985 - 168s - loss: 1.4707e-04 - val_loss: 1.2598e-04 - 168s/epoch - 56ms/step
Epoch 131/200
2985/2985 - 168s - loss: 1.4421e-04 - val_loss: 1.2718e-04 - 168s/epoch - 56ms/step
Epoch 132/200
2985/2985 - 168s - loss: 1.4470e-04 - val_loss: 1.2755e-04 - 168s/epoch - 56ms/step
Epoch 133/200
2985/2985 - 168s - loss: 1.4275e-04 - val_loss: 1.3403e-04 - 168s/epoch - 56ms/step
Epoch 134/200
2985/2985 - 168s - loss: 1.4965e-04 - val_loss: 1.3408e-04 - 168s/epoch - 56ms/step
Epoch 135/200
2985/2985 - 168s - loss: 1.4480e-04 - val_loss: 1.3186e-04 - 168s/epoch - 56ms/step
Epoch 136/200
2985/2985 - 168s - loss: 1.5100e-04 - val_loss: 1.2759e-04 - 168s/epoch - 56ms/step
Epoch 137/200
2985/2985 - 168s - loss: 1.4419e-04 - val_loss: 1.2678e-04 - 168s/epoch - 56ms/step
Epoch 138/200
2985/2985 - 168s - loss: 1.4337e-04 - val_loss: 1.3640e-04 - 168s/epoch - 56ms/step
Epoch 139/200
2985/2985 - 168s - loss: 1.4221e-04 - val_loss: 1.3007e-04 - 168s/epoch - 56ms/step
Epoch 140/200
2985/2985 - 168s - loss: 1.4249e-04 - val_loss: 1.4147e-04 - 168s/epoch - 56ms/step
Epoch 141/200
2985/2985 - 168s - loss: 1.4401e-04 - val_loss: 1.5324e-04 - 168s/epoch - 56ms/step
Epoch 142/200
2985/2985 - 168s - loss: 1.4510e-04 - val_loss: 1.3288e-04 - 168s/epoch - 56ms/step
Epoch 143/200
2985/2985 - 168s - loss: 1.4224e-04 - val_loss: 1.2538e-04 - 168s/epoch - 56ms/step
Epoch 144/200
2985/2985 - 168s - loss: 1.4875e-04 - val_loss: 1.2575e-04 - 168s/epoch - 56ms/step
Epoch 145/200
2985/2985 - 168s - loss: 1.4160e-04 - val_loss: 1.2286e-04 - 168s/epoch - 56ms/step
Epoch 146/200
2985/2985 - 168s - loss: 1.3978e-04 - val_loss: 1.3070e-04 - 168s/epoch - 56ms/step
Epoch 147/200
2985/2985 - 168s - loss: 1.4038e-04 - val_loss: 1.3537e-04 - 168s/epoch - 56ms/step
Epoch 148/200
2985/2985 - 168s - loss: 1.4141e-04 - val_loss: 1.4284e-04 - 168s/epoch - 56ms/step
Epoch 149/200
2985/2985 - 168s - loss: 1.3835e-04 - val_loss: 1.2570e-04 - 168s/epoch - 56ms/step
Epoch 150/200
2985/2985 - 168s - loss: 1.4159e-04 - val_loss: 1.3502e-04 - 168s/epoch - 56ms/step
Epoch 151/200
2985/2985 - 168s - loss: 1.3936e-04 - val_loss: 1.2519e-04 - 168s/epoch - 56ms/step
Epoch 152/200
2985/2985 - 168s - loss: 1.3937e-04 - val_loss: 1.3092e-04 - 168s/epoch - 56ms/step
Epoch 153/200
2985/2985 - 168s - loss: 1.4001e-04 - val_loss: 1.3370e-04 - 168s/epoch - 56ms/step
Epoch 154/200
2985/2985 - 168s - loss: 1.3773e-04 - val_loss: 1.2363e-04 - 168s/epoch - 56ms/step
Epoch 155/200
2985/2985 - 168s - loss: 1.3551e-04 - val_loss: 1.7508e-04 - 168s/epoch - 56ms/step
Epoch 156/200
2985/2985 - 168s - loss: 1.3669e-04 - val_loss: 1.2844e-04 - 168s/epoch - 56ms/step
Epoch 157/200
2985/2985 - 168s - loss: 1.3492e-04 - val_loss: 1.4125e-04 - 168s/epoch - 56ms/step
Epoch 158/200
2985/2985 - 168s - loss: 1.3788e-04 - val_loss: 1.3916e-04 - 168s/epoch - 56ms/step
Epoch 159/200
2985/2985 - 168s - loss: 1.3708e-04 - val_loss: 1.1980e-04 - 168s/epoch - 56ms/step
Epoch 160/200
2985/2985 - 168s - loss: 1.3847e-04 - val_loss: 1.5921e-04 - 168s/epoch - 56ms/step
Epoch 161/200
2985/2985 - 168s - loss: 1.5018e-04 - val_loss: 1.2780e-04 - 168s/epoch - 56ms/step
Epoch 162/200
2985/2985 - 168s - loss: 1.3700e-04 - val_loss: 1.2787e-04 - 168s/epoch - 56ms/step
Epoch 163/200
2985/2985 - 168s - loss: 1.4183e-04 - val_loss: 1.2982e-04 - 168s/epoch - 56ms/step
Epoch 164/200
2985/2985 - 168s - loss: 1.3877e-04 - val_loss: 1.3622e-04 - 168s/epoch - 56ms/step
Epoch 165/200
2985/2985 - 168s - loss: 1.3533e-04 - val_loss: 1.2582e-04 - 168s/epoch - 56ms/step
Epoch 166/200
2985/2985 - 168s - loss: 1.3346e-04 - val_loss: 1.3673e-04 - 168s/epoch - 56ms/step
Epoch 167/200
2985/2985 - 168s - loss: 1.3942e-04 - val_loss: 1.2277e-04 - 168s/epoch - 56ms/step
Epoch 168/200
2985/2985 - 168s - loss: 1.3452e-04 - val_loss: 1.2430e-04 - 168s/epoch - 56ms/step
Epoch 169/200
2985/2985 - 168s - loss: 1.3314e-04 - val_loss: 1.2789e-04 - 168s/epoch - 56ms/step
Epoch 170/200
2985/2985 - 168s - loss: 1.4383e-04 - val_loss: 1.6334e-04 - 168s/epoch - 56ms/step
Epoch 171/200
2985/2985 - 168s - loss: 1.4834e-04 - val_loss: 1.2683e-04 - 168s/epoch - 56ms/step
Epoch 172/200
2985/2985 - 168s - loss: 1.3476e-04 - val_loss: 1.2482e-04 - 168s/epoch - 56ms/step
Epoch 173/200
2985/2985 - 168s - loss: 1.3345e-04 - val_loss: 1.2061e-04 - 168s/epoch - 56ms/step
Epoch 174/200
2985/2985 - 168s - loss: 1.3413e-04 - val_loss: 1.0946e-04 - 168s/epoch - 56ms/step
Epoch 175/200
2985/2985 - 168s - loss: 1.3214e-04 - val_loss: 1.2436e-04 - 168s/epoch - 56ms/step
Epoch 176/200
2985/2985 - 168s - loss: 1.3609e-04 - val_loss: 1.1875e-04 - 168s/epoch - 56ms/step
Epoch 177/200
2985/2985 - 168s - loss: 1.3262e-04 - val_loss: 1.1363e-04 - 168s/epoch - 56ms/step
Epoch 178/200
2985/2985 - 168s - loss: 1.3112e-04 - val_loss: 1.1665e-04 - 168s/epoch - 56ms/step
Epoch 179/200
2985/2985 - 168s - loss: 1.3202e-04 - val_loss: 1.1591e-04 - 168s/epoch - 56ms/step
Epoch 180/200
2985/2985 - 168s - loss: 1.3600e-04 - val_loss: 1.1794e-04 - 168s/epoch - 56ms/step
Epoch 181/200
2985/2985 - 168s - loss: 1.3035e-04 - val_loss: 1.2229e-04 - 168s/epoch - 56ms/step
Epoch 182/200
2985/2985 - 168s - loss: 1.3047e-04 - val_loss: 1.2964e-04 - 168s/epoch - 56ms/step
Epoch 183/200
2985/2985 - 168s - loss: 1.2966e-04 - val_loss: 1.2228e-04 - 168s/epoch - 56ms/step
Epoch 184/200
2985/2985 - 168s - loss: 1.4544e-04 - val_loss: 1.1875e-04 - 168s/epoch - 56ms/step
Epoch 185/200
2985/2985 - 167s - loss: 1.3240e-04 - val_loss: 1.3203e-04 - 167s/epoch - 56ms/step
Epoch 186/200
2985/2985 - 168s - loss: 1.3076e-04 - val_loss: 1.1790e-04 - 168s/epoch - 56ms/step
Epoch 187/200
2985/2985 - 168s - loss: 1.3101e-04 - val_loss: 1.2067e-04 - 168s/epoch - 56ms/step
Epoch 188/200
2985/2985 - 168s - loss: 1.2908e-04 - val_loss: 1.2235e-04 - 168s/epoch - 56ms/step
Epoch 189/200
2985/2985 - 167s - loss: 1.3128e-04 - val_loss: 1.1602e-04 - 167s/epoch - 56ms/step
Epoch 190/200
2985/2985 - 167s - loss: 1.3650e-04 - val_loss: 1.1400e-04 - 167s/epoch - 56ms/step
Epoch 191/200
2985/2985 - 168s - loss: 1.3053e-04 - val_loss: 1.4384e-04 - 168s/epoch - 56ms/step
Epoch 192/200
2985/2985 - 168s - loss: 1.3768e-04 - val_loss: 1.3267e-04 - 168s/epoch - 56ms/step
Epoch 193/200
2985/2985 - 168s - loss: 1.3034e-04 - val_loss: 1.1788e-04 - 168s/epoch - 56ms/step
Epoch 194/200
2985/2985 - 168s - loss: 1.3153e-04 - val_loss: 1.7984e-04 - 168s/epoch - 56ms/step
Epoch 195/200
2985/2985 - 168s - loss: 1.4306e-04 - val_loss: 1.1695e-04 - 168s/epoch - 56ms/step
Epoch 196/200
2985/2985 - 168s - loss: 1.2915e-04 - val_loss: 1.2067e-04 - 168s/epoch - 56ms/step
Epoch 197/200
2985/2985 - 168s - loss: 1.3103e-04 - val_loss: 1.2212e-04 - 168s/epoch - 56ms/step
Epoch 198/200
2985/2985 - 168s - loss: 1.2789e-04 - val_loss: 1.2690e-04 - 168s/epoch - 56ms/step
Epoch 199/200
2985/2985 - 168s - loss: 1.2861e-04 - val_loss: 1.1555e-04 - 168s/epoch - 56ms/step
Epoch 200/200
2985/2985 - 168s - loss: 1.2786e-04 - val_loss: 1.1848e-04 - 168s/epoch - 56ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00011848167923744768
  1/332 [..............................] - ETA: 2:12  7/332 [..............................] - ETA: 2s   14/332 [>.............................] - ETA: 2s 21/332 [>.............................] - ETA: 2s 28/332 [=>............................] - ETA: 2s 35/332 [==>...........................] - ETA: 2s 42/332 [==>...........................] - ETA: 2s 49/332 [===>..........................] - ETA: 2s 56/332 [====>.........................] - ETA: 2s 63/332 [====>.........................] - ETA: 2s 70/332 [=====>........................] - ETA: 2s 77/332 [=====>........................] - ETA: 2s 84/332 [======>.......................] - ETA: 2s 91/332 [=======>......................] - ETA: 1s 98/332 [=======>......................] - ETA: 1s105/332 [========>.....................] - ETA: 1s112/332 [=========>....................] - ETA: 1s119/332 [=========>....................] - ETA: 1s126/332 [==========>...................] - ETA: 1s133/332 [===========>..................] - ETA: 1s140/332 [===========>..................] - ETA: 1s147/332 [============>.................] - ETA: 1s154/332 [============>.................] - ETA: 1s161/332 [=============>................] - ETA: 1s168/332 [==============>...............] - ETA: 1s175/332 [==============>...............] - ETA: 1s182/332 [===============>..............] - ETA: 1s189/332 [================>.............] - ETA: 1s196/332 [================>.............] - ETA: 1s203/332 [=================>............] - ETA: 1s210/332 [=================>............] - ETA: 0s217/332 [==================>...........] - ETA: 0s224/332 [===================>..........] - ETA: 0s231/332 [===================>..........] - ETA: 0s238/332 [====================>.........] - ETA: 0s245/332 [=====================>........] - ETA: 0s252/332 [=====================>........] - ETA: 0s259/332 [======================>.......] - ETA: 0s266/332 [=======================>......] - ETA: 0s273/332 [=======================>......] - ETA: 0s280/332 [========================>.....] - ETA: 0s287/332 [========================>.....] - ETA: 0s294/332 [=========================>....] - ETA: 0s301/332 [==========================>...] - ETA: 0s308/332 [==========================>...] - ETA: 0s315/332 [===========================>..] - ETA: 0s322/332 [============================>.] - ETA: 0s329/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 8ms/step
correlation 0.001338457256903794
cosine 0.0010563834035706618
MAE: 0.0057430873
RMSE: 0.010884921
r2: 0.9923141189182827
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 3792)              4796880   
                                                                 
 batch_normalization_12 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 3792)              2400336   
                                                                 
 batch_normalization_14 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 3792)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Encoder
Model: "model_13"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_14 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 3792)              4796880   
                                                                 
 batch_normalization_12 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
=================================================================
Total params: 7,209,224
Trainable params: 7,201,640
Non-trainable params: 7,584
_________________________________________________________________
Decoder
Model: "model_14"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_15 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 3792)              2400336   
                                                                 
 batch_normalization_14 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 3792)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 7,212,384
Trainable params: 7,203,536
Non-trainable params: 8,848
_________________________________________________________________
['3n_b', 'mse', 32, 200, 0.0005, 0.5, 632, 0.00012785867147613317, 0.00011848167923744768, 0.001338457256903794, 0.0010563834035706618, 0.0057430872693657875, 0.010884921066462994, 0.9923141189182827, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_15 (Dense)            (None, 3792)              4796880   
                                                                 
 batch_normalization_15 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 3792)              2400336   
                                                                 
 batch_normalization_17 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 3792)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Epoch 1/200
2985/2985 - 168s - loss: 0.0089 - val_loss: 0.0024 - 168s/epoch - 56ms/step
Epoch 2/200
2985/2985 - 167s - loss: 0.0030 - val_loss: 0.0019 - 167s/epoch - 56ms/step
Epoch 3/200
2985/2985 - 168s - loss: 0.0020 - val_loss: 0.0012 - 168s/epoch - 56ms/step
Epoch 4/200
2985/2985 - 168s - loss: 0.0014 - val_loss: 0.0012 - 168s/epoch - 56ms/step
Epoch 5/200
2985/2985 - 168s - loss: 0.0010 - val_loss: 7.8363e-04 - 168s/epoch - 56ms/step
Epoch 6/200
2985/2985 - 167s - loss: 8.5145e-04 - val_loss: 6.7972e-04 - 167s/epoch - 56ms/step
Epoch 7/200
2985/2985 - 168s - loss: 7.2832e-04 - val_loss: 6.1295e-04 - 168s/epoch - 56ms/step
Epoch 8/200
2985/2985 - 168s - loss: 6.6139e-04 - val_loss: 5.3263e-04 - 168s/epoch - 56ms/step
Epoch 9/200
2985/2985 - 168s - loss: 5.9734e-04 - val_loss: 4.7968e-04 - 168s/epoch - 56ms/step
Epoch 10/200
2985/2985 - 168s - loss: 5.6097e-04 - val_loss: 4.4930e-04 - 168s/epoch - 56ms/step
Epoch 11/200
2985/2985 - 168s - loss: 5.2565e-04 - val_loss: 4.3737e-04 - 168s/epoch - 56ms/step
Epoch 12/200
2985/2985 - 167s - loss: 4.9731e-04 - val_loss: 4.8288e-04 - 167s/epoch - 56ms/step
Epoch 13/200
2985/2985 - 168s - loss: 4.6923e-04 - val_loss: 3.7209e-04 - 168s/epoch - 56ms/step
Epoch 14/200
2985/2985 - 168s - loss: 4.9505e-04 - val_loss: 3.8728e-04 - 168s/epoch - 56ms/step
Epoch 15/200
2985/2985 - 168s - loss: 4.5573e-04 - val_loss: 3.4076e-04 - 168s/epoch - 56ms/step
Epoch 16/200
2985/2985 - 168s - loss: 4.1657e-04 - val_loss: 3.5971e-04 - 168s/epoch - 56ms/step
Epoch 17/200
2985/2985 - 168s - loss: 4.0129e-04 - val_loss: 3.2814e-04 - 168s/epoch - 56ms/step
Epoch 18/200
2985/2985 - 168s - loss: 3.8204e-04 - val_loss: 3.2260e-04 - 168s/epoch - 56ms/step
Epoch 19/200
2985/2985 - 167s - loss: 4.1501e-04 - val_loss: 3.4104e-04 - 167s/epoch - 56ms/step
Epoch 20/200
2985/2985 - 168s - loss: 3.7314e-04 - val_loss: 3.1317e-04 - 168s/epoch - 56ms/step
Epoch 21/200
2985/2985 - 167s - loss: 3.5993e-04 - val_loss: 2.8769e-04 - 167s/epoch - 56ms/step
Epoch 22/200
2985/2985 - 168s - loss: 3.4667e-04 - val_loss: 3.0621e-04 - 168s/epoch - 56ms/step
Epoch 23/200
2985/2985 - 168s - loss: 3.3814e-04 - val_loss: 2.7423e-04 - 168s/epoch - 56ms/step
Epoch 24/200
2985/2985 - 168s - loss: 3.3610e-04 - val_loss: 2.9843e-04 - 168s/epoch - 56ms/step
Epoch 25/200
2985/2985 - 168s - loss: 3.2749e-04 - val_loss: 2.7400e-04 - 168s/epoch - 56ms/step
Epoch 26/200
2985/2985 - 167s - loss: 3.2084e-04 - val_loss: 2.5235e-04 - 167s/epoch - 56ms/step
Epoch 27/200
2985/2985 - 167s - loss: 3.1337e-04 - val_loss: 2.5288e-04 - 167s/epoch - 56ms/step
Epoch 28/200
2985/2985 - 167s - loss: 3.1415e-04 - val_loss: 2.4676e-04 - 167s/epoch - 56ms/step
Epoch 29/200
2985/2985 - 167s - loss: 2.9918e-04 - val_loss: 2.3886e-04 - 167s/epoch - 56ms/step
Epoch 30/200
2985/2985 - 168s - loss: 2.9668e-04 - val_loss: 2.4917e-04 - 168s/epoch - 56ms/step
Epoch 31/200
2985/2985 - 168s - loss: 2.9250e-04 - val_loss: 2.4225e-04 - 168s/epoch - 56ms/step
Epoch 32/200
2985/2985 - 167s - loss: 3.4163e-04 - val_loss: 2.3338e-04 - 167s/epoch - 56ms/step
Epoch 33/200
2985/2985 - 167s - loss: 2.9483e-04 - val_loss: 2.2772e-04 - 167s/epoch - 56ms/step
Epoch 34/200
2985/2985 - 168s - loss: 2.8619e-04 - val_loss: 2.3472e-04 - 168s/epoch - 56ms/step
Epoch 35/200
2985/2985 - 167s - loss: 2.7878e-04 - val_loss: 2.3434e-04 - 167s/epoch - 56ms/step
Epoch 36/200
2985/2985 - 167s - loss: 3.0413e-04 - val_loss: 2.3014e-04 - 167s/epoch - 56ms/step
Epoch 37/200
2985/2985 - 167s - loss: 2.9100e-04 - val_loss: 2.2231e-04 - 167s/epoch - 56ms/step
Epoch 38/200
2985/2985 - 167s - loss: 2.8074e-04 - val_loss: 2.1711e-04 - 167s/epoch - 56ms/step
Epoch 39/200
2985/2985 - 167s - loss: 2.7039e-04 - val_loss: 2.1189e-04 - 167s/epoch - 56ms/step
Epoch 40/200
2985/2985 - 167s - loss: 2.6236e-04 - val_loss: 2.3703e-04 - 167s/epoch - 56ms/step
Epoch 41/200
2985/2985 - 167s - loss: 2.6267e-04 - val_loss: 2.0986e-04 - 167s/epoch - 56ms/step
Epoch 42/200
2985/2985 - 167s - loss: 2.5735e-04 - val_loss: 2.1436e-04 - 167s/epoch - 56ms/step
Epoch 43/200
2985/2985 - 167s - loss: 2.5530e-04 - val_loss: 2.1667e-04 - 167s/epoch - 56ms/step
Epoch 44/200
2985/2985 - 167s - loss: 2.5288e-04 - val_loss: 2.5482e-04 - 167s/epoch - 56ms/step
Epoch 45/200
2985/2985 - 168s - loss: 2.6717e-04 - val_loss: 2.0561e-04 - 168s/epoch - 56ms/step
Epoch 46/200
2985/2985 - 168s - loss: 2.4847e-04 - val_loss: 2.0539e-04 - 168s/epoch - 56ms/step
Epoch 47/200
2985/2985 - 167s - loss: 2.4454e-04 - val_loss: 2.0171e-04 - 167s/epoch - 56ms/step
Epoch 48/200
2985/2985 - 167s - loss: 2.4680e-04 - val_loss: 2.0528e-04 - 167s/epoch - 56ms/step
Epoch 49/200
2985/2985 - 167s - loss: 2.4551e-04 - val_loss: 2.3329e-04 - 167s/epoch - 56ms/step
Epoch 50/200
2985/2985 - 168s - loss: 2.5204e-04 - val_loss: 2.0242e-04 - 168s/epoch - 56ms/step
Epoch 51/200
2985/2985 - 167s - loss: 2.3904e-04 - val_loss: 1.9392e-04 - 167s/epoch - 56ms/step
Epoch 52/200
2985/2985 - 167s - loss: 2.3862e-04 - val_loss: 1.9298e-04 - 167s/epoch - 56ms/step
Epoch 53/200
2985/2985 - 167s - loss: 2.4030e-04 - val_loss: 1.9706e-04 - 167s/epoch - 56ms/step
Epoch 54/200
2985/2985 - 168s - loss: 2.3855e-04 - val_loss: 2.0180e-04 - 168s/epoch - 56ms/step
Epoch 55/200
2985/2985 - 168s - loss: 2.2878e-04 - val_loss: 1.7842e-04 - 168s/epoch - 56ms/step
Epoch 56/200
2985/2985 - 167s - loss: 2.2847e-04 - val_loss: 1.9230e-04 - 167s/epoch - 56ms/step
Epoch 57/200
2985/2985 - 167s - loss: 2.2478e-04 - val_loss: 1.9101e-04 - 167s/epoch - 56ms/step
Epoch 58/200
2985/2985 - 167s - loss: 2.2590e-04 - val_loss: 1.7980e-04 - 167s/epoch - 56ms/step
Epoch 59/200
2985/2985 - 168s - loss: 2.2201e-04 - val_loss: 1.7878e-04 - 168s/epoch - 56ms/step
Epoch 60/200
2985/2985 - 167s - loss: 2.3008e-04 - val_loss: 1.7945e-04 - 167s/epoch - 56ms/step
Epoch 61/200
2985/2985 - 167s - loss: 2.2167e-04 - val_loss: 1.8240e-04 - 167s/epoch - 56ms/step
Epoch 62/200
2985/2985 - 167s - loss: 2.1964e-04 - val_loss: 1.8711e-04 - 167s/epoch - 56ms/step
Epoch 63/200
2985/2985 - 167s - loss: 2.4400e-04 - val_loss: 1.8682e-04 - 167s/epoch - 56ms/step
Epoch 64/200
2985/2985 - 167s - loss: 2.2354e-04 - val_loss: 1.7943e-04 - 167s/epoch - 56ms/step
Epoch 65/200
2985/2985 - 167s - loss: 2.1727e-04 - val_loss: 1.8379e-04 - 167s/epoch - 56ms/step
Epoch 66/200
2985/2985 - 167s - loss: 2.2488e-04 - val_loss: 1.8048e-04 - 167s/epoch - 56ms/step
Epoch 67/200
2985/2985 - 167s - loss: 2.1889e-04 - val_loss: 1.8092e-04 - 167s/epoch - 56ms/step
Epoch 68/200
2985/2985 - 167s - loss: 2.3717e-04 - val_loss: 1.8881e-04 - 167s/epoch - 56ms/step
Epoch 69/200
2985/2985 - 167s - loss: 2.1576e-04 - val_loss: 1.7036e-04 - 167s/epoch - 56ms/step
Epoch 70/200
2985/2985 - 167s - loss: 2.2203e-04 - val_loss: 1.8232e-04 - 167s/epoch - 56ms/step
Epoch 71/200
2985/2985 - 167s - loss: 2.1183e-04 - val_loss: 1.7322e-04 - 167s/epoch - 56ms/step
Epoch 72/200
2985/2985 - 167s - loss: 2.1154e-04 - val_loss: 2.0986e-04 - 167s/epoch - 56ms/step
Epoch 73/200
2985/2985 - 168s - loss: 2.1861e-04 - val_loss: 1.6167e-04 - 168s/epoch - 56ms/step
Epoch 74/200
2985/2985 - 168s - loss: 2.0816e-04 - val_loss: 1.7371e-04 - 168s/epoch - 56ms/step
Epoch 75/200
2985/2985 - 168s - loss: 2.1177e-04 - val_loss: 1.6919e-04 - 168s/epoch - 56ms/step
Epoch 76/200
2985/2985 - 168s - loss: 2.0834e-04 - val_loss: 1.6832e-04 - 168s/epoch - 56ms/step
Epoch 77/200
2985/2985 - 168s - loss: 2.1302e-04 - val_loss: 2.3015e-04 - 168s/epoch - 56ms/step
Epoch 78/200
2985/2985 - 167s - loss: 2.2832e-04 - val_loss: 1.7060e-04 - 167s/epoch - 56ms/step
Epoch 79/200
2985/2985 - 168s - loss: 2.1009e-04 - val_loss: 2.6499e-04 - 168s/epoch - 56ms/step
Epoch 80/200
2985/2985 - 167s - loss: 2.1870e-04 - val_loss: 1.6991e-04 - 167s/epoch - 56ms/step
Epoch 81/200
2985/2985 - 167s - loss: 2.0307e-04 - val_loss: 1.9004e-04 - 167s/epoch - 56ms/step
Epoch 82/200
2985/2985 - 168s - loss: 2.0077e-04 - val_loss: 1.6517e-04 - 168s/epoch - 56ms/step
Epoch 83/200
2985/2985 - 168s - loss: 2.0335e-04 - val_loss: 1.7229e-04 - 168s/epoch - 56ms/step
Epoch 84/200
2985/2985 - 167s - loss: 2.0680e-04 - val_loss: 1.6033e-04 - 167s/epoch - 56ms/step
Epoch 85/200
2985/2985 - 168s - loss: 2.0302e-04 - val_loss: 1.5643e-04 - 168s/epoch - 56ms/step
Epoch 86/200
2985/2985 - 168s - loss: 1.9769e-04 - val_loss: 1.6076e-04 - 168s/epoch - 56ms/step
Epoch 87/200
2985/2985 - 168s - loss: 1.9599e-04 - val_loss: 1.6190e-04 - 168s/epoch - 56ms/step
Epoch 88/200
2985/2985 - 168s - loss: 1.9717e-04 - val_loss: 1.6109e-04 - 168s/epoch - 56ms/step
Epoch 89/200
2985/2985 - 168s - loss: 1.9787e-04 - val_loss: 1.6580e-04 - 168s/epoch - 56ms/step
Epoch 90/200
2985/2985 - 168s - loss: 2.2797e-04 - val_loss: 1.7461e-04 - 168s/epoch - 56ms/step
Epoch 91/200
2985/2985 - 167s - loss: 2.0506e-04 - val_loss: 1.6483e-04 - 167s/epoch - 56ms/step
Epoch 92/200
2985/2985 - 168s - loss: 2.0013e-04 - val_loss: 1.6453e-04 - 168s/epoch - 56ms/step
Epoch 93/200
2985/2985 - 168s - loss: 1.9841e-04 - val_loss: 1.6553e-04 - 168s/epoch - 56ms/step
Epoch 94/200
2985/2985 - 168s - loss: 1.9333e-04 - val_loss: 1.4880e-04 - 168s/epoch - 56ms/step
Epoch 95/200
2985/2985 - 167s - loss: 2.1376e-04 - val_loss: 1.5828e-04 - 167s/epoch - 56ms/step
Epoch 96/200
2985/2985 - 168s - loss: 2.0343e-04 - val_loss: 1.8153e-04 - 168s/epoch - 56ms/step
Epoch 97/200
2985/2985 - 167s - loss: 1.9417e-04 - val_loss: 1.5568e-04 - 167s/epoch - 56ms/step
Epoch 98/200
2985/2985 - 168s - loss: 1.9078e-04 - val_loss: 1.6486e-04 - 168s/epoch - 56ms/step
Epoch 99/200
2985/2985 - 168s - loss: 1.9117e-04 - val_loss: 1.6127e-04 - 168s/epoch - 56ms/step
Epoch 100/200
2985/2985 - 167s - loss: 1.8882e-04 - val_loss: 1.6143e-04 - 167s/epoch - 56ms/step
Epoch 101/200
2985/2985 - 167s - loss: 1.8759e-04 - val_loss: 1.5255e-04 - 167s/epoch - 56ms/step
Epoch 102/200
2985/2985 - 167s - loss: 1.9340e-04 - val_loss: 1.5477e-04 - 167s/epoch - 56ms/step
Epoch 103/200
2985/2985 - 167s - loss: 1.8964e-04 - val_loss: 1.5540e-04 - 167s/epoch - 56ms/step
Epoch 104/200
2985/2985 - 167s - loss: 1.8683e-04 - val_loss: 1.6203e-04 - 167s/epoch - 56ms/step
Epoch 105/200
2985/2985 - 167s - loss: 1.8801e-04 - val_loss: 1.6259e-04 - 167s/epoch - 56ms/step
Epoch 106/200
2985/2985 - 167s - loss: 1.8631e-04 - val_loss: 1.5952e-04 - 167s/epoch - 56ms/step
Epoch 107/200
2985/2985 - 168s - loss: 1.8256e-04 - val_loss: 1.4757e-04 - 168s/epoch - 56ms/step
Epoch 108/200
2985/2985 - 167s - loss: 1.8924e-04 - val_loss: 1.5856e-04 - 167s/epoch - 56ms/step
Epoch 109/200
2985/2985 - 167s - loss: 1.8820e-04 - val_loss: 1.5462e-04 - 167s/epoch - 56ms/step
Epoch 110/200
2985/2985 - 167s - loss: 1.8571e-04 - val_loss: 1.6432e-04 - 167s/epoch - 56ms/step
Epoch 111/200
2985/2985 - 167s - loss: 1.8506e-04 - val_loss: 1.5441e-04 - 167s/epoch - 56ms/step
Epoch 112/200
2985/2985 - 167s - loss: 1.8406e-04 - val_loss: 1.5653e-04 - 167s/epoch - 56ms/step
Epoch 113/200
2985/2985 - 168s - loss: 1.7964e-04 - val_loss: 1.4713e-04 - 168s/epoch - 56ms/step
Epoch 114/200
2985/2985 - 168s - loss: 1.8379e-04 - val_loss: 1.5620e-04 - 168s/epoch - 56ms/step
Epoch 115/200
2985/2985 - 168s - loss: 1.8071e-04 - val_loss: 1.4945e-04 - 168s/epoch - 56ms/step
Epoch 116/200
2985/2985 - 168s - loss: 1.7908e-04 - val_loss: 1.5110e-04 - 168s/epoch - 56ms/step
Epoch 117/200
2985/2985 - 168s - loss: 1.7623e-04 - val_loss: 1.4433e-04 - 168s/epoch - 56ms/step
Epoch 118/200
2985/2985 - 168s - loss: 1.8332e-04 - val_loss: 1.4965e-04 - 168s/epoch - 56ms/step
Epoch 119/200
2985/2985 - 168s - loss: 1.7903e-04 - val_loss: 1.4131e-04 - 168s/epoch - 56ms/step
Epoch 120/200
2985/2985 - 167s - loss: 1.7647e-04 - val_loss: 1.4807e-04 - 167s/epoch - 56ms/step
Epoch 121/200
2985/2985 - 167s - loss: 1.7880e-04 - val_loss: 1.4783e-04 - 167s/epoch - 56ms/step
Epoch 122/200
2985/2985 - 168s - loss: 1.8007e-04 - val_loss: 1.4242e-04 - 168s/epoch - 56ms/step
Epoch 123/200
2985/2985 - 167s - loss: 1.7926e-04 - val_loss: 1.5296e-04 - 167s/epoch - 56ms/step
Epoch 124/200
2985/2985 - 167s - loss: 1.7601e-04 - val_loss: 1.5250e-04 - 167s/epoch - 56ms/step
Epoch 125/200
2985/2985 - 168s - loss: 1.7763e-04 - val_loss: 1.4716e-04 - 168s/epoch - 56ms/step
Epoch 126/200
2985/2985 - 167s - loss: 1.8068e-04 - val_loss: 1.4473e-04 - 167s/epoch - 56ms/step
Epoch 127/200
2985/2985 - 168s - loss: 1.7598e-04 - val_loss: 1.4574e-04 - 168s/epoch - 56ms/step
Epoch 128/200
2985/2985 - 168s - loss: 1.8079e-04 - val_loss: 1.4995e-04 - 168s/epoch - 56ms/step
Epoch 129/200
2985/2985 - 168s - loss: 1.7703e-04 - val_loss: 1.5045e-04 - 168s/epoch - 56ms/step
Epoch 130/200
2985/2985 - 167s - loss: 1.7512e-04 - val_loss: 1.3719e-04 - 167s/epoch - 56ms/step
Epoch 131/200
2985/2985 - 168s - loss: 1.7325e-04 - val_loss: 1.4958e-04 - 168s/epoch - 56ms/step
Epoch 132/200
2985/2985 - 168s - loss: 1.7330e-04 - val_loss: 1.4053e-04 - 168s/epoch - 56ms/step
Epoch 133/200
2985/2985 - 168s - loss: 1.7028e-04 - val_loss: 1.4787e-04 - 168s/epoch - 56ms/step
Epoch 134/200
2985/2985 - 168s - loss: 1.8071e-04 - val_loss: 1.5361e-04 - 168s/epoch - 56ms/step
Epoch 135/200
2985/2985 - 167s - loss: 1.7532e-04 - val_loss: 1.3940e-04 - 167s/epoch - 56ms/step
Epoch 136/200
2985/2985 - 167s - loss: 1.7592e-04 - val_loss: 1.3649e-04 - 167s/epoch - 56ms/step
Epoch 137/200
2985/2985 - 167s - loss: 1.7278e-04 - val_loss: 1.5049e-04 - 167s/epoch - 56ms/step
Epoch 138/200
2985/2985 - 168s - loss: 1.7604e-04 - val_loss: 1.5668e-04 - 168s/epoch - 56ms/step
Epoch 139/200
2985/2985 - 167s - loss: 1.7245e-04 - val_loss: 1.4470e-04 - 167s/epoch - 56ms/step
Epoch 140/200
2985/2985 - 167s - loss: 1.7234e-04 - val_loss: 1.4752e-04 - 167s/epoch - 56ms/step
Epoch 141/200
2985/2985 - 167s - loss: 1.7228e-04 - val_loss: 1.3875e-04 - 167s/epoch - 56ms/step
Epoch 142/200
2985/2985 - 167s - loss: 1.7055e-04 - val_loss: 1.5217e-04 - 167s/epoch - 56ms/step
Epoch 143/200
2985/2985 - 167s - loss: 1.7580e-04 - val_loss: 1.4474e-04 - 167s/epoch - 56ms/step
Epoch 144/200
2985/2985 - 167s - loss: 1.7014e-04 - val_loss: 1.3397e-04 - 167s/epoch - 56ms/step
Epoch 145/200
2985/2985 - 167s - loss: 1.7088e-04 - val_loss: 1.3941e-04 - 167s/epoch - 56ms/step
Epoch 146/200
2985/2985 - 168s - loss: 1.6954e-04 - val_loss: 1.4582e-04 - 168s/epoch - 56ms/step
Epoch 147/200
2985/2985 - 168s - loss: 1.7577e-04 - val_loss: 2.1369e-04 - 168s/epoch - 56ms/step
Epoch 148/200
2985/2985 - 168s - loss: 1.8476e-04 - val_loss: 1.5540e-04 - 168s/epoch - 56ms/step
Epoch 149/200
2985/2985 - 167s - loss: 1.6995e-04 - val_loss: 1.4485e-04 - 167s/epoch - 56ms/step
Epoch 150/200
2985/2985 - 168s - loss: 1.7595e-04 - val_loss: 1.4380e-04 - 168s/epoch - 56ms/step
Epoch 151/200
2985/2985 - 168s - loss: 1.6721e-04 - val_loss: 1.4190e-04 - 168s/epoch - 56ms/step
Epoch 152/200
2985/2985 - 168s - loss: 1.6740e-04 - val_loss: 1.4692e-04 - 168s/epoch - 56ms/step
Epoch 153/200
2985/2985 - 168s - loss: 1.7273e-04 - val_loss: 1.4021e-04 - 168s/epoch - 56ms/step
Epoch 154/200
2985/2985 - 168s - loss: 1.6739e-04 - val_loss: 1.3923e-04 - 168s/epoch - 56ms/step
Epoch 155/200
2985/2985 - 167s - loss: 1.6946e-04 - val_loss: 1.3494e-04 - 167s/epoch - 56ms/step
Epoch 156/200
2985/2985 - 168s - loss: 1.6420e-04 - val_loss: 1.3489e-04 - 168s/epoch - 56ms/step
Epoch 157/200
2985/2985 - 168s - loss: 1.6281e-04 - val_loss: 1.4731e-04 - 168s/epoch - 56ms/step
Epoch 158/200
2985/2985 - 168s - loss: 1.6299e-04 - val_loss: 1.3829e-04 - 168s/epoch - 56ms/step
Epoch 159/200
2985/2985 - 167s - loss: 1.6575e-04 - val_loss: 1.2959e-04 - 167s/epoch - 56ms/step
Epoch 160/200
2985/2985 - 167s - loss: 1.6425e-04 - val_loss: 1.3604e-04 - 167s/epoch - 56ms/step
Epoch 161/200
2985/2985 - 168s - loss: 1.6348e-04 - val_loss: 1.8821e-04 - 168s/epoch - 56ms/step
Epoch 162/200
2985/2985 - 168s - loss: 1.6372e-04 - val_loss: 1.3736e-04 - 168s/epoch - 56ms/step
Epoch 163/200
2985/2985 - 168s - loss: 1.6223e-04 - val_loss: 1.3678e-04 - 168s/epoch - 56ms/step
Epoch 164/200
2985/2985 - 167s - loss: 1.6640e-04 - val_loss: 1.5395e-04 - 167s/epoch - 56ms/step
Epoch 165/200
2985/2985 - 168s - loss: 1.6408e-04 - val_loss: 1.3437e-04 - 168s/epoch - 56ms/step
Epoch 166/200
2985/2985 - 168s - loss: 1.6954e-04 - val_loss: 1.4091e-04 - 168s/epoch - 56ms/step
Epoch 167/200
2985/2985 - 168s - loss: 1.9057e-04 - val_loss: 1.3600e-04 - 168s/epoch - 56ms/step
Epoch 168/200
2985/2985 - 167s - loss: 1.6640e-04 - val_loss: 1.4150e-04 - 167s/epoch - 56ms/step
Epoch 169/200
2985/2985 - 167s - loss: 1.6327e-04 - val_loss: 1.4264e-04 - 167s/epoch - 56ms/step
Epoch 170/200
2985/2985 - 168s - loss: 1.6185e-04 - val_loss: 1.3432e-04 - 168s/epoch - 56ms/step
Epoch 171/200
2985/2985 - 167s - loss: 1.6257e-04 - val_loss: 1.4024e-04 - 167s/epoch - 56ms/step
Epoch 172/200
2985/2985 - 167s - loss: 1.9164e-04 - val_loss: 1.5408e-04 - 167s/epoch - 56ms/step
Epoch 173/200
2985/2985 - 167s - loss: 1.7099e-04 - val_loss: 1.3809e-04 - 167s/epoch - 56ms/step
Epoch 174/200
2985/2985 - 168s - loss: 1.6578e-04 - val_loss: 1.2974e-04 - 168s/epoch - 56ms/step
Epoch 175/200
2985/2985 - 167s - loss: 1.6301e-04 - val_loss: 1.3026e-04 - 167s/epoch - 56ms/step
Epoch 176/200
2985/2985 - 167s - loss: 1.6275e-04 - val_loss: 1.3647e-04 - 167s/epoch - 56ms/step
Epoch 177/200
2985/2985 - 167s - loss: 1.6353e-04 - val_loss: 1.2675e-04 - 167s/epoch - 56ms/step
Epoch 178/200
2985/2985 - 168s - loss: 1.6508e-04 - val_loss: 1.4011e-04 - 168s/epoch - 56ms/step
Epoch 179/200
2985/2985 - 168s - loss: 1.6234e-04 - val_loss: 1.4075e-04 - 168s/epoch - 56ms/step
Epoch 180/200
2985/2985 - 168s - loss: 1.5987e-04 - val_loss: 1.3335e-04 - 168s/epoch - 56ms/step
Epoch 181/200
2985/2985 - 167s - loss: 1.5817e-04 - val_loss: 1.4180e-04 - 167s/epoch - 56ms/step
Epoch 182/200
2985/2985 - 168s - loss: 1.8374e-04 - val_loss: 1.4959e-04 - 168s/epoch - 56ms/step
Epoch 183/200
2985/2985 - 168s - loss: 1.6607e-04 - val_loss: 1.3457e-04 - 168s/epoch - 56ms/step
Epoch 184/200
2985/2985 - 168s - loss: 1.6263e-04 - val_loss: 1.4483e-04 - 168s/epoch - 56ms/step
Epoch 185/200
2985/2985 - 167s - loss: 1.6060e-04 - val_loss: 1.4956e-04 - 167s/epoch - 56ms/step
Epoch 186/200
2985/2985 - 167s - loss: 1.5792e-04 - val_loss: 1.3083e-04 - 167s/epoch - 56ms/step
Epoch 187/200
2985/2985 - 168s - loss: 1.5858e-04 - val_loss: 1.3048e-04 - 168s/epoch - 56ms/step
Epoch 188/200
2985/2985 - 168s - loss: 1.5924e-04 - val_loss: 1.3767e-04 - 168s/epoch - 56ms/step
Epoch 189/200
2985/2985 - 168s - loss: 1.6054e-04 - val_loss: 1.2743e-04 - 168s/epoch - 56ms/step
Epoch 190/200
2985/2985 - 167s - loss: 1.5624e-04 - val_loss: 1.2946e-04 - 167s/epoch - 56ms/step
Epoch 191/200
2985/2985 - 167s - loss: 1.6045e-04 - val_loss: 2.1236e-04 - 167s/epoch - 56ms/step
Epoch 192/200
2985/2985 - 167s - loss: 1.7734e-04 - val_loss: 1.4885e-04 - 167s/epoch - 56ms/step
Epoch 193/200
2985/2985 - 167s - loss: 1.6083e-04 - val_loss: 2.2175e-04 - 167s/epoch - 56ms/step
Epoch 194/200
2985/2985 - 167s - loss: 1.7563e-04 - val_loss: 1.3755e-04 - 167s/epoch - 56ms/step
Epoch 195/200
2985/2985 - 167s - loss: 1.6136e-04 - val_loss: 1.2767e-04 - 167s/epoch - 56ms/step
Epoch 196/200
2985/2985 - 167s - loss: 1.6046e-04 - val_loss: 1.3755e-04 - 167s/epoch - 56ms/step
Epoch 197/200
2985/2985 - 168s - loss: 1.5682e-04 - val_loss: 1.3825e-04 - 168s/epoch - 56ms/step
Epoch 198/200
2985/2985 - 168s - loss: 1.5785e-04 - val_loss: 1.4869e-04 - 168s/epoch - 56ms/step
Epoch 199/200
2985/2985 - 167s - loss: 1.5937e-04 - val_loss: 1.3370e-04 - 167s/epoch - 56ms/step
Epoch 200/200
2985/2985 - 167s - loss: 1.5742e-04 - val_loss: 1.3903e-04 - 167s/epoch - 56ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0001390322868246585
  1/332 [..............................] - ETA: 2:04  7/332 [..............................] - ETA: 2s   14/332 [>.............................] - ETA: 2s 21/332 [>.............................] - ETA: 2s 28/332 [=>............................] - ETA: 2s 35/332 [==>...........................] - ETA: 2s 42/332 [==>...........................] - ETA: 2s 49/332 [===>..........................] - ETA: 2s 56/332 [====>.........................] - ETA: 2s 63/332 [====>.........................] - ETA: 2s 70/332 [=====>........................] - ETA: 2s 77/332 [=====>........................] - ETA: 2s 84/332 [======>.......................] - ETA: 2s 91/332 [=======>......................] - ETA: 1s 98/332 [=======>......................] - ETA: 1s105/332 [========>.....................] - ETA: 1s112/332 [=========>....................] - ETA: 1s119/332 [=========>....................] - ETA: 1s126/332 [==========>...................] - ETA: 1s133/332 [===========>..................] - ETA: 1s140/332 [===========>..................] - ETA: 1s147/332 [============>.................] - ETA: 1s154/332 [============>.................] - ETA: 1s161/332 [=============>................] - ETA: 1s168/332 [==============>...............] - ETA: 1s175/332 [==============>...............] - ETA: 1s182/332 [===============>..............] - ETA: 1s189/332 [================>.............] - ETA: 1s196/332 [================>.............] - ETA: 1s203/332 [=================>............] - ETA: 1s210/332 [=================>............] - ETA: 0s217/332 [==================>...........] - ETA: 0s224/332 [===================>..........] - ETA: 0s231/332 [===================>..........] - ETA: 0s238/332 [====================>.........] - ETA: 0s245/332 [=====================>........] - ETA: 0s252/332 [=====================>........] - ETA: 0s259/332 [======================>.......] - ETA: 0s266/332 [=======================>......] - ETA: 0s273/332 [=======================>......] - ETA: 0s280/332 [========================>.....] - ETA: 0s287/332 [========================>.....] - ETA: 0s294/332 [=========================>....] - ETA: 0s301/332 [==========================>...] - ETA: 0s308/332 [==========================>...] - ETA: 0s315/332 [===========================>..] - ETA: 0s322/332 [============================>.] - ETA: 0s329/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 8ms/step
correlation 0.0015790564268883605
cosine 0.0012478877632653
MAE: 0.0063482276
RMSE: 0.011791188
r2: 0.990980876749755
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 3792)              4796880   
                                                                 
 batch_normalization_15 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 3792)              2400336   
                                                                 
 batch_normalization_17 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 3792)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Encoder
Model: "model_16"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_17 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 3792)              4796880   
                                                                 
 batch_normalization_15 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
=================================================================
Total params: 7,209,224
Trainable params: 7,201,640
Non-trainable params: 7,584
_________________________________________________________________
Decoder
Model: "model_17"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_18 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 3792)              2400336   
                                                                 
 batch_normalization_17 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 3792)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 7,212,384
Trainable params: 7,203,536
Non-trainable params: 8,848
_________________________________________________________________
['3n_b', 'mse', 32, 200, 0.001, 0.5, 632, 0.00015741701645310968, 0.0001390322868246585, 0.0015790564268883605, 0.0012478877632653, 0.006348227616399527, 0.011791188269853592, 0.990980876749755, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_18 (Dense)            (None, 3792)              4796880   
                                                                 
 batch_normalization_18 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 3792)              2400336   
                                                                 
 batch_normalization_20 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 3792)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Epoch 1/200
2985/2985 - 165s - loss: 0.0145 - val_loss: 0.0045 - 165s/epoch - 55ms/step
Epoch 2/200
2985/2985 - 165s - loss: 0.0034 - val_loss: 0.0021 - 165s/epoch - 55ms/step
Epoch 3/200
2985/2985 - 165s - loss: 0.0022 - val_loss: 0.0015 - 165s/epoch - 55ms/step
Epoch 4/200
2985/2985 - 165s - loss: 0.0018 - val_loss: 0.0013 - 165s/epoch - 55ms/step
Epoch 5/200
2985/2985 - 164s - loss: 0.0015 - val_loss: 0.0011 - 164s/epoch - 55ms/step
Epoch 6/200
2985/2985 - 164s - loss: 0.0014 - val_loss: 0.0010 - 164s/epoch - 55ms/step
Epoch 7/200
2985/2985 - 164s - loss: 0.0012 - val_loss: 9.2515e-04 - 164s/epoch - 55ms/step
Epoch 8/200
2985/2985 - 165s - loss: 0.0011 - val_loss: 8.5385e-04 - 165s/epoch - 55ms/step
Epoch 9/200
2985/2985 - 165s - loss: 0.0011 - val_loss: 8.0196e-04 - 165s/epoch - 55ms/step
Epoch 10/200
2985/2985 - 165s - loss: 9.8148e-04 - val_loss: 7.8275e-04 - 165s/epoch - 55ms/step
Epoch 11/200
2985/2985 - 165s - loss: 9.3279e-04 - val_loss: 7.6860e-04 - 165s/epoch - 55ms/step
Epoch 12/200
2985/2985 - 165s - loss: 8.7517e-04 - val_loss: 6.9756e-04 - 165s/epoch - 55ms/step
Epoch 13/200
2985/2985 - 165s - loss: 9.9587e-04 - val_loss: 0.0014 - 165s/epoch - 55ms/step
Epoch 14/200
2985/2985 - 165s - loss: 0.0010 - val_loss: 6.4806e-04 - 165s/epoch - 55ms/step
Epoch 15/200
2985/2985 - 165s - loss: 8.5156e-04 - val_loss: 6.7946e-04 - 165s/epoch - 55ms/step
Epoch 16/200
2985/2985 - 165s - loss: 8.0794e-04 - val_loss: 6.8366e-04 - 165s/epoch - 55ms/step
Epoch 17/200
2985/2985 - 165s - loss: 7.9186e-04 - val_loss: 6.5433e-04 - 165s/epoch - 55ms/step
Epoch 18/200
2985/2985 - 165s - loss: 8.2756e-04 - val_loss: 6.0016e-04 - 165s/epoch - 55ms/step
Epoch 19/200
2985/2985 - 165s - loss: 7.6442e-04 - val_loss: 6.3607e-04 - 165s/epoch - 55ms/step
Epoch 20/200
2985/2985 - 165s - loss: 7.4116e-04 - val_loss: 5.6918e-04 - 165s/epoch - 55ms/step
Epoch 21/200
2985/2985 - 165s - loss: 7.4192e-04 - val_loss: 5.5904e-04 - 165s/epoch - 55ms/step
Epoch 22/200
2985/2985 - 165s - loss: 7.1338e-04 - val_loss: 5.7307e-04 - 165s/epoch - 55ms/step
Epoch 23/200
2985/2985 - 170s - loss: 7.0348e-04 - val_loss: 5.4883e-04 - 170s/epoch - 57ms/step
Epoch 24/200
2985/2985 - 169s - loss: 7.0015e-04 - val_loss: 5.8989e-04 - 169s/epoch - 56ms/step
Epoch 25/200
2985/2985 - 169s - loss: 7.0006e-04 - val_loss: 5.1130e-04 - 169s/epoch - 57ms/step
Epoch 26/200
2985/2985 - 169s - loss: 6.8194e-04 - val_loss: 5.4141e-04 - 169s/epoch - 57ms/step
Epoch 27/200
2985/2985 - 168s - loss: 7.1871e-04 - val_loss: 4.8561e-04 - 168s/epoch - 56ms/step
Epoch 28/200
2985/2985 - 168s - loss: 6.5471e-04 - val_loss: 5.0821e-04 - 168s/epoch - 56ms/step
Epoch 29/200
2985/2985 - 168s - loss: 6.4405e-04 - val_loss: 5.0835e-04 - 168s/epoch - 56ms/step
Epoch 30/200
2985/2985 - 168s - loss: 6.5380e-04 - val_loss: 4.9629e-04 - 168s/epoch - 56ms/step
Epoch 31/200
2985/2985 - 168s - loss: 6.3417e-04 - val_loss: 4.7844e-04 - 168s/epoch - 56ms/step
Epoch 32/200
2985/2985 - 167s - loss: 6.3731e-04 - val_loss: 4.9469e-04 - 167s/epoch - 56ms/step
Epoch 33/200
2985/2985 - 169s - loss: 6.3980e-04 - val_loss: 4.6510e-04 - 169s/epoch - 57ms/step
Epoch 34/200
2985/2985 - 168s - loss: 6.1633e-04 - val_loss: 4.5714e-04 - 168s/epoch - 56ms/step
Epoch 35/200
2985/2985 - 168s - loss: 6.0977e-04 - val_loss: 6.1745e-04 - 168s/epoch - 56ms/step
Epoch 36/200
2985/2985 - 168s - loss: 6.8859e-04 - val_loss: 4.6510e-04 - 168s/epoch - 56ms/step
Epoch 37/200
2985/2985 - 168s - loss: 6.2576e-04 - val_loss: 5.0061e-04 - 168s/epoch - 56ms/step
Epoch 38/200
2985/2985 - 168s - loss: 6.0626e-04 - val_loss: 4.5598e-04 - 168s/epoch - 56ms/step
Epoch 39/200
2985/2985 - 168s - loss: 5.9708e-04 - val_loss: 4.5361e-04 - 168s/epoch - 56ms/step
Epoch 40/200
2985/2985 - 168s - loss: 5.8710e-04 - val_loss: 4.9224e-04 - 168s/epoch - 56ms/step
Epoch 41/200
2985/2985 - 167s - loss: 6.1980e-04 - val_loss: 4.2656e-04 - 167s/epoch - 56ms/step
Epoch 42/200
2985/2985 - 167s - loss: 5.8193e-04 - val_loss: 4.4156e-04 - 167s/epoch - 56ms/step
Epoch 43/200
2985/2985 - 169s - loss: 5.8193e-04 - val_loss: 4.6243e-04 - 169s/epoch - 56ms/step
Epoch 44/200
2985/2985 - 168s - loss: 5.7860e-04 - val_loss: 4.9777e-04 - 168s/epoch - 56ms/step
Epoch 45/200
2985/2985 - 168s - loss: 6.5984e-04 - val_loss: 4.3748e-04 - 168s/epoch - 56ms/step
Epoch 46/200
2985/2985 - 169s - loss: 5.8420e-04 - val_loss: 4.2938e-04 - 169s/epoch - 56ms/step
Epoch 47/200
2985/2985 - 168s - loss: 5.6749e-04 - val_loss: 4.3910e-04 - 168s/epoch - 56ms/step
Epoch 48/200
2985/2985 - 168s - loss: 5.9354e-04 - val_loss: 4.2043e-04 - 168s/epoch - 56ms/step
Epoch 49/200
2985/2985 - 168s - loss: 5.6930e-04 - val_loss: 4.2364e-04 - 168s/epoch - 56ms/step
Epoch 50/200
2985/2985 - 168s - loss: 5.5876e-04 - val_loss: 4.4358e-04 - 168s/epoch - 56ms/step
Epoch 51/200
2985/2985 - 167s - loss: 6.1049e-04 - val_loss: 4.2336e-04 - 167s/epoch - 56ms/step
Epoch 52/200
2985/2985 - 168s - loss: 5.8784e-04 - val_loss: 4.6031e-04 - 168s/epoch - 56ms/step
Epoch 53/200
2985/2985 - 168s - loss: 5.7185e-04 - val_loss: 4.2113e-04 - 168s/epoch - 56ms/step
Epoch 54/200
2985/2985 - 167s - loss: 5.4680e-04 - val_loss: 4.5131e-04 - 167s/epoch - 56ms/step
Epoch 55/200
2985/2985 - 168s - loss: 5.4695e-04 - val_loss: 3.8140e-04 - 168s/epoch - 56ms/step
Epoch 56/200
2985/2985 - 168s - loss: 5.4657e-04 - val_loss: 3.9338e-04 - 168s/epoch - 56ms/step
Epoch 57/200
2985/2985 - 168s - loss: 5.6834e-04 - val_loss: 4.1371e-04 - 168s/epoch - 56ms/step
Epoch 58/200
2985/2985 - 167s - loss: 5.7920e-04 - val_loss: 4.1324e-04 - 167s/epoch - 56ms/step
Epoch 59/200
2985/2985 - 168s - loss: 5.4290e-04 - val_loss: 4.2533e-04 - 168s/epoch - 56ms/step
Epoch 60/200
2985/2985 - 169s - loss: 5.3805e-04 - val_loss: 3.9818e-04 - 169s/epoch - 56ms/step
Epoch 61/200
2985/2985 - 168s - loss: 5.3652e-04 - val_loss: 4.2409e-04 - 168s/epoch - 56ms/step
Epoch 62/200
2985/2985 - 168s - loss: 5.4973e-04 - val_loss: 4.3477e-04 - 168s/epoch - 56ms/step
Epoch 63/200
2985/2985 - 169s - loss: 5.3056e-04 - val_loss: 4.1506e-04 - 169s/epoch - 57ms/step
Epoch 64/200
2985/2985 - 168s - loss: 5.2827e-04 - val_loss: 4.0085e-04 - 168s/epoch - 56ms/step
Epoch 65/200
2985/2985 - 168s - loss: 5.2450e-04 - val_loss: 4.2694e-04 - 168s/epoch - 56ms/step
Epoch 66/200
2985/2985 - 168s - loss: 5.3565e-04 - val_loss: 3.7846e-04 - 168s/epoch - 56ms/step
Epoch 67/200
2985/2985 - 169s - loss: 5.3738e-04 - val_loss: 4.1268e-04 - 169s/epoch - 57ms/step
Epoch 68/200
2985/2985 - 169s - loss: 5.1693e-04 - val_loss: 3.9404e-04 - 169s/epoch - 56ms/step
Epoch 69/200
2985/2985 - 169s - loss: 5.2655e-04 - val_loss: 4.1771e-04 - 169s/epoch - 56ms/step
Epoch 70/200
2985/2985 - 169s - loss: 5.2334e-04 - val_loss: 4.7765e-04 - 169s/epoch - 56ms/step
Epoch 71/200
2985/2985 - 168s - loss: 5.5184e-04 - val_loss: 3.7888e-04 - 168s/epoch - 56ms/step
Epoch 72/200
2985/2985 - 168s - loss: 5.1858e-04 - val_loss: 3.9582e-04 - 168s/epoch - 56ms/step
Epoch 73/200
2985/2985 - 168s - loss: 5.0831e-04 - val_loss: 3.8241e-04 - 168s/epoch - 56ms/step
Epoch 74/200
2985/2985 - 168s - loss: 5.0897e-04 - val_loss: 3.7610e-04 - 168s/epoch - 56ms/step
Epoch 75/200
2985/2985 - 169s - loss: 5.1795e-04 - val_loss: 0.0022 - 169s/epoch - 56ms/step
Epoch 76/200
2985/2985 - 168s - loss: 5.5742e-04 - val_loss: 3.9191e-04 - 168s/epoch - 56ms/step
Epoch 77/200
2985/2985 - 169s - loss: 5.0985e-04 - val_loss: 3.7981e-04 - 169s/epoch - 57ms/step
Epoch 78/200
2985/2985 - 168s - loss: 5.0465e-04 - val_loss: 3.7439e-04 - 168s/epoch - 56ms/step
Epoch 79/200
2985/2985 - 168s - loss: 4.9794e-04 - val_loss: 3.9602e-04 - 168s/epoch - 56ms/step
Epoch 80/200
2985/2985 - 169s - loss: 5.0110e-04 - val_loss: 3.8878e-04 - 169s/epoch - 57ms/step
Epoch 81/200
2985/2985 - 168s - loss: 5.3796e-04 - val_loss: 3.9296e-04 - 168s/epoch - 56ms/step
Epoch 82/200
2985/2985 - 168s - loss: 4.9812e-04 - val_loss: 3.9326e-04 - 168s/epoch - 56ms/step
Epoch 83/200
2985/2985 - 168s - loss: 5.2788e-04 - val_loss: 3.9506e-04 - 168s/epoch - 56ms/step
Epoch 84/200
2985/2985 - 168s - loss: 5.0523e-04 - val_loss: 3.7158e-04 - 168s/epoch - 56ms/step
Epoch 85/200
2985/2985 - 168s - loss: 4.9429e-04 - val_loss: 3.5677e-04 - 168s/epoch - 56ms/step
Epoch 86/200
2985/2985 - 168s - loss: 5.0451e-04 - val_loss: 4.2779e-04 - 168s/epoch - 56ms/step
Epoch 87/200
2985/2985 - 168s - loss: 4.9343e-04 - val_loss: 3.7097e-04 - 168s/epoch - 56ms/step
Epoch 88/200
2985/2985 - 168s - loss: 4.9282e-04 - val_loss: 3.7952e-04 - 168s/epoch - 56ms/step
Epoch 89/200
2985/2985 - 168s - loss: 5.0231e-04 - val_loss: 3.9510e-04 - 168s/epoch - 56ms/step
Epoch 90/200
2985/2985 - 169s - loss: 4.9129e-04 - val_loss: 3.6327e-04 - 169s/epoch - 57ms/step
Epoch 91/200
2985/2985 - 168s - loss: 4.9140e-04 - val_loss: 3.7228e-04 - 168s/epoch - 56ms/step
Epoch 92/200
2985/2985 - 168s - loss: 5.0921e-04 - val_loss: 7.0324e-04 - 168s/epoch - 56ms/step
Epoch 93/200
2985/2985 - 168s - loss: 5.8198e-04 - val_loss: 3.7433e-04 - 168s/epoch - 56ms/step
Epoch 94/200
2985/2985 - 169s - loss: 5.0478e-04 - val_loss: 3.7542e-04 - 169s/epoch - 57ms/step
Epoch 95/200
2985/2985 - 168s - loss: 4.9569e-04 - val_loss: 3.8124e-04 - 168s/epoch - 56ms/step
Epoch 96/200
2985/2985 - 169s - loss: 5.9859e-04 - val_loss: 5.7678e-04 - 169s/epoch - 57ms/step
Epoch 97/200
2985/2985 - 169s - loss: 5.3474e-04 - val_loss: 3.6375e-04 - 169s/epoch - 56ms/step
Epoch 98/200
2985/2985 - 168s - loss: 4.9791e-04 - val_loss: 3.7681e-04 - 168s/epoch - 56ms/step
Epoch 99/200
2985/2985 - 168s - loss: 4.8910e-04 - val_loss: 3.7656e-04 - 168s/epoch - 56ms/step
Epoch 100/200
2985/2985 - 168s - loss: 4.7984e-04 - val_loss: 3.7489e-04 - 168s/epoch - 56ms/step
Epoch 101/200
2985/2985 - 168s - loss: 4.8627e-04 - val_loss: 3.6127e-04 - 168s/epoch - 56ms/step
Epoch 102/200
2985/2985 - 168s - loss: 4.7882e-04 - val_loss: 3.5535e-04 - 168s/epoch - 56ms/step
Epoch 103/200
2985/2985 - 168s - loss: 5.1934e-04 - val_loss: 6.4250e-04 - 168s/epoch - 56ms/step
Epoch 104/200
2985/2985 - 168s - loss: 5.4377e-04 - val_loss: 3.7263e-04 - 168s/epoch - 56ms/step
Epoch 105/200
2985/2985 - 167s - loss: 4.8432e-04 - val_loss: 3.3760e-04 - 167s/epoch - 56ms/step
Epoch 106/200
2985/2985 - 169s - loss: 5.1456e-04 - val_loss: 3.5254e-04 - 169s/epoch - 57ms/step
Epoch 107/200
2985/2985 - 168s - loss: 4.8084e-04 - val_loss: 3.9370e-04 - 168s/epoch - 56ms/step
Epoch 108/200
2985/2985 - 168s - loss: 4.9432e-04 - val_loss: 3.4968e-04 - 168s/epoch - 56ms/step
Epoch 109/200
2985/2985 - 168s - loss: 4.7537e-04 - val_loss: 4.1776e-04 - 168s/epoch - 56ms/step
Epoch 110/200
2985/2985 - 168s - loss: 5.3528e-04 - val_loss: 3.8636e-04 - 168s/epoch - 56ms/step
Epoch 111/200
2985/2985 - 169s - loss: 4.7908e-04 - val_loss: 3.7785e-04 - 169s/epoch - 56ms/step
Epoch 112/200
2985/2985 - 168s - loss: 4.6866e-04 - val_loss: 3.5456e-04 - 168s/epoch - 56ms/step
Epoch 113/200
2985/2985 - 168s - loss: 4.8791e-04 - val_loss: 3.5521e-04 - 168s/epoch - 56ms/step
Epoch 114/200
2985/2985 - 168s - loss: 4.7090e-04 - val_loss: 3.6943e-04 - 168s/epoch - 56ms/step
Epoch 115/200
2985/2985 - 168s - loss: 4.7487e-04 - val_loss: 4.4487e-04 - 168s/epoch - 56ms/step
Epoch 116/200
2985/2985 - 168s - loss: 5.0612e-04 - val_loss: 3.4096e-04 - 168s/epoch - 56ms/step
Epoch 117/200
2985/2985 - 168s - loss: 4.6783e-04 - val_loss: 3.5553e-04 - 168s/epoch - 56ms/step
Epoch 118/200
2985/2985 - 168s - loss: 4.8837e-04 - val_loss: 3.7313e-04 - 168s/epoch - 56ms/step
Epoch 119/200
2985/2985 - 168s - loss: 4.6321e-04 - val_loss: 3.5439e-04 - 168s/epoch - 56ms/step
Epoch 120/200
2985/2985 - 168s - loss: 5.0474e-04 - val_loss: 3.7910e-04 - 168s/epoch - 56ms/step
Epoch 121/200
2985/2985 - 169s - loss: 4.7111e-04 - val_loss: 3.7393e-04 - 169s/epoch - 57ms/step
Epoch 122/200
2985/2985 - 168s - loss: 4.7184e-04 - val_loss: 3.6631e-04 - 168s/epoch - 56ms/step
Epoch 123/200
2985/2985 - 168s - loss: 4.7027e-04 - val_loss: 3.6191e-04 - 168s/epoch - 56ms/step
Epoch 124/200
2985/2985 - 168s - loss: 4.6250e-04 - val_loss: 3.6471e-04 - 168s/epoch - 56ms/step
Epoch 125/200
2985/2985 - 168s - loss: 4.5448e-04 - val_loss: 3.4428e-04 - 168s/epoch - 56ms/step
Epoch 126/200
2985/2985 - 169s - loss: 4.6473e-04 - val_loss: 3.3956e-04 - 169s/epoch - 57ms/step
Epoch 127/200
2985/2985 - 169s - loss: 4.5539e-04 - val_loss: 3.5080e-04 - 169s/epoch - 57ms/step
Epoch 128/200
2985/2985 - 168s - loss: 4.7489e-04 - val_loss: 3.8885e-04 - 168s/epoch - 56ms/step
Epoch 129/200
2985/2985 - 168s - loss: 4.6657e-04 - val_loss: 3.3566e-04 - 168s/epoch - 56ms/step
Epoch 130/200
2985/2985 - 168s - loss: 4.5649e-04 - val_loss: 3.3309e-04 - 168s/epoch - 56ms/step
Epoch 131/200
2985/2985 - 169s - loss: 4.5160e-04 - val_loss: 3.4473e-04 - 169s/epoch - 57ms/step
Epoch 132/200
2985/2985 - 168s - loss: 4.5637e-04 - val_loss: 3.6185e-04 - 168s/epoch - 56ms/step
Epoch 133/200
2985/2985 - 168s - loss: 4.5312e-04 - val_loss: 3.5521e-04 - 168s/epoch - 56ms/step
Epoch 134/200
2985/2985 - 168s - loss: 4.6467e-04 - val_loss: 5.2565e-04 - 168s/epoch - 56ms/step
Epoch 135/200
2985/2985 - 169s - loss: 4.8316e-04 - val_loss: 3.3316e-04 - 169s/epoch - 56ms/step
Epoch 136/200
2985/2985 - 168s - loss: 4.5578e-04 - val_loss: 3.7000e-04 - 168s/epoch - 56ms/step
Epoch 137/200
2985/2985 - 169s - loss: 4.6975e-04 - val_loss: 6.2246e-04 - 169s/epoch - 57ms/step
Epoch 138/200
2985/2985 - 169s - loss: 5.0870e-04 - val_loss: 3.4743e-04 - 169s/epoch - 56ms/step
Epoch 139/200
2985/2985 - 168s - loss: 4.8150e-04 - val_loss: 3.5939e-04 - 168s/epoch - 56ms/step
Epoch 140/200
2985/2985 - 168s - loss: 4.5122e-04 - val_loss: 3.5393e-04 - 168s/epoch - 56ms/step
Epoch 141/200
2985/2985 - 168s - loss: 4.6430e-04 - val_loss: 3.3748e-04 - 168s/epoch - 56ms/step
Epoch 142/200
2985/2985 - 168s - loss: 4.5942e-04 - val_loss: 3.3317e-04 - 168s/epoch - 56ms/step
Epoch 143/200
2985/2985 - 168s - loss: 4.4905e-04 - val_loss: 3.6931e-04 - 168s/epoch - 56ms/step
Epoch 144/200
2985/2985 - 168s - loss: 4.5377e-04 - val_loss: 3.4973e-04 - 168s/epoch - 56ms/step
Epoch 145/200
2985/2985 - 168s - loss: 4.5570e-04 - val_loss: 3.4093e-04 - 168s/epoch - 56ms/step
Epoch 146/200
2985/2985 - 168s - loss: 4.4883e-04 - val_loss: 3.5057e-04 - 168s/epoch - 56ms/step
Epoch 147/200
2985/2985 - 168s - loss: 4.7985e-04 - val_loss: 4.4828e-04 - 168s/epoch - 56ms/step
Epoch 148/200
2985/2985 - 169s - loss: 4.9091e-04 - val_loss: 3.9883e-04 - 169s/epoch - 57ms/step
Epoch 149/200
2985/2985 - 168s - loss: 4.4914e-04 - val_loss: 3.3159e-04 - 168s/epoch - 56ms/step
Epoch 150/200
2985/2985 - 168s - loss: 4.4936e-04 - val_loss: 3.5595e-04 - 168s/epoch - 56ms/step
Epoch 151/200
2985/2985 - 169s - loss: 4.9646e-04 - val_loss: 3.5360e-04 - 169s/epoch - 57ms/step
Epoch 152/200
2985/2985 - 168s - loss: 4.7574e-04 - val_loss: 5.4432e-04 - 168s/epoch - 56ms/step
Epoch 153/200
2985/2985 - 169s - loss: 4.8885e-04 - val_loss: 3.5078e-04 - 169s/epoch - 57ms/step
Epoch 154/200
2985/2985 - 169s - loss: 4.5643e-04 - val_loss: 3.6536e-04 - 169s/epoch - 57ms/step
Epoch 155/200
2985/2985 - 168s - loss: 4.6254e-04 - val_loss: 3.3181e-04 - 168s/epoch - 56ms/step
Epoch 156/200
2985/2985 - 169s - loss: 4.4618e-04 - val_loss: 3.3501e-04 - 169s/epoch - 57ms/step
Epoch 157/200
2985/2985 - 168s - loss: 4.4632e-04 - val_loss: 3.4728e-04 - 168s/epoch - 56ms/step
Epoch 158/200
2985/2985 - 169s - loss: 4.4197e-04 - val_loss: 3.4005e-04 - 169s/epoch - 56ms/step
Epoch 159/200
2985/2985 - 168s - loss: 4.3864e-04 - val_loss: 3.5939e-04 - 168s/epoch - 56ms/step
Epoch 160/200
2985/2985 - 168s - loss: 4.4020e-04 - val_loss: 3.3395e-04 - 168s/epoch - 56ms/step
Epoch 161/200
2985/2985 - 168s - loss: 4.5869e-04 - val_loss: 3.9078e-04 - 168s/epoch - 56ms/step
Epoch 162/200
2985/2985 - 168s - loss: 4.6305e-04 - val_loss: 3.4601e-04 - 168s/epoch - 56ms/step
Epoch 163/200
2985/2985 - 168s - loss: 4.4488e-04 - val_loss: 3.4781e-04 - 168s/epoch - 56ms/step
Epoch 164/200
2985/2985 - 168s - loss: 4.5336e-04 - val_loss: 4.3120e-04 - 168s/epoch - 56ms/step
Epoch 165/200
2985/2985 - 168s - loss: 5.6700e-04 - val_loss: 3.5754e-04 - 168s/epoch - 56ms/step
Epoch 166/200
2985/2985 - 168s - loss: 4.5713e-04 - val_loss: 3.5011e-04 - 168s/epoch - 56ms/step
Epoch 167/200
2985/2985 - 168s - loss: 4.5673e-04 - val_loss: 3.4297e-04 - 168s/epoch - 56ms/step
Epoch 168/200
2985/2985 - 169s - loss: 4.5912e-04 - val_loss: 3.4065e-04 - 169s/epoch - 57ms/step
Epoch 169/200
2985/2985 - 168s - loss: 4.5398e-04 - val_loss: 3.6359e-04 - 168s/epoch - 56ms/step
Epoch 170/200
2985/2985 - 168s - loss: 4.4500e-04 - val_loss: 3.3234e-04 - 168s/epoch - 56ms/step
Epoch 171/200
2985/2985 - 169s - loss: 4.4788e-04 - val_loss: 4.3615e-04 - 169s/epoch - 56ms/step
Epoch 172/200
2985/2985 - 168s - loss: 4.4226e-04 - val_loss: 3.4587e-04 - 168s/epoch - 56ms/step
Epoch 173/200
2985/2985 - 169s - loss: 4.3956e-04 - val_loss: 3.6500e-04 - 169s/epoch - 57ms/step
Epoch 174/200
2985/2985 - 169s - loss: 4.4860e-04 - val_loss: 3.3020e-04 - 169s/epoch - 57ms/step
Epoch 175/200
2985/2985 - 169s - loss: 4.3701e-04 - val_loss: 3.5672e-04 - 169s/epoch - 56ms/step
Epoch 176/200
2985/2985 - 168s - loss: 4.3849e-04 - val_loss: 3.2663e-04 - 168s/epoch - 56ms/step
Epoch 177/200
2985/2985 - 168s - loss: 4.5115e-04 - val_loss: 3.9649e-04 - 168s/epoch - 56ms/step
Epoch 178/200
2985/2985 - 169s - loss: 4.5190e-04 - val_loss: 3.4467e-04 - 169s/epoch - 57ms/step
Epoch 179/200
2985/2985 - 168s - loss: 4.4285e-04 - val_loss: 3.3719e-04 - 168s/epoch - 56ms/step
Epoch 180/200
2985/2985 - 168s - loss: 4.5237e-04 - val_loss: 3.3133e-04 - 168s/epoch - 56ms/step
Epoch 181/200
2985/2985 - 168s - loss: 4.3692e-04 - val_loss: 3.3856e-04 - 168s/epoch - 56ms/step
Epoch 182/200
2985/2985 - 168s - loss: 4.4865e-04 - val_loss: 3.2393e-04 - 168s/epoch - 56ms/step
Epoch 183/200
2985/2985 - 169s - loss: 4.3945e-04 - val_loss: 3.2034e-04 - 169s/epoch - 57ms/step
Epoch 184/200
2985/2985 - 168s - loss: 4.3735e-04 - val_loss: 3.4665e-04 - 168s/epoch - 56ms/step
Epoch 185/200
2985/2985 - 169s - loss: 4.3716e-04 - val_loss: 3.4803e-04 - 169s/epoch - 57ms/step
Epoch 186/200
2985/2985 - 169s - loss: 4.3095e-04 - val_loss: 3.4079e-04 - 169s/epoch - 57ms/step
Epoch 187/200
2985/2985 - 169s - loss: 4.5991e-04 - val_loss: 3.4541e-04 - 169s/epoch - 57ms/step
Epoch 188/200
2985/2985 - 169s - loss: 4.4542e-04 - val_loss: 7.0792e-04 - 169s/epoch - 57ms/step
Epoch 189/200
2985/2985 - 169s - loss: 4.8515e-04 - val_loss: 3.4107e-04 - 169s/epoch - 57ms/step
Epoch 190/200
2985/2985 - 169s - loss: 5.0106e-04 - val_loss: 0.0010 - 169s/epoch - 57ms/step
Epoch 191/200
2985/2985 - 169s - loss: 4.9121e-04 - val_loss: 7.1558e-04 - 169s/epoch - 57ms/step
Epoch 192/200
2985/2985 - 169s - loss: 4.8565e-04 - val_loss: 3.3549e-04 - 169s/epoch - 56ms/step
Epoch 193/200
2985/2985 - 169s - loss: 4.5178e-04 - val_loss: 4.0113e-04 - 169s/epoch - 57ms/step
Epoch 194/200
2985/2985 - 169s - loss: 4.4356e-04 - val_loss: 3.6713e-04 - 169s/epoch - 57ms/step
Epoch 195/200
2985/2985 - 168s - loss: 4.3598e-04 - val_loss: 3.4458e-04 - 168s/epoch - 56ms/step
Epoch 196/200
2985/2985 - 169s - loss: 4.7202e-04 - val_loss: 3.4367e-04 - 169s/epoch - 57ms/step
Epoch 197/200
2985/2985 - 168s - loss: 4.6796e-04 - val_loss: 3.5871e-04 - 168s/epoch - 56ms/step
Epoch 198/200
2985/2985 - 169s - loss: 4.3986e-04 - val_loss: 3.8417e-04 - 169s/epoch - 57ms/step
Epoch 199/200
2985/2985 - 169s - loss: 4.4732e-04 - val_loss: 3.7488e-04 - 169s/epoch - 56ms/step
Epoch 200/200
2985/2985 - 168s - loss: 4.4337e-04 - val_loss: 5.8659e-04 - 168s/epoch - 56ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0005865931161679327
  1/332 [..............................] - ETA: 3:15  7/332 [..............................] - ETA: 2s   13/332 [>.............................] - ETA: 2s 19/332 [>.............................] - ETA: 2s 25/332 [=>............................] - ETA: 2s 31/332 [=>............................] - ETA: 2s 37/332 [==>...........................] - ETA: 2s 43/332 [==>...........................] - ETA: 2s 49/332 [===>..........................] - ETA: 2s 55/332 [===>..........................] - ETA: 2s 61/332 [====>.........................] - ETA: 2s 67/332 [=====>........................] - ETA: 2s 73/332 [=====>........................] - ETA: 2s 79/332 [======>.......................] - ETA: 2s 85/332 [======>.......................] - ETA: 2s 91/332 [=======>......................] - ETA: 2s 98/332 [=======>......................] - ETA: 2s104/332 [========>.....................] - ETA: 2s110/332 [========>.....................] - ETA: 1s116/332 [=========>....................] - ETA: 1s122/332 [==========>...................] - ETA: 1s128/332 [==========>...................] - ETA: 1s134/332 [===========>..................] - ETA: 1s140/332 [===========>..................] - ETA: 1s146/332 [============>.................] - ETA: 1s152/332 [============>.................] - ETA: 1s158/332 [=============>................] - ETA: 1s164/332 [=============>................] - ETA: 1s170/332 [==============>...............] - ETA: 1s176/332 [==============>...............] - ETA: 1s182/332 [===============>..............] - ETA: 1s188/332 [===============>..............] - ETA: 1s195/332 [================>.............] - ETA: 1s201/332 [=================>............] - ETA: 1s207/332 [=================>............] - ETA: 1s213/332 [==================>...........] - ETA: 1s219/332 [==================>...........] - ETA: 0s226/332 [===================>..........] - ETA: 0s233/332 [====================>.........] - ETA: 0s239/332 [====================>.........] - ETA: 0s245/332 [=====================>........] - ETA: 0s251/332 [=====================>........] - ETA: 0s257/332 [======================>.......] - ETA: 0s263/332 [======================>.......] - ETA: 0s269/332 [=======================>......] - ETA: 0s275/332 [=======================>......] - ETA: 0s281/332 [========================>.....] - ETA: 0s287/332 [========================>.....] - ETA: 0s293/332 [=========================>....] - ETA: 0s300/332 [==========================>...] - ETA: 0s307/332 [==========================>...] - ETA: 0s313/332 [===========================>..] - ETA: 0s319/332 [===========================>..] - ETA: 0s325/332 [============================>.] - ETA: 0s332/332 [==============================] - ETA: 0s332/332 [==============================] - 3s 9ms/step
correlation 0.0066014209581778446
cosine 0.005271104565666075
MAE: 0.016311022
RMSE: 0.024219682
r2: 0.9619467908648534
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 3792)              4796880   
                                                                 
 batch_normalization_18 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 3792)              2400336   
                                                                 
 batch_normalization_20 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 3792)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Encoder
Model: "model_19"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_20 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 3792)              4796880   
                                                                 
 batch_normalization_18 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
=================================================================
Total params: 7,209,224
Trainable params: 7,201,640
Non-trainable params: 7,584
_________________________________________________________________
Decoder
Model: "model_20"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_21 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 3792)              2400336   
                                                                 
 batch_normalization_20 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 3792)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 7,212,384
Trainable params: 7,203,536
Non-trainable params: 8,848
_________________________________________________________________
['3n_b', 'mse', 32, 200, 0.002, 0.5, 632, 0.00044337447616271675, 0.0005865931161679327, 0.0066014209581778446, 0.005271104565666075, 0.016311021521687508, 0.024219682440161705, 0.9619467908648534, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_21 (Dense)            (None, 3792)              4796880   
                                                                 
 batch_normalization_21 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_22 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 632)               0         
                                                                 
 dense_22 (Dense)            (None, 3792)              2400336   
                                                                 
 batch_normalization_23 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 3792)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Epoch 1/300
2985/2985 - 173s - loss: 0.0087 - val_loss: 0.0032 - 173s/epoch - 58ms/step
Epoch 2/300
2985/2985 - 171s - loss: 0.0034 - val_loss: 0.0024 - 171s/epoch - 57ms/step
Epoch 3/300
2985/2985 - 172s - loss: 0.0027 - val_loss: 0.0015 - 172s/epoch - 58ms/step
Epoch 4/300
2985/2985 - 172s - loss: 0.0019 - val_loss: 0.0012 - 172s/epoch - 57ms/step
Epoch 5/300
2985/2985 - 172s - loss: 0.0015 - val_loss: 9.5733e-04 - 172s/epoch - 58ms/step
Epoch 6/300
2985/2985 - 172s - loss: 0.0011 - val_loss: 9.0913e-04 - 172s/epoch - 58ms/step
Epoch 7/300
2985/2985 - 172s - loss: 9.2943e-04 - val_loss: 6.6496e-04 - 172s/epoch - 58ms/step
Epoch 8/300
2985/2985 - 172s - loss: 7.8982e-04 - val_loss: 6.4974e-04 - 172s/epoch - 58ms/step
Epoch 9/300
2985/2985 - 172s - loss: 6.9889e-04 - val_loss: 5.1901e-04 - 172s/epoch - 58ms/step
Epoch 10/300
2985/2985 - 173s - loss: 6.2484e-04 - val_loss: 4.8267e-04 - 173s/epoch - 58ms/step
Epoch 11/300
2985/2985 - 172s - loss: 5.7574e-04 - val_loss: 4.8659e-04 - 172s/epoch - 58ms/step
Epoch 12/300
2985/2985 - 172s - loss: 5.2675e-04 - val_loss: 7.2691e-04 - 172s/epoch - 58ms/step
Epoch 13/300
2985/2985 - 172s - loss: 5.1908e-04 - val_loss: 4.5143e-04 - 172s/epoch - 58ms/step
Epoch 14/300
2985/2985 - 172s - loss: 4.6958e-04 - val_loss: 3.9148e-04 - 172s/epoch - 58ms/step
Epoch 15/300
2985/2985 - 171s - loss: 4.4814e-04 - val_loss: 3.7370e-04 - 171s/epoch - 57ms/step
Epoch 16/300
2985/2985 - 172s - loss: 4.2094e-04 - val_loss: 3.8337e-04 - 172s/epoch - 57ms/step
Epoch 17/300
2985/2985 - 172s - loss: 4.0076e-04 - val_loss: 3.3500e-04 - 172s/epoch - 57ms/step
Epoch 18/300
2985/2985 - 171s - loss: 3.8451e-04 - val_loss: 3.4929e-04 - 171s/epoch - 57ms/step
Epoch 19/300
2985/2985 - 171s - loss: 3.6991e-04 - val_loss: 3.4462e-04 - 171s/epoch - 57ms/step
Epoch 20/300
2985/2985 - 171s - loss: 3.6941e-04 - val_loss: 3.0977e-04 - 171s/epoch - 57ms/step
Epoch 21/300
2985/2985 - 173s - loss: 3.4663e-04 - val_loss: 2.8655e-04 - 173s/epoch - 58ms/step
Epoch 22/300
2985/2985 - 172s - loss: 3.3475e-04 - val_loss: 2.9691e-04 - 172s/epoch - 58ms/step
Epoch 23/300
2985/2985 - 172s - loss: 3.3449e-04 - val_loss: 2.9203e-04 - 172s/epoch - 58ms/step
Epoch 24/300
2985/2985 - 172s - loss: 3.1751e-04 - val_loss: 2.8600e-04 - 172s/epoch - 58ms/step
Epoch 25/300
2985/2985 - 171s - loss: 3.1356e-04 - val_loss: 3.0930e-04 - 171s/epoch - 57ms/step
Epoch 26/300
2985/2985 - 171s - loss: 3.4249e-04 - val_loss: 2.4788e-04 - 171s/epoch - 57ms/step
Epoch 27/300
2985/2985 - 171s - loss: 3.0757e-04 - val_loss: 2.6509e-04 - 171s/epoch - 57ms/step
Epoch 28/300
2985/2985 - 171s - loss: 2.9516e-04 - val_loss: 2.4499e-04 - 171s/epoch - 57ms/step
Epoch 29/300
2985/2985 - 172s - loss: 2.8482e-04 - val_loss: 2.4388e-04 - 172s/epoch - 58ms/step
Epoch 30/300
2985/2985 - 172s - loss: 2.8253e-04 - val_loss: 2.5459e-04 - 172s/epoch - 58ms/step
Epoch 31/300
2985/2985 - 171s - loss: 2.7400e-04 - val_loss: 2.5812e-04 - 171s/epoch - 57ms/step
Epoch 32/300
2985/2985 - 171s - loss: 2.7531e-04 - val_loss: 2.2219e-04 - 171s/epoch - 57ms/step
Epoch 33/300
2985/2985 - 171s - loss: 2.6480e-04 - val_loss: 2.2611e-04 - 171s/epoch - 57ms/step
Epoch 34/300
2985/2985 - 172s - loss: 2.6600e-04 - val_loss: 2.3041e-04 - 172s/epoch - 58ms/step
Epoch 35/300
2985/2985 - 171s - loss: 2.5392e-04 - val_loss: 2.1825e-04 - 171s/epoch - 57ms/step
Epoch 36/300
2985/2985 - 172s - loss: 2.5162e-04 - val_loss: 2.3068e-04 - 172s/epoch - 57ms/step
Epoch 37/300
2985/2985 - 172s - loss: 2.6495e-04 - val_loss: 2.2021e-04 - 172s/epoch - 57ms/step
Epoch 38/300
2985/2985 - 171s - loss: 2.4764e-04 - val_loss: 2.0569e-04 - 171s/epoch - 57ms/step
Epoch 39/300
2985/2985 - 172s - loss: 2.4162e-04 - val_loss: 2.0663e-04 - 172s/epoch - 58ms/step
Epoch 40/300
2985/2985 - 172s - loss: 2.3965e-04 - val_loss: 2.1650e-04 - 172s/epoch - 58ms/step
Epoch 41/300
2985/2985 - 172s - loss: 2.3747e-04 - val_loss: 1.9802e-04 - 172s/epoch - 57ms/step
Epoch 42/300
2985/2985 - 172s - loss: 2.6246e-04 - val_loss: 1.9218e-04 - 172s/epoch - 58ms/step
Epoch 43/300
2985/2985 - 171s - loss: 2.3152e-04 - val_loss: 2.1071e-04 - 171s/epoch - 57ms/step
Epoch 44/300
2985/2985 - 172s - loss: 2.3212e-04 - val_loss: 2.0858e-04 - 172s/epoch - 58ms/step
Epoch 45/300
2985/2985 - 172s - loss: 2.2823e-04 - val_loss: 2.0095e-04 - 172s/epoch - 57ms/step
Epoch 46/300
2985/2985 - 171s - loss: 2.2180e-04 - val_loss: 1.9944e-04 - 171s/epoch - 57ms/step
Epoch 47/300
2985/2985 - 172s - loss: 2.2392e-04 - val_loss: 1.8781e-04 - 172s/epoch - 58ms/step
Epoch 48/300
2985/2985 - 172s - loss: 2.2288e-04 - val_loss: 1.8578e-04 - 172s/epoch - 58ms/step
Epoch 49/300
2985/2985 - 172s - loss: 2.2067e-04 - val_loss: 1.9365e-04 - 172s/epoch - 57ms/step
Epoch 50/300
2985/2985 - 172s - loss: 2.1570e-04 - val_loss: 2.5898e-04 - 172s/epoch - 57ms/step
Epoch 51/300
2985/2985 - 171s - loss: 2.2368e-04 - val_loss: 1.7431e-04 - 171s/epoch - 57ms/step
Epoch 52/300
2985/2985 - 172s - loss: 2.0872e-04 - val_loss: 1.7267e-04 - 172s/epoch - 57ms/step
Epoch 53/300
2985/2985 - 171s - loss: 2.0621e-04 - val_loss: 1.7916e-04 - 171s/epoch - 57ms/step
Epoch 54/300
2985/2985 - 171s - loss: 2.0613e-04 - val_loss: 1.8248e-04 - 171s/epoch - 57ms/step
Epoch 55/300
2985/2985 - 171s - loss: 2.0062e-04 - val_loss: 1.7314e-04 - 171s/epoch - 57ms/step
Epoch 56/300
2985/2985 - 171s - loss: 2.1496e-04 - val_loss: 1.7193e-04 - 171s/epoch - 57ms/step
Epoch 57/300
2985/2985 - 172s - loss: 2.0089e-04 - val_loss: 1.6759e-04 - 172s/epoch - 58ms/step
Epoch 58/300
2985/2985 - 171s - loss: 1.9812e-04 - val_loss: 1.8813e-04 - 171s/epoch - 57ms/step
Epoch 59/300
2985/2985 - 172s - loss: 1.9304e-04 - val_loss: 1.6681e-04 - 172s/epoch - 58ms/step
Epoch 60/300
2985/2985 - 172s - loss: 2.0134e-04 - val_loss: 1.6606e-04 - 172s/epoch - 58ms/step
Epoch 61/300
2985/2985 - 171s - loss: 1.9706e-04 - val_loss: 1.6674e-04 - 171s/epoch - 57ms/step
Epoch 62/300
2985/2985 - 172s - loss: 1.9474e-04 - val_loss: 1.9141e-04 - 172s/epoch - 58ms/step
Epoch 63/300
2985/2985 - 172s - loss: 1.9169e-04 - val_loss: 2.7100e-04 - 172s/epoch - 58ms/step
Epoch 64/300
2985/2985 - 171s - loss: 1.9055e-04 - val_loss: 1.5759e-04 - 171s/epoch - 57ms/step
Epoch 65/300
2985/2985 - 172s - loss: 1.8855e-04 - val_loss: 1.7028e-04 - 172s/epoch - 58ms/step
Epoch 66/300
2985/2985 - 171s - loss: 1.8739e-04 - val_loss: 1.6916e-04 - 171s/epoch - 57ms/step
Epoch 67/300
2985/2985 - 172s - loss: 1.8753e-04 - val_loss: 1.5459e-04 - 172s/epoch - 58ms/step
Epoch 68/300
2985/2985 - 172s - loss: 1.8614e-04 - val_loss: 1.6727e-04 - 172s/epoch - 57ms/step
Epoch 69/300
2985/2985 - 172s - loss: 1.8279e-04 - val_loss: 1.6417e-04 - 172s/epoch - 58ms/step
Epoch 70/300
2985/2985 - 172s - loss: 1.8141e-04 - val_loss: 1.9739e-04 - 172s/epoch - 58ms/step
Epoch 71/300
2985/2985 - 172s - loss: 1.8225e-04 - val_loss: 1.5989e-04 - 172s/epoch - 58ms/step
Epoch 72/300
2985/2985 - 172s - loss: 1.8655e-04 - val_loss: 1.5399e-04 - 172s/epoch - 58ms/step
Epoch 73/300
2985/2985 - 172s - loss: 1.9189e-04 - val_loss: 2.6695e-04 - 172s/epoch - 57ms/step
Epoch 74/300
2985/2985 - 171s - loss: 2.1337e-04 - val_loss: 1.5531e-04 - 171s/epoch - 57ms/step
Epoch 75/300
2985/2985 - 172s - loss: 1.8472e-04 - val_loss: 1.5614e-04 - 172s/epoch - 58ms/step
Epoch 76/300
2985/2985 - 172s - loss: 1.7864e-04 - val_loss: 1.6539e-04 - 172s/epoch - 58ms/step
Epoch 77/300
2985/2985 - 171s - loss: 1.8711e-04 - val_loss: 4.4433e-04 - 171s/epoch - 57ms/step
Epoch 78/300
2985/2985 - 172s - loss: 2.3754e-04 - val_loss: 1.6858e-04 - 172s/epoch - 57ms/step
Epoch 79/300
2985/2985 - 171s - loss: 1.8473e-04 - val_loss: 1.8742e-04 - 171s/epoch - 57ms/step
Epoch 80/300
2985/2985 - 172s - loss: 1.8221e-04 - val_loss: 1.6612e-04 - 172s/epoch - 58ms/step
Epoch 81/300
2985/2985 - 172s - loss: 1.7768e-04 - val_loss: 1.6923e-04 - 172s/epoch - 58ms/step
Epoch 82/300
2985/2985 - 172s - loss: 1.7288e-04 - val_loss: 1.6728e-04 - 172s/epoch - 58ms/step
Epoch 83/300
2985/2985 - 172s - loss: 1.7259e-04 - val_loss: 1.6498e-04 - 172s/epoch - 58ms/step
Epoch 84/300
2985/2985 - 171s - loss: 1.7183e-04 - val_loss: 1.5183e-04 - 171s/epoch - 57ms/step
Epoch 85/300
2985/2985 - 172s - loss: 1.6885e-04 - val_loss: 1.4548e-04 - 172s/epoch - 57ms/step
Epoch 86/300
2985/2985 - 172s - loss: 1.6975e-04 - val_loss: 1.5452e-04 - 172s/epoch - 58ms/step
Epoch 87/300
2985/2985 - 172s - loss: 1.7057e-04 - val_loss: 1.6120e-04 - 172s/epoch - 57ms/step
Epoch 88/300
2985/2985 - 172s - loss: 1.7306e-04 - val_loss: 1.4898e-04 - 172s/epoch - 57ms/step
Epoch 89/300
2985/2985 - 172s - loss: 1.6643e-04 - val_loss: 1.4995e-04 - 172s/epoch - 58ms/step
Epoch 90/300
2985/2985 - 171s - loss: 1.7016e-04 - val_loss: 1.4499e-04 - 171s/epoch - 57ms/step
Epoch 91/300
2985/2985 - 171s - loss: 1.6310e-04 - val_loss: 1.4204e-04 - 171s/epoch - 57ms/step
Epoch 92/300
2985/2985 - 172s - loss: 1.6389e-04 - val_loss: 1.4849e-04 - 172s/epoch - 58ms/step
Epoch 93/300
2985/2985 - 172s - loss: 1.6258e-04 - val_loss: 1.4959e-04 - 172s/epoch - 58ms/step
Epoch 94/300
2985/2985 - 171s - loss: 1.6117e-04 - val_loss: 1.3926e-04 - 171s/epoch - 57ms/step
Epoch 95/300
2985/2985 - 172s - loss: 1.6534e-04 - val_loss: 1.4645e-04 - 172s/epoch - 57ms/step
Epoch 96/300
2985/2985 - 172s - loss: 1.6332e-04 - val_loss: 1.5237e-04 - 172s/epoch - 58ms/step
Epoch 97/300
2985/2985 - 171s - loss: 1.5971e-04 - val_loss: 1.4574e-04 - 171s/epoch - 57ms/step
Epoch 98/300
2985/2985 - 172s - loss: 1.6276e-04 - val_loss: 1.9364e-04 - 172s/epoch - 58ms/step
Epoch 99/300
2985/2985 - 172s - loss: 1.7223e-04 - val_loss: 1.3664e-04 - 172s/epoch - 58ms/step
Epoch 100/300
2985/2985 - 171s - loss: 1.6305e-04 - val_loss: 1.3882e-04 - 171s/epoch - 57ms/step
Epoch 101/300
2985/2985 - 172s - loss: 1.5931e-04 - val_loss: 1.4503e-04 - 172s/epoch - 58ms/step
Epoch 102/300
2985/2985 - 172s - loss: 1.5844e-04 - val_loss: 1.3776e-04 - 172s/epoch - 58ms/step
Epoch 103/300
2985/2985 - 172s - loss: 1.5653e-04 - val_loss: 1.4220e-04 - 172s/epoch - 58ms/step
Epoch 104/300
2985/2985 - 172s - loss: 1.5798e-04 - val_loss: 1.6043e-04 - 172s/epoch - 58ms/step
Epoch 105/300
2985/2985 - 172s - loss: 1.6010e-04 - val_loss: 1.4968e-04 - 172s/epoch - 58ms/step
Epoch 106/300
2985/2985 - 172s - loss: 1.5816e-04 - val_loss: 1.5094e-04 - 172s/epoch - 58ms/step
Epoch 107/300
2985/2985 - 172s - loss: 1.5431e-04 - val_loss: 1.4902e-04 - 172s/epoch - 57ms/step
Epoch 108/300
2985/2985 - 172s - loss: 1.5491e-04 - val_loss: 1.4681e-04 - 172s/epoch - 57ms/step
Epoch 109/300
2985/2985 - 171s - loss: 1.5850e-04 - val_loss: 1.5300e-04 - 171s/epoch - 57ms/step
Epoch 110/300
2985/2985 - 171s - loss: 1.5539e-04 - val_loss: 1.4746e-04 - 171s/epoch - 57ms/step
Epoch 111/300
2985/2985 - 172s - loss: 1.5534e-04 - val_loss: 1.3842e-04 - 172s/epoch - 57ms/step
Epoch 112/300
slurmstepd: error: *** JOB 35411193 ON mb-icg101 CANCELLED AT 2023-01-04T23:59:37 DUE TO TIME LIMIT ***
