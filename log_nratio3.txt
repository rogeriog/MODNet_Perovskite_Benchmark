start
Wed Dec 28 16:06:42 CET 2022
2022-12-28 16:06:43.191391: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-28 16:06:43.267349: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-12-28 16:07:15,839 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fdc95a2c8b0> object, created with modnet version 0.1.12
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
Shape of dataset to encode: (106113, 1264)
2022-12-28 16:07:18.909665: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense (Dense)               (None, 1896)              2398440   
                                                                 
 batch_normalization (BatchN  (None, 1896)             7584      
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 1896)              0         
                                                                 
 bottleneck (Dense)          (None, 1011)              1917867   
                                                                 
 batch_normalization_1 (Batc  (None, 1011)             4044      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 1011)              0         
                                                                 
 dense_1 (Dense)             (None, 1896)              1918752   
                                                                 
 batch_normalization_2 (Batc  (None, 1896)             7584      
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 1896)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              2397808   
                                                                 
=================================================================
Total params: 8,652,079
Trainable params: 8,642,473
Non-trainable params: 9,606
_________________________________________________________________
Epoch 1/200
1493/1493 - 53s - loss: 0.0089 - val_loss: 0.0042 - 53s/epoch - 35ms/step
Epoch 2/200
1493/1493 - 52s - loss: 0.0028 - val_loss: 0.0027 - 52s/epoch - 35ms/step
Epoch 3/200
1493/1493 - 52s - loss: 0.0019 - val_loss: 0.0016 - 52s/epoch - 35ms/step
Epoch 4/200
1493/1493 - 52s - loss: 0.0016 - val_loss: 0.0028 - 52s/epoch - 35ms/step
Epoch 5/200
1493/1493 - 52s - loss: 0.0015 - val_loss: 0.0013 - 52s/epoch - 35ms/step
Epoch 6/200
1493/1493 - 52s - loss: 0.0012 - val_loss: 0.0012 - 52s/epoch - 35ms/step
Epoch 7/200
1493/1493 - 52s - loss: 0.0012 - val_loss: 0.0011 - 52s/epoch - 35ms/step
Epoch 8/200
1493/1493 - 52s - loss: 0.0011 - val_loss: 0.0013 - 52s/epoch - 35ms/step
Epoch 9/200
1493/1493 - 52s - loss: 9.8815e-04 - val_loss: 0.0020 - 52s/epoch - 35ms/step
Epoch 10/200
1493/1493 - 51s - loss: 0.0011 - val_loss: 8.3217e-04 - 51s/epoch - 34ms/step
Epoch 11/200
1493/1493 - 51s - loss: 8.5351e-04 - val_loss: 7.8621e-04 - 51s/epoch - 34ms/step
Epoch 12/200
1493/1493 - 51s - loss: 7.6548e-04 - val_loss: 9.1466e-04 - 51s/epoch - 34ms/step
Epoch 13/200
1493/1493 - 51s - loss: 7.6047e-04 - val_loss: 0.0020 - 51s/epoch - 34ms/step
Epoch 14/200
1493/1493 - 52s - loss: 7.8548e-04 - val_loss: 0.0012 - 52s/epoch - 35ms/step
Epoch 15/200
1493/1493 - 51s - loss: 7.3741e-04 - val_loss: 9.6485e-04 - 51s/epoch - 34ms/step
Epoch 16/200
1493/1493 - 51s - loss: 6.3843e-04 - val_loss: 5.5670e-04 - 51s/epoch - 34ms/step
Epoch 17/200
1493/1493 - 51s - loss: 5.8132e-04 - val_loss: 6.1091e-04 - 51s/epoch - 34ms/step
Epoch 18/200
1493/1493 - 51s - loss: 5.5129e-04 - val_loss: 7.6683e-04 - 51s/epoch - 34ms/step
Epoch 19/200
1493/1493 - 52s - loss: 5.4592e-04 - val_loss: 6.0212e-04 - 52s/epoch - 35ms/step
Epoch 20/200
1493/1493 - 51s - loss: 5.1377e-04 - val_loss: 5.0966e-04 - 51s/epoch - 34ms/step
Epoch 21/200
1493/1493 - 51s - loss: 4.9019e-04 - val_loss: 6.0148e-04 - 51s/epoch - 34ms/step
Epoch 22/200
1493/1493 - 52s - loss: 4.6607e-04 - val_loss: 5.5361e-04 - 52s/epoch - 35ms/step
Epoch 23/200
1493/1493 - 52s - loss: 4.5455e-04 - val_loss: 0.0010 - 52s/epoch - 35ms/step
Epoch 24/200
1493/1493 - 52s - loss: 4.6440e-04 - val_loss: 4.3684e-04 - 52s/epoch - 35ms/step
Epoch 25/200
1493/1493 - 51s - loss: 4.2881e-04 - val_loss: 4.4110e-04 - 51s/epoch - 34ms/step
Epoch 26/200
1493/1493 - 51s - loss: 3.9918e-04 - val_loss: 5.0041e-04 - 51s/epoch - 34ms/step
Epoch 27/200
1493/1493 - 52s - loss: 3.8208e-04 - val_loss: 4.0824e-04 - 52s/epoch - 35ms/step
Epoch 28/200
1493/1493 - 52s - loss: 3.6429e-04 - val_loss: 5.8755e-04 - 52s/epoch - 35ms/step
Epoch 29/200
1493/1493 - 52s - loss: 3.5932e-04 - val_loss: 5.3100e-04 - 52s/epoch - 35ms/step
Epoch 30/200
1493/1493 - 51s - loss: 3.6480e-04 - val_loss: 4.0133e-04 - 51s/epoch - 34ms/step
Epoch 31/200
1493/1493 - 51s - loss: 3.3533e-04 - val_loss: 7.8078e-04 - 51s/epoch - 34ms/step
Epoch 32/200
1493/1493 - 51s - loss: 3.8997e-04 - val_loss: 3.3494e-04 - 51s/epoch - 34ms/step
Epoch 33/200
1493/1493 - 52s - loss: 3.2467e-04 - val_loss: 3.9692e-04 - 52s/epoch - 35ms/step
Epoch 34/200
1493/1493 - 52s - loss: 3.1469e-04 - val_loss: 2.9164e-04 - 52s/epoch - 35ms/step
Epoch 35/200
1493/1493 - 52s - loss: 3.0111e-04 - val_loss: 5.9725e-04 - 52s/epoch - 35ms/step
Epoch 36/200
1493/1493 - 52s - loss: 3.1867e-04 - val_loss: 4.5931e-04 - 52s/epoch - 35ms/step
Epoch 37/200
1493/1493 - 52s - loss: 2.9310e-04 - val_loss: 8.9580e-04 - 52s/epoch - 35ms/step
Epoch 38/200
1493/1493 - 51s - loss: 3.4202e-04 - val_loss: 2.8357e-04 - 51s/epoch - 34ms/step
Epoch 39/200
1493/1493 - 52s - loss: 2.7892e-04 - val_loss: 2.4935e-04 - 52s/epoch - 35ms/step
Epoch 40/200
1493/1493 - 52s - loss: 2.6622e-04 - val_loss: 3.4661e-04 - 52s/epoch - 35ms/step
Epoch 41/200
1493/1493 - 52s - loss: 2.7020e-04 - val_loss: 2.4739e-04 - 52s/epoch - 35ms/step
Epoch 42/200
1493/1493 - 52s - loss: 2.5827e-04 - val_loss: 2.4893e-04 - 52s/epoch - 35ms/step
Epoch 43/200
1493/1493 - 52s - loss: 2.5860e-04 - val_loss: 4.6041e-04 - 52s/epoch - 35ms/step
Epoch 44/200
1493/1493 - 52s - loss: 2.6670e-04 - val_loss: 3.5595e-04 - 52s/epoch - 35ms/step
Epoch 45/200
1493/1493 - 52s - loss: 2.5719e-04 - val_loss: 2.1272e-04 - 52s/epoch - 35ms/step
Epoch 46/200
1493/1493 - 51s - loss: 2.4185e-04 - val_loss: 2.2750e-04 - 51s/epoch - 34ms/step
Epoch 47/200
1493/1493 - 52s - loss: 2.3916e-04 - val_loss: 2.4349e-04 - 52s/epoch - 35ms/step
Epoch 48/200
1493/1493 - 51s - loss: 2.3929e-04 - val_loss: 2.9793e-04 - 51s/epoch - 34ms/step
Epoch 49/200
1493/1493 - 52s - loss: 2.3425e-04 - val_loss: 4.8375e-04 - 52s/epoch - 35ms/step
Epoch 50/200
1493/1493 - 52s - loss: 2.4106e-04 - val_loss: 3.7051e-04 - 52s/epoch - 35ms/step
Epoch 51/200
1493/1493 - 52s - loss: 2.3336e-04 - val_loss: 2.6605e-04 - 52s/epoch - 35ms/step
Epoch 52/200
1493/1493 - 52s - loss: 2.1783e-04 - val_loss: 2.3922e-04 - 52s/epoch - 35ms/step
Epoch 53/200
1493/1493 - 52s - loss: 2.1914e-04 - val_loss: 2.0021e-04 - 52s/epoch - 35ms/step
Epoch 54/200
1493/1493 - 52s - loss: 2.1431e-04 - val_loss: 2.5749e-04 - 52s/epoch - 35ms/step
Epoch 55/200
1493/1493 - 51s - loss: 2.0679e-04 - val_loss: 2.1125e-04 - 51s/epoch - 34ms/step
Epoch 56/200
1493/1493 - 51s - loss: 2.0827e-04 - val_loss: 2.9420e-04 - 51s/epoch - 34ms/step
Epoch 57/200
1493/1493 - 51s - loss: 2.0193e-04 - val_loss: 2.3142e-04 - 51s/epoch - 34ms/step
Epoch 58/200
1493/1493 - 51s - loss: 2.0104e-04 - val_loss: 1.6835e-04 - 51s/epoch - 34ms/step
Epoch 59/200
