start
Wed Dec 28 16:06:42 CET 2022
2022-12-28 16:06:43.191448: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-28 16:06:43.268985: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-12-28 16:07:15,690 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f69ff2dd8b0> object, created with modnet version 0.1.12
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
Shape of dataset to encode: (106113, 1264)
2022-12-28 16:07:18.681631: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense (Dense)               (None, 1896)              2398440   
                                                                 
 batch_normalization (BatchN  (None, 1896)             7584      
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 1896)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1198904   
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 1896)              1200168   
                                                                 
 batch_normalization_2 (Batc  (None, 1896)             7584      
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 1896)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              2397808   
                                                                 
=================================================================
Total params: 7,213,016
Trainable params: 7,204,168
Non-trainable params: 8,848
_________________________________________________________________
Epoch 1/200
1493/1493 - 42s - loss: 0.0094 - val_loss: 0.0044 - 42s/epoch - 28ms/step
Epoch 2/200
1493/1493 - 41s - loss: 0.0030 - val_loss: 0.0027 - 41s/epoch - 27ms/step
Epoch 3/200
1493/1493 - 41s - loss: 0.0021 - val_loss: 0.0018 - 41s/epoch - 27ms/step
Epoch 4/200
1493/1493 - 41s - loss: 0.0017 - val_loss: 0.0035 - 41s/epoch - 28ms/step
Epoch 5/200
1493/1493 - 41s - loss: 0.0017 - val_loss: 0.0011 - 41s/epoch - 28ms/step
Epoch 6/200
1493/1493 - 41s - loss: 0.0013 - val_loss: 0.0014 - 41s/epoch - 28ms/step
Epoch 7/200
1493/1493 - 41s - loss: 0.0012 - val_loss: 9.6893e-04 - 41s/epoch - 28ms/step
Epoch 8/200
1493/1493 - 41s - loss: 0.0011 - val_loss: 0.0012 - 41s/epoch - 28ms/step
Epoch 9/200
1493/1493 - 41s - loss: 0.0010 - val_loss: 0.0010 - 41s/epoch - 28ms/step
Epoch 10/200
1493/1493 - 41s - loss: 9.7256e-04 - val_loss: 8.6949e-04 - 41s/epoch - 27ms/step
Epoch 11/200
1493/1493 - 41s - loss: 9.4740e-04 - val_loss: 8.5472e-04 - 41s/epoch - 27ms/step
Epoch 12/200
1493/1493 - 41s - loss: 8.0531e-04 - val_loss: 0.0012 - 41s/epoch - 27ms/step
Epoch 13/200
1493/1493 - 41s - loss: 8.0689e-04 - val_loss: 0.0014 - 41s/epoch - 27ms/step
Epoch 14/200
1493/1493 - 41s - loss: 7.6837e-04 - val_loss: 9.7125e-04 - 41s/epoch - 27ms/step
Epoch 15/200
1493/1493 - 41s - loss: 7.4101e-04 - val_loss: 8.1052e-04 - 41s/epoch - 27ms/step
Epoch 16/200
1493/1493 - 41s - loss: 6.5887e-04 - val_loss: 6.4313e-04 - 41s/epoch - 27ms/step
Epoch 17/200
1493/1493 - 41s - loss: 6.2398e-04 - val_loss: 7.5626e-04 - 41s/epoch - 27ms/step
Epoch 18/200
1493/1493 - 41s - loss: 6.0050e-04 - val_loss: 9.5803e-04 - 41s/epoch - 27ms/step
Epoch 19/200
1493/1493 - 41s - loss: 5.9081e-04 - val_loss: 6.5072e-04 - 41s/epoch - 27ms/step
Epoch 20/200
1493/1493 - 41s - loss: 5.5863e-04 - val_loss: 5.3073e-04 - 41s/epoch - 27ms/step
Epoch 21/200
1493/1493 - 41s - loss: 5.2290e-04 - val_loss: 7.4610e-04 - 41s/epoch - 27ms/step
Epoch 22/200
1493/1493 - 41s - loss: 5.0307e-04 - val_loss: 7.6720e-04 - 41s/epoch - 27ms/step
Epoch 23/200
1493/1493 - 41s - loss: 5.1008e-04 - val_loss: 8.7727e-04 - 41s/epoch - 27ms/step
Epoch 24/200
1493/1493 - 41s - loss: 4.8545e-04 - val_loss: 4.7725e-04 - 41s/epoch - 27ms/step
Epoch 25/200
1493/1493 - 41s - loss: 4.6094e-04 - val_loss: 4.5247e-04 - 41s/epoch - 27ms/step
Epoch 26/200
1493/1493 - 41s - loss: 4.2857e-04 - val_loss: 5.2477e-04 - 41s/epoch - 27ms/step
Epoch 27/200
1493/1493 - 41s - loss: 4.1238e-04 - val_loss: 4.0318e-04 - 41s/epoch - 27ms/step
Epoch 28/200
1493/1493 - 41s - loss: 3.9399e-04 - val_loss: 6.7365e-04 - 41s/epoch - 27ms/step
Epoch 29/200
1493/1493 - 41s - loss: 3.8358e-04 - val_loss: 5.3483e-04 - 41s/epoch - 27ms/step
Epoch 30/200
1493/1493 - 41s - loss: 3.8078e-04 - val_loss: 4.3145e-04 - 41s/epoch - 27ms/step
Epoch 31/200
1493/1493 - 41s - loss: 3.6143e-04 - val_loss: 5.9179e-04 - 41s/epoch - 27ms/step
Epoch 32/200
1493/1493 - 41s - loss: 3.9819e-04 - val_loss: 3.6697e-04 - 41s/epoch - 27ms/step
Epoch 33/200
1493/1493 - 41s - loss: 3.4909e-04 - val_loss: 3.5472e-04 - 41s/epoch - 27ms/step
Epoch 34/200
1493/1493 - 41s - loss: 3.3533e-04 - val_loss: 3.1103e-04 - 41s/epoch - 27ms/step
Epoch 35/200
1493/1493 - 41s - loss: 3.1998e-04 - val_loss: 0.0011 - 41s/epoch - 27ms/step
Epoch 36/200
1493/1493 - 41s - loss: 3.6604e-04 - val_loss: 3.9682e-04 - 41s/epoch - 27ms/step
Epoch 37/200
1493/1493 - 41s - loss: 3.1513e-04 - val_loss: 6.3140e-04 - 41s/epoch - 27ms/step
Epoch 38/200
1493/1493 - 41s - loss: 3.3699e-04 - val_loss: 2.8728e-04 - 41s/epoch - 27ms/step
Epoch 39/200
1493/1493 - 41s - loss: 2.9741e-04 - val_loss: 2.9459e-04 - 41s/epoch - 27ms/step
Epoch 40/200
1493/1493 - 41s - loss: 2.8643e-04 - val_loss: 4.2044e-04 - 41s/epoch - 27ms/step
Epoch 41/200
1493/1493 - 41s - loss: 2.9143e-04 - val_loss: 2.6076e-04 - 41s/epoch - 27ms/step
Epoch 42/200
1493/1493 - 41s - loss: 2.7959e-04 - val_loss: 2.6049e-04 - 41s/epoch - 27ms/step
Epoch 43/200
1493/1493 - 41s - loss: 2.7489e-04 - val_loss: 4.0429e-04 - 41s/epoch - 27ms/step
Epoch 44/200
1493/1493 - 41s - loss: 2.7927e-04 - val_loss: 4.4057e-04 - 41s/epoch - 27ms/step
Epoch 45/200
1493/1493 - 41s - loss: 2.7967e-04 - val_loss: 2.3936e-04 - 41s/epoch - 27ms/step
Epoch 46/200
1493/1493 - 41s - loss: 2.6175e-04 - val_loss: 2.5200e-04 - 41s/epoch - 27ms/step
Epoch 47/200
1493/1493 - 41s - loss: 2.5509e-04 - val_loss: 3.1548e-04 - 41s/epoch - 27ms/step
Epoch 48/200
1493/1493 - 41s - loss: 2.5547e-04 - val_loss: 6.1538e-04 - 41s/epoch - 27ms/step
Epoch 49/200
1493/1493 - 41s - loss: 2.5033e-04 - val_loss: 4.7747e-04 - 41s/epoch - 27ms/step
Epoch 50/200
1493/1493 - 41s - loss: 2.5532e-04 - val_loss: 3.6883e-04 - 41s/epoch - 27ms/step
Epoch 51/200
1493/1493 - 41s - loss: 2.5420e-04 - val_loss: 2.7023e-04 - 41s/epoch - 27ms/step
Epoch 52/200
1493/1493 - 41s - loss: 2.3468e-04 - val_loss: 2.3504e-04 - 41s/epoch - 27ms/step
Epoch 53/200
1493/1493 - 41s - loss: 2.3826e-04 - val_loss: 8.4469e-04 - 41s/epoch - 27ms/step
Epoch 54/200
1493/1493 - 41s - loss: 2.8817e-04 - val_loss: 2.6141e-04 - 41s/epoch - 27ms/step
Epoch 55/200
1493/1493 - 41s - loss: 2.2806e-04 - val_loss: 2.0961e-04 - 41s/epoch - 27ms/step
Epoch 56/200
1493/1493 - 41s - loss: 2.2872e-04 - val_loss: 2.6459e-04 - 41s/epoch - 27ms/step
Epoch 57/200
1493/1493 - 41s - loss: 2.2023e-04 - val_loss: 2.5489e-04 - 41s/epoch - 27ms/step
Epoch 58/200
1493/1493 - 41s - loss: 2.1946e-04 - val_loss: 2.0927e-04 - 41s/epoch - 27ms/step
Epoch 59/200
1493/1493 - 41s - loss: 2.1541e-04 - val_loss: 2.4910e-04 - 41s/epoch - 27ms/step
Epoch 60/200
1493/1493 - 41s - loss: 2.1715e-04 - val_loss: 6.9434e-04 - 41s/epoch - 27ms/step
Epoch 61/200
1493/1493 - 41s - loss: 3.0422e-04 - val_loss: 1.9223e-04 - 41s/epoch - 27ms/step
Epoch 62/200
1493/1493 - 41s - loss: 2.1604e-04 - val_loss: 2.4983e-04 - 41s/epoch - 27ms/step
Epoch 63/200
1493/1493 - 41s - loss: 2.0912e-04 - val_loss: 2.4825e-04 - 41s/epoch - 27ms/step
Epoch 64/200
1493/1493 - 41s - loss: 2.1255e-04 - val_loss: 0.0012 - 41s/epoch - 27ms/step
Epoch 65/200
1493/1493 - 41s - loss: 3.5099e-04 - val_loss: 2.1192e-04 - 41s/epoch - 27ms/step
Epoch 66/200
1493/1493 - 41s - loss: 2.1553e-04 - val_loss: 2.3696e-04 - 41s/epoch - 27ms/step
Epoch 67/200
1493/1493 - 41s - loss: 2.0797e-04 - val_loss: 2.0636e-04 - 41s/epoch - 27ms/step
Epoch 68/200
1493/1493 - 41s - loss: 2.0395e-04 - val_loss: 0.0010 - 41s/epoch - 27ms/step
Epoch 69/200
1493/1493 - 41s - loss: 2.7686e-04 - val_loss: 2.4132e-04 - 41s/epoch - 27ms/step
Epoch 70/200
1493/1493 - 41s - loss: 2.0654e-04 - val_loss: 6.2395e-04 - 41s/epoch - 27ms/step
Epoch 71/200
1493/1493 - 41s - loss: 2.4617e-04 - val_loss: 1.9408e-04 - 41s/epoch - 27ms/step
Epoch 72/200
1493/1493 - 41s - loss: 2.0214e-04 - val_loss: 2.5744e-04 - 41s/epoch - 27ms/step
Epoch 73/200
1493/1493 - 41s - loss: 2.0123e-04 - val_loss: 2.3258e-04 - 41s/epoch - 27ms/step
Epoch 74/200
1493/1493 - 41s - loss: 1.9742e-04 - val_loss: 2.0839e-04 - 41s/epoch - 27ms/step
Epoch 75/200
1493/1493 - 41s - loss: 1.8860e-04 - val_loss: 2.3615e-04 - 41s/epoch - 27ms/step
Epoch 76/200
1493/1493 - 41s - loss: 1.9276e-04 - val_loss: 2.0018e-04 - 41s/epoch - 27ms/step
Epoch 77/200
1493/1493 - 41s - loss: 1.8664e-04 - val_loss: 4.7022e-04 - 41s/epoch - 27ms/step
Epoch 78/200
1493/1493 - 41s - loss: 1.9231e-04 - val_loss: 6.5690e-04 - 41s/epoch - 27ms/step
Epoch 79/200
1493/1493 - 42s - loss: 2.2244e-04 - val_loss: 4.0648e-04 - 42s/epoch - 28ms/step
Epoch 80/200
1493/1493 - 42s - loss: 2.2532e-04 - val_loss: 1.5576e-04 - 42s/epoch - 28ms/step
Epoch 81/200
1493/1493 - 41s - loss: 1.8275e-04 - val_loss: 3.2738e-04 - 41s/epoch - 27ms/step
Epoch 82/200
1493/1493 - 41s - loss: 1.8055e-04 - val_loss: 1.8270e-04 - 41s/epoch - 27ms/step
Epoch 83/200
1493/1493 - 41s - loss: 1.7583e-04 - val_loss: 1.9759e-04 - 41s/epoch - 27ms/step
Epoch 84/200
1493/1493 - 41s - loss: 1.7525e-04 - val_loss: 2.0122e-04 - 41s/epoch - 27ms/step
Epoch 85/200
1493/1493 - 41s - loss: 1.7943e-04 - val_loss: 2.2060e-04 - 41s/epoch - 27ms/step
Epoch 86/200
1493/1493 - 41s - loss: 1.7487e-04 - val_loss: 1.7382e-04 - 41s/epoch - 27ms/step
Epoch 87/200
1493/1493 - 41s - loss: 1.7054e-04 - val_loss: 1.6464e-04 - 41s/epoch - 27ms/step
Epoch 88/200
1493/1493 - 41s - loss: 1.7009e-04 - val_loss: 2.1483e-04 - 41s/epoch - 27ms/step
Epoch 89/200
1493/1493 - 41s - loss: 1.7188e-04 - val_loss: 1.7444e-04 - 41s/epoch - 27ms/step
Epoch 90/200
1493/1493 - 41s - loss: 1.6584e-04 - val_loss: 1.8349e-04 - 41s/epoch - 27ms/step
Epoch 91/200
1493/1493 - 41s - loss: 1.6493e-04 - val_loss: 1.8813e-04 - 41s/epoch - 27ms/step
Epoch 92/200
1493/1493 - 41s - loss: 1.6378e-04 - val_loss: 5.5131e-04 - 41s/epoch - 27ms/step
Epoch 93/200
1493/1493 - 42s - loss: 2.0690e-04 - val_loss: 3.1860e-04 - 42s/epoch - 28ms/step
Epoch 94/200
1493/1493 - 41s - loss: 1.8131e-04 - val_loss: 1.3276e-04 - 41s/epoch - 28ms/step
Epoch 95/200
1493/1493 - 41s - loss: 1.6283e-04 - val_loss: 1.9998e-04 - 41s/epoch - 28ms/step
Epoch 96/200
1493/1493 - 41s - loss: 1.6321e-04 - val_loss: 2.0031e-04 - 41s/epoch - 28ms/step
Epoch 97/200
1493/1493 - 41s - loss: 1.6081e-04 - val_loss: 1.8029e-04 - 41s/epoch - 28ms/step
Epoch 98/200
1493/1493 - 41s - loss: 1.6285e-04 - val_loss: 8.5878e-04 - 41s/epoch - 28ms/step
Epoch 99/200
1493/1493 - 41s - loss: 2.0796e-04 - val_loss: 1.6876e-04 - 41s/epoch - 27ms/step
Epoch 100/200
1493/1493 - 41s - loss: 1.6082e-04 - val_loss: 1.6997e-04 - 41s/epoch - 27ms/step
Epoch 101/200
1493/1493 - 41s - loss: 1.5761e-04 - val_loss: 3.5394e-04 - 41s/epoch - 27ms/step
Epoch 102/200
1493/1493 - 41s - loss: 1.7793e-04 - val_loss: 2.1429e-04 - 41s/epoch - 27ms/step
Epoch 103/200
1493/1493 - 41s - loss: 1.6003e-04 - val_loss: 1.5597e-04 - 41s/epoch - 27ms/step
Epoch 104/200
1493/1493 - 41s - loss: 1.5341e-04 - val_loss: 2.6000e-04 - 41s/epoch - 27ms/step
Epoch 105/200
1493/1493 - 41s - loss: 1.6159e-04 - val_loss: 1.6256e-04 - 41s/epoch - 27ms/step
Epoch 106/200
1493/1493 - 41s - loss: 1.5133e-04 - val_loss: 1.6361e-04 - 41s/epoch - 27ms/step
Epoch 107/200
1493/1493 - 41s - loss: 1.5036e-04 - val_loss: 1.7316e-04 - 41s/epoch - 27ms/step
Epoch 108/200
1493/1493 - 41s - loss: 1.5029e-04 - val_loss: 1.6286e-04 - 41s/epoch - 27ms/step
Epoch 109/200
1493/1493 - 41s - loss: 1.4952e-04 - val_loss: 1.7217e-04 - 41s/epoch - 27ms/step
Epoch 110/200
1493/1493 - 41s - loss: 1.4665e-04 - val_loss: 1.6992e-04 - 41s/epoch - 27ms/step
Epoch 111/200
1493/1493 - 41s - loss: 1.4799e-04 - val_loss: 4.3083e-04 - 41s/epoch - 27ms/step
Epoch 112/200
1493/1493 - 41s - loss: 1.8151e-04 - val_loss: 1.3766e-04 - 41s/epoch - 27ms/step
Epoch 113/200
1493/1493 - 41s - loss: 1.4809e-04 - val_loss: 6.5415e-04 - 41s/epoch - 27ms/step
Epoch 114/200
1493/1493 - 41s - loss: 2.0359e-04 - val_loss: 1.5274e-04 - 41s/epoch - 27ms/step
Epoch 115/200
1493/1493 - 41s - loss: 1.4986e-04 - val_loss: 2.4529e-04 - 41s/epoch - 27ms/step
Epoch 116/200
1493/1493 - 41s - loss: 1.4743e-04 - val_loss: 1.6642e-04 - 41s/epoch - 27ms/step
Epoch 117/200
1493/1493 - 41s - loss: 1.4433e-04 - val_loss: 1.8764e-04 - 41s/epoch - 27ms/step
Epoch 118/200
1493/1493 - 41s - loss: 1.4779e-04 - val_loss: 1.5656e-04 - 41s/epoch - 27ms/step
Epoch 119/200
1493/1493 - 41s - loss: 1.4238e-04 - val_loss: 1.4664e-04 - 41s/epoch - 27ms/step
Epoch 120/200
1493/1493 - 41s - loss: 1.4090e-04 - val_loss: 1.3254e-04 - 41s/epoch - 27ms/step
Epoch 121/200
1493/1493 - 41s - loss: 1.4035e-04 - val_loss: 2.3974e-04 - 41s/epoch - 27ms/step
Epoch 122/200
1493/1493 - 41s - loss: 1.4760e-04 - val_loss: 1.4489e-04 - 41s/epoch - 27ms/step
Epoch 123/200
1493/1493 - 41s - loss: 1.3938e-04 - val_loss: 1.5889e-04 - 41s/epoch - 27ms/step
Epoch 124/200
1493/1493 - 41s - loss: 1.3873e-04 - val_loss: 5.8564e-04 - 41s/epoch - 27ms/step
Epoch 125/200
1493/1493 - 41s - loss: 1.6828e-04 - val_loss: 1.3610e-04 - 41s/epoch - 27ms/step
Epoch 126/200
1493/1493 - 41s - loss: 1.4099e-04 - val_loss: 4.6102e-04 - 41s/epoch - 27ms/step
Epoch 127/200
1493/1493 - 41s - loss: 1.6165e-04 - val_loss: 3.3629e-04 - 41s/epoch - 27ms/step
Epoch 128/200
1493/1493 - 41s - loss: 1.5650e-04 - val_loss: 1.1807e-04 - 41s/epoch - 27ms/step
Epoch 129/200
1493/1493 - 41s - loss: 1.3746e-04 - val_loss: 1.2965e-04 - 41s/epoch - 27ms/step
Epoch 130/200
1493/1493 - 41s - loss: 1.3565e-04 - val_loss: 1.3520e-04 - 41s/epoch - 27ms/step
Epoch 131/200
1493/1493 - 41s - loss: 1.3807e-04 - val_loss: 3.3282e-04 - 41s/epoch - 27ms/step
Epoch 132/200
1493/1493 - 41s - loss: 1.6036e-04 - val_loss: 1.2187e-04 - 41s/epoch - 27ms/step
Epoch 133/200
1493/1493 - 41s - loss: 1.3386e-04 - val_loss: 1.5937e-04 - 41s/epoch - 27ms/step
Epoch 134/200
1493/1493 - 41s - loss: 1.3354e-04 - val_loss: 1.3912e-04 - 41s/epoch - 27ms/step
Epoch 135/200
1493/1493 - 41s - loss: 1.3133e-04 - val_loss: 1.3069e-04 - 41s/epoch - 27ms/step
Epoch 136/200
1493/1493 - 41s - loss: 1.3167e-04 - val_loss: 2.2814e-04 - 41s/epoch - 27ms/step
Epoch 137/200
1493/1493 - 41s - loss: 1.5330e-04 - val_loss: 2.6976e-04 - 41s/epoch - 27ms/step
Epoch 138/200
1493/1493 - 41s - loss: 1.6478e-04 - val_loss: 2.0630e-04 - 41s/epoch - 27ms/step
Epoch 139/200
1493/1493 - 41s - loss: 1.4980e-04 - val_loss: 2.6342e-04 - 41s/epoch - 27ms/step
Epoch 140/200
1493/1493 - 41s - loss: 1.6628e-04 - val_loss: 1.2151e-04 - 41s/epoch - 27ms/step
Epoch 141/200
1493/1493 - 41s - loss: 1.3489e-04 - val_loss: 1.7539e-04 - 41s/epoch - 27ms/step
Epoch 142/200
1493/1493 - 41s - loss: 1.3649e-04 - val_loss: 1.4030e-04 - 41s/epoch - 27ms/step
Epoch 143/200
1493/1493 - 41s - loss: 1.3085e-04 - val_loss: 1.9070e-04 - 41s/epoch - 27ms/step
Epoch 144/200
1493/1493 - 41s - loss: 1.3980e-04 - val_loss: 1.4192e-04 - 41s/epoch - 27ms/step
Epoch 145/200
1493/1493 - 41s - loss: 1.3016e-04 - val_loss: 1.3143e-04 - 41s/epoch - 27ms/step
Epoch 146/200
1493/1493 - 41s - loss: 1.2813e-04 - val_loss: 1.4861e-04 - 41s/epoch - 27ms/step
Epoch 147/200
1493/1493 - 41s - loss: 1.2689e-04 - val_loss: 1.5510e-04 - 41s/epoch - 27ms/step
Epoch 148/200
1493/1493 - 41s - loss: 1.2607e-04 - val_loss: 2.3347e-04 - 41s/epoch - 27ms/step
Epoch 149/200
1493/1493 - 41s - loss: 1.2555e-04 - val_loss: 1.2886e-04 - 41s/epoch - 27ms/step
Epoch 150/200
1493/1493 - 41s - loss: 1.2421e-04 - val_loss: 1.2438e-04 - 41s/epoch - 27ms/step
Epoch 151/200
1493/1493 - 41s - loss: 1.3326e-04 - val_loss: 1.3255e-04 - 41s/epoch - 27ms/step
Epoch 152/200
1493/1493 - 41s - loss: 1.2523e-04 - val_loss: 1.8325e-04 - 41s/epoch - 27ms/step
Epoch 153/200
1493/1493 - 41s - loss: 1.2649e-04 - val_loss: 1.4104e-04 - 41s/epoch - 27ms/step
Epoch 154/200
1493/1493 - 41s - loss: 1.2357e-04 - val_loss: 2.7250e-04 - 41s/epoch - 27ms/step
Epoch 155/200
1493/1493 - 41s - loss: 1.4553e-04 - val_loss: 1.1849e-04 - 41s/epoch - 27ms/step
Epoch 156/200
1493/1493 - 41s - loss: 1.2663e-04 - val_loss: 1.4502e-04 - 41s/epoch - 27ms/step
Epoch 157/200
1493/1493 - 41s - loss: 1.2153e-04 - val_loss: 1.3153e-04 - 41s/epoch - 27ms/step
Epoch 158/200
1493/1493 - 41s - loss: 1.2315e-04 - val_loss: 1.3573e-04 - 41s/epoch - 27ms/step
Epoch 159/200
1493/1493 - 41s - loss: 1.2136e-04 - val_loss: 1.1594e-04 - 41s/epoch - 27ms/step
Epoch 160/200
1493/1493 - 41s - loss: 1.2094e-04 - val_loss: 1.4054e-04 - 41s/epoch - 27ms/step
Epoch 161/200
1493/1493 - 41s - loss: 1.2690e-04 - val_loss: 1.2213e-04 - 41s/epoch - 27ms/step
Epoch 162/200
1493/1493 - 41s - loss: 1.2036e-04 - val_loss: 1.1786e-04 - 41s/epoch - 27ms/step
Epoch 163/200
1493/1493 - 41s - loss: 1.2150e-04 - val_loss: 3.2697e-04 - 41s/epoch - 27ms/step
Epoch 164/200
1493/1493 - 41s - loss: 1.4476e-04 - val_loss: 1.2296e-04 - 41s/epoch - 27ms/step
Epoch 165/200
1493/1493 - 41s - loss: 1.2602e-04 - val_loss: 1.2224e-04 - 41s/epoch - 27ms/step
Epoch 166/200
1493/1493 - 41s - loss: 1.2197e-04 - val_loss: 1.2045e-04 - 41s/epoch - 27ms/step
Epoch 167/200
1493/1493 - 41s - loss: 1.2007e-04 - val_loss: 1.1403e-04 - 41s/epoch - 27ms/step
Epoch 168/200
1493/1493 - 41s - loss: 1.2131e-04 - val_loss: 1.2077e-04 - 41s/epoch - 27ms/step
Epoch 169/200
1493/1493 - 41s - loss: 1.1873e-04 - val_loss: 1.7077e-04 - 41s/epoch - 27ms/step
Epoch 170/200
1493/1493 - 41s - loss: 1.2069e-04 - val_loss: 3.2297e-04 - 41s/epoch - 27ms/step
Epoch 171/200
1493/1493 - 41s - loss: 1.6811e-04 - val_loss: 2.9635e-04 - 41s/epoch - 27ms/step
Epoch 172/200
1493/1493 - 41s - loss: 1.3952e-04 - val_loss: 1.1725e-04 - 41s/epoch - 27ms/step
Epoch 173/200
1493/1493 - 41s - loss: 1.1985e-04 - val_loss: 1.1100e-04 - 41s/epoch - 27ms/step
Epoch 174/200
1493/1493 - 41s - loss: 1.1754e-04 - val_loss: 1.0898e-04 - 41s/epoch - 27ms/step
Epoch 175/200
1493/1493 - 41s - loss: 1.1922e-04 - val_loss: 1.3816e-04 - 41s/epoch - 27ms/step
Epoch 176/200
1493/1493 - 41s - loss: 1.1917e-04 - val_loss: 1.9384e-04 - 41s/epoch - 27ms/step
Epoch 177/200
1493/1493 - 41s - loss: 1.2818e-04 - val_loss: 1.0524e-04 - 41s/epoch - 27ms/step
Epoch 178/200
1493/1493 - 41s - loss: 1.1597e-04 - val_loss: 1.1788e-04 - 41s/epoch - 27ms/step
Epoch 179/200
1493/1493 - 41s - loss: 1.1496e-04 - val_loss: 1.2546e-04 - 41s/epoch - 27ms/step
Epoch 180/200
1493/1493 - 41s - loss: 1.1483e-04 - val_loss: 1.2382e-04 - 41s/epoch - 27ms/step
Epoch 181/200
1493/1493 - 41s - loss: 1.1595e-04 - val_loss: 1.2968e-04 - 41s/epoch - 27ms/step
Epoch 182/200
1493/1493 - 41s - loss: 1.1466e-04 - val_loss: 1.3490e-04 - 41s/epoch - 27ms/step
Epoch 183/200
1493/1493 - 41s - loss: 1.1288e-04 - val_loss: 1.4675e-04 - 41s/epoch - 27ms/step
Epoch 184/200
1493/1493 - 41s - loss: 1.1637e-04 - val_loss: 1.0274e-04 - 41s/epoch - 27ms/step
Epoch 185/200
1493/1493 - 41s - loss: 1.1380e-04 - val_loss: 3.4108e-04 - 41s/epoch - 27ms/step
Epoch 186/200
1493/1493 - 41s - loss: 1.4350e-04 - val_loss: 1.0187e-04 - 41s/epoch - 27ms/step
Epoch 187/200
1493/1493 - 41s - loss: 1.1469e-04 - val_loss: 1.1990e-04 - 41s/epoch - 27ms/step
Epoch 188/200
1493/1493 - 41s - loss: 1.1842e-04 - val_loss: 2.6222e-04 - 41s/epoch - 27ms/step
Epoch 189/200
1493/1493 - 41s - loss: 1.2544e-04 - val_loss: 1.3545e-04 - 41s/epoch - 27ms/step
Epoch 190/200
1493/1493 - 41s - loss: 1.1573e-04 - val_loss: 1.4575e-04 - 41s/epoch - 27ms/step
Epoch 191/200
1493/1493 - 41s - loss: 1.1709e-04 - val_loss: 2.2622e-04 - 41s/epoch - 27ms/step
Epoch 192/200
1493/1493 - 41s - loss: 1.2131e-04 - val_loss: 1.2053e-04 - 41s/epoch - 27ms/step
Epoch 193/200
1493/1493 - 41s - loss: 1.1639e-04 - val_loss: 2.2335e-04 - 41s/epoch - 27ms/step
Epoch 194/200
1493/1493 - 41s - loss: 1.2805e-04 - val_loss: 3.0525e-04 - 41s/epoch - 27ms/step
Epoch 195/200
1493/1493 - 41s - loss: 1.3593e-04 - val_loss: 1.6482e-04 - 41s/epoch - 27ms/step
Epoch 196/200
1493/1493 - 41s - loss: 1.2030e-04 - val_loss: 1.2281e-04 - 41s/epoch - 27ms/step
Epoch 197/200
1493/1493 - 41s - loss: 1.1377e-04 - val_loss: 1.3433e-04 - 41s/epoch - 27ms/step
Epoch 198/200
1493/1493 - 41s - loss: 1.1467e-04 - val_loss: 1.5493e-04 - 41s/epoch - 27ms/step
Epoch 199/200
1493/1493 - 41s - loss: 1.1623e-04 - val_loss: 1.0428e-04 - 41s/epoch - 27ms/step
Epoch 200/200
1493/1493 - 41s - loss: 1.1116e-04 - val_loss: 1.1096e-04 - 41s/epoch - 27ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0001109605364035815
  1/332 [..............................] - ETA: 40s  9/332 [..............................] - ETA: 2s  18/332 [>.............................] - ETA: 1s 28/332 [=>............................] - ETA: 1s 39/332 [==>...........................] - ETA: 1s 51/332 [===>..........................] - ETA: 1s 63/332 [====>.........................] - ETA: 1s 75/332 [=====>........................] - ETA: 1s 87/332 [======>.......................] - ETA: 1s 99/332 [=======>......................] - ETA: 1s111/332 [=========>....................] - ETA: 1s122/332 [==========>...................] - ETA: 1s133/332 [===========>..................] - ETA: 0s145/332 [============>.................] - ETA: 0s157/332 [=============>................] - ETA: 0s169/332 [==============>...............] - ETA: 0s180/332 [===============>..............] - ETA: 0s192/332 [================>.............] - ETA: 0s203/332 [=================>............] - ETA: 0s215/332 [==================>...........] - ETA: 0s227/332 [===================>..........] - ETA: 0s239/332 [====================>.........] - ETA: 0s251/332 [=====================>........] - ETA: 0s263/332 [======================>.......] - ETA: 0s275/332 [=======================>......] - ETA: 0s287/332 [========================>.....] - ETA: 0s299/332 [==========================>...] - ETA: 0s311/332 [===========================>..] - ETA: 0s323/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 5ms/step
correlation 0.001264664926171258
cosine 0.0009956750225400728
MAE: 0.005886184
RMSE: 0.010533778
r2: 0.9928024280986263
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 1896)              2398440   
                                                                 
 batch_normalization (BatchN  (None, 1896)             7584      
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 1896)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1198904   
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 1896)              1200168   
                                                                 
 batch_normalization_2 (Batc  (None, 1896)             7584      
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 1896)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              2397808   
                                                                 
=================================================================
Total params: 7,213,016
Trainable params: 7,204,168
Non-trainable params: 8,848
_________________________________________________________________
Encoder
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 1896)              2398440   
                                                                 
 batch_normalization (BatchN  (None, 1896)             7584      
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 1896)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1198904   
                                                                 
=================================================================
Total params: 3,604,928
Trainable params: 3,601,136
Non-trainable params: 3,792
_________________________________________________________________
Decoder
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 1896)              1200168   
                                                                 
 batch_normalization_2 (Batc  (None, 1896)             7584      
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 1896)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              2397808   
                                                                 
=================================================================
Total params: 3,608,088
Trainable params: 3,603,032
Non-trainable params: 5,056
_________________________________________________________________
['1.5custom_n_b', 'mse', 64, 200, 0.0005, 0.5, 632, 0.00011116487439721823, 0.0001109605364035815, 0.001264664926171258, 0.0009956750225400728, 0.005886184051632881, 0.010533777996897697, 0.9928024280986263, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats2_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_3 (Dense)             (None, 2022)              2557830   
                                                                 
 batch_normalization_3 (Batc  (None, 2022)             8088      
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2022)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1278536   
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 2022)              1279926   
                                                                 
 batch_normalization_5 (Batc  (None, 2022)             8088      
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2022)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              2557072   
                                                                 
=================================================================
Total params: 7,692,068
Trainable params: 7,682,716
Non-trainable params: 9,352
_________________________________________________________________
Epoch 1/200
1493/1493 - 44s - loss: 0.0093 - val_loss: 0.0050 - 44s/epoch - 30ms/step
Epoch 2/200
1493/1493 - 43s - loss: 0.0031 - val_loss: 0.0027 - 43s/epoch - 29ms/step
Epoch 3/200
1493/1493 - 43s - loss: 0.0020 - val_loss: 0.0017 - 43s/epoch - 29ms/step
Epoch 4/200
1493/1493 - 43s - loss: 0.0017 - val_loss: 0.0023 - 43s/epoch - 29ms/step
Epoch 5/200
1493/1493 - 43s - loss: 0.0015 - val_loss: 0.0012 - 43s/epoch - 29ms/step
Epoch 6/200
1493/1493 - 43s - loss: 0.0013 - val_loss: 0.0013 - 43s/epoch - 29ms/step
Epoch 7/200
1493/1493 - 43s - loss: 0.0013 - val_loss: 9.8579e-04 - 43s/epoch - 29ms/step
Epoch 8/200
1493/1493 - 43s - loss: 0.0011 - val_loss: 0.0017 - 43s/epoch - 29ms/step
Epoch 9/200
1493/1493 - 43s - loss: 0.0010 - val_loss: 9.0623e-04 - 43s/epoch - 29ms/step
Epoch 10/200
1493/1493 - 43s - loss: 9.6137e-04 - val_loss: 0.0012 - 43s/epoch - 29ms/step
Epoch 11/200
1493/1493 - 43s - loss: 9.5168e-04 - val_loss: 8.5100e-04 - 43s/epoch - 29ms/step
Epoch 12/200
1493/1493 - 43s - loss: 7.9626e-04 - val_loss: 0.0010 - 43s/epoch - 29ms/step
Epoch 13/200
1493/1493 - 43s - loss: 8.0232e-04 - val_loss: 0.0028 - 43s/epoch - 29ms/step
Epoch 14/200
1493/1493 - 43s - loss: 8.9948e-04 - val_loss: 9.8651e-04 - 43s/epoch - 29ms/step
Epoch 15/200
1493/1493 - 43s - loss: 7.5930e-04 - val_loss: 7.0160e-04 - 43s/epoch - 29ms/step
Epoch 16/200
1493/1493 - 43s - loss: 6.5335e-04 - val_loss: 6.1847e-04 - 43s/epoch - 29ms/step
Epoch 17/200
1493/1493 - 43s - loss: 6.1388e-04 - val_loss: 9.2192e-04 - 43s/epoch - 29ms/step
Epoch 18/200
1493/1493 - 43s - loss: 6.2587e-04 - val_loss: 7.0900e-04 - 43s/epoch - 29ms/step
Epoch 19/200
1493/1493 - 43s - loss: 5.7110e-04 - val_loss: 7.0110e-04 - 43s/epoch - 29ms/step
Epoch 20/200
1493/1493 - 43s - loss: 5.4985e-04 - val_loss: 5.9173e-04 - 43s/epoch - 29ms/step
Epoch 21/200
1493/1493 - 43s - loss: 5.1111e-04 - val_loss: 7.2937e-04 - 43s/epoch - 29ms/step
Epoch 22/200
1493/1493 - 43s - loss: 4.9430e-04 - val_loss: 7.2782e-04 - 43s/epoch - 29ms/step
Epoch 23/200
1493/1493 - 43s - loss: 5.0299e-04 - val_loss: 0.0016 - 43s/epoch - 29ms/step
Epoch 24/200
1493/1493 - 43s - loss: 5.4644e-04 - val_loss: 4.5633e-04 - 43s/epoch - 29ms/step
Epoch 25/200
1493/1493 - 43s - loss: 4.5403e-04 - val_loss: 4.4989e-04 - 43s/epoch - 29ms/step
Epoch 26/200
1493/1493 - 44s - loss: 4.2665e-04 - val_loss: 5.0790e-04 - 44s/epoch - 29ms/step
Epoch 27/200
1493/1493 - 43s - loss: 3.9910e-04 - val_loss: 4.0696e-04 - 43s/epoch - 29ms/step
Epoch 28/200
1493/1493 - 43s - loss: 3.8286e-04 - val_loss: 8.0032e-04 - 43s/epoch - 29ms/step
Epoch 29/200
1493/1493 - 43s - loss: 3.7978e-04 - val_loss: 5.9346e-04 - 43s/epoch - 29ms/step
Epoch 30/200
1493/1493 - 44s - loss: 3.8276e-04 - val_loss: 3.9822e-04 - 44s/epoch - 29ms/step
Epoch 31/200
1493/1493 - 43s - loss: 3.5136e-04 - val_loss: 6.5044e-04 - 43s/epoch - 29ms/step
Epoch 32/200
1493/1493 - 43s - loss: 4.1014e-04 - val_loss: 3.4137e-04 - 43s/epoch - 29ms/step
Epoch 33/200
1493/1493 - 43s - loss: 3.4448e-04 - val_loss: 3.4130e-04 - 43s/epoch - 29ms/step
Epoch 34/200
1493/1493 - 43s - loss: 3.2601e-04 - val_loss: 3.1960e-04 - 43s/epoch - 29ms/step
Epoch 35/200
1493/1493 - 43s - loss: 3.1205e-04 - val_loss: 7.7617e-04 - 43s/epoch - 29ms/step
Epoch 36/200
1493/1493 - 43s - loss: 3.4540e-04 - val_loss: 4.1592e-04 - 43s/epoch - 29ms/step
Epoch 37/200
1493/1493 - 43s - loss: 3.0623e-04 - val_loss: 4.1275e-04 - 43s/epoch - 29ms/step
Epoch 38/200
1493/1493 - 43s - loss: 3.0617e-04 - val_loss: 2.9023e-04 - 43s/epoch - 29ms/step
Epoch 39/200
1493/1493 - 43s - loss: 2.8574e-04 - val_loss: 2.6782e-04 - 43s/epoch - 29ms/step
Epoch 40/200
1493/1493 - 43s - loss: 2.7676e-04 - val_loss: 3.8156e-04 - 43s/epoch - 29ms/step
Epoch 41/200
1493/1493 - 44s - loss: 2.7701e-04 - val_loss: 2.5686e-04 - 44s/epoch - 29ms/step
Epoch 42/200
1493/1493 - 43s - loss: 2.6778e-04 - val_loss: 2.4750e-04 - 43s/epoch - 29ms/step
Epoch 43/200
1493/1493 - 44s - loss: 2.6346e-04 - val_loss: 3.9029e-04 - 44s/epoch - 29ms/step
Epoch 44/200
1493/1493 - 44s - loss: 2.7015e-04 - val_loss: 3.6138e-04 - 44s/epoch - 29ms/step
Epoch 45/200
1493/1493 - 43s - loss: 2.6526e-04 - val_loss: 2.2483e-04 - 43s/epoch - 29ms/step
Epoch 46/200
1493/1493 - 43s - loss: 2.5179e-04 - val_loss: 2.3722e-04 - 43s/epoch - 29ms/step
Epoch 47/200
1493/1493 - 43s - loss: 2.4461e-04 - val_loss: 2.5841e-04 - 43s/epoch - 29ms/step
Epoch 48/200
1493/1493 - 43s - loss: 2.4975e-04 - val_loss: 3.2201e-04 - 43s/epoch - 29ms/step
Epoch 49/200
1493/1493 - 44s - loss: 2.4231e-04 - val_loss: 4.9244e-04 - 44s/epoch - 29ms/step
Epoch 50/200
1493/1493 - 43s - loss: 2.5810e-04 - val_loss: 3.6487e-04 - 43s/epoch - 29ms/step
Epoch 51/200
1493/1493 - 43s - loss: 2.6178e-04 - val_loss: 2.4200e-04 - 43s/epoch - 29ms/step
Epoch 52/200
1493/1493 - 43s - loss: 2.2618e-04 - val_loss: 2.1510e-04 - 43s/epoch - 29ms/step
Epoch 53/200
1493/1493 - 43s - loss: 2.3057e-04 - val_loss: 2.9810e-04 - 43s/epoch - 29ms/step
Epoch 54/200
1493/1493 - 43s - loss: 2.3023e-04 - val_loss: 2.7658e-04 - 43s/epoch - 29ms/step
Epoch 55/200
1493/1493 - 43s - loss: 2.1697e-04 - val_loss: 2.4204e-04 - 43s/epoch - 29ms/step
Epoch 56/200
1493/1493 - 43s - loss: 2.1694e-04 - val_loss: 2.9378e-04 - 43s/epoch - 29ms/step
Epoch 57/200
1493/1493 - 43s - loss: 2.1084e-04 - val_loss: 2.7226e-04 - 43s/epoch - 29ms/step
Epoch 58/200
1493/1493 - 43s - loss: 2.0971e-04 - val_loss: 2.0534e-04 - 43s/epoch - 29ms/step
Epoch 59/200
1493/1493 - 43s - loss: 2.0500e-04 - val_loss: 2.2017e-04 - 43s/epoch - 29ms/step
Epoch 60/200
1493/1493 - 43s - loss: 2.0725e-04 - val_loss: 7.2975e-04 - 43s/epoch - 29ms/step
Epoch 61/200
1493/1493 - 44s - loss: 4.3570e-04 - val_loss: 1.8944e-04 - 44s/epoch - 29ms/step
Epoch 62/200
1493/1493 - 43s - loss: 2.1893e-04 - val_loss: 2.2565e-04 - 43s/epoch - 29ms/step
Epoch 63/200
1493/1493 - 44s - loss: 2.0571e-04 - val_loss: 2.4914e-04 - 44s/epoch - 29ms/step
Epoch 64/200
1493/1493 - 44s - loss: 2.0523e-04 - val_loss: 0.0027 - 44s/epoch - 29ms/step
Epoch 65/200
1493/1493 - 43s - loss: 3.2425e-04 - val_loss: 2.6797e-04 - 43s/epoch - 29ms/step
Epoch 66/200
1493/1493 - 43s - loss: 2.1196e-04 - val_loss: 2.1048e-04 - 43s/epoch - 29ms/step
Epoch 67/200
1493/1493 - 43s - loss: 1.9843e-04 - val_loss: 1.9807e-04 - 43s/epoch - 29ms/step
Epoch 68/200
1493/1493 - 43s - loss: 1.9575e-04 - val_loss: 7.6552e-04 - 43s/epoch - 29ms/step
Epoch 69/200
1493/1493 - 43s - loss: 2.5862e-04 - val_loss: 2.0838e-04 - 43s/epoch - 29ms/step
Epoch 70/200
1493/1493 - 43s - loss: 1.9524e-04 - val_loss: 7.1690e-04 - 43s/epoch - 29ms/step
Epoch 71/200
1493/1493 - 43s - loss: 2.4842e-04 - val_loss: 1.7660e-04 - 43s/epoch - 29ms/step
Epoch 72/200
1493/1493 - 43s - loss: 1.9614e-04 - val_loss: 2.4770e-04 - 43s/epoch - 29ms/step
Epoch 73/200
1493/1493 - 43s - loss: 1.9320e-04 - val_loss: 1.9720e-04 - 43s/epoch - 29ms/step
Epoch 74/200
1493/1493 - 43s - loss: 1.8803e-04 - val_loss: 1.8749e-04 - 43s/epoch - 29ms/step
Epoch 75/200
1493/1493 - 43s - loss: 1.8046e-04 - val_loss: 1.9674e-04 - 43s/epoch - 29ms/step
Epoch 76/200
1493/1493 - 43s - loss: 1.7899e-04 - val_loss: 1.9799e-04 - 43s/epoch - 29ms/step
Epoch 77/200
1493/1493 - 43s - loss: 1.8040e-04 - val_loss: 4.1772e-04 - 43s/epoch - 29ms/step
Epoch 78/200
1493/1493 - 43s - loss: 1.8222e-04 - val_loss: 3.4964e-04 - 43s/epoch - 29ms/step
Epoch 79/200
1493/1493 - 44s - loss: 1.9263e-04 - val_loss: 3.3712e-04 - 44s/epoch - 29ms/step
Epoch 80/200
1493/1493 - 43s - loss: 1.9917e-04 - val_loss: 1.6234e-04 - 43s/epoch - 29ms/step
Epoch 81/200
1493/1493 - 43s - loss: 1.7431e-04 - val_loss: 3.0924e-04 - 43s/epoch - 29ms/step
Epoch 82/200
1493/1493 - 43s - loss: 1.7038e-04 - val_loss: 1.7670e-04 - 43s/epoch - 29ms/step
Epoch 83/200
1493/1493 - 43s - loss: 1.6607e-04 - val_loss: 2.0174e-04 - 43s/epoch - 29ms/step
Epoch 84/200
1493/1493 - 43s - loss: 1.6600e-04 - val_loss: 1.9873e-04 - 43s/epoch - 29ms/step
Epoch 85/200
1493/1493 - 43s - loss: 1.6722e-04 - val_loss: 2.1755e-04 - 43s/epoch - 29ms/step
Epoch 86/200
1493/1493 - 43s - loss: 1.6596e-04 - val_loss: 1.7522e-04 - 43s/epoch - 29ms/step
Epoch 87/200
1493/1493 - 44s - loss: 1.6094e-04 - val_loss: 1.6027e-04 - 44s/epoch - 29ms/step
Epoch 88/200
1493/1493 - 43s - loss: 1.6002e-04 - val_loss: 2.8751e-04 - 43s/epoch - 29ms/step
Epoch 89/200
1493/1493 - 43s - loss: 1.7166e-04 - val_loss: 1.7072e-04 - 43s/epoch - 29ms/step
Epoch 90/200
1493/1493 - 44s - loss: 1.5793e-04 - val_loss: 2.0532e-04 - 44s/epoch - 29ms/step
Epoch 91/200
1493/1493 - 44s - loss: 1.5662e-04 - val_loss: 1.7725e-04 - 44s/epoch - 29ms/step
Epoch 92/200
1493/1493 - 43s - loss: 1.5660e-04 - val_loss: 4.9415e-04 - 43s/epoch - 29ms/step
Epoch 93/200
1493/1493 - 43s - loss: 2.0379e-04 - val_loss: 1.7636e-04 - 43s/epoch - 29ms/step
Epoch 94/200
1493/1493 - 43s - loss: 1.5902e-04 - val_loss: 1.3166e-04 - 43s/epoch - 29ms/step
Epoch 95/200
1493/1493 - 43s - loss: 1.5463e-04 - val_loss: 1.8601e-04 - 43s/epoch - 29ms/step
Epoch 96/200
1493/1493 - 43s - loss: 1.5402e-04 - val_loss: 1.8687e-04 - 43s/epoch - 29ms/step
Epoch 97/200
1493/1493 - 43s - loss: 1.5248e-04 - val_loss: 1.5738e-04 - 43s/epoch - 29ms/step
Epoch 98/200
1493/1493 - 43s - loss: 1.5465e-04 - val_loss: 0.0011 - 43s/epoch - 29ms/step
Epoch 99/200
1493/1493 - 43s - loss: 2.2610e-04 - val_loss: 1.5343e-04 - 43s/epoch - 29ms/step
Epoch 100/200
1493/1493 - 43s - loss: 1.6211e-04 - val_loss: 1.4601e-04 - 43s/epoch - 29ms/step
Epoch 101/200
1493/1493 - 44s - loss: 1.5209e-04 - val_loss: 1.9138e-04 - 44s/epoch - 29ms/step
Epoch 102/200
1493/1493 - 43s - loss: 1.5489e-04 - val_loss: 2.2664e-04 - 43s/epoch - 29ms/step
Epoch 103/200
1493/1493 - 43s - loss: 1.5308e-04 - val_loss: 1.6784e-04 - 43s/epoch - 29ms/step
Epoch 104/200
1493/1493 - 43s - loss: 1.4688e-04 - val_loss: 3.0917e-04 - 43s/epoch - 29ms/step
Epoch 105/200
1493/1493 - 43s - loss: 1.5532e-04 - val_loss: 1.5906e-04 - 43s/epoch - 29ms/step
Epoch 106/200
1493/1493 - 43s - loss: 1.4401e-04 - val_loss: 1.6602e-04 - 43s/epoch - 29ms/step
Epoch 107/200
1493/1493 - 43s - loss: 1.4300e-04 - val_loss: 1.9749e-04 - 43s/epoch - 29ms/step
Epoch 108/200
1493/1493 - 43s - loss: 1.4518e-04 - val_loss: 1.5892e-04 - 43s/epoch - 29ms/step
Epoch 109/200
1493/1493 - 43s - loss: 1.4377e-04 - val_loss: 1.7111e-04 - 43s/epoch - 29ms/step
Epoch 110/200
1493/1493 - 43s - loss: 1.4041e-04 - val_loss: 1.5488e-04 - 43s/epoch - 29ms/step
Epoch 111/200
1493/1493 - 43s - loss: 1.4133e-04 - val_loss: 4.1875e-04 - 43s/epoch - 29ms/step
Epoch 112/200
1493/1493 - 43s - loss: 1.7390e-04 - val_loss: 1.3631e-04 - 43s/epoch - 29ms/step
Epoch 113/200
1493/1493 - 43s - loss: 1.4167e-04 - val_loss: 4.6952e-04 - 43s/epoch - 29ms/step
Epoch 114/200
1493/1493 - 43s - loss: 1.7801e-04 - val_loss: 1.5113e-04 - 43s/epoch - 29ms/step
Epoch 115/200
1493/1493 - 43s - loss: 1.4408e-04 - val_loss: 2.4173e-04 - 43s/epoch - 29ms/step
Epoch 116/200
1493/1493 - 44s - loss: 1.3998e-04 - val_loss: 1.9002e-04 - 44s/epoch - 29ms/step
Epoch 117/200
1493/1493 - 43s - loss: 1.3802e-04 - val_loss: 2.3318e-04 - 43s/epoch - 29ms/step
Epoch 118/200
1493/1493 - 43s - loss: 1.4936e-04 - val_loss: 1.5454e-04 - 43s/epoch - 29ms/step
Epoch 119/200
1493/1493 - 43s - loss: 1.3696e-04 - val_loss: 1.3271e-04 - 43s/epoch - 29ms/step
Epoch 120/200
1493/1493 - 43s - loss: 1.3484e-04 - val_loss: 1.3415e-04 - 43s/epoch - 29ms/step
Epoch 121/200
1493/1493 - 43s - loss: 1.3454e-04 - val_loss: 1.9721e-04 - 43s/epoch - 29ms/step
Epoch 122/200
1493/1493 - 43s - loss: 1.3774e-04 - val_loss: 1.5480e-04 - 43s/epoch - 29ms/step
Epoch 123/200
1493/1493 - 44s - loss: 1.3640e-04 - val_loss: 1.4931e-04 - 44s/epoch - 29ms/step
Epoch 124/200
1493/1493 - 43s - loss: 1.3166e-04 - val_loss: 3.8261e-04 - 43s/epoch - 29ms/step
Epoch 125/200
1493/1493 - 43s - loss: 1.5319e-04 - val_loss: 1.3038e-04 - 43s/epoch - 29ms/step
Epoch 126/200
1493/1493 - 43s - loss: 1.3288e-04 - val_loss: 3.9046e-04 - 43s/epoch - 29ms/step
Epoch 127/200
1493/1493 - 43s - loss: 1.5580e-04 - val_loss: 6.1616e-04 - 43s/epoch - 29ms/step
Epoch 128/200
1493/1493 - 43s - loss: 1.7512e-04 - val_loss: 1.1749e-04 - 43s/epoch - 29ms/step
Epoch 129/200
1493/1493 - 44s - loss: 1.3364e-04 - val_loss: 1.3408e-04 - 44s/epoch - 29ms/step
Epoch 130/200
1493/1493 - 44s - loss: 1.3097e-04 - val_loss: 1.3802e-04 - 44s/epoch - 29ms/step
Epoch 131/200
1493/1493 - 43s - loss: 1.3412e-04 - val_loss: 3.4459e-04 - 43s/epoch - 29ms/step
Epoch 132/200
1493/1493 - 43s - loss: 1.6827e-04 - val_loss: 1.2677e-04 - 43s/epoch - 29ms/step
Epoch 133/200
1493/1493 - 43s - loss: 1.3186e-04 - val_loss: 1.3519e-04 - 43s/epoch - 29ms/step
Epoch 134/200
1493/1493 - 43s - loss: 1.2821e-04 - val_loss: 1.2969e-04 - 43s/epoch - 29ms/step
Epoch 135/200
1493/1493 - 44s - loss: 1.2720e-04 - val_loss: 1.3025e-04 - 44s/epoch - 29ms/step
Epoch 136/200
1493/1493 - 44s - loss: 1.2752e-04 - val_loss: 1.5231e-04 - 44s/epoch - 29ms/step
Epoch 137/200
1493/1493 - 43s - loss: 1.4004e-04 - val_loss: 2.9510e-04 - 43s/epoch - 29ms/step
Epoch 138/200
1493/1493 - 43s - loss: 1.5052e-04 - val_loss: 1.4939e-04 - 43s/epoch - 29ms/step
Epoch 139/200
1493/1493 - 43s - loss: 1.3550e-04 - val_loss: 2.5970e-04 - 43s/epoch - 29ms/step
Epoch 140/200
1493/1493 - 44s - loss: 1.5977e-04 - val_loss: 1.1748e-04 - 44s/epoch - 29ms/step
Epoch 141/200
1493/1493 - 43s - loss: 1.3178e-04 - val_loss: 1.4950e-04 - 43s/epoch - 29ms/step
Epoch 142/200
1493/1493 - 44s - loss: 1.3004e-04 - val_loss: 1.3859e-04 - 44s/epoch - 29ms/step
Epoch 143/200
1493/1493 - 43s - loss: 1.2476e-04 - val_loss: 1.5287e-04 - 43s/epoch - 29ms/step
Epoch 144/200
1493/1493 - 43s - loss: 1.2868e-04 - val_loss: 1.4528e-04 - 43s/epoch - 29ms/step
Epoch 145/200
1493/1493 - 43s - loss: 1.2517e-04 - val_loss: 1.1965e-04 - 43s/epoch - 29ms/step
Epoch 146/200
1493/1493 - 43s - loss: 1.2339e-04 - val_loss: 1.2755e-04 - 43s/epoch - 29ms/step
Epoch 147/200
1493/1493 - 43s - loss: 1.2175e-04 - val_loss: 1.5001e-04 - 43s/epoch - 29ms/step
Epoch 148/200
1493/1493 - 43s - loss: 1.2132e-04 - val_loss: 2.2920e-04 - 43s/epoch - 29ms/step
Epoch 149/200
1493/1493 - 43s - loss: 1.2106e-04 - val_loss: 1.1956e-04 - 43s/epoch - 29ms/step
Epoch 150/200
1493/1493 - 43s - loss: 1.2013e-04 - val_loss: 1.1235e-04 - 43s/epoch - 29ms/step
Epoch 151/200
1493/1493 - 43s - loss: 1.2597e-04 - val_loss: 1.4753e-04 - 43s/epoch - 29ms/step
Epoch 152/200
1493/1493 - 43s - loss: 1.2010e-04 - val_loss: 1.7733e-04 - 43s/epoch - 29ms/step
Epoch 153/200
1493/1493 - 44s - loss: 1.2041e-04 - val_loss: 1.6019e-04 - 44s/epoch - 29ms/step
Epoch 154/200
1493/1493 - 43s - loss: 1.1897e-04 - val_loss: 2.1968e-04 - 43s/epoch - 29ms/step
Epoch 155/200
1493/1493 - 43s - loss: 1.3270e-04 - val_loss: 1.2635e-04 - 43s/epoch - 29ms/step
Epoch 156/200
1493/1493 - 44s - loss: 1.2064e-04 - val_loss: 1.4429e-04 - 44s/epoch - 29ms/step
Epoch 157/200
1493/1493 - 44s - loss: 1.1645e-04 - val_loss: 1.3713e-04 - 44s/epoch - 29ms/step
Epoch 158/200
1493/1493 - 43s - loss: 1.1920e-04 - val_loss: 1.1765e-04 - 43s/epoch - 29ms/step
Epoch 159/200
1493/1493 - 43s - loss: 1.1694e-04 - val_loss: 1.1693e-04 - 43s/epoch - 29ms/step
Epoch 160/200
1493/1493 - 43s - loss: 1.2885e-04 - val_loss: 1.6380e-04 - 43s/epoch - 29ms/step
Epoch 161/200
1493/1493 - 43s - loss: 1.3086e-04 - val_loss: 1.1556e-04 - 43s/epoch - 29ms/step
Epoch 162/200
1493/1493 - 44s - loss: 1.1714e-04 - val_loss: 1.1567e-04 - 44s/epoch - 29ms/step
Epoch 163/200
1493/1493 - 43s - loss: 1.1817e-04 - val_loss: 2.5691e-04 - 43s/epoch - 29ms/step
Epoch 164/200
1493/1493 - 43s - loss: 1.3691e-04 - val_loss: 1.2683e-04 - 43s/epoch - 29ms/step
Epoch 165/200
1493/1493 - 44s - loss: 1.2233e-04 - val_loss: 1.2010e-04 - 44s/epoch - 29ms/step
Epoch 166/200
1493/1493 - 43s - loss: 1.1644e-04 - val_loss: 1.2894e-04 - 43s/epoch - 29ms/step
Epoch 167/200
1493/1493 - 43s - loss: 1.1528e-04 - val_loss: 1.2192e-04 - 43s/epoch - 29ms/step
Epoch 168/200
1493/1493 - 43s - loss: 1.1528e-04 - val_loss: 1.1958e-04 - 43s/epoch - 29ms/step
Epoch 169/200
1493/1493 - 43s - loss: 1.1417e-04 - val_loss: 1.4343e-04 - 43s/epoch - 29ms/step
Epoch 170/200
1493/1493 - 43s - loss: 1.1825e-04 - val_loss: 4.2779e-04 - 43s/epoch - 29ms/step
Epoch 171/200
1493/1493 - 43s - loss: 1.6928e-04 - val_loss: 3.5288e-04 - 43s/epoch - 29ms/step
Epoch 172/200
1493/1493 - 44s - loss: 1.4308e-04 - val_loss: 1.4960e-04 - 44s/epoch - 29ms/step
Epoch 173/200
1493/1493 - 43s - loss: 1.2055e-04 - val_loss: 1.1013e-04 - 43s/epoch - 29ms/step
Epoch 174/200
1493/1493 - 43s - loss: 1.1426e-04 - val_loss: 1.1502e-04 - 43s/epoch - 29ms/step
Epoch 175/200
1493/1493 - 43s - loss: 1.1485e-04 - val_loss: 1.5026e-04 - 43s/epoch - 29ms/step
Epoch 176/200
1493/1493 - 43s - loss: 1.1670e-04 - val_loss: 1.3203e-04 - 43s/epoch - 29ms/step
Epoch 177/200
1493/1493 - 43s - loss: 1.1528e-04 - val_loss: 1.0503e-04 - 43s/epoch - 29ms/step
Epoch 178/200
1493/1493 - 44s - loss: 1.1188e-04 - val_loss: 1.1794e-04 - 44s/epoch - 29ms/step
Epoch 179/200
1493/1493 - 43s - loss: 1.1175e-04 - val_loss: 1.2894e-04 - 43s/epoch - 29ms/step
Epoch 180/200
1493/1493 - 43s - loss: 1.1143e-04 - val_loss: 1.8558e-04 - 43s/epoch - 29ms/step
Epoch 181/200
1493/1493 - 43s - loss: 1.1812e-04 - val_loss: 1.3351e-04 - 43s/epoch - 29ms/step
Epoch 182/200
1493/1493 - 43s - loss: 1.1139e-04 - val_loss: 1.3650e-04 - 43s/epoch - 29ms/step
Epoch 183/200
1493/1493 - 43s - loss: 1.0979e-04 - val_loss: 1.4730e-04 - 43s/epoch - 29ms/step
Epoch 184/200
1493/1493 - 43s - loss: 1.1270e-04 - val_loss: 1.0537e-04 - 43s/epoch - 29ms/step
Epoch 185/200
1493/1493 - 43s - loss: 1.0794e-04 - val_loss: 1.4863e-04 - 43s/epoch - 29ms/step
Epoch 186/200
1493/1493 - 43s - loss: 1.0922e-04 - val_loss: 1.1820e-04 - 43s/epoch - 29ms/step
Epoch 187/200
1493/1493 - 44s - loss: 1.0801e-04 - val_loss: 1.7106e-04 - 44s/epoch - 29ms/step
Epoch 188/200
1493/1493 - 43s - loss: 1.1954e-04 - val_loss: 1.4258e-04 - 43s/epoch - 29ms/step
Epoch 189/200
1493/1493 - 43s - loss: 1.1151e-04 - val_loss: 1.2400e-04 - 43s/epoch - 29ms/step
Epoch 190/200
1493/1493 - 43s - loss: 1.1306e-04 - val_loss: 1.4306e-04 - 43s/epoch - 29ms/step
Epoch 191/200
1493/1493 - 43s - loss: 1.1559e-04 - val_loss: 1.7583e-04 - 43s/epoch - 29ms/step
Epoch 192/200
1493/1493 - 43s - loss: 1.1213e-04 - val_loss: 1.5230e-04 - 43s/epoch - 29ms/step
Epoch 193/200
1493/1493 - 43s - loss: 1.1529e-04 - val_loss: 2.8305e-04 - 43s/epoch - 29ms/step
Epoch 194/200
1493/1493 - 43s - loss: 1.3839e-04 - val_loss: 4.7899e-04 - 43s/epoch - 29ms/step
Epoch 195/200
1493/1493 - 43s - loss: 1.6213e-04 - val_loss: 2.7587e-04 - 43s/epoch - 29ms/step
Epoch 196/200
1493/1493 - 43s - loss: 1.3344e-04 - val_loss: 1.0210e-04 - 43s/epoch - 29ms/step
Epoch 197/200
1493/1493 - 43s - loss: 1.1272e-04 - val_loss: 1.7072e-04 - 43s/epoch - 29ms/step
Epoch 198/200
1493/1493 - 43s - loss: 1.1667e-04 - val_loss: 1.2973e-04 - 43s/epoch - 29ms/step
Epoch 199/200
1493/1493 - 43s - loss: 1.1017e-04 - val_loss: 1.1312e-04 - 43s/epoch - 29ms/step
Epoch 200/200
1493/1493 - 43s - loss: 1.0821e-04 - val_loss: 1.1479e-04 - 43s/epoch - 29ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00011479422391857952
  1/332 [..............................] - ETA: 34s  9/332 [..............................] - ETA: 2s  18/332 [>.............................] - ETA: 1s 28/332 [=>............................] - ETA: 1s 38/332 [==>...........................] - ETA: 1s 49/332 [===>..........................] - ETA: 1s 60/332 [====>.........................] - ETA: 1s 71/332 [=====>........................] - ETA: 1s 82/332 [======>.......................] - ETA: 1s 93/332 [=======>......................] - ETA: 1s104/332 [========>.....................] - ETA: 1s115/332 [=========>....................] - ETA: 1s126/332 [==========>...................] - ETA: 1s137/332 [===========>..................] - ETA: 0s148/332 [============>.................] - ETA: 0s159/332 [=============>................] - ETA: 0s170/332 [==============>...............] - ETA: 0s181/332 [===============>..............] - ETA: 0s192/332 [================>.............] - ETA: 0s203/332 [=================>............] - ETA: 0s214/332 [==================>...........] - ETA: 0s225/332 [===================>..........] - ETA: 0s236/332 [====================>.........] - ETA: 0s247/332 [=====================>........] - ETA: 0s258/332 [======================>.......] - ETA: 0s269/332 [=======================>......] - ETA: 0s280/332 [========================>.....] - ETA: 0s291/332 [=========================>....] - ETA: 0s302/332 [==========================>...] - ETA: 0s313/332 [===========================>..] - ETA: 0s324/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 5ms/step
correlation 0.001305897568155584
cosine 0.0010282624803576162
MAE: 0.0060560247
RMSE: 0.010714201
r2: 0.9925538801418433
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 2022)              2557830   
                                                                 
 batch_normalization_3 (Batc  (None, 2022)             8088      
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2022)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1278536   
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 2022)              1279926   
                                                                 
 batch_normalization_5 (Batc  (None, 2022)             8088      
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2022)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              2557072   
                                                                 
=================================================================
Total params: 7,692,068
Trainable params: 7,682,716
Non-trainable params: 9,352
_________________________________________________________________
Encoder
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 2022)              2557830   
                                                                 
 batch_normalization_3 (Batc  (None, 2022)             8088      
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2022)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1278536   
                                                                 
=================================================================
Total params: 3,844,454
Trainable params: 3,840,410
Non-trainable params: 4,044
_________________________________________________________________
Decoder
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 2022)              1279926   
                                                                 
 batch_normalization_5 (Batc  (None, 2022)             8088      
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2022)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              2557072   
                                                                 
=================================================================
Total params: 3,847,614
Trainable params: 3,842,306
Non-trainable params: 5,308
_________________________________________________________________
['1.6custom_n_b', 'mse', 64, 200, 0.0005, 0.5, 632, 0.00010821171599673107, 0.00011479422391857952, 0.001305897568155584, 0.0010282624803576162, 0.006056024692952633, 0.010714201256632805, 0.9925538801418433, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats2_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_6 (Dense)             (None, 2148)              2717220   
                                                                 
 batch_normalization_6 (Batc  (None, 2148)             8592      
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2148)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1358168   
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 2148)              1359684   
                                                                 
 batch_normalization_8 (Batc  (None, 2148)             8592      
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2148)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              2716336   
                                                                 
=================================================================
Total params: 8,171,120
Trainable params: 8,161,264
Non-trainable params: 9,856
_________________________________________________________________
Epoch 1/200
1493/1493 - 47s - loss: 0.0094 - val_loss: 0.0052 - 47s/epoch - 31ms/step
Epoch 2/200
1493/1493 - 46s - loss: 0.0031 - val_loss: 0.0028 - 46s/epoch - 31ms/step
Epoch 3/200
1493/1493 - 45s - loss: 0.0021 - val_loss: 0.0018 - 45s/epoch - 30ms/step
Epoch 4/200
1493/1493 - 45s - loss: 0.0017 - val_loss: 0.0026 - 45s/epoch - 30ms/step
Epoch 5/200
1493/1493 - 45s - loss: 0.0016 - val_loss: 0.0012 - 45s/epoch - 30ms/step
Epoch 6/200
1493/1493 - 45s - loss: 0.0013 - val_loss: 0.0016 - 45s/epoch - 30ms/step
Epoch 7/200
1493/1493 - 45s - loss: 0.0013 - val_loss: 9.3978e-04 - 45s/epoch - 30ms/step
Epoch 8/200
1493/1493 - 46s - loss: 0.0011 - val_loss: 0.0015 - 46s/epoch - 30ms/step
Epoch 9/200
1493/1493 - 46s - loss: 0.0010 - val_loss: 0.0010 - 46s/epoch - 31ms/step
Epoch 10/200
1493/1493 - 46s - loss: 9.6929e-04 - val_loss: 8.9165e-04 - 46s/epoch - 31ms/step
Epoch 11/200
1493/1493 - 45s - loss: 0.0010 - val_loss: 9.8211e-04 - 45s/epoch - 30ms/step
Epoch 12/200
1493/1493 - 46s - loss: 8.0325e-04 - val_loss: 0.0015 - 46s/epoch - 31ms/step
Epoch 13/200
1493/1493 - 46s - loss: 8.0532e-04 - val_loss: 0.0011 - 46s/epoch - 31ms/step
Epoch 14/200
1493/1493 - 46s - loss: 7.8122e-04 - val_loss: 6.9510e-04 - 46s/epoch - 31ms/step
Epoch 15/200
1493/1493 - 45s - loss: 7.1397e-04 - val_loss: 6.8210e-04 - 45s/epoch - 30ms/step
Epoch 16/200
1493/1493 - 46s - loss: 6.3110e-04 - val_loss: 6.4556e-04 - 46s/epoch - 30ms/step
Epoch 17/200
1493/1493 - 46s - loss: 5.9509e-04 - val_loss: 7.4663e-04 - 46s/epoch - 31ms/step
Epoch 18/200
1493/1493 - 46s - loss: 5.8804e-04 - val_loss: 9.5065e-04 - 46s/epoch - 30ms/step
Epoch 19/200
1493/1493 - 46s - loss: 5.8180e-04 - val_loss: 7.0060e-04 - 46s/epoch - 31ms/step
Epoch 20/200
1493/1493 - 45s - loss: 5.3375e-04 - val_loss: 5.2478e-04 - 45s/epoch - 30ms/step
Epoch 21/200
1493/1493 - 46s - loss: 4.9328e-04 - val_loss: 6.5435e-04 - 46s/epoch - 30ms/step
Epoch 22/200
1493/1493 - 46s - loss: 4.7490e-04 - val_loss: 9.0567e-04 - 46s/epoch - 31ms/step
Epoch 23/200
1493/1493 - 46s - loss: 5.1952e-04 - val_loss: 0.0013 - 46s/epoch - 30ms/step
Epoch 24/200
1493/1493 - 45s - loss: 5.0428e-04 - val_loss: 4.0833e-04 - 45s/epoch - 30ms/step
Epoch 25/200
1493/1493 - 45s - loss: 4.2549e-04 - val_loss: 4.5515e-04 - 45s/epoch - 30ms/step
Epoch 26/200
1493/1493 - 46s - loss: 3.9901e-04 - val_loss: 5.1430e-04 - 46s/epoch - 31ms/step
Epoch 27/200
1493/1493 - 46s - loss: 3.8145e-04 - val_loss: 3.9060e-04 - 46s/epoch - 30ms/step
Epoch 28/200
1493/1493 - 46s - loss: 3.6605e-04 - val_loss: 6.7056e-04 - 46s/epoch - 31ms/step
Epoch 29/200
1493/1493 - 45s - loss: 3.6075e-04 - val_loss: 5.8331e-04 - 45s/epoch - 30ms/step
Epoch 30/200
1493/1493 - 46s - loss: 3.6021e-04 - val_loss: 3.8962e-04 - 46s/epoch - 31ms/step
Epoch 31/200
1493/1493 - 45s - loss: 3.3503e-04 - val_loss: 4.1639e-04 - 45s/epoch - 30ms/step
Epoch 32/200
1493/1493 - 46s - loss: 3.7227e-04 - val_loss: 3.4139e-04 - 46s/epoch - 30ms/step
Epoch 33/200
1493/1493 - 46s - loss: 3.2876e-04 - val_loss: 3.1124e-04 - 46s/epoch - 31ms/step
Epoch 34/200
1493/1493 - 46s - loss: 3.1101e-04 - val_loss: 2.7599e-04 - 46s/epoch - 31ms/step
Epoch 35/200
1493/1493 - 46s - loss: 2.9623e-04 - val_loss: 9.5065e-04 - 46s/epoch - 31ms/step
Epoch 36/200
1493/1493 - 46s - loss: 3.4630e-04 - val_loss: 3.3228e-04 - 46s/epoch - 31ms/step
Epoch 37/200
1493/1493 - 46s - loss: 2.9187e-04 - val_loss: 2.9936e-04 - 46s/epoch - 31ms/step
Epoch 38/200
1493/1493 - 46s - loss: 2.8454e-04 - val_loss: 2.8301e-04 - 46s/epoch - 31ms/step
Epoch 39/200
1493/1493 - 46s - loss: 2.7418e-04 - val_loss: 2.7589e-04 - 46s/epoch - 31ms/step
Epoch 40/200
1493/1493 - 46s - loss: 2.6407e-04 - val_loss: 3.8219e-04 - 46s/epoch - 31ms/step
Epoch 41/200
1493/1493 - 46s - loss: 2.6858e-04 - val_loss: 2.4539e-04 - 46s/epoch - 31ms/step
Epoch 42/200
1493/1493 - 45s - loss: 2.5598e-04 - val_loss: 2.3658e-04 - 45s/epoch - 30ms/step
Epoch 43/200
1493/1493 - 46s - loss: 2.5470e-04 - val_loss: 3.3280e-04 - 46s/epoch - 31ms/step
Epoch 44/200
1493/1493 - 46s - loss: 2.5988e-04 - val_loss: 2.7475e-04 - 46s/epoch - 30ms/step
Epoch 45/200
1493/1493 - 46s - loss: 2.4831e-04 - val_loss: 2.2061e-04 - 46s/epoch - 31ms/step
Epoch 46/200
1493/1493 - 46s - loss: 2.4377e-04 - val_loss: 2.2777e-04 - 46s/epoch - 30ms/step
Epoch 47/200
1493/1493 - 45s - loss: 2.3621e-04 - val_loss: 2.6989e-04 - 45s/epoch - 30ms/step
Epoch 48/200
1493/1493 - 46s - loss: 2.3744e-04 - val_loss: 3.0125e-04 - 46s/epoch - 31ms/step
Epoch 49/200
1493/1493 - 45s - loss: 2.3206e-04 - val_loss: 5.0176e-04 - 45s/epoch - 30ms/step
Epoch 50/200
1493/1493 - 46s - loss: 2.5113e-04 - val_loss: 4.9710e-04 - 46s/epoch - 31ms/step
Epoch 51/200
1493/1493 - 46s - loss: 2.5359e-04 - val_loss: 2.3015e-04 - 46s/epoch - 31ms/step
Epoch 52/200
1493/1493 - 46s - loss: 2.1751e-04 - val_loss: 2.1132e-04 - 46s/epoch - 31ms/step
Epoch 53/200
1493/1493 - 46s - loss: 2.2132e-04 - val_loss: 3.0691e-04 - 46s/epoch - 30ms/step
Epoch 54/200
1493/1493 - 46s - loss: 2.2608e-04 - val_loss: 2.4962e-04 - 46s/epoch - 31ms/step
Epoch 55/200
1493/1493 - 46s - loss: 2.0896e-04 - val_loss: 2.1188e-04 - 46s/epoch - 31ms/step
Epoch 56/200
1493/1493 - 46s - loss: 2.1005e-04 - val_loss: 2.6489e-04 - 46s/epoch - 31ms/step
Epoch 57/200
1493/1493 - 46s - loss: 2.0290e-04 - val_loss: 2.5589e-04 - 46s/epoch - 31ms/step
Epoch 58/200
1493/1493 - 46s - loss: 2.0330e-04 - val_loss: 1.7299e-04 - 46s/epoch - 31ms/step
Epoch 59/200
1493/1493 - 46s - loss: 1.9801e-04 - val_loss: 2.2422e-04 - 46s/epoch - 31ms/step
Epoch 60/200
1493/1493 - 46s - loss: 2.0121e-04 - val_loss: 8.3542e-04 - 46s/epoch - 31ms/step
Epoch 61/200
1493/1493 - 46s - loss: 3.0679e-04 - val_loss: 1.7843e-04 - 46s/epoch - 31ms/step
Epoch 62/200
1493/1493 - 46s - loss: 2.0146e-04 - val_loss: 2.0931e-04 - 46s/epoch - 31ms/step
Epoch 63/200
1493/1493 - 46s - loss: 1.9416e-04 - val_loss: 2.3616e-04 - 46s/epoch - 30ms/step
Epoch 64/200
1493/1493 - 46s - loss: 1.9297e-04 - val_loss: 0.0020 - 46s/epoch - 30ms/step
Epoch 65/200
1493/1493 - 46s - loss: 2.7227e-04 - val_loss: 3.6530e-04 - 46s/epoch - 31ms/step
Epoch 66/200
1493/1493 - 46s - loss: 2.0561e-04 - val_loss: 2.0111e-04 - 46s/epoch - 31ms/step
Epoch 67/200
1493/1493 - 46s - loss: 1.8796e-04 - val_loss: 1.7321e-04 - 46s/epoch - 31ms/step
Epoch 68/200
1493/1493 - 46s - loss: 1.8422e-04 - val_loss: 3.5799e-04 - 46s/epoch - 30ms/step
Epoch 69/200
1493/1493 - 46s - loss: 2.0811e-04 - val_loss: 2.1697e-04 - 46s/epoch - 31ms/step
Epoch 70/200
1493/1493 - 46s - loss: 1.8484e-04 - val_loss: 3.2698e-04 - 46s/epoch - 31ms/step
Epoch 71/200
1493/1493 - 45s - loss: 2.2094e-04 - val_loss: 1.7360e-04 - 45s/epoch - 30ms/step
Epoch 72/200
1493/1493 - 46s - loss: 1.8352e-04 - val_loss: 2.0355e-04 - 46s/epoch - 30ms/step
Epoch 73/200
1493/1493 - 46s - loss: 1.8213e-04 - val_loss: 1.9419e-04 - 46s/epoch - 30ms/step
Epoch 74/200
1493/1493 - 46s - loss: 1.7959e-04 - val_loss: 1.8415e-04 - 46s/epoch - 31ms/step
Epoch 75/200
1493/1493 - 46s - loss: 1.7252e-04 - val_loss: 2.1041e-04 - 46s/epoch - 31ms/step
Epoch 76/200
1493/1493 - 46s - loss: 1.7643e-04 - val_loss: 1.7251e-04 - 46s/epoch - 31ms/step
Epoch 77/200
1493/1493 - 46s - loss: 1.7185e-04 - val_loss: 5.1641e-04 - 46s/epoch - 31ms/step
Epoch 78/200
1493/1493 - 46s - loss: 1.7816e-04 - val_loss: 4.8667e-04 - 46s/epoch - 31ms/step
Epoch 79/200
1493/1493 - 45s - loss: 1.9808e-04 - val_loss: 2.8307e-04 - 45s/epoch - 30ms/step
Epoch 80/200
1493/1493 - 46s - loss: 1.9420e-04 - val_loss: 1.4667e-04 - 46s/epoch - 31ms/step
Epoch 81/200
1493/1493 - 46s - loss: 1.6760e-04 - val_loss: 2.9987e-04 - 46s/epoch - 31ms/step
Epoch 82/200
1493/1493 - 46s - loss: 1.6389e-04 - val_loss: 1.6646e-04 - 46s/epoch - 31ms/step
Epoch 83/200
1493/1493 - 46s - loss: 1.6002e-04 - val_loss: 1.8563e-04 - 46s/epoch - 31ms/step
Epoch 84/200
1493/1493 - 46s - loss: 1.5997e-04 - val_loss: 1.7337e-04 - 46s/epoch - 31ms/step
Epoch 85/200
1493/1493 - 46s - loss: 1.6327e-04 - val_loss: 1.8389e-04 - 46s/epoch - 31ms/step
Epoch 86/200
1493/1493 - 46s - loss: 1.5846e-04 - val_loss: 1.6171e-04 - 46s/epoch - 31ms/step
Epoch 87/200
1493/1493 - 46s - loss: 1.5589e-04 - val_loss: 1.4803e-04 - 46s/epoch - 31ms/step
Epoch 88/200
1493/1493 - 46s - loss: 1.5517e-04 - val_loss: 3.1122e-04 - 46s/epoch - 30ms/step
Epoch 89/200
1493/1493 - 46s - loss: 1.6637e-04 - val_loss: 1.6246e-04 - 46s/epoch - 30ms/step
Epoch 90/200
1493/1493 - 46s - loss: 1.5302e-04 - val_loss: 1.7646e-04 - 46s/epoch - 31ms/step
Epoch 91/200
1493/1493 - 46s - loss: 1.5189e-04 - val_loss: 1.7843e-04 - 46s/epoch - 31ms/step
Epoch 92/200
1493/1493 - 45s - loss: 1.5301e-04 - val_loss: 7.5433e-04 - 45s/epoch - 30ms/step
Epoch 93/200
1493/1493 - 46s - loss: 2.2000e-04 - val_loss: 1.9067e-04 - 46s/epoch - 31ms/step
Epoch 94/200
1493/1493 - 46s - loss: 1.5837e-04 - val_loss: 1.2921e-04 - 46s/epoch - 31ms/step
Epoch 95/200
1493/1493 - 46s - loss: 1.5160e-04 - val_loss: 1.8877e-04 - 46s/epoch - 31ms/step
Epoch 96/200
1493/1493 - 46s - loss: 1.5049e-04 - val_loss: 1.7379e-04 - 46s/epoch - 31ms/step
Epoch 97/200
1493/1493 - 46s - loss: 1.4797e-04 - val_loss: 1.4477e-04 - 46s/epoch - 31ms/step
Epoch 98/200
1493/1493 - 45s - loss: 1.5365e-04 - val_loss: 0.0011 - 45s/epoch - 30ms/step
Epoch 99/200
1493/1493 - 46s - loss: 2.4261e-04 - val_loss: 1.3987e-04 - 46s/epoch - 30ms/step
Epoch 100/200
1493/1493 - 46s - loss: 1.6481e-04 - val_loss: 1.5527e-04 - 46s/epoch - 31ms/step
Epoch 101/200
1493/1493 - 45s - loss: 1.5144e-04 - val_loss: 5.9274e-04 - 45s/epoch - 30ms/step
Epoch 102/200
1493/1493 - 46s - loss: 1.7520e-04 - val_loss: 1.6555e-04 - 46s/epoch - 31ms/step
Epoch 103/200
1493/1493 - 46s - loss: 1.4829e-04 - val_loss: 1.4745e-04 - 46s/epoch - 31ms/step
Epoch 104/200
1493/1493 - 46s - loss: 1.4460e-04 - val_loss: 3.2074e-04 - 46s/epoch - 30ms/step
Epoch 105/200
1493/1493 - 46s - loss: 1.5547e-04 - val_loss: 1.3748e-04 - 46s/epoch - 31ms/step
Epoch 106/200
1493/1493 - 46s - loss: 1.4202e-04 - val_loss: 1.6431e-04 - 46s/epoch - 31ms/step
Epoch 107/200
1493/1493 - 45s - loss: 1.4040e-04 - val_loss: 1.4302e-04 - 45s/epoch - 30ms/step
Epoch 108/200
1493/1493 - 46s - loss: 1.4223e-04 - val_loss: 1.4871e-04 - 46s/epoch - 31ms/step
Epoch 109/200
1493/1493 - 46s - loss: 1.3980e-04 - val_loss: 1.7050e-04 - 46s/epoch - 30ms/step
Epoch 110/200
1493/1493 - 46s - loss: 1.3680e-04 - val_loss: 1.3770e-04 - 46s/epoch - 31ms/step
Epoch 111/200
1493/1493 - 46s - loss: 1.3722e-04 - val_loss: 1.8907e-04 - 46s/epoch - 31ms/step
Epoch 112/200
1493/1493 - 46s - loss: 1.4581e-04 - val_loss: 1.2613e-04 - 46s/epoch - 31ms/step
Epoch 113/200
1493/1493 - 46s - loss: 1.3670e-04 - val_loss: 5.1226e-04 - 46s/epoch - 31ms/step
Epoch 114/200
1493/1493 - 46s - loss: 1.7938e-04 - val_loss: 1.4217e-04 - 46s/epoch - 30ms/step
Epoch 115/200
1493/1493 - 46s - loss: 1.4180e-04 - val_loss: 2.0513e-04 - 46s/epoch - 31ms/step
Epoch 116/200
1493/1493 - 46s - loss: 1.3513e-04 - val_loss: 1.6240e-04 - 46s/epoch - 31ms/step
Epoch 117/200
1493/1493 - 46s - loss: 1.3394e-04 - val_loss: 1.8168e-04 - 46s/epoch - 30ms/step
Epoch 118/200
1493/1493 - 46s - loss: 1.4071e-04 - val_loss: 1.4316e-04 - 46s/epoch - 31ms/step
Epoch 119/200
1493/1493 - 46s - loss: 1.3357e-04 - val_loss: 1.2943e-04 - 46s/epoch - 31ms/step
Epoch 120/200
1493/1493 - 46s - loss: 1.3100e-04 - val_loss: 1.4070e-04 - 46s/epoch - 31ms/step
Epoch 121/200
1493/1493 - 46s - loss: 1.3066e-04 - val_loss: 1.9521e-04 - 46s/epoch - 31ms/step
Epoch 122/200
1493/1493 - 46s - loss: 1.3491e-04 - val_loss: 1.2999e-04 - 46s/epoch - 31ms/step
Epoch 123/200
1493/1493 - 46s - loss: 1.3011e-04 - val_loss: 1.4015e-04 - 46s/epoch - 30ms/step
Epoch 124/200
1493/1493 - 46s - loss: 1.2864e-04 - val_loss: 3.9554e-04 - 46s/epoch - 31ms/step
Epoch 125/200
1493/1493 - 46s - loss: 1.5249e-04 - val_loss: 1.2789e-04 - 46s/epoch - 31ms/step
Epoch 126/200
1493/1493 - 46s - loss: 1.3078e-04 - val_loss: 3.8822e-04 - 46s/epoch - 31ms/step
Epoch 127/200
1493/1493 - 46s - loss: 1.5401e-04 - val_loss: 3.4414e-04 - 46s/epoch - 31ms/step
Epoch 128/200
1493/1493 - 46s - loss: 1.4733e-04 - val_loss: 1.1267e-04 - 46s/epoch - 31ms/step
Epoch 129/200
1493/1493 - 46s - loss: 1.2852e-04 - val_loss: 1.2808e-04 - 46s/epoch - 31ms/step
Epoch 130/200
1493/1493 - 46s - loss: 1.2719e-04 - val_loss: 1.2618e-04 - 46s/epoch - 30ms/step
Epoch 131/200
1493/1493 - 46s - loss: 1.2965e-04 - val_loss: 5.3253e-04 - 46s/epoch - 31ms/step
Epoch 132/200
1493/1493 - 46s - loss: 1.7293e-04 - val_loss: 1.1707e-04 - 46s/epoch - 31ms/step
Epoch 133/200
1493/1493 - 46s - loss: 1.2816e-04 - val_loss: 1.3010e-04 - 46s/epoch - 31ms/step
Epoch 134/200
1493/1493 - 46s - loss: 1.2479e-04 - val_loss: 1.2386e-04 - 46s/epoch - 31ms/step
Epoch 135/200
1493/1493 - 46s - loss: 1.2296e-04 - val_loss: 1.1928e-04 - 46s/epoch - 31ms/step
Epoch 136/200
1493/1493 - 46s - loss: 1.2484e-04 - val_loss: 2.5335e-04 - 46s/epoch - 31ms/step
Epoch 137/200
1493/1493 - 46s - loss: 1.5116e-04 - val_loss: 2.3243e-04 - 46s/epoch - 31ms/step
Epoch 138/200
1493/1493 - 46s - loss: 1.4918e-04 - val_loss: 1.6674e-04 - 46s/epoch - 31ms/step
Epoch 139/200
1493/1493 - 46s - loss: 1.3506e-04 - val_loss: 2.2670e-04 - 46s/epoch - 31ms/step
Epoch 140/200
1493/1493 - 46s - loss: 1.3897e-04 - val_loss: 1.0951e-04 - 46s/epoch - 31ms/step
Epoch 141/200
1493/1493 - 46s - loss: 1.2335e-04 - val_loss: 1.6603e-04 - 46s/epoch - 31ms/step
Epoch 142/200
1493/1493 - 46s - loss: 1.2772e-04 - val_loss: 1.2434e-04 - 46s/epoch - 31ms/step
Epoch 143/200
1493/1493 - 46s - loss: 1.2092e-04 - val_loss: 1.6410e-04 - 46s/epoch - 31ms/step
Epoch 144/200
1493/1493 - 46s - loss: 1.2703e-04 - val_loss: 1.2112e-04 - 46s/epoch - 31ms/step
Epoch 145/200
1493/1493 - 46s - loss: 1.2040e-04 - val_loss: 1.1072e-04 - 46s/epoch - 30ms/step
Epoch 146/200
1493/1493 - 45s - loss: 1.1932e-04 - val_loss: 1.2504e-04 - 45s/epoch - 30ms/step
Epoch 147/200
1493/1493 - 46s - loss: 1.1818e-04 - val_loss: 1.4180e-04 - 46s/epoch - 31ms/step
Epoch 148/200
1493/1493 - 46s - loss: 1.2026e-04 - val_loss: 1.9244e-04 - 46s/epoch - 31ms/step
Epoch 149/200
1493/1493 - 46s - loss: 1.1732e-04 - val_loss: 1.2029e-04 - 46s/epoch - 31ms/step
Epoch 150/200
1493/1493 - 46s - loss: 1.1672e-04 - val_loss: 1.0580e-04 - 46s/epoch - 31ms/step
Epoch 151/200
1493/1493 - 46s - loss: 1.1609e-04 - val_loss: 1.2042e-04 - 46s/epoch - 31ms/step
Epoch 152/200
1493/1493 - 46s - loss: 1.1542e-04 - val_loss: 1.4216e-04 - 46s/epoch - 31ms/step
Epoch 153/200
1493/1493 - 46s - loss: 1.1664e-04 - val_loss: 1.3357e-04 - 46s/epoch - 31ms/step
Epoch 154/200
1493/1493 - 46s - loss: 1.1584e-04 - val_loss: 2.3363e-04 - 46s/epoch - 31ms/step
Epoch 155/200
1493/1493 - 46s - loss: 1.3835e-04 - val_loss: 1.1175e-04 - 46s/epoch - 31ms/step
Epoch 156/200
1493/1493 - 46s - loss: 1.1952e-04 - val_loss: 1.1668e-04 - 46s/epoch - 31ms/step
Epoch 157/200
1493/1493 - 46s - loss: 1.1345e-04 - val_loss: 1.2452e-04 - 46s/epoch - 31ms/step
Epoch 158/200
1493/1493 - 46s - loss: 1.1617e-04 - val_loss: 1.2248e-04 - 46s/epoch - 31ms/step
Epoch 159/200
1493/1493 - 46s - loss: 1.1357e-04 - val_loss: 1.1266e-04 - 46s/epoch - 31ms/step
Epoch 160/200
1493/1493 - 46s - loss: 1.1491e-04 - val_loss: 1.6736e-04 - 46s/epoch - 31ms/step
Epoch 161/200
1493/1493 - 46s - loss: 1.3842e-04 - val_loss: 1.0519e-04 - 46s/epoch - 31ms/step
Epoch 162/200
1493/1493 - 46s - loss: 1.1354e-04 - val_loss: 1.1567e-04 - 46s/epoch - 31ms/step
Epoch 163/200
1493/1493 - 46s - loss: 1.1331e-04 - val_loss: 1.5979e-04 - 46s/epoch - 31ms/step
Epoch 164/200
1493/1493 - 46s - loss: 1.2066e-04 - val_loss: 1.2188e-04 - 46s/epoch - 31ms/step
Epoch 165/200
1493/1493 - 46s - loss: 1.1808e-04 - val_loss: 1.0840e-04 - 46s/epoch - 31ms/step
Epoch 166/200
1493/1493 - 46s - loss: 1.1240e-04 - val_loss: 1.1919e-04 - 46s/epoch - 31ms/step
Epoch 167/200
1493/1493 - 46s - loss: 1.1170e-04 - val_loss: 1.0548e-04 - 46s/epoch - 31ms/step
Epoch 168/200
1493/1493 - 46s - loss: 1.1223e-04 - val_loss: 1.1661e-04 - 46s/epoch - 30ms/step
Epoch 169/200
1493/1493 - 46s - loss: 1.1066e-04 - val_loss: 1.2902e-04 - 46s/epoch - 31ms/step
Epoch 170/200
1493/1493 - 46s - loss: 1.1469e-04 - val_loss: 3.7124e-04 - 46s/epoch - 31ms/step
Epoch 171/200
1493/1493 - 46s - loss: 1.6520e-04 - val_loss: 6.4032e-04 - 46s/epoch - 31ms/step
Epoch 172/200
1493/1493 - 46s - loss: 1.7512e-04 - val_loss: 1.5343e-04 - 46s/epoch - 31ms/step
Epoch 173/200
1493/1493 - 46s - loss: 1.2066e-04 - val_loss: 9.6942e-05 - 46s/epoch - 30ms/step
Epoch 174/200
1493/1493 - 46s - loss: 1.1242e-04 - val_loss: 1.2485e-04 - 46s/epoch - 31ms/step
Epoch 175/200
1493/1493 - 46s - loss: 1.1278e-04 - val_loss: 1.3248e-04 - 46s/epoch - 31ms/step
Epoch 176/200
1493/1493 - 46s - loss: 1.1317e-04 - val_loss: 1.3198e-04 - 46s/epoch - 31ms/step
Epoch 177/200
1493/1493 - 46s - loss: 1.1467e-04 - val_loss: 1.0592e-04 - 46s/epoch - 31ms/step
Epoch 178/200
1493/1493 - 46s - loss: 1.0895e-04 - val_loss: 1.1943e-04 - 46s/epoch - 31ms/step
Epoch 179/200
1493/1493 - 46s - loss: 1.0846e-04 - val_loss: 1.1405e-04 - 46s/epoch - 31ms/step
Epoch 180/200
1493/1493 - 46s - loss: 1.0851e-04 - val_loss: 1.6026e-04 - 46s/epoch - 31ms/step
Epoch 181/200
1493/1493 - 46s - loss: 1.1524e-04 - val_loss: 1.1169e-04 - 46s/epoch - 31ms/step
Epoch 182/200
1493/1493 - 46s - loss: 1.0848e-04 - val_loss: 1.3172e-04 - 46s/epoch - 31ms/step
Epoch 183/200
1493/1493 - 46s - loss: 1.0787e-04 - val_loss: 1.3949e-04 - 46s/epoch - 31ms/step
Epoch 184/200
1493/1493 - 46s - loss: 1.0893e-04 - val_loss: 1.0159e-04 - 46s/epoch - 31ms/step
Epoch 185/200
1493/1493 - 46s - loss: 1.0490e-04 - val_loss: 1.3977e-04 - 46s/epoch - 31ms/step
Epoch 186/200
1493/1493 - 46s - loss: 1.0588e-04 - val_loss: 1.0972e-04 - 46s/epoch - 31ms/step
Epoch 187/200
1493/1493 - 46s - loss: 1.0496e-04 - val_loss: 1.1308e-04 - 46s/epoch - 31ms/step
Epoch 188/200
1493/1493 - 46s - loss: 1.0915e-04 - val_loss: 1.9188e-04 - 46s/epoch - 31ms/step
Epoch 189/200
1493/1493 - 46s - loss: 1.1168e-04 - val_loss: 1.2996e-04 - 46s/epoch - 31ms/step
Epoch 190/200
1493/1493 - 46s - loss: 1.0714e-04 - val_loss: 1.9390e-04 - 46s/epoch - 30ms/step
Epoch 191/200
1493/1493 - 46s - loss: 1.1464e-04 - val_loss: 1.2932e-04 - 46s/epoch - 31ms/step
Epoch 192/200
1493/1493 - 46s - loss: 1.0812e-04 - val_loss: 1.8377e-04 - 46s/epoch - 31ms/step
Epoch 193/200
1493/1493 - 46s - loss: 1.1666e-04 - val_loss: 3.9609e-04 - 46s/epoch - 31ms/step
Epoch 194/200
1493/1493 - 46s - loss: 1.4953e-04 - val_loss: 3.8247e-04 - 46s/epoch - 31ms/step
Epoch 195/200
1493/1493 - 46s - loss: 1.5367e-04 - val_loss: 2.5094e-04 - 46s/epoch - 31ms/step
Epoch 196/200
1493/1493 - 46s - loss: 1.2922e-04 - val_loss: 1.0165e-04 - 46s/epoch - 31ms/step
Epoch 197/200
1493/1493 - 46s - loss: 1.0948e-04 - val_loss: 1.0249e-04 - 46s/epoch - 31ms/step
Epoch 198/200
1493/1493 - 46s - loss: 1.0830e-04 - val_loss: 1.3534e-04 - 46s/epoch - 31ms/step
Epoch 199/200
1493/1493 - 46s - loss: 1.0723e-04 - val_loss: 1.0530e-04 - 46s/epoch - 31ms/step
Epoch 200/200
1493/1493 - 46s - loss: 1.0475e-04 - val_loss: 1.1064e-04 - 46s/epoch - 31ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00011063892452511936
  1/332 [..............................] - ETA: 55s  8/332 [..............................] - ETA: 2s  16/332 [>.............................] - ETA: 2s 25/332 [=>............................] - ETA: 1s 35/332 [==>...........................] - ETA: 1s 46/332 [===>..........................] - ETA: 1s 57/332 [====>.........................] - ETA: 1s 67/332 [=====>........................] - ETA: 1s 78/332 [======>.......................] - ETA: 1s 88/332 [======>.......................] - ETA: 1s 99/332 [=======>......................] - ETA: 1s110/332 [========>.....................] - ETA: 1s121/332 [=========>....................] - ETA: 1s132/332 [==========>...................] - ETA: 1s143/332 [===========>..................] - ETA: 0s153/332 [============>.................] - ETA: 0s164/332 [=============>................] - ETA: 0s175/332 [==============>...............] - ETA: 0s186/332 [===============>..............] - ETA: 0s197/332 [================>.............] - ETA: 0s208/332 [=================>............] - ETA: 0s219/332 [==================>...........] - ETA: 0s230/332 [===================>..........] - ETA: 0s241/332 [====================>.........] - ETA: 0s252/332 [=====================>........] - ETA: 0s263/332 [======================>.......] - ETA: 0s274/332 [=======================>......] - ETA: 0s285/332 [========================>.....] - ETA: 0s296/332 [=========================>....] - ETA: 0s306/332 [==========================>...] - ETA: 0s316/332 [===========================>..] - ETA: 0s327/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 5ms/step
correlation 0.0012574849198819823
cosine 0.0009906499387181754
MAE: 0.0059102937
RMSE: 0.010518499
r2: 0.9928232279596557
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 2148)              2717220   
                                                                 
 batch_normalization_6 (Batc  (None, 2148)             8592      
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2148)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1358168   
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 2148)              1359684   
                                                                 
 batch_normalization_8 (Batc  (None, 2148)             8592      
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2148)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              2716336   
                                                                 
=================================================================
Total params: 8,171,120
Trainable params: 8,161,264
Non-trainable params: 9,856
_________________________________________________________________
Encoder
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 2148)              2717220   
                                                                 
 batch_normalization_6 (Batc  (None, 2148)             8592      
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2148)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1358168   
                                                                 
=================================================================
Total params: 4,083,980
Trainable params: 4,079,684
Non-trainable params: 4,296
_________________________________________________________________
Decoder
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 2148)              1359684   
                                                                 
 batch_normalization_8 (Batc  (None, 2148)             8592      
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2148)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              2716336   
                                                                 
=================================================================
Total params: 4,087,140
Trainable params: 4,081,580
Non-trainable params: 5,560
_________________________________________________________________
['1.7custom_n_b', 'mse', 64, 200, 0.0005, 0.5, 632, 0.00010474623559275642, 0.00011063892452511936, 0.0012574849198819823, 0.0009906499387181754, 0.005910293664783239, 0.010518498718738556, 0.9928232279596557, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats2_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_9 (Dense)             (None, 2275)              2877875   
                                                                 
 batch_normalization_9 (Batc  (None, 2275)             9100      
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2275)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1438432   
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 2275)              1440075   
                                                                 
 batch_normalization_11 (Bat  (None, 2275)             9100      
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2275)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              2876864   
                                                                 
=================================================================
Total params: 8,653,974
Trainable params: 8,643,610
Non-trainable params: 10,364
_________________________________________________________________
Epoch 1/200
1493/1493 - 49s - loss: 0.0093 - val_loss: 0.0050 - 49s/epoch - 33ms/step
Epoch 2/200
1493/1493 - 48s - loss: 0.0030 - val_loss: 0.0030 - 48s/epoch - 32ms/step
Epoch 3/200
1493/1493 - 48s - loss: 0.0021 - val_loss: 0.0016 - 48s/epoch - 32ms/step
Epoch 4/200
1493/1493 - 48s - loss: 0.0017 - val_loss: 0.0018 - 48s/epoch - 32ms/step
Epoch 5/200
1493/1493 - 48s - loss: 0.0015 - val_loss: 0.0015 - 48s/epoch - 32ms/step
Epoch 6/200
1493/1493 - 48s - loss: 0.0013 - val_loss: 0.0013 - 48s/epoch - 32ms/step
Epoch 7/200
1493/1493 - 48s - loss: 0.0013 - val_loss: 0.0011 - 48s/epoch - 32ms/step
Epoch 8/200
1493/1493 - 48s - loss: 0.0011 - val_loss: 0.0012 - 48s/epoch - 32ms/step
Epoch 9/200
1493/1493 - 48s - loss: 0.0010 - val_loss: 9.5549e-04 - 48s/epoch - 32ms/step
Epoch 10/200
1493/1493 - 48s - loss: 9.5382e-04 - val_loss: 8.5095e-04 - 48s/epoch - 32ms/step
Epoch 11/200
1493/1493 - 48s - loss: 8.8901e-04 - val_loss: 7.3160e-04 - 48s/epoch - 32ms/step
Epoch 12/200
1493/1493 - 48s - loss: 8.0078e-04 - val_loss: 0.0029 - 48s/epoch - 32ms/step
Epoch 13/200
1493/1493 - 48s - loss: 8.3967e-04 - val_loss: 0.0030 - 48s/epoch - 32ms/step
Epoch 14/200
1493/1493 - 48s - loss: 8.6509e-04 - val_loss: 7.7146e-04 - 48s/epoch - 32ms/step
Epoch 15/200
1493/1493 - 48s - loss: 6.9789e-04 - val_loss: 0.0010 - 48s/epoch - 32ms/step
Epoch 16/200
1493/1493 - 48s - loss: 6.4486e-04 - val_loss: 5.8492e-04 - 48s/epoch - 32ms/step
Epoch 17/200
1493/1493 - 48s - loss: 5.8438e-04 - val_loss: 9.4145e-04 - 48s/epoch - 32ms/step
Epoch 18/200
1493/1493 - 48s - loss: 6.0430e-04 - val_loss: 5.8747e-04 - 48s/epoch - 32ms/step
Epoch 19/200
1493/1493 - 48s - loss: 5.2979e-04 - val_loss: 8.1762e-04 - 48s/epoch - 32ms/step
Epoch 20/200
1493/1493 - 48s - loss: 5.2450e-04 - val_loss: 4.6446e-04 - 48s/epoch - 32ms/step
Epoch 21/200
1493/1493 - 48s - loss: 4.8004e-04 - val_loss: 6.4355e-04 - 48s/epoch - 32ms/step
Epoch 22/200
1493/1493 - 48s - loss: 4.6431e-04 - val_loss: 5.3593e-04 - 48s/epoch - 32ms/step
Epoch 23/200
1493/1493 - 48s - loss: 4.5014e-04 - val_loss: 7.9487e-04 - 48s/epoch - 32ms/step
Epoch 24/200
1493/1493 - 48s - loss: 4.5962e-04 - val_loss: 3.8961e-04 - 48s/epoch - 32ms/step
Epoch 25/200
1493/1493 - 48s - loss: 3.9569e-04 - val_loss: 3.5799e-04 - 48s/epoch - 32ms/step
Epoch 26/200
1493/1493 - 48s - loss: 3.7870e-04 - val_loss: 4.2536e-04 - 48s/epoch - 32ms/step
Epoch 27/200
1493/1493 - 48s - loss: 3.6987e-04 - val_loss: 3.5977e-04 - 48s/epoch - 32ms/step
Epoch 28/200
1493/1493 - 48s - loss: 3.5403e-04 - val_loss: 4.2249e-04 - 48s/epoch - 32ms/step
Epoch 29/200
1493/1493 - 48s - loss: 3.3695e-04 - val_loss: 5.4902e-04 - 48s/epoch - 32ms/step
Epoch 30/200
1493/1493 - 48s - loss: 3.5556e-04 - val_loss: 3.7936e-04 - 48s/epoch - 32ms/step
Epoch 31/200
1493/1493 - 48s - loss: 3.1915e-04 - val_loss: 4.8834e-04 - 48s/epoch - 32ms/step
Epoch 32/200
1493/1493 - 48s - loss: 3.2507e-04 - val_loss: 3.1194e-04 - 48s/epoch - 32ms/step
Epoch 33/200
1493/1493 - 48s - loss: 3.0920e-04 - val_loss: 2.9852e-04 - 48s/epoch - 32ms/step
Epoch 34/200
1493/1493 - 48s - loss: 2.9757e-04 - val_loss: 2.6565e-04 - 48s/epoch - 32ms/step
Epoch 35/200
1493/1493 - 48s - loss: 2.8316e-04 - val_loss: 9.7396e-04 - 48s/epoch - 32ms/step
Epoch 36/200
1493/1493 - 48s - loss: 3.3391e-04 - val_loss: 4.6215e-04 - 48s/epoch - 32ms/step
Epoch 37/200
1493/1493 - 48s - loss: 2.8991e-04 - val_loss: 4.2762e-04 - 48s/epoch - 32ms/step
Epoch 38/200
1493/1493 - 48s - loss: 2.8944e-04 - val_loss: 2.8424e-04 - 48s/epoch - 32ms/step
Epoch 39/200
1493/1493 - 48s - loss: 2.6383e-04 - val_loss: 2.6590e-04 - 48s/epoch - 32ms/step
Epoch 40/200
1493/1493 - 48s - loss: 2.5399e-04 - val_loss: 3.9683e-04 - 48s/epoch - 32ms/step
Epoch 41/200
1493/1493 - 48s - loss: 2.5452e-04 - val_loss: 2.2532e-04 - 48s/epoch - 32ms/step
Epoch 42/200
1493/1493 - 48s - loss: 2.4950e-04 - val_loss: 2.3634e-04 - 48s/epoch - 32ms/step
Epoch 43/200
1493/1493 - 48s - loss: 2.4625e-04 - val_loss: 4.0749e-04 - 48s/epoch - 32ms/step
Epoch 44/200
1493/1493 - 48s - loss: 2.5630e-04 - val_loss: 4.0010e-04 - 48s/epoch - 32ms/step
Epoch 45/200
1493/1493 - 48s - loss: 2.4865e-04 - val_loss: 2.1297e-04 - 48s/epoch - 32ms/step
Epoch 46/200
1493/1493 - 48s - loss: 2.3011e-04 - val_loss: 2.2628e-04 - 48s/epoch - 32ms/step
Epoch 47/200
1493/1493 - 48s - loss: 2.2753e-04 - val_loss: 2.6245e-04 - 48s/epoch - 32ms/step
Epoch 48/200
1493/1493 - 48s - loss: 2.2663e-04 - val_loss: 5.2488e-04 - 48s/epoch - 32ms/step
Epoch 49/200
1493/1493 - 48s - loss: 2.2446e-04 - val_loss: 0.0011 - 48s/epoch - 32ms/step
Epoch 50/200
1493/1493 - 48s - loss: 3.1429e-04 - val_loss: 9.5602e-04 - 48s/epoch - 32ms/step
Epoch 51/200
1493/1493 - 48s - loss: 2.5418e-04 - val_loss: 2.4060e-04 - 48s/epoch - 32ms/step
Epoch 52/200
1493/1493 - 48s - loss: 2.1320e-04 - val_loss: 2.1902e-04 - 48s/epoch - 32ms/step
Epoch 53/200
1493/1493 - 48s - loss: 2.1573e-04 - val_loss: 2.2890e-04 - 48s/epoch - 32ms/step
Epoch 54/200
1493/1493 - 48s - loss: 2.0796e-04 - val_loss: 2.3717e-04 - 48s/epoch - 32ms/step
Epoch 55/200
1493/1493 - 48s - loss: 2.0359e-04 - val_loss: 2.0435e-04 - 48s/epoch - 32ms/step
Epoch 56/200
1493/1493 - 48s - loss: 2.0037e-04 - val_loss: 2.4381e-04 - 48s/epoch - 32ms/step
Epoch 57/200
1493/1493 - 48s - loss: 1.9623e-04 - val_loss: 2.1364e-04 - 48s/epoch - 32ms/step
Epoch 58/200
1493/1493 - 48s - loss: 1.9688e-04 - val_loss: 1.7906e-04 - 48s/epoch - 32ms/step
Epoch 59/200
1493/1493 - 48s - loss: 1.9178e-04 - val_loss: 2.1387e-04 - 48s/epoch - 32ms/step
Epoch 60/200
1493/1493 - 48s - loss: 1.9170e-04 - val_loss: 5.4194e-04 - 48s/epoch - 32ms/step
Epoch 61/200
1493/1493 - 48s - loss: 3.0380e-04 - val_loss: 1.7410e-04 - 48s/epoch - 32ms/step
Epoch 62/200
1493/1493 - 48s - loss: 1.9454e-04 - val_loss: 2.3983e-04 - 48s/epoch - 32ms/step
Epoch 63/200
1493/1493 - 48s - loss: 1.9027e-04 - val_loss: 2.6267e-04 - 48s/epoch - 32ms/step
Epoch 64/200
1493/1493 - 48s - loss: 1.9049e-04 - val_loss: 0.0018 - 48s/epoch - 32ms/step
Epoch 65/200
1493/1493 - 48s - loss: 3.3995e-04 - val_loss: 2.4746e-04 - 48s/epoch - 32ms/step
Epoch 66/200
1493/1493 - 48s - loss: 2.0110e-04 - val_loss: 1.8315e-04 - 48s/epoch - 32ms/step
Epoch 67/200
1493/1493 - 48s - loss: 1.8663e-04 - val_loss: 1.6664e-04 - 48s/epoch - 32ms/step
Epoch 68/200
1493/1493 - 48s - loss: 1.8203e-04 - val_loss: 4.4430e-04 - 48s/epoch - 32ms/step
Epoch 69/200
1493/1493 - 48s - loss: 2.1812e-04 - val_loss: 1.9545e-04 - 48s/epoch - 32ms/step
Epoch 70/200
1493/1493 - 48s - loss: 1.8172e-04 - val_loss: 4.7821e-04 - 48s/epoch - 32ms/step
Epoch 71/200
1493/1493 - 48s - loss: 2.3261e-04 - val_loss: 1.8351e-04 - 48s/epoch - 32ms/step
Epoch 72/200
1493/1493 - 48s - loss: 1.8492e-04 - val_loss: 2.4269e-04 - 48s/epoch - 32ms/step
Epoch 73/200
1493/1493 - 48s - loss: 1.8068e-04 - val_loss: 1.8033e-04 - 48s/epoch - 32ms/step
Epoch 74/200
1493/1493 - 48s - loss: 1.7621e-04 - val_loss: 1.7745e-04 - 48s/epoch - 32ms/step
Epoch 75/200
1493/1493 - 48s - loss: 1.6913e-04 - val_loss: 1.8164e-04 - 48s/epoch - 32ms/step
Epoch 76/200
1493/1493 - 48s - loss: 1.7200e-04 - val_loss: 1.7239e-04 - 48s/epoch - 32ms/step
Epoch 77/200
1493/1493 - 48s - loss: 1.6255e-04 - val_loss: 3.7477e-04 - 48s/epoch - 32ms/step
Epoch 78/200
1493/1493 - 48s - loss: 1.7397e-04 - val_loss: 2.3611e-04 - 48s/epoch - 32ms/step
Epoch 79/200
1493/1493 - 48s - loss: 1.7065e-04 - val_loss: 3.5340e-04 - 48s/epoch - 32ms/step
Epoch 80/200
1493/1493 - 48s - loss: 1.9597e-04 - val_loss: 1.7275e-04 - 48s/epoch - 32ms/step
Epoch 81/200
1493/1493 - 48s - loss: 1.6239e-04 - val_loss: 3.8616e-04 - 48s/epoch - 32ms/step
Epoch 82/200
1493/1493 - 48s - loss: 1.6327e-04 - val_loss: 1.6379e-04 - 48s/epoch - 32ms/step
Epoch 83/200
1493/1493 - 48s - loss: 1.5621e-04 - val_loss: 1.7933e-04 - 48s/epoch - 32ms/step
Epoch 84/200
1493/1493 - 48s - loss: 1.5540e-04 - val_loss: 1.7483e-04 - 48s/epoch - 32ms/step
Epoch 85/200
1493/1493 - 48s - loss: 1.5595e-04 - val_loss: 2.0117e-04 - 48s/epoch - 32ms/step
Epoch 86/200
1493/1493 - 48s - loss: 1.5458e-04 - val_loss: 1.8866e-04 - 48s/epoch - 32ms/step
Epoch 87/200
1493/1493 - 48s - loss: 1.5029e-04 - val_loss: 1.4514e-04 - 48s/epoch - 32ms/step
Epoch 88/200
1493/1493 - 48s - loss: 1.4959e-04 - val_loss: 2.1027e-04 - 48s/epoch - 32ms/step
Epoch 89/200
1493/1493 - 48s - loss: 1.5477e-04 - val_loss: 1.5935e-04 - 48s/epoch - 32ms/step
Epoch 90/200
1493/1493 - 48s - loss: 1.4736e-04 - val_loss: 1.7938e-04 - 48s/epoch - 32ms/step
Epoch 91/200
1493/1493 - 48s - loss: 1.4611e-04 - val_loss: 1.5480e-04 - 48s/epoch - 32ms/step
Epoch 92/200
1493/1493 - 48s - loss: 1.5076e-04 - val_loss: 5.6449e-04 - 48s/epoch - 32ms/step
Epoch 93/200
1493/1493 - 48s - loss: 1.9487e-04 - val_loss: 6.4881e-04 - 48s/epoch - 32ms/step
Epoch 94/200
1493/1493 - 48s - loss: 1.8532e-04 - val_loss: 1.2345e-04 - 48s/epoch - 32ms/step
Epoch 95/200
1493/1493 - 48s - loss: 1.4776e-04 - val_loss: 2.5308e-04 - 48s/epoch - 32ms/step
Epoch 96/200
1493/1493 - 48s - loss: 1.5294e-04 - val_loss: 1.7330e-04 - 48s/epoch - 32ms/step
Epoch 97/200
1493/1493 - 48s - loss: 1.4450e-04 - val_loss: 1.4967e-04 - 48s/epoch - 32ms/step
Epoch 98/200
1493/1493 - 48s - loss: 1.4846e-04 - val_loss: 7.0747e-04 - 48s/epoch - 32ms/step
Epoch 99/200
1493/1493 - 48s - loss: 2.3004e-04 - val_loss: 1.2948e-04 - 48s/epoch - 32ms/step
Epoch 100/200
1493/1493 - 48s - loss: 1.5647e-04 - val_loss: 1.3825e-04 - 48s/epoch - 32ms/step
Epoch 101/200
1493/1493 - 48s - loss: 1.4392e-04 - val_loss: 3.3169e-04 - 48s/epoch - 32ms/step
Epoch 102/200
1493/1493 - 48s - loss: 1.5475e-04 - val_loss: 2.9149e-04 - 48s/epoch - 32ms/step
Epoch 103/200
1493/1493 - 48s - loss: 1.5674e-04 - val_loss: 1.3935e-04 - 48s/epoch - 32ms/step
Epoch 104/200
1493/1493 - 48s - loss: 1.4101e-04 - val_loss: 6.5148e-04 - 48s/epoch - 32ms/step
Epoch 105/200
1493/1493 - 48s - loss: 1.8383e-04 - val_loss: 1.2207e-04 - 48s/epoch - 32ms/step
Epoch 106/200
1493/1493 - 48s - loss: 1.4077e-04 - val_loss: 1.5290e-04 - 48s/epoch - 32ms/step
Epoch 107/200
1493/1493 - 48s - loss: 1.3854e-04 - val_loss: 1.6593e-04 - 48s/epoch - 32ms/step
Epoch 108/200
1493/1493 - 48s - loss: 1.4265e-04 - val_loss: 1.4066e-04 - 48s/epoch - 32ms/step
Epoch 109/200
1493/1493 - 48s - loss: 1.3652e-04 - val_loss: 1.5217e-04 - 48s/epoch - 32ms/step
Epoch 110/200
1493/1493 - 48s - loss: 1.3446e-04 - val_loss: 1.4745e-04 - 48s/epoch - 32ms/step
Epoch 111/200
1493/1493 - 48s - loss: 1.3332e-04 - val_loss: 1.8794e-04 - 48s/epoch - 32ms/step
Epoch 112/200
1493/1493 - 48s - loss: 1.4119e-04 - val_loss: 1.3131e-04 - 48s/epoch - 32ms/step
Epoch 113/200
1493/1493 - 48s - loss: 1.2971e-04 - val_loss: 2.2569e-04 - 48s/epoch - 32ms/step
Epoch 114/200
1493/1493 - 48s - loss: 1.4068e-04 - val_loss: 1.5030e-04 - 48s/epoch - 32ms/step
Epoch 115/200
1493/1493 - 48s - loss: 1.3255e-04 - val_loss: 2.0555e-04 - 48s/epoch - 32ms/step
Epoch 116/200
1493/1493 - 48s - loss: 1.3006e-04 - val_loss: 1.6414e-04 - 48s/epoch - 32ms/step
Epoch 117/200
1493/1493 - 48s - loss: 1.3026e-04 - val_loss: 4.0322e-04 - 48s/epoch - 32ms/step
Epoch 118/200
1493/1493 - 48s - loss: 1.6303e-04 - val_loss: 1.4202e-04 - 48s/epoch - 32ms/step
Epoch 119/200
1493/1493 - 48s - loss: 1.3084e-04 - val_loss: 1.2449e-04 - 48s/epoch - 32ms/step
Epoch 120/200
1493/1493 - 48s - loss: 1.2794e-04 - val_loss: 1.4061e-04 - 48s/epoch - 32ms/step
Epoch 121/200
1493/1493 - 48s - loss: 1.2720e-04 - val_loss: 1.9929e-04 - 48s/epoch - 32ms/step
Epoch 122/200
1493/1493 - 48s - loss: 1.2778e-04 - val_loss: 1.2614e-04 - 48s/epoch - 32ms/step
Epoch 123/200
1493/1493 - 48s - loss: 1.2454e-04 - val_loss: 1.4359e-04 - 48s/epoch - 32ms/step
Epoch 124/200
1493/1493 - 48s - loss: 1.2372e-04 - val_loss: 3.0226e-04 - 48s/epoch - 32ms/step
Epoch 125/200
1493/1493 - 48s - loss: 1.3604e-04 - val_loss: 1.4319e-04 - 48s/epoch - 32ms/step
Epoch 126/200
1493/1493 - 48s - loss: 1.2659e-04 - val_loss: 1.8118e-04 - 48s/epoch - 32ms/step
Epoch 127/200
1493/1493 - 48s - loss: 1.2779e-04 - val_loss: 3.3787e-04 - 48s/epoch - 32ms/step
Epoch 128/200
1493/1493 - 48s - loss: 1.5108e-04 - val_loss: 1.1369e-04 - 48s/epoch - 32ms/step
Epoch 129/200
1493/1493 - 48s - loss: 1.2302e-04 - val_loss: 1.2818e-04 - 48s/epoch - 32ms/step
Epoch 130/200
1493/1493 - 48s - loss: 1.2173e-04 - val_loss: 1.1761e-04 - 48s/epoch - 32ms/step
Epoch 131/200
1493/1493 - 48s - loss: 1.2345e-04 - val_loss: 3.0293e-04 - 48s/epoch - 32ms/step
Epoch 132/200
1493/1493 - 48s - loss: 1.4240e-04 - val_loss: 1.1678e-04 - 48s/epoch - 32ms/step
Epoch 133/200
1493/1493 - 48s - loss: 1.2112e-04 - val_loss: 1.3465e-04 - 48s/epoch - 32ms/step
Epoch 134/200
1493/1493 - 48s - loss: 1.1907e-04 - val_loss: 1.2396e-04 - 48s/epoch - 32ms/step
Epoch 135/200
1493/1493 - 48s - loss: 1.1923e-04 - val_loss: 1.1688e-04 - 48s/epoch - 32ms/step
Epoch 136/200
1493/1493 - 48s - loss: 1.1899e-04 - val_loss: 1.9760e-04 - 48s/epoch - 32ms/step
Epoch 137/200
1493/1493 - 48s - loss: 1.3505e-04 - val_loss: 2.1972e-04 - 48s/epoch - 32ms/step
Epoch 138/200
1493/1493 - 48s - loss: 1.3626e-04 - val_loss: 2.4436e-04 - 48s/epoch - 32ms/step
Epoch 139/200
1493/1493 - 48s - loss: 1.4271e-04 - val_loss: 2.3688e-04 - 48s/epoch - 32ms/step
Epoch 140/200
1493/1493 - 48s - loss: 1.4011e-04 - val_loss: 1.0069e-04 - 48s/epoch - 32ms/step
Epoch 141/200
1493/1493 - 48s - loss: 1.1929e-04 - val_loss: 1.6081e-04 - 48s/epoch - 32ms/step
Epoch 142/200
1493/1493 - 48s - loss: 1.2235e-04 - val_loss: 1.3757e-04 - 48s/epoch - 32ms/step
Epoch 143/200
1493/1493 - 48s - loss: 1.1689e-04 - val_loss: 1.4352e-04 - 48s/epoch - 32ms/step
Epoch 144/200
1493/1493 - 48s - loss: 1.1853e-04 - val_loss: 1.2479e-04 - 48s/epoch - 32ms/step
Epoch 145/200
1493/1493 - 48s - loss: 1.1545e-04 - val_loss: 1.2099e-04 - 48s/epoch - 32ms/step
Epoch 146/200
1493/1493 - 48s - loss: 1.1519e-04 - val_loss: 1.3313e-04 - 48s/epoch - 32ms/step
Epoch 147/200
1493/1493 - 48s - loss: 1.1362e-04 - val_loss: 1.3023e-04 - 48s/epoch - 32ms/step
Epoch 148/200
1493/1493 - 48s - loss: 1.1296e-04 - val_loss: 2.5928e-04 - 48s/epoch - 32ms/step
Epoch 149/200
1493/1493 - 48s - loss: 1.1281e-04 - val_loss: 1.2425e-04 - 48s/epoch - 32ms/step
Epoch 150/200
1493/1493 - 48s - loss: 1.1216e-04 - val_loss: 1.1077e-04 - 48s/epoch - 32ms/step
Epoch 151/200
1493/1493 - 48s - loss: 1.1307e-04 - val_loss: 1.6500e-04 - 48s/epoch - 32ms/step
Epoch 152/200
1493/1493 - 48s - loss: 1.1464e-04 - val_loss: 1.6075e-04 - 48s/epoch - 32ms/step
Epoch 153/200
1493/1493 - 48s - loss: 1.1258e-04 - val_loss: 1.4318e-04 - 48s/epoch - 32ms/step
Epoch 154/200
1493/1493 - 48s - loss: 1.1094e-04 - val_loss: 1.7866e-04 - 48s/epoch - 32ms/step
Epoch 155/200
1493/1493 - 48s - loss: 1.2239e-04 - val_loss: 1.0633e-04 - 48s/epoch - 32ms/step
Epoch 156/200
1493/1493 - 48s - loss: 1.1289e-04 - val_loss: 1.2293e-04 - 48s/epoch - 32ms/step
Epoch 157/200
1493/1493 - 48s - loss: 1.0882e-04 - val_loss: 1.3466e-04 - 48s/epoch - 32ms/step
Epoch 158/200
1493/1493 - 48s - loss: 1.1060e-04 - val_loss: 1.3878e-04 - 48s/epoch - 32ms/step
Epoch 159/200
1493/1493 - 48s - loss: 1.0871e-04 - val_loss: 1.1852e-04 - 48s/epoch - 32ms/step
Epoch 160/200
1493/1493 - 47s - loss: 1.0981e-04 - val_loss: 1.7386e-04 - 47s/epoch - 32ms/step
Epoch 161/200
1493/1493 - 48s - loss: 1.3119e-04 - val_loss: 1.0485e-04 - 48s/epoch - 32ms/step
Epoch 162/200
1493/1493 - 48s - loss: 1.1021e-04 - val_loss: 1.1873e-04 - 48s/epoch - 32ms/step
Epoch 163/200
1493/1493 - 48s - loss: 1.1176e-04 - val_loss: 4.0198e-04 - 48s/epoch - 32ms/step
Epoch 164/200
1493/1493 - 48s - loss: 1.4951e-04 - val_loss: 1.1269e-04 - 48s/epoch - 32ms/step
Epoch 165/200
1493/1493 - 48s - loss: 1.2235e-04 - val_loss: 1.0493e-04 - 48s/epoch - 32ms/step
Epoch 166/200
1493/1493 - 48s - loss: 1.1131e-04 - val_loss: 1.0923e-04 - 48s/epoch - 32ms/step
Epoch 167/200
1493/1493 - 48s - loss: 1.0855e-04 - val_loss: 1.0274e-04 - 48s/epoch - 32ms/step
Epoch 168/200
1493/1493 - 48s - loss: 1.0782e-04 - val_loss: 1.1269e-04 - 48s/epoch - 32ms/step
Epoch 169/200
1493/1493 - 48s - loss: 1.0791e-04 - val_loss: 1.5243e-04 - 48s/epoch - 32ms/step
Epoch 170/200
1493/1493 - 48s - loss: 1.0973e-04 - val_loss: 2.7221e-04 - 48s/epoch - 32ms/step
Epoch 171/200
1493/1493 - 48s - loss: 1.4734e-04 - val_loss: 2.9986e-04 - 48s/epoch - 32ms/step
Epoch 172/200
1493/1493 - 48s - loss: 1.3242e-04 - val_loss: 1.2672e-04 - 48s/epoch - 32ms/step
Epoch 173/200
1493/1493 - 48s - loss: 1.1180e-04 - val_loss: 9.5953e-05 - 48s/epoch - 32ms/step
Epoch 174/200
1493/1493 - 48s - loss: 1.0690e-04 - val_loss: 1.1503e-04 - 48s/epoch - 32ms/step
Epoch 175/200
1493/1493 - 48s - loss: 1.0826e-04 - val_loss: 1.3027e-04 - 48s/epoch - 32ms/step
Epoch 176/200
1493/1493 - 48s - loss: 1.0821e-04 - val_loss: 1.2171e-04 - 48s/epoch - 32ms/step
Epoch 177/200
1493/1493 - 48s - loss: 1.0868e-04 - val_loss: 9.5294e-05 - 48s/epoch - 32ms/step
Epoch 178/200
1493/1493 - 48s - loss: 1.0546e-04 - val_loss: 1.0331e-04 - 48s/epoch - 32ms/step
Epoch 179/200
1493/1493 - 48s - loss: 1.0365e-04 - val_loss: 1.1250e-04 - 48s/epoch - 32ms/step
Epoch 180/200
1493/1493 - 48s - loss: 1.0414e-04 - val_loss: 1.7454e-04 - 48s/epoch - 32ms/step
Epoch 181/200
1493/1493 - 48s - loss: 1.1096e-04 - val_loss: 1.1388e-04 - 48s/epoch - 32ms/step
Epoch 182/200
1493/1493 - 48s - loss: 1.0541e-04 - val_loss: 1.1544e-04 - 48s/epoch - 32ms/step
Epoch 183/200
1493/1493 - 48s - loss: 1.0328e-04 - val_loss: 1.2609e-04 - 48s/epoch - 32ms/step
Epoch 184/200
1493/1493 - 48s - loss: 1.0546e-04 - val_loss: 1.0894e-04 - 48s/epoch - 32ms/step
Epoch 185/200
1493/1493 - 48s - loss: 1.0156e-04 - val_loss: 1.4533e-04 - 48s/epoch - 32ms/step
Epoch 186/200
1493/1493 - 48s - loss: 1.0407e-04 - val_loss: 1.0197e-04 - 48s/epoch - 32ms/step
Epoch 187/200
1493/1493 - 48s - loss: 1.0138e-04 - val_loss: 1.1034e-04 - 48s/epoch - 32ms/step
Epoch 188/200
1493/1493 - 48s - loss: 1.0389e-04 - val_loss: 1.2641e-04 - 48s/epoch - 32ms/step
Epoch 189/200
1493/1493 - 48s - loss: 1.0349e-04 - val_loss: 1.2302e-04 - 48s/epoch - 32ms/step
Epoch 190/200
1493/1493 - 48s - loss: 1.0287e-04 - val_loss: 1.6907e-04 - 48s/epoch - 32ms/step
Epoch 191/200
1493/1493 - 48s - loss: 1.0681e-04 - val_loss: 1.4074e-04 - 48s/epoch - 32ms/step
Epoch 192/200
1493/1493 - 48s - loss: 1.0397e-04 - val_loss: 1.4849e-04 - 48s/epoch - 32ms/step
Epoch 193/200
1493/1493 - 48s - loss: 1.0869e-04 - val_loss: 3.2612e-04 - 48s/epoch - 32ms/step
Epoch 194/200
1493/1493 - 48s - loss: 1.4150e-04 - val_loss: 8.5929e-04 - 48s/epoch - 32ms/step
Epoch 195/200
1493/1493 - 48s - loss: 1.7279e-04 - val_loss: 3.6673e-04 - 48s/epoch - 32ms/step
Epoch 196/200
1493/1493 - 48s - loss: 1.7448e-04 - val_loss: 9.2066e-05 - 48s/epoch - 32ms/step
Epoch 197/200
1493/1493 - 48s - loss: 1.1267e-04 - val_loss: 1.0855e-04 - 48s/epoch - 32ms/step
Epoch 198/200
1493/1493 - 48s - loss: 1.1046e-04 - val_loss: 1.3995e-04 - 48s/epoch - 32ms/step
Epoch 199/200
1493/1493 - 48s - loss: 1.0947e-04 - val_loss: 9.6652e-05 - 48s/epoch - 32ms/step
Epoch 200/200
1493/1493 - 48s - loss: 1.0375e-04 - val_loss: 1.0621e-04 - 48s/epoch - 32ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00010620507237035781
  1/332 [..............................] - ETA: 58s  9/332 [..............................] - ETA: 2s  19/332 [>.............................] - ETA: 1s 29/332 [=>............................] - ETA: 1s 39/332 [==>...........................] - ETA: 1s 49/332 [===>..........................] - ETA: 1s 59/332 [====>.........................] - ETA: 1s 69/332 [=====>........................] - ETA: 1s 78/332 [======>.......................] - ETA: 1s 88/332 [======>.......................] - ETA: 1s 98/332 [=======>......................] - ETA: 1s108/332 [========>.....................] - ETA: 1s118/332 [=========>....................] - ETA: 1s128/332 [==========>...................] - ETA: 1s138/332 [===========>..................] - ETA: 1s148/332 [============>.................] - ETA: 1s158/332 [=============>................] - ETA: 0s168/332 [==============>...............] - ETA: 0s178/332 [===============>..............] - ETA: 0s188/332 [===============>..............] - ETA: 0s198/332 [================>.............] - ETA: 0s208/332 [=================>............] - ETA: 0s218/332 [==================>...........] - ETA: 0s228/332 [===================>..........] - ETA: 0s238/332 [====================>.........] - ETA: 0s248/332 [=====================>........] - ETA: 0s258/332 [======================>.......] - ETA: 0s268/332 [=======================>......] - ETA: 0s278/332 [========================>.....] - ETA: 0s288/332 [=========================>....] - ETA: 0s298/332 [=========================>....] - ETA: 0s308/332 [==========================>...] - ETA: 0s318/332 [===========================>..] - ETA: 0s328/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 5ms/step
correlation 0.0012073085987024204
cosine 0.0009501185275184583
MAE: 0.00581959
RMSE: 0.010305582
r2: 0.9931118525875603
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 2275)              2877875   
                                                                 
 batch_normalization_9 (Batc  (None, 2275)             9100      
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2275)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1438432   
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 2275)              1440075   
                                                                 
 batch_normalization_11 (Bat  (None, 2275)             9100      
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2275)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              2876864   
                                                                 
=================================================================
Total params: 8,653,974
Trainable params: 8,643,610
Non-trainable params: 10,364
_________________________________________________________________
Encoder
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 2275)              2877875   
                                                                 
 batch_normalization_9 (Batc  (None, 2275)             9100      
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2275)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1438432   
                                                                 
=================================================================
Total params: 4,325,407
Trainable params: 4,320,857
Non-trainable params: 4,550
_________________________________________________________________
Decoder
Model: "model_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 2275)              1440075   
                                                                 
 batch_normalization_11 (Bat  (None, 2275)             9100      
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2275)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              2876864   
                                                                 
=================================================================
Total params: 4,328,567
Trainable params: 4,322,753
Non-trainable params: 5,814
_________________________________________________________________
['1.8custom_n_b', 'mse', 64, 200, 0.0005, 0.5, 632, 0.00010374555859016255, 0.00010620507237035781, 0.0012073085987024204, 0.0009501185275184583, 0.005819589830935001, 0.010305581614375114, 0.9931118525875603, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats2_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_12 (Dense)            (None, 2401)              3037265   
                                                                 
 batch_normalization_12 (Bat  (None, 2401)             9604      
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2401)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1518064   
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 2401)              1519833   
                                                                 
 batch_normalization_14 (Bat  (None, 2401)             9604      
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2401)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3036128   
                                                                 
=================================================================
Total params: 9,133,026
Trainable params: 9,122,158
Non-trainable params: 10,868
_________________________________________________________________
Epoch 1/200
1493/1493 - 52s - loss: 0.0093 - val_loss: 0.0041 - 52s/epoch - 35ms/step
Epoch 2/200
1493/1493 - 51s - loss: 0.0029 - val_loss: 0.0028 - 51s/epoch - 34ms/step
Epoch 3/200
1493/1493 - 51s - loss: 0.0021 - val_loss: 0.0016 - 51s/epoch - 34ms/step
Epoch 4/200
1493/1493 - 51s - loss: 0.0017 - val_loss: 0.0024 - 51s/epoch - 34ms/step
Epoch 5/200
1493/1493 - 51s - loss: 0.0015 - val_loss: 0.0015 - 51s/epoch - 34ms/step
Epoch 6/200
1493/1493 - 51s - loss: 0.0014 - val_loss: 0.0015 - 51s/epoch - 34ms/step
Epoch 7/200
1493/1493 - 51s - loss: 0.0013 - val_loss: 0.0012 - 51s/epoch - 34ms/step
Epoch 8/200
1493/1493 - 51s - loss: 0.0012 - val_loss: 0.0012 - 51s/epoch - 34ms/step
Epoch 9/200
1493/1493 - 51s - loss: 0.0011 - val_loss: 0.0011 - 51s/epoch - 34ms/step
Epoch 10/200
1493/1493 - 51s - loss: 0.0010 - val_loss: 9.8553e-04 - 51s/epoch - 34ms/step
Epoch 11/200
1493/1493 - 51s - loss: 9.3569e-04 - val_loss: 7.3660e-04 - 51s/epoch - 34ms/step
Epoch 12/200
1493/1493 - 51s - loss: 7.8611e-04 - val_loss: 9.5381e-04 - 51s/epoch - 34ms/step
Epoch 13/200
1493/1493 - 51s - loss: 7.7955e-04 - val_loss: 0.0024 - 51s/epoch - 34ms/step
Epoch 14/200
1493/1493 - 51s - loss: 9.2166e-04 - val_loss: 5.9764e-04 - 51s/epoch - 34ms/step
Epoch 15/200
1493/1493 - 51s - loss: 6.9255e-04 - val_loss: 9.4257e-04 - 51s/epoch - 34ms/step
Epoch 16/200
1493/1493 - 51s - loss: 6.4038e-04 - val_loss: 5.5092e-04 - 51s/epoch - 34ms/step
Epoch 17/200
1493/1493 - 51s - loss: 5.7945e-04 - val_loss: 0.0010 - 51s/epoch - 34ms/step
Epoch 18/200
1493/1493 - 51s - loss: 6.2260e-04 - val_loss: 8.1231e-04 - 51s/epoch - 34ms/step
Epoch 19/200
1493/1493 - 51s - loss: 5.5291e-04 - val_loss: 5.8762e-04 - 51s/epoch - 34ms/step
Epoch 20/200
1493/1493 - 51s - loss: 5.0891e-04 - val_loss: 5.0924e-04 - 51s/epoch - 34ms/step
Epoch 21/200
1493/1493 - 51s - loss: 4.7124e-04 - val_loss: 5.2250e-04 - 51s/epoch - 34ms/step
Epoch 22/200
1493/1493 - 51s - loss: 4.4762e-04 - val_loss: 4.8581e-04 - 51s/epoch - 34ms/step
Epoch 23/200
1493/1493 - 51s - loss: 4.4257e-04 - val_loss: 8.4196e-04 - 51s/epoch - 34ms/step
Epoch 24/200
1493/1493 - 51s - loss: 4.3531e-04 - val_loss: 3.6298e-04 - 51s/epoch - 34ms/step
Epoch 25/200
1493/1493 - 51s - loss: 3.9075e-04 - val_loss: 3.4677e-04 - 51s/epoch - 34ms/step
Epoch 26/200
1493/1493 - 51s - loss: 3.7409e-04 - val_loss: 4.4277e-04 - 51s/epoch - 34ms/step
Epoch 27/200
1493/1493 - 51s - loss: 3.5809e-04 - val_loss: 3.5384e-04 - 51s/epoch - 34ms/step
Epoch 28/200
1493/1493 - 51s - loss: 3.4101e-04 - val_loss: 8.2244e-04 - 51s/epoch - 34ms/step
Epoch 29/200
1493/1493 - 51s - loss: 3.3944e-04 - val_loss: 4.1606e-04 - 51s/epoch - 34ms/step
Epoch 30/200
1493/1493 - 51s - loss: 3.3228e-04 - val_loss: 4.0153e-04 - 51s/epoch - 34ms/step
Epoch 31/200
1493/1493 - 51s - loss: 3.1167e-04 - val_loss: 4.6963e-04 - 51s/epoch - 34ms/step
Epoch 32/200
1493/1493 - 51s - loss: 3.4367e-04 - val_loss: 3.7563e-04 - 51s/epoch - 34ms/step
Epoch 33/200
1493/1493 - 51s - loss: 3.1200e-04 - val_loss: 2.9833e-04 - 51s/epoch - 34ms/step
Epoch 34/200
1493/1493 - 51s - loss: 2.9016e-04 - val_loss: 2.8451e-04 - 51s/epoch - 34ms/step
Epoch 35/200
1493/1493 - 51s - loss: 2.8340e-04 - val_loss: 0.0011 - 51s/epoch - 34ms/step
Epoch 36/200
1493/1493 - 51s - loss: 3.4974e-04 - val_loss: 3.4912e-04 - 51s/epoch - 34ms/step
Epoch 37/200
1493/1493 - 51s - loss: 2.7692e-04 - val_loss: 2.9454e-04 - 51s/epoch - 34ms/step
Epoch 38/200
1493/1493 - 51s - loss: 2.7974e-04 - val_loss: 2.7103e-04 - 51s/epoch - 34ms/step
Epoch 39/200
1493/1493 - 51s - loss: 2.6187e-04 - val_loss: 2.5053e-04 - 51s/epoch - 34ms/step
Epoch 40/200
1493/1493 - 51s - loss: 2.5156e-04 - val_loss: 3.9446e-04 - 51s/epoch - 34ms/step
Epoch 41/200
1493/1493 - 51s - loss: 2.5247e-04 - val_loss: 2.2942e-04 - 51s/epoch - 34ms/step
Epoch 42/200
1493/1493 - 51s - loss: 2.4608e-04 - val_loss: 2.4211e-04 - 51s/epoch - 34ms/step
Epoch 43/200
1493/1493 - 51s - loss: 2.4530e-04 - val_loss: 5.9722e-04 - 51s/epoch - 34ms/step
Epoch 44/200
1493/1493 - 51s - loss: 2.6979e-04 - val_loss: 4.3245e-04 - 51s/epoch - 34ms/step
Epoch 45/200
1493/1493 - 51s - loss: 2.5243e-04 - val_loss: 2.0191e-04 - 51s/epoch - 34ms/step
Epoch 46/200
1493/1493 - 51s - loss: 2.2847e-04 - val_loss: 2.1975e-04 - 51s/epoch - 34ms/step
Epoch 47/200
1493/1493 - 51s - loss: 2.2589e-04 - val_loss: 2.6771e-04 - 51s/epoch - 34ms/step
Epoch 48/200
1493/1493 - 51s - loss: 2.2833e-04 - val_loss: 4.1119e-04 - 51s/epoch - 34ms/step
Epoch 49/200
1493/1493 - 51s - loss: 2.2671e-04 - val_loss: 9.1968e-04 - 51s/epoch - 34ms/step
Epoch 50/200
1493/1493 - 51s - loss: 2.9907e-04 - val_loss: 9.2109e-04 - 51s/epoch - 34ms/step
Epoch 51/200
1493/1493 - 51s - loss: 2.5203e-04 - val_loss: 2.2152e-04 - 51s/epoch - 34ms/step
Epoch 52/200
1493/1493 - 51s - loss: 2.1015e-04 - val_loss: 2.0918e-04 - 51s/epoch - 34ms/step
Epoch 53/200
1493/1493 - 51s - loss: 2.1041e-04 - val_loss: 2.0771e-04 - 51s/epoch - 34ms/step
Epoch 54/200
1493/1493 - 51s - loss: 2.1067e-04 - val_loss: 2.3906e-04 - 51s/epoch - 34ms/step
Epoch 55/200
1493/1493 - 51s - loss: 1.9975e-04 - val_loss: 1.9914e-04 - 51s/epoch - 34ms/step
Epoch 56/200
1493/1493 - 51s - loss: 2.0009e-04 - val_loss: 2.4345e-04 - 51s/epoch - 34ms/step
Epoch 57/200
1493/1493 - 51s - loss: 1.9548e-04 - val_loss: 2.0575e-04 - 51s/epoch - 34ms/step
Epoch 58/200
1493/1493 - 51s - loss: 1.9496e-04 - val_loss: 1.9006e-04 - 51s/epoch - 34ms/step
Epoch 59/200
1493/1493 - 51s - loss: 1.9071e-04 - val_loss: 2.0425e-04 - 51s/epoch - 34ms/step
Epoch 60/200
1493/1493 - 51s - loss: 1.9090e-04 - val_loss: 4.9753e-04 - 51s/epoch - 34ms/step
Epoch 61/200
1493/1493 - 51s - loss: 2.7563e-04 - val_loss: 1.7567e-04 - 51s/epoch - 34ms/step
Epoch 62/200
1493/1493 - 51s - loss: 1.8974e-04 - val_loss: 2.0343e-04 - 51s/epoch - 34ms/step
Epoch 63/200
1493/1493 - 51s - loss: 1.8491e-04 - val_loss: 2.8161e-04 - 51s/epoch - 34ms/step
Epoch 64/200
1493/1493 - 51s - loss: 1.8929e-04 - val_loss: 0.0023 - 51s/epoch - 34ms/step
Epoch 65/200
1493/1493 - 51s - loss: 3.3586e-04 - val_loss: 2.7145e-04 - 51s/epoch - 34ms/step
Epoch 66/200
1493/1493 - 51s - loss: 2.0194e-04 - val_loss: 2.3428e-04 - 51s/epoch - 34ms/step
Epoch 67/200
1493/1493 - 51s - loss: 1.8579e-04 - val_loss: 1.6368e-04 - 51s/epoch - 34ms/step
Epoch 68/200
1493/1493 - 51s - loss: 1.8013e-04 - val_loss: 3.1248e-04 - 51s/epoch - 34ms/step
Epoch 69/200
1493/1493 - 51s - loss: 1.9616e-04 - val_loss: 2.0719e-04 - 51s/epoch - 34ms/step
Epoch 70/200
1493/1493 - 51s - loss: 1.7740e-04 - val_loss: 2.1927e-04 - 51s/epoch - 34ms/step
Epoch 71/200
1493/1493 - 51s - loss: 1.7644e-04 - val_loss: 1.6543e-04 - 51s/epoch - 34ms/step
Epoch 72/200
1493/1493 - 51s - loss: 1.7556e-04 - val_loss: 2.1499e-04 - 51s/epoch - 34ms/step
Epoch 73/200
1493/1493 - 51s - loss: 1.7526e-04 - val_loss: 1.8783e-04 - 51s/epoch - 34ms/step
Epoch 74/200
1493/1493 - 51s - loss: 1.7457e-04 - val_loss: 1.7751e-04 - 51s/epoch - 34ms/step
Epoch 75/200
1493/1493 - 51s - loss: 1.6598e-04 - val_loss: 1.6557e-04 - 51s/epoch - 34ms/step
Epoch 76/200
1493/1493 - 51s - loss: 1.6482e-04 - val_loss: 1.6027e-04 - 51s/epoch - 34ms/step
Epoch 77/200
1493/1493 - 51s - loss: 1.6046e-04 - val_loss: 2.8846e-04 - 51s/epoch - 34ms/step
Epoch 78/200
1493/1493 - 51s - loss: 1.6659e-04 - val_loss: 2.1000e-04 - 51s/epoch - 34ms/step
Epoch 79/200
1493/1493 - 51s - loss: 1.6519e-04 - val_loss: 3.8283e-04 - 51s/epoch - 34ms/step
Epoch 80/200
1493/1493 - 51s - loss: 1.9865e-04 - val_loss: 1.6346e-04 - 51s/epoch - 34ms/step
Epoch 81/200
1493/1493 - 51s - loss: 1.6075e-04 - val_loss: 2.9153e-04 - 51s/epoch - 34ms/step
Epoch 82/200
1493/1493 - 51s - loss: 1.6007e-04 - val_loss: 1.6369e-04 - 51s/epoch - 34ms/step
Epoch 83/200
1493/1493 - 51s - loss: 1.5499e-04 - val_loss: 1.7516e-04 - 51s/epoch - 34ms/step
Epoch 84/200
1493/1493 - 51s - loss: 1.5459e-04 - val_loss: 1.7713e-04 - 51s/epoch - 34ms/step
Epoch 85/200
1493/1493 - 51s - loss: 1.5355e-04 - val_loss: 1.9857e-04 - 51s/epoch - 34ms/step
Epoch 86/200
1493/1493 - 51s - loss: 1.5663e-04 - val_loss: 1.6831e-04 - 51s/epoch - 34ms/step
Epoch 87/200
1493/1493 - 51s - loss: 1.5095e-04 - val_loss: 1.5505e-04 - 51s/epoch - 34ms/step
Epoch 88/200
1493/1493 - 51s - loss: 1.5056e-04 - val_loss: 2.4313e-04 - 51s/epoch - 34ms/step
Epoch 89/200
1493/1493 - 51s - loss: 1.5664e-04 - val_loss: 1.6289e-04 - 51s/epoch - 34ms/step
Epoch 90/200
1493/1493 - 51s - loss: 1.4737e-04 - val_loss: 1.8057e-04 - 51s/epoch - 34ms/step
Epoch 91/200
1493/1493 - 51s - loss: 1.4671e-04 - val_loss: 1.6320e-04 - 51s/epoch - 34ms/step
Epoch 92/200
1493/1493 - 51s - loss: 1.4858e-04 - val_loss: 6.2359e-04 - 51s/epoch - 34ms/step
Epoch 93/200
1493/1493 - 51s - loss: 2.0767e-04 - val_loss: 9.8991e-04 - 51s/epoch - 34ms/step
Epoch 94/200
1493/1493 - 51s - loss: 2.1195e-04 - val_loss: 1.2718e-04 - 51s/epoch - 34ms/step
Epoch 95/200
1493/1493 - 51s - loss: 1.5087e-04 - val_loss: 1.4833e-04 - 51s/epoch - 34ms/step
Epoch 96/200
1493/1493 - 51s - loss: 1.4838e-04 - val_loss: 1.6705e-04 - 51s/epoch - 34ms/step
Epoch 97/200
1493/1493 - 51s - loss: 1.4525e-04 - val_loss: 1.4218e-04 - 51s/epoch - 34ms/step
Epoch 98/200
1493/1493 - 51s - loss: 1.5000e-04 - val_loss: 7.5887e-04 - 51s/epoch - 34ms/step
Epoch 99/200
1493/1493 - 51s - loss: 2.3332e-04 - val_loss: 1.3315e-04 - 51s/epoch - 34ms/step
Epoch 100/200
1493/1493 - 51s - loss: 1.6478e-04 - val_loss: 1.4204e-04 - 51s/epoch - 34ms/step
Epoch 101/200
1493/1493 - 51s - loss: 1.4602e-04 - val_loss: 2.1474e-04 - 51s/epoch - 34ms/step
Epoch 102/200
1493/1493 - 51s - loss: 1.4961e-04 - val_loss: 1.8867e-04 - 51s/epoch - 34ms/step
Epoch 103/200
1493/1493 - 51s - loss: 1.4697e-04 - val_loss: 1.5187e-04 - 51s/epoch - 34ms/step
Epoch 104/200
1493/1493 - 51s - loss: 1.4036e-04 - val_loss: 2.6814e-04 - 51s/epoch - 34ms/step
Epoch 105/200
1493/1493 - 51s - loss: 1.4873e-04 - val_loss: 1.6071e-04 - 51s/epoch - 34ms/step
Epoch 106/200
1493/1493 - 51s - loss: 1.3819e-04 - val_loss: 1.6453e-04 - 51s/epoch - 34ms/step
Epoch 107/200
1493/1493 - 51s - loss: 1.3662e-04 - val_loss: 1.5476e-04 - 51s/epoch - 34ms/step
Epoch 108/200
1493/1493 - 51s - loss: 1.3914e-04 - val_loss: 1.4610e-04 - 51s/epoch - 34ms/step
Epoch 109/200
1493/1493 - 51s - loss: 1.3596e-04 - val_loss: 1.5423e-04 - 51s/epoch - 34ms/step
Epoch 110/200
1493/1493 - 51s - loss: 1.3341e-04 - val_loss: 1.4258e-04 - 51s/epoch - 34ms/step
Epoch 111/200
1493/1493 - 52s - loss: 1.3286e-04 - val_loss: 1.7394e-04 - 52s/epoch - 35ms/step
Epoch 112/200
1493/1493 - 51s - loss: 1.3488e-04 - val_loss: 1.2930e-04 - 51s/epoch - 34ms/step
Epoch 113/200
1493/1493 - 51s - loss: 1.3259e-04 - val_loss: 4.9709e-04 - 51s/epoch - 34ms/step
Epoch 114/200
1493/1493 - 51s - loss: 1.8033e-04 - val_loss: 1.2470e-04 - 51s/epoch - 34ms/step
Epoch 115/200
1493/1493 - 52s - loss: 1.3475e-04 - val_loss: 1.7632e-04 - 52s/epoch - 35ms/step
Epoch 116/200
1493/1493 - 51s - loss: 1.3294e-04 - val_loss: 1.6474e-04 - 51s/epoch - 34ms/step
Epoch 117/200
1493/1493 - 52s - loss: 1.3051e-04 - val_loss: 2.8556e-04 - 52s/epoch - 35ms/step
Epoch 118/200
1493/1493 - 52s - loss: 1.4669e-04 - val_loss: 1.3419e-04 - 52s/epoch - 35ms/step
Epoch 119/200
1493/1493 - 51s - loss: 1.2982e-04 - val_loss: 1.2660e-04 - 51s/epoch - 34ms/step
Epoch 120/200
1493/1493 - 51s - loss: 1.2878e-04 - val_loss: 1.3641e-04 - 51s/epoch - 34ms/step
Epoch 121/200
1493/1493 - 51s - loss: 1.2814e-04 - val_loss: 2.1550e-04 - 51s/epoch - 34ms/step
Epoch 122/200
1493/1493 - 52s - loss: 1.3068e-04 - val_loss: 1.1787e-04 - 52s/epoch - 35ms/step
Epoch 123/200
1493/1493 - 52s - loss: 1.2544e-04 - val_loss: 1.3198e-04 - 52s/epoch - 35ms/step
Epoch 124/200
1493/1493 - 52s - loss: 1.2650e-04 - val_loss: 4.1797e-04 - 52s/epoch - 35ms/step
Epoch 125/200
1493/1493 - 52s - loss: 1.5632e-04 - val_loss: 1.7024e-04 - 52s/epoch - 35ms/step
Epoch 126/200
1493/1493 - 52s - loss: 1.3203e-04 - val_loss: 2.1671e-04 - 52s/epoch - 35ms/step
Epoch 127/200
1493/1493 - 52s - loss: 1.3260e-04 - val_loss: 5.7941e-04 - 52s/epoch - 35ms/step
Epoch 128/200
1493/1493 - 52s - loss: 1.7211e-04 - val_loss: 1.1467e-04 - 52s/epoch - 35ms/step
Epoch 129/200
1493/1493 - 51s - loss: 1.2773e-04 - val_loss: 1.2277e-04 - 51s/epoch - 34ms/step
Epoch 130/200
1493/1493 - 51s - loss: 1.2524e-04 - val_loss: 1.2025e-04 - 51s/epoch - 34ms/step
Epoch 131/200
1493/1493 - 51s - loss: 1.2701e-04 - val_loss: 3.7195e-04 - 51s/epoch - 34ms/step
Epoch 132/200
1493/1493 - 51s - loss: 1.5850e-04 - val_loss: 1.1619e-04 - 51s/epoch - 34ms/step
Epoch 133/200
1493/1493 - 51s - loss: 1.2500e-04 - val_loss: 1.1950e-04 - 51s/epoch - 34ms/step
Epoch 134/200
1493/1493 - 52s - loss: 1.2224e-04 - val_loss: 1.1717e-04 - 52s/epoch - 35ms/step
Epoch 135/200
1493/1493 - 51s - loss: 1.2111e-04 - val_loss: 1.1184e-04 - 51s/epoch - 34ms/step
Epoch 136/200
1493/1493 - 51s - loss: 1.2253e-04 - val_loss: 2.7295e-04 - 51s/epoch - 34ms/step
Epoch 137/200
1493/1493 - 51s - loss: 1.6161e-04 - val_loss: 2.4381e-04 - 51s/epoch - 34ms/step
Epoch 138/200
1493/1493 - 51s - loss: 1.4462e-04 - val_loss: 1.8617e-04 - 51s/epoch - 34ms/step
Epoch 139/200
1493/1493 - 51s - loss: 1.3653e-04 - val_loss: 1.8663e-04 - 51s/epoch - 34ms/step
Epoch 140/200
1493/1493 - 51s - loss: 1.3163e-04 - val_loss: 1.1101e-04 - 51s/epoch - 34ms/step
Epoch 141/200
1493/1493 - 52s - loss: 1.2074e-04 - val_loss: 1.5728e-04 - 52s/epoch - 35ms/step
Epoch 142/200
1493/1493 - 51s - loss: 1.2459e-04 - val_loss: 1.2220e-04 - 51s/epoch - 34ms/step
Epoch 143/200
1493/1493 - 51s - loss: 1.1788e-04 - val_loss: 1.4415e-04 - 51s/epoch - 34ms/step
Epoch 144/200
1493/1493 - 51s - loss: 1.2158e-04 - val_loss: 1.1567e-04 - 51s/epoch - 34ms/step
Epoch 145/200
1493/1493 - 51s - loss: 1.1715e-04 - val_loss: 1.1759e-04 - 51s/epoch - 34ms/step
Epoch 146/200
1493/1493 - 51s - loss: 1.1688e-04 - val_loss: 1.2627e-04 - 51s/epoch - 34ms/step
Epoch 147/200
1493/1493 - 51s - loss: 1.1511e-04 - val_loss: 1.2939e-04 - 51s/epoch - 34ms/step
Epoch 148/200
1493/1493 - 51s - loss: 1.1497e-04 - val_loss: 2.0221e-04 - 51s/epoch - 34ms/step
Epoch 149/200
1493/1493 - 52s - loss: 1.1463e-04 - val_loss: 1.1821e-04 - 52s/epoch - 35ms/step
Epoch 150/200
1493/1493 - 51s - loss: 1.1347e-04 - val_loss: 1.0448e-04 - 51s/epoch - 34ms/step
Epoch 151/200
1493/1493 - 52s - loss: 1.1438e-04 - val_loss: 1.5109e-04 - 52s/epoch - 35ms/step
Epoch 152/200
1493/1493 - 52s - loss: 1.1759e-04 - val_loss: 1.4577e-04 - 52s/epoch - 34ms/step
Epoch 153/200
1493/1493 - 51s - loss: 1.1512e-04 - val_loss: 1.2022e-04 - 51s/epoch - 34ms/step
Epoch 154/200
1493/1493 - 51s - loss: 1.1327e-04 - val_loss: 2.0045e-04 - 51s/epoch - 34ms/step
Epoch 155/200
1493/1493 - 52s - loss: 1.3106e-04 - val_loss: 1.0978e-04 - 52s/epoch - 35ms/step
Epoch 156/200
1493/1493 - 52s - loss: 1.1437e-04 - val_loss: 1.1691e-04 - 52s/epoch - 35ms/step
Epoch 157/200
1493/1493 - 51s - loss: 1.1111e-04 - val_loss: 1.1884e-04 - 51s/epoch - 34ms/step
Epoch 158/200
1493/1493 - 52s - loss: 1.1196e-04 - val_loss: 1.2331e-04 - 52s/epoch - 34ms/step
Epoch 159/200
1493/1493 - 51s - loss: 1.1078e-04 - val_loss: 1.1593e-04 - 51s/epoch - 34ms/step
Epoch 160/200
1493/1493 - 51s - loss: 1.1145e-04 - val_loss: 1.6651e-04 - 51s/epoch - 34ms/step
Epoch 161/200
1493/1493 - 51s - loss: 1.2919e-04 - val_loss: 1.0558e-04 - 51s/epoch - 34ms/step
Epoch 162/200
1493/1493 - 51s - loss: 1.1104e-04 - val_loss: 1.0536e-04 - 51s/epoch - 34ms/step
Epoch 163/200
1493/1493 - 51s - loss: 1.1183e-04 - val_loss: 2.8724e-04 - 51s/epoch - 34ms/step
Epoch 164/200
1493/1493 - 51s - loss: 1.3121e-04 - val_loss: 1.1661e-04 - 51s/epoch - 34ms/step
Epoch 165/200
1493/1493 - 51s - loss: 1.2311e-04 - val_loss: 1.1240e-04 - 51s/epoch - 34ms/step
Epoch 166/200
1493/1493 - 52s - loss: 1.1125e-04 - val_loss: 1.1486e-04 - 52s/epoch - 35ms/step
Epoch 167/200
1493/1493 - 52s - loss: 1.0966e-04 - val_loss: 9.8396e-05 - 52s/epoch - 35ms/step
Epoch 168/200
1493/1493 - 51s - loss: 1.0893e-04 - val_loss: 1.0363e-04 - 51s/epoch - 34ms/step
Epoch 169/200
1493/1493 - 51s - loss: 1.0895e-04 - val_loss: 1.3225e-04 - 51s/epoch - 34ms/step
Epoch 170/200
1493/1493 - 51s - loss: 1.0928e-04 - val_loss: 2.2829e-04 - 51s/epoch - 34ms/step
Epoch 171/200
1493/1493 - 51s - loss: 1.3660e-04 - val_loss: 4.1532e-04 - 51s/epoch - 34ms/step
Epoch 172/200
1493/1493 - 51s - loss: 1.4022e-04 - val_loss: 1.7988e-04 - 51s/epoch - 34ms/step
Epoch 173/200
1493/1493 - 51s - loss: 1.1600e-04 - val_loss: 1.0484e-04 - 51s/epoch - 34ms/step
Epoch 174/200
1493/1493 - 51s - loss: 1.0786e-04 - val_loss: 1.0127e-04 - 51s/epoch - 34ms/step
Epoch 175/200
1493/1493 - 52s - loss: 1.0921e-04 - val_loss: 1.2729e-04 - 52s/epoch - 35ms/step
Epoch 176/200
1493/1493 - 51s - loss: 1.0929e-04 - val_loss: 1.2209e-04 - 51s/epoch - 34ms/step
Epoch 177/200
1493/1493 - 51s - loss: 1.1075e-04 - val_loss: 9.9597e-05 - 51s/epoch - 34ms/step
Epoch 178/200
1493/1493 - 52s - loss: 1.0644e-04 - val_loss: 1.1222e-04 - 52s/epoch - 35ms/step
Epoch 179/200
1493/1493 - 52s - loss: 1.0536e-04 - val_loss: 1.1732e-04 - 52s/epoch - 35ms/step
Epoch 180/200
1493/1493 - 51s - loss: 1.0564e-04 - val_loss: 1.1822e-04 - 51s/epoch - 34ms/step
Epoch 181/200
1493/1493 - 51s - loss: 1.0730e-04 - val_loss: 1.0580e-04 - 51s/epoch - 34ms/step
Epoch 182/200
1493/1493 - 51s - loss: 1.0557e-04 - val_loss: 1.4538e-04 - 51s/epoch - 34ms/step
Epoch 183/200
1493/1493 - 52s - loss: 1.0567e-04 - val_loss: 1.3333e-04 - 52s/epoch - 35ms/step
Epoch 184/200
1493/1493 - 52s - loss: 1.0506e-04 - val_loss: 9.8779e-05 - 52s/epoch - 35ms/step
Epoch 185/200
1493/1493 - 52s - loss: 1.0200e-04 - val_loss: 1.4169e-04 - 52s/epoch - 34ms/step
Epoch 186/200
1493/1493 - 51s - loss: 1.0378e-04 - val_loss: 1.1069e-04 - 51s/epoch - 34ms/step
Epoch 187/200
1493/1493 - 51s - loss: 1.0206e-04 - val_loss: 1.1417e-04 - 51s/epoch - 34ms/step
Epoch 188/200
1493/1493 - 52s - loss: 1.0450e-04 - val_loss: 1.7116e-04 - 52s/epoch - 35ms/step
Epoch 189/200
1493/1493 - 52s - loss: 1.1460e-04 - val_loss: 1.2476e-04 - 52s/epoch - 35ms/step
Epoch 190/200
1493/1493 - 51s - loss: 1.0490e-04 - val_loss: 1.4299e-04 - 51s/epoch - 34ms/step
Epoch 191/200
1493/1493 - 51s - loss: 1.1352e-04 - val_loss: 1.4425e-04 - 51s/epoch - 34ms/step
Epoch 192/200
1493/1493 - 51s - loss: 1.0577e-04 - val_loss: 1.3003e-04 - 51s/epoch - 34ms/step
Epoch 193/200
1493/1493 - 51s - loss: 1.0745e-04 - val_loss: 2.3718e-04 - 51s/epoch - 34ms/step
Epoch 194/200
1493/1493 - 51s - loss: 1.3986e-04 - val_loss: 4.6456e-04 - 51s/epoch - 34ms/step
Epoch 195/200
1493/1493 - 51s - loss: 1.5089e-04 - val_loss: 2.4401e-04 - 51s/epoch - 34ms/step
Epoch 196/200
1493/1493 - 52s - loss: 1.3393e-04 - val_loss: 9.1311e-05 - 52s/epoch - 34ms/step
Epoch 197/200
1493/1493 - 52s - loss: 1.0803e-04 - val_loss: 1.8860e-04 - 52s/epoch - 35ms/step
Epoch 198/200
1493/1493 - 51s - loss: 1.1512e-04 - val_loss: 1.2438e-04 - 51s/epoch - 34ms/step
Epoch 199/200
1493/1493 - 51s - loss: 1.0609e-04 - val_loss: 9.3467e-05 - 51s/epoch - 34ms/step
Epoch 200/200
1493/1493 - 51s - loss: 1.0270e-04 - val_loss: 1.0376e-04 - 51s/epoch - 34ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00010376232967246324
  1/332 [..............................] - ETA: 51s 10/332 [..............................] - ETA: 1s  20/332 [>.............................] - ETA: 1s 30/332 [=>............................] - ETA: 1s 41/332 [==>...........................] - ETA: 1s 51/332 [===>..........................] - ETA: 1s 61/332 [====>.........................] - ETA: 1s 70/332 [=====>........................] - ETA: 1s 79/332 [======>.......................] - ETA: 1s 88/332 [======>.......................] - ETA: 1s 97/332 [=======>......................] - ETA: 1s106/332 [========>.....................] - ETA: 1s115/332 [=========>....................] - ETA: 1s124/332 [==========>...................] - ETA: 1s133/332 [===========>..................] - ETA: 1s142/332 [===========>..................] - ETA: 1s151/332 [============>.................] - ETA: 1s160/332 [=============>................] - ETA: 0s169/332 [==============>...............] - ETA: 0s178/332 [===============>..............] - ETA: 0s187/332 [===============>..............] - ETA: 0s196/332 [================>.............] - ETA: 0s205/332 [=================>............] - ETA: 0s214/332 [==================>...........] - ETA: 0s223/332 [===================>..........] - ETA: 0s232/332 [===================>..........] - ETA: 0s241/332 [====================>.........] - ETA: 0s250/332 [=====================>........] - ETA: 0s259/332 [======================>.......] - ETA: 0s268/332 [=======================>......] - ETA: 0s277/332 [========================>.....] - ETA: 0s286/332 [========================>.....] - ETA: 0s295/332 [=========================>....] - ETA: 0s304/332 [==========================>...] - ETA: 0s313/332 [===========================>..] - ETA: 0s322/332 [============================>.] - ETA: 0s331/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 6ms/step
correlation 0.0011807432994274567
cosine 0.0009309539492068271
MAE: 0.005702279
RMSE: 0.010186374
r2: 0.993270080518188
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 2401)              3037265   
                                                                 
 batch_normalization_12 (Bat  (None, 2401)             9604      
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2401)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1518064   
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 2401)              1519833   
                                                                 
 batch_normalization_14 (Bat  (None, 2401)             9604      
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2401)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3036128   
                                                                 
=================================================================
Total params: 9,133,026
Trainable params: 9,122,158
Non-trainable params: 10,868
_________________________________________________________________
Encoder
Model: "model_13"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_14 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 2401)              3037265   
                                                                 
 batch_normalization_12 (Bat  (None, 2401)             9604      
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2401)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1518064   
                                                                 
=================================================================
Total params: 4,564,933
Trainable params: 4,560,131
Non-trainable params: 4,802
_________________________________________________________________
Decoder
Model: "model_14"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_15 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 2401)              1519833   
                                                                 
 batch_normalization_14 (Bat  (None, 2401)             9604      
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2401)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3036128   
                                                                 
=================================================================
Total params: 4,568,093
Trainable params: 4,562,027
Non-trainable params: 6,066
_________________________________________________________________
['1.9custom_n_b', 'mse', 64, 200, 0.0005, 0.5, 632, 0.00010269886115565896, 0.00010376232967246324, 0.0011807432994274567, 0.0009309539492068271, 0.005702279042452574, 0.010186374187469482, 0.993270080518188, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats2_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
1493/1493 - 52s - loss: 0.0093 - val_loss: 0.0042 - 52s/epoch - 35ms/step
Epoch 2/200
1493/1493 - 51s - loss: 0.0030 - val_loss: 0.0041 - 51s/epoch - 34ms/step
Epoch 3/200
1493/1493 - 51s - loss: 0.0022 - val_loss: 0.0016 - 51s/epoch - 34ms/step
Epoch 4/200
1493/1493 - 51s - loss: 0.0017 - val_loss: 0.0019 - 51s/epoch - 34ms/step
Epoch 5/200
1493/1493 - 51s - loss: 0.0016 - val_loss: 0.0014 - 51s/epoch - 34ms/step
Epoch 6/200
1493/1493 - 51s - loss: 0.0014 - val_loss: 0.0013 - 51s/epoch - 34ms/step
Epoch 7/200
1493/1493 - 51s - loss: 0.0013 - val_loss: 0.0010 - 51s/epoch - 34ms/step
Epoch 8/200
1493/1493 - 51s - loss: 0.0012 - val_loss: 0.0012 - 51s/epoch - 34ms/step
Epoch 9/200
1493/1493 - 51s - loss: 0.0011 - val_loss: 0.0025 - 51s/epoch - 34ms/step
Epoch 10/200
1493/1493 - 51s - loss: 0.0013 - val_loss: 8.2638e-04 - 51s/epoch - 34ms/step
Epoch 11/200
1493/1493 - 51s - loss: 9.1915e-04 - val_loss: 9.2517e-04 - 51s/epoch - 34ms/step
Epoch 12/200
1493/1493 - 51s - loss: 8.0298e-04 - val_loss: 9.6806e-04 - 51s/epoch - 34ms/step
Epoch 13/200
1493/1493 - 51s - loss: 7.6099e-04 - val_loss: 0.0025 - 51s/epoch - 34ms/step
Epoch 14/200
1493/1493 - 51s - loss: 8.7543e-04 - val_loss: 6.5891e-04 - 51s/epoch - 34ms/step
Epoch 15/200
1493/1493 - 51s - loss: 6.9745e-04 - val_loss: 0.0013 - 51s/epoch - 34ms/step
Epoch 16/200
1493/1493 - 51s - loss: 6.5313e-04 - val_loss: 5.4161e-04 - 51s/epoch - 34ms/step
Epoch 17/200
1493/1493 - 51s - loss: 5.6414e-04 - val_loss: 7.6277e-04 - 51s/epoch - 34ms/step
Epoch 18/200
1493/1493 - 51s - loss: 5.4215e-04 - val_loss: 6.8130e-04 - 51s/epoch - 34ms/step
Epoch 19/200
1493/1493 - 51s - loss: 5.4057e-04 - val_loss: 6.6425e-04 - 51s/epoch - 34ms/step
Epoch 20/200
1493/1493 - 51s - loss: 4.8612e-04 - val_loss: 4.9361e-04 - 51s/epoch - 34ms/step
Epoch 21/200
1493/1493 - 51s - loss: 4.5794e-04 - val_loss: 5.3272e-04 - 51s/epoch - 34ms/step
Epoch 22/200
1493/1493 - 51s - loss: 4.3618e-04 - val_loss: 5.4406e-04 - 51s/epoch - 34ms/step
Epoch 23/200
1493/1493 - 51s - loss: 4.3842e-04 - val_loss: 9.5521e-04 - 51s/epoch - 34ms/step
Epoch 24/200
1493/1493 - 51s - loss: 4.4472e-04 - val_loss: 3.5294e-04 - 51s/epoch - 34ms/step
Epoch 25/200
1493/1493 - 51s - loss: 3.8138e-04 - val_loss: 3.8734e-04 - 51s/epoch - 34ms/step
Epoch 26/200
1493/1493 - 51s - loss: 3.6025e-04 - val_loss: 4.5326e-04 - 51s/epoch - 34ms/step
Epoch 27/200
1493/1493 - 51s - loss: 3.4683e-04 - val_loss: 3.7634e-04 - 51s/epoch - 34ms/step
Epoch 28/200
1493/1493 - 51s - loss: 3.3135e-04 - val_loss: 6.7412e-04 - 51s/epoch - 34ms/step
Epoch 29/200
1493/1493 - 51s - loss: 3.3362e-04 - val_loss: 3.5851e-04 - 51s/epoch - 34ms/step
Epoch 30/200
1493/1493 - 51s - loss: 3.2214e-04 - val_loss: 3.3432e-04 - 51s/epoch - 34ms/step
Epoch 31/200
1493/1493 - 51s - loss: 3.0572e-04 - val_loss: 5.7466e-04 - 51s/epoch - 34ms/step
Epoch 32/200
1493/1493 - 51s - loss: 3.3630e-04 - val_loss: 3.8341e-04 - 51s/epoch - 34ms/step
Epoch 33/200
1493/1493 - 51s - loss: 3.0485e-04 - val_loss: 2.8091e-04 - 51s/epoch - 34ms/step
Epoch 34/200
1493/1493 - 51s - loss: 2.8546e-04 - val_loss: 2.7117e-04 - 51s/epoch - 34ms/step
Epoch 35/200
1493/1493 - 51s - loss: 2.7389e-04 - val_loss: 7.9082e-04 - 51s/epoch - 34ms/step
Epoch 36/200
1493/1493 - 51s - loss: 3.1304e-04 - val_loss: 3.1637e-04 - 51s/epoch - 34ms/step
Epoch 37/200
1493/1493 - 51s - loss: 2.7146e-04 - val_loss: 5.4178e-04 - 51s/epoch - 34ms/step
Epoch 38/200
1493/1493 - 51s - loss: 3.0201e-04 - val_loss: 2.3648e-04 - 51s/epoch - 34ms/step
Epoch 39/200
1493/1493 - 51s - loss: 2.5462e-04 - val_loss: 2.3441e-04 - 51s/epoch - 34ms/step
Epoch 40/200
1493/1493 - 51s - loss: 2.4755e-04 - val_loss: 3.3428e-04 - 51s/epoch - 34ms/step
Epoch 41/200
1493/1493 - 51s - loss: 2.4845e-04 - val_loss: 2.3176e-04 - 51s/epoch - 34ms/step
Epoch 42/200
1493/1493 - 51s - loss: 2.4077e-04 - val_loss: 2.3121e-04 - 51s/epoch - 34ms/step
Epoch 43/200
1493/1493 - 51s - loss: 2.4286e-04 - val_loss: 3.6862e-04 - 51s/epoch - 34ms/step
Epoch 44/200
1493/1493 - 51s - loss: 2.4347e-04 - val_loss: 6.7243e-04 - 51s/epoch - 34ms/step
Epoch 45/200
1493/1493 - 51s - loss: 2.6981e-04 - val_loss: 1.9582e-04 - 51s/epoch - 34ms/step
Epoch 46/200
1493/1493 - 51s - loss: 2.2971e-04 - val_loss: 2.0064e-04 - 51s/epoch - 34ms/step
Epoch 47/200
1493/1493 - 51s - loss: 2.2497e-04 - val_loss: 2.2518e-04 - 51s/epoch - 34ms/step
Epoch 48/200
1493/1493 - 51s - loss: 2.2007e-04 - val_loss: 2.6849e-04 - 51s/epoch - 34ms/step
Epoch 49/200
1493/1493 - 51s - loss: 2.2342e-04 - val_loss: 0.0011 - 51s/epoch - 34ms/step
Epoch 50/200
1493/1493 - 51s - loss: 3.3386e-04 - val_loss: 9.7224e-04 - 51s/epoch - 34ms/step
Epoch 51/200
1493/1493 - 51s - loss: 2.7179e-04 - val_loss: 1.9526e-04 - 51s/epoch - 34ms/step
Epoch 52/200
1493/1493 - 51s - loss: 2.1260e-04 - val_loss: 2.0505e-04 - 51s/epoch - 34ms/step
Epoch 53/200
1493/1493 - 51s - loss: 2.1196e-04 - val_loss: 1.8834e-04 - 51s/epoch - 34ms/step
Epoch 54/200
1493/1493 - 51s - loss: 2.1193e-04 - val_loss: 2.4539e-04 - 51s/epoch - 34ms/step
Epoch 55/200
1493/1493 - 51s - loss: 2.0256e-04 - val_loss: 1.9553e-04 - 51s/epoch - 34ms/step
Epoch 56/200
1493/1493 - 51s - loss: 2.0047e-04 - val_loss: 2.3329e-04 - 51s/epoch - 34ms/step
Epoch 57/200
1493/1493 - 51s - loss: 1.9362e-04 - val_loss: 2.1640e-04 - 51s/epoch - 34ms/step
Epoch 58/200
1493/1493 - 51s - loss: 1.9396e-04 - val_loss: 1.7248e-04 - 51s/epoch - 34ms/step
Epoch 59/200
1493/1493 - 51s - loss: 1.8870e-04 - val_loss: 1.9769e-04 - 51s/epoch - 34ms/step
Epoch 60/200
1493/1493 - 51s - loss: 1.8815e-04 - val_loss: 3.1533e-04 - 51s/epoch - 34ms/step
Epoch 61/200
1493/1493 - 51s - loss: 2.4200e-04 - val_loss: 1.6917e-04 - 51s/epoch - 34ms/step
Epoch 62/200
1493/1493 - 51s - loss: 1.8802e-04 - val_loss: 2.1142e-04 - 51s/epoch - 34ms/step
Epoch 63/200
1493/1493 - 51s - loss: 1.8071e-04 - val_loss: 2.3794e-04 - 51s/epoch - 34ms/step
Epoch 64/200
1493/1493 - 51s - loss: 1.8323e-04 - val_loss: 0.0010 - 51s/epoch - 34ms/step
Epoch 65/200
1493/1493 - 51s - loss: 2.5418e-04 - val_loss: 0.0012 - 51s/epoch - 34ms/step
Epoch 66/200
1493/1493 - 51s - loss: 2.4730e-04 - val_loss: 2.0734e-04 - 51s/epoch - 34ms/step
Epoch 67/200
1493/1493 - 51s - loss: 1.8198e-04 - val_loss: 1.6642e-04 - 51s/epoch - 34ms/step
Epoch 68/200
1493/1493 - 51s - loss: 1.7900e-04 - val_loss: 5.7890e-04 - 51s/epoch - 34ms/step
Epoch 69/200
1493/1493 - 51s - loss: 2.4760e-04 - val_loss: 2.2805e-04 - 51s/epoch - 34ms/step
Epoch 70/200
1493/1493 - 51s - loss: 1.7921e-04 - val_loss: 2.9459e-04 - 51s/epoch - 34ms/step
Epoch 71/200
1493/1493 - 51s - loss: 1.8706e-04 - val_loss: 1.5868e-04 - 51s/epoch - 34ms/step
Epoch 72/200
1493/1493 - 51s - loss: 1.7574e-04 - val_loss: 2.0376e-04 - 51s/epoch - 34ms/step
Epoch 73/200
1493/1493 - 51s - loss: 1.7570e-04 - val_loss: 1.7757e-04 - 51s/epoch - 34ms/step
Epoch 74/200
1493/1493 - 51s - loss: 1.7130e-04 - val_loss: 1.6873e-04 - 51s/epoch - 34ms/step
Epoch 75/200
1493/1493 - 51s - loss: 1.6487e-04 - val_loss: 1.6528e-04 - 51s/epoch - 34ms/step
Epoch 76/200
1493/1493 - 51s - loss: 1.6309e-04 - val_loss: 1.5192e-04 - 51s/epoch - 34ms/step
Epoch 77/200
1493/1493 - 51s - loss: 1.6628e-04 - val_loss: 4.6038e-04 - 51s/epoch - 34ms/step
Epoch 78/200
1493/1493 - 51s - loss: 1.6791e-04 - val_loss: 2.7943e-04 - 51s/epoch - 34ms/step
Epoch 79/200
1493/1493 - 51s - loss: 1.7120e-04 - val_loss: 2.6208e-04 - 51s/epoch - 34ms/step
Epoch 80/200
1493/1493 - 51s - loss: 1.7361e-04 - val_loss: 1.5601e-04 - 51s/epoch - 34ms/step
Epoch 81/200
1493/1493 - 51s - loss: 1.6190e-04 - val_loss: 2.2652e-04 - 51s/epoch - 34ms/step
Epoch 82/200
1493/1493 - 51s - loss: 1.5615e-04 - val_loss: 1.6709e-04 - 51s/epoch - 34ms/step
Epoch 83/200
1493/1493 - 51s - loss: 1.5304e-04 - val_loss: 1.6492e-04 - 51s/epoch - 34ms/step
Epoch 84/200
1493/1493 - 51s - loss: 1.5299e-04 - val_loss: 1.7511e-04 - 51s/epoch - 34ms/step
Epoch 85/200
1493/1493 - 51s - loss: 1.5617e-04 - val_loss: 1.7425e-04 - 51s/epoch - 34ms/step
Epoch 86/200
1493/1493 - 51s - loss: 1.5282e-04 - val_loss: 1.5055e-04 - 51s/epoch - 34ms/step
Epoch 87/200
1493/1493 - 51s - loss: 1.4696e-04 - val_loss: 1.4678e-04 - 51s/epoch - 34ms/step
Epoch 88/200
1493/1493 - 51s - loss: 1.4580e-04 - val_loss: 3.0026e-04 - 51s/epoch - 34ms/step
Epoch 89/200
1493/1493 - 51s - loss: 1.7069e-04 - val_loss: 1.4790e-04 - 51s/epoch - 34ms/step
Epoch 90/200
1493/1493 - 51s - loss: 1.4589e-04 - val_loss: 1.4963e-04 - 51s/epoch - 34ms/step
Epoch 91/200
1493/1493 - 51s - loss: 1.4555e-04 - val_loss: 1.5636e-04 - 51s/epoch - 34ms/step
Epoch 92/200
1493/1493 - 51s - loss: 1.4272e-04 - val_loss: 2.8931e-04 - 51s/epoch - 34ms/step
Epoch 93/200
1493/1493 - 51s - loss: 1.6023e-04 - val_loss: 1.6412e-04 - 51s/epoch - 34ms/step
Epoch 94/200
1493/1493 - 51s - loss: 1.4650e-04 - val_loss: 1.2770e-04 - 51s/epoch - 34ms/step
Epoch 95/200
1493/1493 - 51s - loss: 1.4174e-04 - val_loss: 1.6461e-04 - 51s/epoch - 34ms/step
Epoch 96/200
1493/1493 - 51s - loss: 1.4174e-04 - val_loss: 1.6921e-04 - 51s/epoch - 34ms/step
Epoch 97/200
1493/1493 - 51s - loss: 1.4041e-04 - val_loss: 1.5174e-04 - 51s/epoch - 34ms/step
Epoch 98/200
1493/1493 - 51s - loss: 1.4575e-04 - val_loss: 8.6886e-04 - 51s/epoch - 34ms/step
Epoch 99/200
1493/1493 - 51s - loss: 2.4598e-04 - val_loss: 1.2855e-04 - 51s/epoch - 34ms/step
Epoch 100/200
1493/1493 - 51s - loss: 1.5997e-04 - val_loss: 1.3146e-04 - 51s/epoch - 34ms/step
Epoch 101/200
1493/1493 - 51s - loss: 1.4780e-04 - val_loss: 7.2670e-04 - 51s/epoch - 34ms/step
Epoch 102/200
1493/1493 - 51s - loss: 1.9804e-04 - val_loss: 1.4131e-04 - 51s/epoch - 34ms/step
Epoch 103/200
1493/1493 - 51s - loss: 1.4472e-04 - val_loss: 1.3153e-04 - 51s/epoch - 34ms/step
Epoch 104/200
1493/1493 - 51s - loss: 1.3946e-04 - val_loss: 2.6202e-04 - 51s/epoch - 34ms/step
Epoch 105/200
1493/1493 - 51s - loss: 1.4966e-04 - val_loss: 1.2738e-04 - 51s/epoch - 34ms/step
Epoch 106/200
1493/1493 - 51s - loss: 1.3644e-04 - val_loss: 1.6134e-04 - 51s/epoch - 34ms/step
Epoch 107/200
1493/1493 - 51s - loss: 1.3488e-04 - val_loss: 1.4555e-04 - 51s/epoch - 34ms/step
Epoch 108/200
1493/1493 - 51s - loss: 1.3689e-04 - val_loss: 1.5714e-04 - 51s/epoch - 34ms/step
Epoch 109/200
1493/1493 - 51s - loss: 1.3501e-04 - val_loss: 1.5773e-04 - 51s/epoch - 34ms/step
Epoch 110/200
1493/1493 - 51s - loss: 1.3216e-04 - val_loss: 1.4072e-04 - 51s/epoch - 34ms/step
Epoch 111/200
1493/1493 - 51s - loss: 1.3118e-04 - val_loss: 1.8229e-04 - 51s/epoch - 34ms/step
Epoch 112/200
1493/1493 - 51s - loss: 1.3853e-04 - val_loss: 1.1766e-04 - 51s/epoch - 34ms/step
Epoch 113/200
1493/1493 - 51s - loss: 1.2822e-04 - val_loss: 2.4506e-04 - 51s/epoch - 34ms/step
Epoch 114/200
1493/1493 - 51s - loss: 1.4305e-04 - val_loss: 1.5182e-04 - 51s/epoch - 34ms/step
Epoch 115/200
1493/1493 - 51s - loss: 1.3129e-04 - val_loss: 2.0715e-04 - 51s/epoch - 34ms/step
Epoch 116/200
1493/1493 - 51s - loss: 1.2969e-04 - val_loss: 1.5194e-04 - 51s/epoch - 34ms/step
Epoch 117/200
1493/1493 - 51s - loss: 1.2745e-04 - val_loss: 3.4005e-04 - 51s/epoch - 34ms/step
Epoch 118/200
1493/1493 - 51s - loss: 1.5218e-04 - val_loss: 1.2756e-04 - 51s/epoch - 34ms/step
Epoch 119/200
1493/1493 - 51s - loss: 1.2804e-04 - val_loss: 1.1940e-04 - 51s/epoch - 34ms/step
Epoch 120/200
1493/1493 - 51s - loss: 1.2611e-04 - val_loss: 1.2901e-04 - 51s/epoch - 34ms/step
Epoch 121/200
1493/1493 - 51s - loss: 1.2574e-04 - val_loss: 2.3653e-04 - 51s/epoch - 34ms/step
Epoch 122/200
1493/1493 - 51s - loss: 1.2791e-04 - val_loss: 1.2884e-04 - 51s/epoch - 34ms/step
Epoch 123/200
1493/1493 - 51s - loss: 1.2491e-04 - val_loss: 1.3354e-04 - 51s/epoch - 34ms/step
Epoch 124/200
1493/1493 - 51s - loss: 1.2279e-04 - val_loss: 2.6127e-04 - 51s/epoch - 34ms/step
Epoch 125/200
1493/1493 - 51s - loss: 1.3436e-04 - val_loss: 1.3721e-04 - 51s/epoch - 34ms/step
Epoch 126/200
1493/1493 - 51s - loss: 1.2543e-04 - val_loss: 2.3000e-04 - 51s/epoch - 34ms/step
Epoch 127/200
1493/1493 - 51s - loss: 1.4020e-04 - val_loss: 5.9127e-04 - 51s/epoch - 34ms/step
Epoch 128/200
1493/1493 - 51s - loss: 1.6903e-04 - val_loss: 1.1420e-04 - 51s/epoch - 34ms/step
Epoch 129/200
1493/1493 - 51s - loss: 1.2492e-04 - val_loss: 1.2813e-04 - 51s/epoch - 34ms/step
Epoch 130/200
1493/1493 - 51s - loss: 1.2271e-04 - val_loss: 1.1780e-04 - 51s/epoch - 34ms/step
Epoch 131/200
1493/1493 - 51s - loss: 1.2450e-04 - val_loss: 3.1142e-04 - 51s/epoch - 34ms/step
Epoch 132/200
1493/1493 - 51s - loss: 1.4306e-04 - val_loss: 1.1034e-04 - 51s/epoch - 34ms/step
Epoch 133/200
1493/1493 - 51s - loss: 1.2118e-04 - val_loss: 1.3430e-04 - 51s/epoch - 34ms/step
Epoch 134/200
1493/1493 - 51s - loss: 1.2000e-04 - val_loss: 1.3048e-04 - 51s/epoch - 34ms/step
Epoch 135/200
1493/1493 - 51s - loss: 1.1809e-04 - val_loss: 1.2405e-04 - 51s/epoch - 34ms/step
Epoch 136/200
1493/1493 - 51s - loss: 1.1864e-04 - val_loss: 1.7754e-04 - 51s/epoch - 34ms/step
Epoch 137/200
1493/1493 - 51s - loss: 1.3496e-04 - val_loss: 1.7349e-04 - 51s/epoch - 34ms/step
Epoch 138/200
1493/1493 - 51s - loss: 1.3253e-04 - val_loss: 1.6386e-04 - 51s/epoch - 34ms/step
Epoch 139/200
1493/1493 - 51s - loss: 1.2885e-04 - val_loss: 1.7519e-04 - 51s/epoch - 34ms/step
Epoch 140/200
1493/1493 - 51s - loss: 1.3016e-04 - val_loss: 1.0428e-04 - 51s/epoch - 34ms/step
Epoch 141/200
1493/1493 - 51s - loss: 1.1751e-04 - val_loss: 1.5107e-04 - 51s/epoch - 34ms/step
Epoch 142/200
1493/1493 - 51s - loss: 1.2228e-04 - val_loss: 1.2798e-04 - 51s/epoch - 34ms/step
Epoch 143/200
1493/1493 - 51s - loss: 1.1513e-04 - val_loss: 1.6121e-04 - 51s/epoch - 34ms/step
Epoch 144/200
1493/1493 - 51s - loss: 1.2147e-04 - val_loss: 1.2488e-04 - 51s/epoch - 34ms/step
Epoch 145/200
1493/1493 - 51s - loss: 1.1683e-04 - val_loss: 1.1369e-04 - 51s/epoch - 34ms/step
Epoch 146/200
1493/1493 - 51s - loss: 1.1442e-04 - val_loss: 1.3654e-04 - 51s/epoch - 34ms/step
Epoch 147/200
1493/1493 - 51s - loss: 1.1330e-04 - val_loss: 1.2090e-04 - 51s/epoch - 34ms/step
Epoch 148/200
1493/1493 - 51s - loss: 1.1290e-04 - val_loss: 2.0311e-04 - 51s/epoch - 34ms/step
Epoch 149/200
1493/1493 - 51s - loss: 1.1232e-04 - val_loss: 1.1916e-04 - 51s/epoch - 34ms/step
Epoch 150/200
1493/1493 - 51s - loss: 1.1121e-04 - val_loss: 1.0887e-04 - 51s/epoch - 34ms/step
Epoch 151/200
1493/1493 - 51s - loss: 1.1344e-04 - val_loss: 1.2801e-04 - 51s/epoch - 34ms/step
Epoch 152/200
1493/1493 - 51s - loss: 1.1273e-04 - val_loss: 1.8465e-04 - 51s/epoch - 34ms/step
Epoch 153/200
1493/1493 - 51s - loss: 1.1756e-04 - val_loss: 1.2075e-04 - 51s/epoch - 34ms/step
Epoch 154/200
1493/1493 - 51s - loss: 1.1022e-04 - val_loss: 1.5885e-04 - 51s/epoch - 34ms/step
Epoch 155/200
1493/1493 - 50s - loss: 1.1729e-04 - val_loss: 1.1093e-04 - 50s/epoch - 34ms/step
Epoch 156/200
1493/1493 - 50s - loss: 1.1348e-04 - val_loss: 1.1198e-04 - 50s/epoch - 34ms/step
Epoch 157/200
1493/1493 - 50s - loss: 1.0929e-04 - val_loss: 1.3404e-04 - 50s/epoch - 34ms/step
Epoch 158/200
1493/1493 - 50s - loss: 1.1148e-04 - val_loss: 1.1843e-04 - 50s/epoch - 34ms/step
Epoch 159/200
1493/1493 - 50s - loss: 1.0845e-04 - val_loss: 1.1257e-04 - 50s/epoch - 34ms/step
Epoch 160/200
1493/1493 - 51s - loss: 1.2468e-04 - val_loss: 1.3768e-04 - 51s/epoch - 34ms/step
Epoch 161/200
1493/1493 - 50s - loss: 1.2375e-04 - val_loss: 1.0437e-04 - 50s/epoch - 34ms/step
Epoch 162/200
1493/1493 - 50s - loss: 1.0949e-04 - val_loss: 1.1130e-04 - 50s/epoch - 34ms/step
Epoch 163/200
1493/1493 - 50s - loss: 1.1072e-04 - val_loss: 2.8591e-04 - 50s/epoch - 34ms/step
Epoch 164/200
1493/1493 - 50s - loss: 1.3193e-04 - val_loss: 1.0769e-04 - 50s/epoch - 34ms/step
Epoch 165/200
1493/1493 - 50s - loss: 1.1256e-04 - val_loss: 1.0391e-04 - 50s/epoch - 34ms/step
Epoch 166/200
1493/1493 - 50s - loss: 1.0848e-04 - val_loss: 1.1454e-04 - 50s/epoch - 34ms/step
Epoch 167/200
1493/1493 - 50s - loss: 1.0763e-04 - val_loss: 1.0296e-04 - 50s/epoch - 34ms/step
Epoch 168/200
1493/1493 - 50s - loss: 1.0752e-04 - val_loss: 1.0698e-04 - 50s/epoch - 34ms/step
Epoch 169/200
1493/1493 - 50s - loss: 1.0662e-04 - val_loss: 1.2547e-04 - 50s/epoch - 34ms/step
Epoch 170/200
1493/1493 - 50s - loss: 1.0971e-04 - val_loss: 3.0818e-04 - 50s/epoch - 34ms/step
Epoch 171/200
1493/1493 - 50s - loss: 1.6285e-04 - val_loss: 1.9015e-04 - 50s/epoch - 34ms/step
Epoch 172/200
1493/1493 - 50s - loss: 1.2665e-04 - val_loss: 1.0446e-04 - 50s/epoch - 34ms/step
Epoch 173/200
1493/1493 - 50s - loss: 1.0981e-04 - val_loss: 9.8075e-05 - 50s/epoch - 34ms/step
Epoch 174/200
1493/1493 - 50s - loss: 1.0724e-04 - val_loss: 1.0682e-04 - 50s/epoch - 34ms/step
Epoch 175/200
1493/1493 - 50s - loss: 1.0813e-04 - val_loss: 1.1130e-04 - 50s/epoch - 34ms/step
Epoch 176/200
1493/1493 - 50s - loss: 1.0852e-04 - val_loss: 1.0973e-04 - 50s/epoch - 34ms/step
Epoch 177/200
1493/1493 - 50s - loss: 1.0784e-04 - val_loss: 9.8586e-05 - 50s/epoch - 34ms/step
Epoch 178/200
1493/1493 - 50s - loss: 1.0472e-04 - val_loss: 1.0627e-04 - 50s/epoch - 34ms/step
Epoch 179/200
1493/1493 - 50s - loss: 1.0377e-04 - val_loss: 1.0946e-04 - 50s/epoch - 34ms/step
Epoch 180/200
1493/1493 - 50s - loss: 1.0408e-04 - val_loss: 1.1854e-04 - 50s/epoch - 34ms/step
Epoch 181/200
1493/1493 - 51s - loss: 1.0443e-04 - val_loss: 1.2258e-04 - 51s/epoch - 34ms/step
Epoch 182/200
1493/1493 - 50s - loss: 1.0379e-04 - val_loss: 1.3879e-04 - 50s/epoch - 34ms/step
Epoch 183/200
1493/1493 - 50s - loss: 1.0470e-04 - val_loss: 1.4244e-04 - 50s/epoch - 34ms/step
Epoch 184/200
1493/1493 - 50s - loss: 1.0622e-04 - val_loss: 1.0181e-04 - 50s/epoch - 34ms/step
Epoch 185/200
1493/1493 - 50s - loss: 1.0082e-04 - val_loss: 1.2529e-04 - 50s/epoch - 34ms/step
Epoch 186/200
1493/1493 - 50s - loss: 1.0187e-04 - val_loss: 1.0122e-04 - 50s/epoch - 34ms/step
Epoch 187/200
1493/1493 - 50s - loss: 1.0095e-04 - val_loss: 1.2159e-04 - 50s/epoch - 34ms/step
Epoch 188/200
1493/1493 - 50s - loss: 1.0463e-04 - val_loss: 1.4269e-04 - 50s/epoch - 34ms/step
Epoch 189/200
1493/1493 - 50s - loss: 1.0302e-04 - val_loss: 1.1768e-04 - 50s/epoch - 34ms/step
Epoch 190/200
1493/1493 - 50s - loss: 1.0320e-04 - val_loss: 1.3073e-04 - 50s/epoch - 34ms/step
Epoch 191/200
1493/1493 - 50s - loss: 1.1256e-04 - val_loss: 1.5829e-04 - 50s/epoch - 34ms/step
Epoch 192/200
1493/1493 - 50s - loss: 1.0376e-04 - val_loss: 1.4135e-04 - 50s/epoch - 34ms/step
Epoch 193/200
1493/1493 - 50s - loss: 1.0895e-04 - val_loss: 3.1146e-04 - 50s/epoch - 34ms/step
Epoch 194/200
1493/1493 - 50s - loss: 1.5372e-04 - val_loss: 6.6309e-04 - 50s/epoch - 34ms/step
Epoch 195/200
1493/1493 - 50s - loss: 1.5207e-04 - val_loss: 2.2986e-04 - 50s/epoch - 34ms/step
Epoch 196/200
1493/1493 - 50s - loss: 1.2221e-04 - val_loss: 9.2279e-05 - 50s/epoch - 34ms/step
Epoch 197/200
1493/1493 - 51s - loss: 1.0528e-04 - val_loss: 9.6961e-05 - 51s/epoch - 34ms/step
Epoch 198/200
1493/1493 - 50s - loss: 1.0492e-04 - val_loss: 1.2367e-04 - 50s/epoch - 34ms/step
Epoch 199/200
1493/1493 - 50s - loss: 1.0583e-04 - val_loss: 9.6278e-05 - 50s/epoch - 34ms/step
Epoch 200/200
1493/1493 - 50s - loss: 1.0137e-04 - val_loss: 1.0687e-04 - 50s/epoch - 34ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00010686571476981044
  1/332 [..............................] - ETA: 35s  7/332 [..............................] - ETA: 2s  15/332 [>.............................] - ETA: 2s 23/332 [=>............................] - ETA: 2s 32/332 [=>............................] - ETA: 2s 42/332 [==>...........................] - ETA: 1s 52/332 [===>..........................] - ETA: 1s 61/332 [====>.........................] - ETA: 1s 70/332 [=====>........................] - ETA: 1s 80/332 [======>.......................] - ETA: 1s 90/332 [=======>......................] - ETA: 1s100/332 [========>.....................] - ETA: 1s110/332 [========>.....................] - ETA: 1s120/332 [=========>....................] - ETA: 1s130/332 [==========>...................] - ETA: 1s140/332 [===========>..................] - ETA: 1s150/332 [============>.................] - ETA: 1s160/332 [=============>................] - ETA: 0s170/332 [==============>...............] - ETA: 0s180/332 [===============>..............] - ETA: 0s190/332 [================>.............] - ETA: 0s200/332 [=================>............] - ETA: 0s210/332 [=================>............] - ETA: 0s220/332 [==================>...........] - ETA: 0s230/332 [===================>..........] - ETA: 0s240/332 [====================>.........] - ETA: 0s250/332 [=====================>........] - ETA: 0s260/332 [======================>.......] - ETA: 0s270/332 [=======================>......] - ETA: 0s280/332 [========================>.....] - ETA: 0s290/332 [=========================>....] - ETA: 0s300/332 [==========================>...] - ETA: 0s310/332 [===========================>..] - ETA: 0s320/332 [===========================>..] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 6ms/step
correlation 0.001204080236573692
cosine 0.0009501829569266959
MAE: 0.0057935906
RMSE: 0.010337584
r2: 0.9930687436368045
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_16"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_17 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_17"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_18 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['2.0custom_n_b', 'mse', 64, 200, 0.0005, 0.5, 632, 0.00010136917262570933, 0.00010686571476981044, 0.001204080236573692, 0.0009501829569266959, 0.005793590564280748, 0.010337583720684052, 0.9930687436368045, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats2_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_18 (Dense)            (None, 2654)              3357310   
                                                                 
 batch_normalization_18 (Bat  (None, 2654)             10616     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2654)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1677960   
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 2654)              1679982   
                                                                 
 batch_normalization_20 (Bat  (None, 2654)             10616     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2654)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3355920   
                                                                 
=================================================================
Total params: 10,094,932
Trainable params: 10,083,052
Non-trainable params: 11,880
_________________________________________________________________
Epoch 1/200
1493/1493 - 59s - loss: 0.0094 - val_loss: 0.0040 - 59s/epoch - 40ms/step
Epoch 2/200
1493/1493 - 58s - loss: 0.0030 - val_loss: 0.0031 - 58s/epoch - 39ms/step
Epoch 3/200
1493/1493 - 58s - loss: 0.0021 - val_loss: 0.0017 - 58s/epoch - 39ms/step
Epoch 4/200
1493/1493 - 58s - loss: 0.0017 - val_loss: 0.0050 - 58s/epoch - 39ms/step
Epoch 5/200
1493/1493 - 58s - loss: 0.0018 - val_loss: 0.0014 - 58s/epoch - 39ms/step
Epoch 6/200
1493/1493 - 58s - loss: 0.0014 - val_loss: 0.0015 - 58s/epoch - 39ms/step
Epoch 7/200
1493/1493 - 58s - loss: 0.0013 - val_loss: 0.0013 - 58s/epoch - 39ms/step
Epoch 8/200
1493/1493 - 58s - loss: 0.0012 - val_loss: 0.0015 - 58s/epoch - 39ms/step
Epoch 9/200
1493/1493 - 58s - loss: 0.0011 - val_loss: 0.0047 - 58s/epoch - 39ms/step
Epoch 10/200
1493/1493 - 58s - loss: 0.0017 - val_loss: 7.6705e-04 - 58s/epoch - 39ms/step
Epoch 11/200
1493/1493 - 58s - loss: 9.1818e-04 - val_loss: 7.8623e-04 - 58s/epoch - 39ms/step
Epoch 12/200
1493/1493 - 58s - loss: 7.9932e-04 - val_loss: 0.0011 - 58s/epoch - 39ms/step
Epoch 13/200
1493/1493 - 58s - loss: 7.6188e-04 - val_loss: 0.0014 - 58s/epoch - 39ms/step
Epoch 14/200
1493/1493 - 58s - loss: 7.8719e-04 - val_loss: 6.2355e-04 - 58s/epoch - 39ms/step
Epoch 15/200
1493/1493 - 58s - loss: 6.7188e-04 - val_loss: 0.0012 - 58s/epoch - 39ms/step
Epoch 16/200
1493/1493 - 58s - loss: 6.3888e-04 - val_loss: 5.4620e-04 - 58s/epoch - 39ms/step
Epoch 17/200
1493/1493 - 58s - loss: 5.5142e-04 - val_loss: 6.7910e-04 - 58s/epoch - 39ms/step
Epoch 18/200
1493/1493 - 58s - loss: 5.2736e-04 - val_loss: 0.0012 - 58s/epoch - 39ms/step
Epoch 19/200
1493/1493 - 58s - loss: 5.5944e-04 - val_loss: 9.0537e-04 - 58s/epoch - 39ms/step
Epoch 20/200
1493/1493 - 58s - loss: 4.9734e-04 - val_loss: 4.3186e-04 - 58s/epoch - 39ms/step
Epoch 21/200
1493/1493 - 58s - loss: 4.4910e-04 - val_loss: 5.2845e-04 - 58s/epoch - 39ms/step
Epoch 22/200
1493/1493 - 58s - loss: 4.2960e-04 - val_loss: 4.1955e-04 - 58s/epoch - 39ms/step
Epoch 23/200
1493/1493 - 58s - loss: 4.0622e-04 - val_loss: 4.6842e-04 - 58s/epoch - 39ms/step
Epoch 24/200
1493/1493 - 58s - loss: 4.0071e-04 - val_loss: 3.4650e-04 - 58s/epoch - 39ms/step
Epoch 25/200
1493/1493 - 58s - loss: 3.7709e-04 - val_loss: 4.1687e-04 - 58s/epoch - 39ms/step
Epoch 26/200
1493/1493 - 58s - loss: 3.5707e-04 - val_loss: 4.4164e-04 - 58s/epoch - 39ms/step
Epoch 27/200
1493/1493 - 58s - loss: 3.4269e-04 - val_loss: 3.5606e-04 - 58s/epoch - 39ms/step
Epoch 28/200
1493/1493 - 58s - loss: 3.2519e-04 - val_loss: 7.6098e-04 - 58s/epoch - 39ms/step
Epoch 29/200
1493/1493 - 58s - loss: 3.2733e-04 - val_loss: 3.7115e-04 - 58s/epoch - 39ms/step
Epoch 30/200
1493/1493 - 58s - loss: 3.0871e-04 - val_loss: 3.3507e-04 - 58s/epoch - 39ms/step
Epoch 31/200
1493/1493 - 58s - loss: 3.0090e-04 - val_loss: 7.0151e-04 - 58s/epoch - 39ms/step
Epoch 32/200
1493/1493 - 58s - loss: 3.4447e-04 - val_loss: 3.4671e-04 - 58s/epoch - 39ms/step
Epoch 33/200
1493/1493 - 58s - loss: 3.0803e-04 - val_loss: 2.7553e-04 - 58s/epoch - 39ms/step
Epoch 34/200
1493/1493 - 58s - loss: 2.8250e-04 - val_loss: 2.7402e-04 - 58s/epoch - 39ms/step
Epoch 35/200
1493/1493 - 58s - loss: 2.6946e-04 - val_loss: 4.4491e-04 - 58s/epoch - 39ms/step
Epoch 36/200
1493/1493 - 58s - loss: 2.8298e-04 - val_loss: 2.9659e-04 - 58s/epoch - 39ms/step
Epoch 37/200
1493/1493 - 58s - loss: 2.6833e-04 - val_loss: 5.4198e-04 - 58s/epoch - 39ms/step
Epoch 38/200
1493/1493 - 58s - loss: 2.8233e-04 - val_loss: 2.3732e-04 - 58s/epoch - 39ms/step
Epoch 39/200
1493/1493 - 58s - loss: 2.4851e-04 - val_loss: 2.3806e-04 - 58s/epoch - 39ms/step
Epoch 40/200
1493/1493 - 58s - loss: 2.4099e-04 - val_loss: 3.0340e-04 - 58s/epoch - 39ms/step
Epoch 41/200
1493/1493 - 58s - loss: 2.4153e-04 - val_loss: 2.2792e-04 - 58s/epoch - 39ms/step
Epoch 42/200
1493/1493 - 58s - loss: 2.3732e-04 - val_loss: 2.1518e-04 - 58s/epoch - 39ms/step
Epoch 43/200
1493/1493 - 58s - loss: 2.3436e-04 - val_loss: 5.9482e-04 - 58s/epoch - 39ms/step
Epoch 44/200
1493/1493 - 58s - loss: 2.4924e-04 - val_loss: 2.3785e-04 - 58s/epoch - 39ms/step
Epoch 45/200
1493/1493 - 58s - loss: 2.2565e-04 - val_loss: 1.9093e-04 - 58s/epoch - 39ms/step
Epoch 46/200
1493/1493 - 58s - loss: 2.2294e-04 - val_loss: 2.1225e-04 - 58s/epoch - 39ms/step
Epoch 47/200
1493/1493 - 58s - loss: 2.1751e-04 - val_loss: 2.5272e-04 - 58s/epoch - 39ms/step
Epoch 48/200
1493/1493 - 58s - loss: 2.1694e-04 - val_loss: 2.8116e-04 - 58s/epoch - 39ms/step
Epoch 49/200
1493/1493 - 58s - loss: 2.2170e-04 - val_loss: 0.0011 - 58s/epoch - 39ms/step
Epoch 50/200
1493/1493 - 58s - loss: 3.2961e-04 - val_loss: 0.0013 - 58s/epoch - 39ms/step
Epoch 51/200
1493/1493 - 58s - loss: 2.9121e-04 - val_loss: 2.1807e-04 - 58s/epoch - 39ms/step
Epoch 52/200
1493/1493 - 58s - loss: 2.1231e-04 - val_loss: 2.1463e-04 - 58s/epoch - 39ms/step
Epoch 53/200
1493/1493 - 58s - loss: 2.1544e-04 - val_loss: 2.2356e-04 - 58s/epoch - 39ms/step
Epoch 54/200
1493/1493 - 58s - loss: 2.2134e-04 - val_loss: 2.0661e-04 - 58s/epoch - 39ms/step
Epoch 55/200
1493/1493 - 58s - loss: 1.9830e-04 - val_loss: 1.8427e-04 - 58s/epoch - 39ms/step
Epoch 56/200
1493/1493 - 58s - loss: 1.9605e-04 - val_loss: 2.1322e-04 - 58s/epoch - 39ms/step
Epoch 57/200
1493/1493 - 58s - loss: 1.9158e-04 - val_loss: 2.2060e-04 - 58s/epoch - 39ms/step
Epoch 58/200
1493/1493 - 58s - loss: 1.9262e-04 - val_loss: 1.6993e-04 - 58s/epoch - 39ms/step
Epoch 59/200
1493/1493 - 58s - loss: 1.8596e-04 - val_loss: 1.8594e-04 - 58s/epoch - 39ms/step
Epoch 60/200
1493/1493 - 58s - loss: 1.8914e-04 - val_loss: 5.2037e-04 - 58s/epoch - 39ms/step
Epoch 61/200
1493/1493 - 59s - loss: 2.3320e-04 - val_loss: 1.7554e-04 - 59s/epoch - 39ms/step
Epoch 62/200
1493/1493 - 59s - loss: 1.8450e-04 - val_loss: 1.9857e-04 - 59s/epoch - 39ms/step
Epoch 63/200
1493/1493 - 59s - loss: 1.7910e-04 - val_loss: 2.2650e-04 - 59s/epoch - 39ms/step
Epoch 64/200
1493/1493 - 58s - loss: 1.7960e-04 - val_loss: 5.3075e-04 - 58s/epoch - 39ms/step
Epoch 65/200
1493/1493 - 58s - loss: 2.0824e-04 - val_loss: 2.0417e-04 - 58s/epoch - 39ms/step
Epoch 66/200
1493/1493 - 58s - loss: 1.8407e-04 - val_loss: 2.0121e-04 - 58s/epoch - 39ms/step
Epoch 67/200
1493/1493 - 59s - loss: 1.7358e-04 - val_loss: 1.8396e-04 - 59s/epoch - 39ms/step
Epoch 68/200
1493/1493 - 58s - loss: 1.7253e-04 - val_loss: 6.1649e-04 - 58s/epoch - 39ms/step
Epoch 69/200
1493/1493 - 58s - loss: 2.6295e-04 - val_loss: 1.7838e-04 - 58s/epoch - 39ms/step
Epoch 70/200
1493/1493 - 59s - loss: 1.7522e-04 - val_loss: 1.8967e-04 - 59s/epoch - 39ms/step
Epoch 71/200
1493/1493 - 58s - loss: 1.7472e-04 - val_loss: 1.5598e-04 - 58s/epoch - 39ms/step
Epoch 72/200
1493/1493 - 58s - loss: 1.7142e-04 - val_loss: 2.1618e-04 - 58s/epoch - 39ms/step
Epoch 73/200
1493/1493 - 59s - loss: 1.7077e-04 - val_loss: 1.8219e-04 - 59s/epoch - 39ms/step
Epoch 74/200
1493/1493 - 58s - loss: 1.6718e-04 - val_loss: 1.6707e-04 - 58s/epoch - 39ms/step
Epoch 75/200
1493/1493 - 58s - loss: 1.6158e-04 - val_loss: 1.5151e-04 - 58s/epoch - 39ms/step
Epoch 76/200
1493/1493 - 58s - loss: 1.5942e-04 - val_loss: 1.5105e-04 - 58s/epoch - 39ms/step
Epoch 77/200
1493/1493 - 58s - loss: 1.5730e-04 - val_loss: 3.5163e-04 - 58s/epoch - 39ms/step
Epoch 78/200
1493/1493 - 59s - loss: 1.6570e-04 - val_loss: 1.5376e-04 - 59s/epoch - 39ms/step
Epoch 79/200
1493/1493 - 59s - loss: 1.5698e-04 - val_loss: 3.0764e-04 - 59s/epoch - 39ms/step
Epoch 80/200
1493/1493 - 58s - loss: 1.7323e-04 - val_loss: 1.6668e-04 - 58s/epoch - 39ms/step
Epoch 81/200
1493/1493 - 58s - loss: 1.5621e-04 - val_loss: 2.8725e-04 - 58s/epoch - 39ms/step
Epoch 82/200
1493/1493 - 58s - loss: 1.5516e-04 - val_loss: 1.5732e-04 - 58s/epoch - 39ms/step
Epoch 83/200
1493/1493 - 58s - loss: 1.4974e-04 - val_loss: 1.5502e-04 - 58s/epoch - 39ms/step
Epoch 84/200
1493/1493 - 58s - loss: 1.4918e-04 - val_loss: 1.8322e-04 - 58s/epoch - 39ms/step
Epoch 85/200
1493/1493 - 59s - loss: 1.5512e-04 - val_loss: 1.8426e-04 - 59s/epoch - 39ms/step
Epoch 86/200
1493/1493 - 59s - loss: 1.5607e-04 - val_loss: 1.4451e-04 - 59s/epoch - 39ms/step
Epoch 87/200
1493/1493 - 59s - loss: 1.4523e-04 - val_loss: 1.4735e-04 - 59s/epoch - 39ms/step
Epoch 88/200
1493/1493 - 59s - loss: 1.4400e-04 - val_loss: 2.4571e-04 - 59s/epoch - 39ms/step
Epoch 89/200
1493/1493 - 58s - loss: 1.5763e-04 - val_loss: 1.4029e-04 - 58s/epoch - 39ms/step
Epoch 90/200
1493/1493 - 58s - loss: 1.4341e-04 - val_loss: 1.5311e-04 - 58s/epoch - 39ms/step
Epoch 91/200
1493/1493 - 58s - loss: 1.4228e-04 - val_loss: 1.5658e-04 - 58s/epoch - 39ms/step
Epoch 92/200
1493/1493 - 59s - loss: 1.4166e-04 - val_loss: 3.5983e-04 - 59s/epoch - 39ms/step
Epoch 93/200
1493/1493 - 58s - loss: 1.6372e-04 - val_loss: 1.3763e-04 - 58s/epoch - 39ms/step
Epoch 94/200
1493/1493 - 58s - loss: 1.4292e-04 - val_loss: 1.2305e-04 - 58s/epoch - 39ms/step
Epoch 95/200
1493/1493 - 58s - loss: 1.3962e-04 - val_loss: 1.6167e-04 - 58s/epoch - 39ms/step
Epoch 96/200
1493/1493 - 59s - loss: 1.3969e-04 - val_loss: 1.6271e-04 - 59s/epoch - 39ms/step
Epoch 97/200
1493/1493 - 58s - loss: 1.3798e-04 - val_loss: 1.3905e-04 - 58s/epoch - 39ms/step
Epoch 98/200
1493/1493 - 59s - loss: 1.4096e-04 - val_loss: 5.3510e-04 - 59s/epoch - 39ms/step
Epoch 99/200
1493/1493 - 59s - loss: 2.1322e-04 - val_loss: 1.3125e-04 - 59s/epoch - 39ms/step
Epoch 100/200
1493/1493 - 58s - loss: 1.6252e-04 - val_loss: 1.2353e-04 - 58s/epoch - 39ms/step
Epoch 101/200
1493/1493 - 58s - loss: 1.4110e-04 - val_loss: 1.8191e-04 - 58s/epoch - 39ms/step
Epoch 102/200
1493/1493 - 58s - loss: 1.4581e-04 - val_loss: 2.2164e-04 - 58s/epoch - 39ms/step
Epoch 103/200
1493/1493 - 59s - loss: 1.4693e-04 - val_loss: 1.2895e-04 - 59s/epoch - 39ms/step
Epoch 104/200
1493/1493 - 59s - loss: 1.3572e-04 - val_loss: 2.8667e-04 - 59s/epoch - 39ms/step
Epoch 105/200
1493/1493 - 58s - loss: 1.4726e-04 - val_loss: 1.3085e-04 - 58s/epoch - 39ms/step
Epoch 106/200
1493/1493 - 58s - loss: 1.3350e-04 - val_loss: 1.5134e-04 - 58s/epoch - 39ms/step
Epoch 107/200
1493/1493 - 58s - loss: 1.3265e-04 - val_loss: 1.3066e-04 - 58s/epoch - 39ms/step
Epoch 108/200
1493/1493 - 58s - loss: 1.3733e-04 - val_loss: 1.3846e-04 - 58s/epoch - 39ms/step
Epoch 109/200
1493/1493 - 59s - loss: 1.3204e-04 - val_loss: 1.5209e-04 - 59s/epoch - 39ms/step
Epoch 110/200
1493/1493 - 58s - loss: 1.2998e-04 - val_loss: 1.4523e-04 - 58s/epoch - 39ms/step
Epoch 111/200
1493/1493 - 59s - loss: 1.2969e-04 - val_loss: 2.3300e-04 - 59s/epoch - 39ms/step
Epoch 112/200
1493/1493 - 59s - loss: 1.4617e-04 - val_loss: 1.1833e-04 - 59s/epoch - 39ms/step
Epoch 113/200
1493/1493 - 58s - loss: 1.2891e-04 - val_loss: 2.7644e-04 - 58s/epoch - 39ms/step
Epoch 114/200
1493/1493 - 58s - loss: 1.6072e-04 - val_loss: 1.3388e-04 - 58s/epoch - 39ms/step
Epoch 115/200
1493/1493 - 58s - loss: 1.3463e-04 - val_loss: 2.0960e-04 - 58s/epoch - 39ms/step
Epoch 116/200
1493/1493 - 59s - loss: 1.2910e-04 - val_loss: 1.4375e-04 - 59s/epoch - 39ms/step
Epoch 117/200
1493/1493 - 58s - loss: 1.2700e-04 - val_loss: 1.9415e-04 - 58s/epoch - 39ms/step
Epoch 118/200
1493/1493 - 59s - loss: 1.3396e-04 - val_loss: 1.3080e-04 - 59s/epoch - 39ms/step
Epoch 119/200
1493/1493 - 58s - loss: 1.2616e-04 - val_loss: 1.2357e-04 - 58s/epoch - 39ms/step
Epoch 120/200
1493/1493 - 58s - loss: 1.2498e-04 - val_loss: 1.2818e-04 - 58s/epoch - 39ms/step
Epoch 121/200
1493/1493 - 58s - loss: 1.2441e-04 - val_loss: 1.7013e-04 - 58s/epoch - 39ms/step
Epoch 122/200
1493/1493 - 59s - loss: 1.2475e-04 - val_loss: 1.1658e-04 - 59s/epoch - 39ms/step
Epoch 123/200
1493/1493 - 58s - loss: 1.2292e-04 - val_loss: 1.2745e-04 - 58s/epoch - 39ms/step
Epoch 124/200
1493/1493 - 58s - loss: 1.2251e-04 - val_loss: 3.7306e-04 - 58s/epoch - 39ms/step
Epoch 125/200
1493/1493 - 58s - loss: 1.6266e-04 - val_loss: 1.5596e-04 - 58s/epoch - 39ms/step
Epoch 126/200
1493/1493 - 58s - loss: 1.2844e-04 - val_loss: 2.2832e-04 - 58s/epoch - 39ms/step
Epoch 127/200
1493/1493 - 58s - loss: 1.2811e-04 - val_loss: 1.2426e-04 - 58s/epoch - 39ms/step
Epoch 128/200
1493/1493 - 58s - loss: 1.1990e-04 - val_loss: 1.2420e-04 - 58s/epoch - 39ms/step
Epoch 129/200
1493/1493 - 58s - loss: 1.1946e-04 - val_loss: 1.1490e-04 - 58s/epoch - 39ms/step
Epoch 130/200
1493/1493 - 58s - loss: 1.1930e-04 - val_loss: 1.1907e-04 - 58s/epoch - 39ms/step
Epoch 131/200
1493/1493 - 58s - loss: 1.2139e-04 - val_loss: 2.7161e-04 - 58s/epoch - 39ms/step
Epoch 132/200
1493/1493 - 58s - loss: 1.3933e-04 - val_loss: 1.1030e-04 - 58s/epoch - 39ms/step
Epoch 133/200
1493/1493 - 58s - loss: 1.1933e-04 - val_loss: 1.2778e-04 - 58s/epoch - 39ms/step
Epoch 134/200
1493/1493 - 58s - loss: 1.1796e-04 - val_loss: 1.2756e-04 - 58s/epoch - 39ms/step
Epoch 135/200
1493/1493 - 58s - loss: 1.1665e-04 - val_loss: 1.0702e-04 - 58s/epoch - 39ms/step
Epoch 136/200
1493/1493 - 59s - loss: 1.1641e-04 - val_loss: 1.4498e-04 - 59s/epoch - 39ms/step
Epoch 137/200
1493/1493 - 58s - loss: 1.2765e-04 - val_loss: 2.6496e-04 - 58s/epoch - 39ms/step
Epoch 138/200
1493/1493 - 58s - loss: 1.4043e-04 - val_loss: 2.4212e-04 - 58s/epoch - 39ms/step
Epoch 139/200
1493/1493 - 58s - loss: 1.3900e-04 - val_loss: 2.3489e-04 - 58s/epoch - 39ms/step
Epoch 140/200
1493/1493 - 59s - loss: 1.3868e-04 - val_loss: 1.0673e-04 - 59s/epoch - 39ms/step
Epoch 141/200
1493/1493 - 58s - loss: 1.1856e-04 - val_loss: 1.4715e-04 - 58s/epoch - 39ms/step
Epoch 142/200
1493/1493 - 58s - loss: 1.2228e-04 - val_loss: 1.2485e-04 - 58s/epoch - 39ms/step
Epoch 143/200
1493/1493 - 58s - loss: 1.1499e-04 - val_loss: 1.3583e-04 - 58s/epoch - 39ms/step
Epoch 144/200
1493/1493 - 58s - loss: 1.1775e-04 - val_loss: 1.5122e-04 - 58s/epoch - 39ms/step
Epoch 145/200
1493/1493 - 59s - loss: 1.2127e-04 - val_loss: 1.0521e-04 - 59s/epoch - 39ms/step
Epoch 146/200
1493/1493 - 58s - loss: 1.1388e-04 - val_loss: 1.1062e-04 - 58s/epoch - 39ms/step
Epoch 147/200
1493/1493 - 58s - loss: 1.1242e-04 - val_loss: 1.4418e-04 - 58s/epoch - 39ms/step
Epoch 148/200
1493/1493 - 58s - loss: 1.1419e-04 - val_loss: 2.0794e-04 - 58s/epoch - 39ms/step
Epoch 149/200
1493/1493 - 58s - loss: 1.1139e-04 - val_loss: 1.1625e-04 - 58s/epoch - 39ms/step
Epoch 150/200
1493/1493 - 58s - loss: 1.1082e-04 - val_loss: 1.1813e-04 - 58s/epoch - 39ms/step
Epoch 151/200
1493/1493 - 58s - loss: 1.1216e-04 - val_loss: 1.3706e-04 - 58s/epoch - 39ms/step
Epoch 152/200
1493/1493 - 59s - loss: 1.1172e-04 - val_loss: 1.6569e-04 - 59s/epoch - 39ms/step
Epoch 153/200
1493/1493 - 58s - loss: 1.1558e-04 - val_loss: 1.3049e-04 - 58s/epoch - 39ms/step
Epoch 154/200
1493/1493 - 58s - loss: 1.0976e-04 - val_loss: 1.3555e-04 - 58s/epoch - 39ms/step
Epoch 155/200
1493/1493 - 59s - loss: 1.1113e-04 - val_loss: 1.1150e-04 - 59s/epoch - 39ms/step
Epoch 156/200
1493/1493 - 58s - loss: 1.0977e-04 - val_loss: 1.1583e-04 - 58s/epoch - 39ms/step
Epoch 157/200
1493/1493 - 58s - loss: 1.0776e-04 - val_loss: 1.2072e-04 - 58s/epoch - 39ms/step
Epoch 158/200
1493/1493 - 58s - loss: 1.0870e-04 - val_loss: 1.1624e-04 - 58s/epoch - 39ms/step
Epoch 159/200
1493/1493 - 58s - loss: 1.0743e-04 - val_loss: 1.0768e-04 - 58s/epoch - 39ms/step
Epoch 160/200
1493/1493 - 59s - loss: 1.0737e-04 - val_loss: 1.6914e-04 - 59s/epoch - 39ms/step
Epoch 161/200
1493/1493 - 58s - loss: 1.3583e-04 - val_loss: 1.0173e-04 - 58s/epoch - 39ms/step
Epoch 162/200
1493/1493 - 59s - loss: 1.0841e-04 - val_loss: 1.1094e-04 - 59s/epoch - 39ms/step
Epoch 163/200
1493/1493 - 59s - loss: 1.0716e-04 - val_loss: 1.3469e-04 - 59s/epoch - 39ms/step
Epoch 164/200
1493/1493 - 58s - loss: 1.1042e-04 - val_loss: 1.1665e-04 - 58s/epoch - 39ms/step
Epoch 165/200
1493/1493 - 59s - loss: 1.2887e-04 - val_loss: 1.1274e-04 - 59s/epoch - 39ms/step
Epoch 166/200
1493/1493 - 58s - loss: 1.0784e-04 - val_loss: 1.0970e-04 - 58s/epoch - 39ms/step
Epoch 167/200
1493/1493 - 58s - loss: 1.0654e-04 - val_loss: 9.7721e-05 - 58s/epoch - 39ms/step
Epoch 168/200
1493/1493 - 58s - loss: 1.0606e-04 - val_loss: 9.8271e-05 - 58s/epoch - 39ms/step
Epoch 169/200
1493/1493 - 58s - loss: 1.0554e-04 - val_loss: 1.3579e-04 - 58s/epoch - 39ms/step
Epoch 170/200
1493/1493 - 58s - loss: 1.0584e-04 - val_loss: 2.5479e-04 - 58s/epoch - 39ms/step
Epoch 171/200
1493/1493 - 58s - loss: 1.3126e-04 - val_loss: 4.8394e-04 - 58s/epoch - 39ms/step
Epoch 172/200
1493/1493 - 58s - loss: 1.5879e-04 - val_loss: 1.2042e-04 - 58s/epoch - 39ms/step
Epoch 173/200
1493/1493 - 58s - loss: 1.1316e-04 - val_loss: 1.0300e-04 - 58s/epoch - 39ms/step
Epoch 174/200
1493/1493 - 59s - loss: 1.0713e-04 - val_loss: 1.0044e-04 - 59s/epoch - 39ms/step
Epoch 175/200
1493/1493 - 58s - loss: 1.0709e-04 - val_loss: 1.1943e-04 - 58s/epoch - 39ms/step
Epoch 176/200
1493/1493 - 59s - loss: 1.0675e-04 - val_loss: 1.0758e-04 - 59s/epoch - 39ms/step
Epoch 177/200
1493/1493 - 58s - loss: 1.0499e-04 - val_loss: 9.3742e-05 - 58s/epoch - 39ms/step
Epoch 178/200
1493/1493 - 58s - loss: 1.0322e-04 - val_loss: 1.0716e-04 - 58s/epoch - 39ms/step
Epoch 179/200
1493/1493 - 59s - loss: 1.0246e-04 - val_loss: 1.3627e-04 - 59s/epoch - 39ms/step
Epoch 180/200
1493/1493 - 58s - loss: 1.0694e-04 - val_loss: 1.1733e-04 - 58s/epoch - 39ms/step
Epoch 181/200
1493/1493 - 58s - loss: 1.0230e-04 - val_loss: 1.2520e-04 - 58s/epoch - 39ms/step
Epoch 182/200
1493/1493 - 58s - loss: 1.0345e-04 - val_loss: 1.4387e-04 - 58s/epoch - 39ms/step
Epoch 183/200
1493/1493 - 59s - loss: 1.0336e-04 - val_loss: 1.3412e-04 - 59s/epoch - 39ms/step
Epoch 184/200
1493/1493 - 58s - loss: 1.0363e-04 - val_loss: 9.8224e-05 - 58s/epoch - 39ms/step
Epoch 185/200
1493/1493 - 59s - loss: 9.9562e-05 - val_loss: 1.3476e-04 - 59s/epoch - 39ms/step
Epoch 186/200
1493/1493 - 58s - loss: 1.0099e-04 - val_loss: 1.0721e-04 - 58s/epoch - 39ms/step
Epoch 187/200
1493/1493 - 59s - loss: 1.0069e-04 - val_loss: 1.4833e-04 - 59s/epoch - 39ms/step
Epoch 188/200
1493/1493 - 58s - loss: 1.1965e-04 - val_loss: 1.1979e-04 - 58s/epoch - 39ms/step
Epoch 189/200
1493/1493 - 58s - loss: 1.0619e-04 - val_loss: 1.1445e-04 - 58s/epoch - 39ms/step
Epoch 190/200
1493/1493 - 58s - loss: 1.0291e-04 - val_loss: 1.1169e-04 - 58s/epoch - 39ms/step
Epoch 191/200
1493/1493 - 59s - loss: 1.0374e-04 - val_loss: 2.9783e-04 - 59s/epoch - 39ms/step
Epoch 192/200
1493/1493 - 58s - loss: 1.3672e-04 - val_loss: 1.0484e-04 - 58s/epoch - 39ms/step
Epoch 193/200
1493/1493 - 58s - loss: 1.0801e-04 - val_loss: 2.6514e-04 - 58s/epoch - 39ms/step
Epoch 194/200
1493/1493 - 58s - loss: 1.2247e-04 - val_loss: 3.8378e-04 - 58s/epoch - 39ms/step
Epoch 195/200
1493/1493 - 58s - loss: 1.4478e-04 - val_loss: 1.4797e-04 - 58s/epoch - 39ms/step
Epoch 196/200
1493/1493 - 58s - loss: 1.1620e-04 - val_loss: 8.8640e-05 - 58s/epoch - 39ms/step
Epoch 197/200
1493/1493 - 58s - loss: 1.0507e-04 - val_loss: 1.8247e-04 - 58s/epoch - 39ms/step
Epoch 198/200
1493/1493 - 58s - loss: 1.1159e-04 - val_loss: 1.8264e-04 - 58s/epoch - 39ms/step
Epoch 199/200
1493/1493 - 58s - loss: 1.1622e-04 - val_loss: 9.6612e-05 - 58s/epoch - 39ms/step
Epoch 200/200
1493/1493 - 58s - loss: 1.0221e-04 - val_loss: 9.9616e-05 - 58s/epoch - 39ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 9.961600153474137e-05
  1/332 [..............................] - ETA: 35s  7/332 [..............................] - ETA: 2s  14/332 [>.............................] - ETA: 2s 22/332 [>.............................] - ETA: 2s 31/332 [=>............................] - ETA: 2s 39/332 [==>...........................] - ETA: 2s 48/332 [===>..........................] - ETA: 1s 57/332 [====>.........................] - ETA: 1s 66/332 [====>.........................] - ETA: 1s 75/332 [=====>........................] - ETA: 1s 84/332 [======>.......................] - ETA: 1s 93/332 [=======>......................] - ETA: 1s102/332 [========>.....................] - ETA: 1s111/332 [=========>....................] - ETA: 1s120/332 [=========>....................] - ETA: 1s129/332 [==========>...................] - ETA: 1s138/332 [===========>..................] - ETA: 1s147/332 [============>.................] - ETA: 1s156/332 [=============>................] - ETA: 1s165/332 [=============>................] - ETA: 1s174/332 [==============>...............] - ETA: 0s183/332 [===============>..............] - ETA: 0s192/332 [================>.............] - ETA: 0s201/332 [=================>............] - ETA: 0s210/332 [=================>............] - ETA: 0s219/332 [==================>...........] - ETA: 0s228/332 [===================>..........] - ETA: 0s237/332 [====================>.........] - ETA: 0s246/332 [=====================>........] - ETA: 0s255/332 [======================>.......] - ETA: 0s264/332 [======================>.......] - ETA: 0s273/332 [=======================>......] - ETA: 0s282/332 [========================>.....] - ETA: 0s291/332 [=========================>....] - ETA: 0s300/332 [==========================>...] - ETA: 0s309/332 [==========================>...] - ETA: 0s318/332 [===========================>..] - ETA: 0s327/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 6ms/step
correlation 0.00113040403716752
cosine 0.0008901338947075427
MAE: 0.0055326736
RMSE: 0.009980777
r2: 0.9935386708060221
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 2654)              3357310   
                                                                 
 batch_normalization_18 (Bat  (None, 2654)             10616     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2654)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1677960   
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 2654)              1679982   
                                                                 
 batch_normalization_20 (Bat  (None, 2654)             10616     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2654)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3355920   
                                                                 
=================================================================
Total params: 10,094,932
Trainable params: 10,083,052
Non-trainable params: 11,880
_________________________________________________________________
Encoder
Model: "model_19"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_20 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 2654)              3357310   
                                                                 
 batch_normalization_18 (Bat  (None, 2654)             10616     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2654)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1677960   
                                                                 
=================================================================
Total params: 5,045,886
Trainable params: 5,040,578
Non-trainable params: 5,308
_________________________________________________________________
Decoder
Model: "model_20"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_21 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 2654)              1679982   
                                                                 
 batch_normalization_20 (Bat  (None, 2654)             10616     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2654)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3355920   
                                                                 
=================================================================
Total params: 5,049,046
Trainable params: 5,042,474
Non-trainable params: 6,572
_________________________________________________________________
['2.1custom_n_b', 'mse', 64, 200, 0.0005, 0.5, 632, 0.00010221423144685104, 9.961600153474137e-05, 0.00113040403716752, 0.0008901338947075427, 0.0055326735600829124, 0.009980777278542519, 0.9935386708060221, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats2_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_21 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_21 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1757592   
                                                                 
 batch_normalization_22 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 632)               0         
                                                                 
 dense_22 (Dense)            (None, 2780)              1759740   
                                                                 
 batch_normalization_23 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2780)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 10,573,984
Trainable params: 10,561,600
Non-trainable params: 12,384
_________________________________________________________________
Epoch 1/200
1493/1493 - 64s - loss: 0.0095 - val_loss: 0.0048 - 64s/epoch - 43ms/step
Epoch 2/200
1493/1493 - 62s - loss: 0.0030 - val_loss: 0.0032 - 62s/epoch - 42ms/step
Epoch 3/200
1493/1493 - 62s - loss: 0.0021 - val_loss: 0.0017 - 62s/epoch - 41ms/step
Epoch 4/200
1493/1493 - 62s - loss: 0.0017 - val_loss: 0.0037 - 62s/epoch - 42ms/step
Epoch 5/200
1493/1493 - 62s - loss: 0.0017 - val_loss: 0.0017 - 62s/epoch - 41ms/step
Epoch 6/200
1493/1493 - 62s - loss: 0.0014 - val_loss: 0.0015 - 62s/epoch - 42ms/step
Epoch 7/200
1493/1493 - 62s - loss: 0.0014 - val_loss: 0.0010 - 62s/epoch - 41ms/step
Epoch 8/200
1493/1493 - 62s - loss: 0.0012 - val_loss: 9.8246e-04 - 62s/epoch - 42ms/step
Epoch 9/200
1493/1493 - 62s - loss: 0.0011 - val_loss: 0.0023 - 62s/epoch - 42ms/step
Epoch 10/200
1493/1493 - 62s - loss: 0.0012 - val_loss: 7.9692e-04 - 62s/epoch - 42ms/step
Epoch 11/200
1493/1493 - 62s - loss: 9.0400e-04 - val_loss: 9.1228e-04 - 62s/epoch - 42ms/step
Epoch 12/200
1493/1493 - 62s - loss: 7.8192e-04 - val_loss: 0.0015 - 62s/epoch - 41ms/step
Epoch 13/200
1493/1493 - 62s - loss: 7.8568e-04 - val_loss: 0.0014 - 62s/epoch - 42ms/step
Epoch 14/200
1493/1493 - 62s - loss: 7.7047e-04 - val_loss: 7.0110e-04 - 62s/epoch - 41ms/step
Epoch 15/200
1493/1493 - 62s - loss: 6.5208e-04 - val_loss: 5.4491e-04 - 62s/epoch - 42ms/step
Epoch 16/200
1493/1493 - 62s - loss: 5.8294e-04 - val_loss: 5.3382e-04 - 62s/epoch - 42ms/step
Epoch 17/200
1493/1493 - 62s - loss: 5.3323e-04 - val_loss: 6.1665e-04 - 62s/epoch - 42ms/step
Epoch 18/200
1493/1493 - 62s - loss: 5.0521e-04 - val_loss: 7.0244e-04 - 62s/epoch - 41ms/step
Epoch 19/200
1493/1493 - 62s - loss: 4.9114e-04 - val_loss: 6.3356e-04 - 62s/epoch - 41ms/step
Epoch 20/200
1493/1493 - 62s - loss: 4.6546e-04 - val_loss: 4.3900e-04 - 62s/epoch - 42ms/step
Epoch 21/200
1493/1493 - 62s - loss: 4.3155e-04 - val_loss: 5.0861e-04 - 62s/epoch - 42ms/step
Epoch 22/200
1493/1493 - 62s - loss: 4.0885e-04 - val_loss: 7.8032e-04 - 62s/epoch - 42ms/step
Epoch 23/200
1493/1493 - 62s - loss: 4.6623e-04 - val_loss: 9.2473e-04 - 62s/epoch - 42ms/step
Epoch 24/200
1493/1493 - 62s - loss: 4.2891e-04 - val_loss: 3.5682e-04 - 62s/epoch - 41ms/step
Epoch 25/200
1493/1493 - 62s - loss: 3.6570e-04 - val_loss: 3.2448e-04 - 62s/epoch - 42ms/step
Epoch 26/200
1493/1493 - 62s - loss: 3.4825e-04 - val_loss: 4.4230e-04 - 62s/epoch - 41ms/step
Epoch 27/200
1493/1493 - 62s - loss: 3.3503e-04 - val_loss: 3.2382e-04 - 62s/epoch - 42ms/step
Epoch 28/200
1493/1493 - 62s - loss: 3.2064e-04 - val_loss: 3.9327e-04 - 62s/epoch - 41ms/step
Epoch 29/200
1493/1493 - 62s - loss: 3.0774e-04 - val_loss: 4.6310e-04 - 62s/epoch - 41ms/step
Epoch 30/200
1493/1493 - 62s - loss: 3.2025e-04 - val_loss: 3.3209e-04 - 62s/epoch - 42ms/step
Epoch 31/200
1493/1493 - 62s - loss: 2.9105e-04 - val_loss: 6.4829e-04 - 62s/epoch - 42ms/step
Epoch 32/200
1493/1493 - 62s - loss: 3.2506e-04 - val_loss: 3.7623e-04 - 62s/epoch - 42ms/step
Epoch 33/200
1493/1493 - 62s - loss: 2.8979e-04 - val_loss: 2.8507e-04 - 62s/epoch - 41ms/step
Epoch 34/200
1493/1493 - 62s - loss: 2.7076e-04 - val_loss: 2.3406e-04 - 62s/epoch - 42ms/step
Epoch 35/200
1493/1493 - 62s - loss: 2.6069e-04 - val_loss: 7.2006e-04 - 62s/epoch - 42ms/step
Epoch 36/200
1493/1493 - 62s - loss: 3.0285e-04 - val_loss: 2.9008e-04 - 62s/epoch - 41ms/step
Epoch 37/200
1493/1493 - 62s - loss: 2.5567e-04 - val_loss: 2.7232e-04 - 62s/epoch - 41ms/step
Epoch 38/200
1493/1493 - 62s - loss: 2.5860e-04 - val_loss: 2.4703e-04 - 62s/epoch - 41ms/step
Epoch 39/200
1493/1493 - 62s - loss: 2.4246e-04 - val_loss: 2.4994e-04 - 62s/epoch - 41ms/step
Epoch 40/200
1493/1493 - 62s - loss: 2.3224e-04 - val_loss: 3.3990e-04 - 62s/epoch - 42ms/step
Epoch 41/200
1493/1493 - 62s - loss: 2.3273e-04 - val_loss: 2.0896e-04 - 62s/epoch - 41ms/step
Epoch 42/200
1493/1493 - 62s - loss: 2.2728e-04 - val_loss: 2.3571e-04 - 62s/epoch - 42ms/step
Epoch 43/200
1493/1493 - 62s - loss: 2.3668e-04 - val_loss: 3.4006e-04 - 62s/epoch - 41ms/step
Epoch 44/200
1493/1493 - 62s - loss: 2.3079e-04 - val_loss: 7.2728e-04 - 62s/epoch - 42ms/step
Epoch 45/200
1493/1493 - 62s - loss: 2.6106e-04 - val_loss: 1.8788e-04 - 62s/epoch - 42ms/step
Epoch 46/200
1493/1493 - 62s - loss: 2.1347e-04 - val_loss: 2.0943e-04 - 62s/epoch - 41ms/step
Epoch 47/200
1493/1493 - 62s - loss: 2.1002e-04 - val_loss: 2.6673e-04 - 62s/epoch - 41ms/step
Epoch 48/200
1493/1493 - 62s - loss: 2.1125e-04 - val_loss: 2.5341e-04 - 62s/epoch - 41ms/step
Epoch 49/200
1493/1493 - 62s - loss: 2.0716e-04 - val_loss: 8.2407e-04 - 62s/epoch - 41ms/step
Epoch 50/200
1493/1493 - 62s - loss: 2.9635e-04 - val_loss: 0.0011 - 62s/epoch - 41ms/step
Epoch 51/200
1493/1493 - 62s - loss: 2.7140e-04 - val_loss: 2.3061e-04 - 62s/epoch - 41ms/step
Epoch 52/200
1493/1493 - 62s - loss: 2.0332e-04 - val_loss: 2.1448e-04 - 62s/epoch - 41ms/step
Epoch 53/200
1493/1493 - 62s - loss: 2.0000e-04 - val_loss: 1.8293e-04 - 62s/epoch - 42ms/step
Epoch 54/200
1493/1493 - 62s - loss: 1.9234e-04 - val_loss: 2.3324e-04 - 62s/epoch - 42ms/step
Epoch 55/200
1493/1493 - 62s - loss: 1.8892e-04 - val_loss: 1.9249e-04 - 62s/epoch - 41ms/step
Epoch 56/200
1493/1493 - 62s - loss: 1.8718e-04 - val_loss: 2.2845e-04 - 62s/epoch - 42ms/step
Epoch 57/200
1493/1493 - 62s - loss: 1.8379e-04 - val_loss: 2.0207e-04 - 62s/epoch - 41ms/step
Epoch 58/200
1493/1493 - 62s - loss: 1.8292e-04 - val_loss: 1.6450e-04 - 62s/epoch - 42ms/step
Epoch 59/200
1493/1493 - 62s - loss: 1.7762e-04 - val_loss: 1.8982e-04 - 62s/epoch - 42ms/step
Epoch 60/200
1493/1493 - 62s - loss: 1.7714e-04 - val_loss: 3.5905e-04 - 62s/epoch - 42ms/step
Epoch 61/200
1493/1493 - 62s - loss: 2.6956e-04 - val_loss: 1.7357e-04 - 62s/epoch - 41ms/step
Epoch 62/200
1493/1493 - 62s - loss: 1.7709e-04 - val_loss: 1.8964e-04 - 62s/epoch - 41ms/step
Epoch 63/200
1493/1493 - 62s - loss: 1.7330e-04 - val_loss: 2.0575e-04 - 62s/epoch - 41ms/step
Epoch 64/200
1493/1493 - 62s - loss: 1.7343e-04 - val_loss: 0.0011 - 62s/epoch - 41ms/step
Epoch 65/200
1493/1493 - 62s - loss: 2.5046e-04 - val_loss: 6.1991e-04 - 62s/epoch - 41ms/step
Epoch 66/200
1493/1493 - 62s - loss: 2.0867e-04 - val_loss: 1.8204e-04 - 62s/epoch - 41ms/step
Epoch 67/200
1493/1493 - 62s - loss: 1.7264e-04 - val_loss: 1.6262e-04 - 62s/epoch - 42ms/step
Epoch 68/200
1493/1493 - 62s - loss: 1.6873e-04 - val_loss: 4.8611e-04 - 62s/epoch - 42ms/step
Epoch 69/200
1493/1493 - 62s - loss: 2.1803e-04 - val_loss: 1.8380e-04 - 62s/epoch - 41ms/step
Epoch 70/200
1493/1493 - 62s - loss: 1.6452e-04 - val_loss: 1.8363e-04 - 62s/epoch - 41ms/step
Epoch 71/200
1493/1493 - 62s - loss: 1.6400e-04 - val_loss: 1.6127e-04 - 62s/epoch - 41ms/step
Epoch 72/200
1493/1493 - 61s - loss: 1.6195e-04 - val_loss: 1.7232e-04 - 61s/epoch - 41ms/step
Epoch 73/200
1493/1493 - 62s - loss: 1.6323e-04 - val_loss: 3.9724e-04 - 62s/epoch - 41ms/step
Epoch 74/200
1493/1493 - 61s - loss: 1.9029e-04 - val_loss: 1.5159e-04 - 61s/epoch - 41ms/step
Epoch 75/200
1493/1493 - 62s - loss: 1.5530e-04 - val_loss: 1.5581e-04 - 62s/epoch - 41ms/step
Epoch 76/200
1493/1493 - 61s - loss: 1.5356e-04 - val_loss: 1.4028e-04 - 61s/epoch - 41ms/step
Epoch 77/200
1493/1493 - 62s - loss: 1.4987e-04 - val_loss: 3.0099e-04 - 62s/epoch - 41ms/step
Epoch 78/200
1493/1493 - 61s - loss: 1.5747e-04 - val_loss: 2.5112e-04 - 61s/epoch - 41ms/step
Epoch 79/200
1493/1493 - 62s - loss: 1.5836e-04 - val_loss: 3.6111e-04 - 62s/epoch - 41ms/step
Epoch 80/200
1493/1493 - 62s - loss: 1.8199e-04 - val_loss: 1.6752e-04 - 62s/epoch - 41ms/step
Epoch 81/200
1493/1493 - 61s - loss: 1.5027e-04 - val_loss: 3.0942e-04 - 61s/epoch - 41ms/step
Epoch 82/200
1493/1493 - 61s - loss: 1.5351e-04 - val_loss: 1.4506e-04 - 61s/epoch - 41ms/step
Epoch 83/200
1493/1493 - 62s - loss: 1.4425e-04 - val_loss: 1.5383e-04 - 62s/epoch - 41ms/step
Epoch 84/200
1493/1493 - 62s - loss: 1.4350e-04 - val_loss: 1.6299e-04 - 62s/epoch - 41ms/step
Epoch 85/200
1493/1493 - 61s - loss: 1.4246e-04 - val_loss: 1.7231e-04 - 61s/epoch - 41ms/step
Epoch 86/200
1493/1493 - 62s - loss: 1.4210e-04 - val_loss: 1.7770e-04 - 62s/epoch - 41ms/step
Epoch 87/200
1493/1493 - 61s - loss: 1.4026e-04 - val_loss: 1.4789e-04 - 61s/epoch - 41ms/step
Epoch 88/200
1493/1493 - 62s - loss: 1.4074e-04 - val_loss: 1.5315e-04 - 62s/epoch - 41ms/step
Epoch 89/200
1493/1493 - 62s - loss: 1.4559e-04 - val_loss: 1.5537e-04 - 62s/epoch - 41ms/step
Epoch 90/200
1493/1493 - 62s - loss: 1.3623e-04 - val_loss: 1.5379e-04 - 62s/epoch - 41ms/step
Epoch 91/200
1493/1493 - 61s - loss: 1.3577e-04 - val_loss: 1.5791e-04 - 61s/epoch - 41ms/step
Epoch 92/200
1493/1493 - 61s - loss: 1.3571e-04 - val_loss: 5.2789e-04 - 61s/epoch - 41ms/step
Epoch 93/200
1493/1493 - 62s - loss: 1.8648e-04 - val_loss: 2.0080e-04 - 62s/epoch - 41ms/step
Epoch 94/200
1493/1493 - 62s - loss: 1.4558e-04 - val_loss: 1.1801e-04 - 62s/epoch - 41ms/step
Epoch 95/200
1493/1493 - 62s - loss: 1.3499e-04 - val_loss: 4.4554e-04 - 62s/epoch - 41ms/step
Epoch 96/200
1493/1493 - 62s - loss: 1.5334e-04 - val_loss: 1.5667e-04 - 62s/epoch - 41ms/step
Epoch 97/200
1493/1493 - 61s - loss: 1.3644e-04 - val_loss: 1.4239e-04 - 61s/epoch - 41ms/step
Epoch 98/200
1493/1493 - 62s - loss: 1.3486e-04 - val_loss: 4.0001e-04 - 62s/epoch - 41ms/step
Epoch 99/200
1493/1493 - 61s - loss: 1.7922e-04 - val_loss: 1.2066e-04 - 61s/epoch - 41ms/step
Epoch 100/200
1493/1493 - 62s - loss: 1.3823e-04 - val_loss: 1.3133e-04 - 62s/epoch - 41ms/step
Epoch 101/200
1493/1493 - 61s - loss: 1.3108e-04 - val_loss: 1.6949e-04 - 61s/epoch - 41ms/step
Epoch 102/200
1493/1493 - 61s - loss: 1.3514e-04 - val_loss: 2.5297e-04 - 61s/epoch - 41ms/step
Epoch 103/200
1493/1493 - 61s - loss: 1.4257e-04 - val_loss: 1.3818e-04 - 61s/epoch - 41ms/step
Epoch 104/200
1493/1493 - 62s - loss: 1.2865e-04 - val_loss: 3.9381e-04 - 62s/epoch - 41ms/step
Epoch 105/200
1493/1493 - 61s - loss: 1.4637e-04 - val_loss: 1.2200e-04 - 61s/epoch - 41ms/step
Epoch 106/200
1493/1493 - 62s - loss: 1.2696e-04 - val_loss: 1.4273e-04 - 62s/epoch - 41ms/step
Epoch 107/200
1493/1493 - 61s - loss: 1.2596e-04 - val_loss: 1.3842e-04 - 61s/epoch - 41ms/step
Epoch 108/200
1493/1493 - 62s - loss: 1.2697e-04 - val_loss: 1.4334e-04 - 62s/epoch - 41ms/step
Epoch 109/200
1493/1493 - 61s - loss: 1.2457e-04 - val_loss: 1.4773e-04 - 61s/epoch - 41ms/step
Epoch 110/200
1493/1493 - 62s - loss: 1.2277e-04 - val_loss: 1.4165e-04 - 62s/epoch - 41ms/step
Epoch 111/200
1493/1493 - 61s - loss: 1.2264e-04 - val_loss: 2.1719e-04 - 61s/epoch - 41ms/step
Epoch 112/200
1493/1493 - 62s - loss: 1.3493e-04 - val_loss: 1.1841e-04 - 62s/epoch - 41ms/step
Epoch 113/200
1493/1493 - 61s - loss: 1.2087e-04 - val_loss: 2.8296e-04 - 61s/epoch - 41ms/step
Epoch 114/200
1493/1493 - 62s - loss: 1.4735e-04 - val_loss: 1.2092e-04 - 62s/epoch - 41ms/step
Epoch 115/200
1493/1493 - 61s - loss: 1.2353e-04 - val_loss: 1.8707e-04 - 61s/epoch - 41ms/step
Epoch 116/200
1493/1493 - 61s - loss: 1.2159e-04 - val_loss: 1.4528e-04 - 61s/epoch - 41ms/step
Epoch 117/200
1493/1493 - 62s - loss: 1.1951e-04 - val_loss: 2.9457e-04 - 62s/epoch - 41ms/step
Epoch 118/200
1493/1493 - 61s - loss: 1.4020e-04 - val_loss: 1.2023e-04 - 61s/epoch - 41ms/step
Epoch 119/200
1493/1493 - 62s - loss: 1.2036e-04 - val_loss: 1.1709e-04 - 62s/epoch - 41ms/step
Epoch 120/200
1493/1493 - 62s - loss: 1.1890e-04 - val_loss: 1.2827e-04 - 62s/epoch - 41ms/step
Epoch 121/200
1493/1493 - 62s - loss: 1.1776e-04 - val_loss: 2.2236e-04 - 62s/epoch - 41ms/step
Epoch 122/200
1493/1493 - 62s - loss: 1.2250e-04 - val_loss: 1.0849e-04 - 62s/epoch - 41ms/step
Epoch 123/200
1493/1493 - 62s - loss: 1.1564e-04 - val_loss: 1.2659e-04 - 62s/epoch - 41ms/step
Epoch 124/200
1493/1493 - 61s - loss: 1.1622e-04 - val_loss: 3.7806e-04 - 61s/epoch - 41ms/step
Epoch 125/200
1493/1493 - 61s - loss: 1.5341e-04 - val_loss: 1.7449e-04 - 61s/epoch - 41ms/step
Epoch 126/200
1493/1493 - 61s - loss: 1.2609e-04 - val_loss: 3.6846e-04 - 61s/epoch - 41ms/step
Epoch 127/200
1493/1493 - 62s - loss: 1.4866e-04 - val_loss: 4.4298e-04 - 62s/epoch - 41ms/step
Epoch 128/200
1493/1493 - 62s - loss: 1.3608e-04 - val_loss: 1.0274e-04 - 62s/epoch - 41ms/step
Epoch 129/200
1493/1493 - 61s - loss: 1.1565e-04 - val_loss: 1.2047e-04 - 61s/epoch - 41ms/step
Epoch 130/200
1493/1493 - 62s - loss: 1.1423e-04 - val_loss: 1.1185e-04 - 62s/epoch - 41ms/step
Epoch 131/200
1493/1493 - 61s - loss: 1.1671e-04 - val_loss: 3.4274e-04 - 61s/epoch - 41ms/step
Epoch 132/200
1493/1493 - 62s - loss: 1.4879e-04 - val_loss: 1.0845e-04 - 62s/epoch - 41ms/step
Epoch 133/200
1493/1493 - 62s - loss: 1.1537e-04 - val_loss: 1.1238e-04 - 62s/epoch - 41ms/step
Epoch 134/200
1493/1493 - 61s - loss: 1.1226e-04 - val_loss: 1.1683e-04 - 61s/epoch - 41ms/step
Epoch 135/200
1493/1493 - 62s - loss: 1.1210e-04 - val_loss: 1.0613e-04 - 62s/epoch - 41ms/step
Epoch 136/200
1493/1493 - 62s - loss: 1.1091e-04 - val_loss: 1.1979e-04 - 62s/epoch - 41ms/step
Epoch 137/200
1493/1493 - 62s - loss: 1.1627e-04 - val_loss: 1.5853e-04 - 62s/epoch - 41ms/step
Epoch 138/200
1493/1493 - 61s - loss: 1.2134e-04 - val_loss: 1.4290e-04 - 61s/epoch - 41ms/step
Epoch 139/200
1493/1493 - 62s - loss: 1.1930e-04 - val_loss: 2.5889e-04 - 62s/epoch - 41ms/step
Epoch 140/200
1493/1493 - 62s - loss: 1.3433e-04 - val_loss: 1.0748e-04 - 62s/epoch - 41ms/step
Epoch 141/200
1493/1493 - 62s - loss: 1.1163e-04 - val_loss: 1.6354e-04 - 62s/epoch - 41ms/step
Epoch 142/200
1493/1493 - 61s - loss: 1.1729e-04 - val_loss: 1.2548e-04 - 61s/epoch - 41ms/step
Epoch 143/200
1493/1493 - 61s - loss: 1.0868e-04 - val_loss: 1.3144e-04 - 61s/epoch - 41ms/step
Epoch 144/200
1493/1493 - 62s - loss: 1.1204e-04 - val_loss: 1.1566e-04 - 62s/epoch - 41ms/step
Epoch 145/200
1493/1493 - 62s - loss: 1.0783e-04 - val_loss: 1.0845e-04 - 62s/epoch - 41ms/step
Epoch 146/200
1493/1493 - 62s - loss: 1.0701e-04 - val_loss: 1.0820e-04 - 62s/epoch - 41ms/step
Epoch 147/200
1493/1493 - 62s - loss: 1.0537e-04 - val_loss: 1.2861e-04 - 62s/epoch - 41ms/step
Epoch 148/200
1493/1493 - 62s - loss: 1.0582e-04 - val_loss: 1.8152e-04 - 62s/epoch - 41ms/step
Epoch 149/200
1493/1493 - 62s - loss: 1.0534e-04 - val_loss: 1.1718e-04 - 62s/epoch - 41ms/step
Epoch 150/200
1493/1493 - 62s - loss: 1.0434e-04 - val_loss: 1.0311e-04 - 62s/epoch - 41ms/step
Epoch 151/200
1493/1493 - 62s - loss: 1.0924e-04 - val_loss: 1.2280e-04 - 62s/epoch - 41ms/step
Epoch 152/200
1493/1493 - 61s - loss: 1.0592e-04 - val_loss: 2.6271e-04 - 61s/epoch - 41ms/step
Epoch 153/200
1493/1493 - 62s - loss: 1.2138e-04 - val_loss: 1.1298e-04 - 62s/epoch - 41ms/step
Epoch 154/200
1493/1493 - 63s - loss: 1.0413e-04 - val_loss: 1.1648e-04 - 63s/epoch - 42ms/step
Epoch 155/200
1493/1493 - 63s - loss: 1.0517e-04 - val_loss: 1.0694e-04 - 63s/epoch - 42ms/step
Epoch 156/200
1493/1493 - 62s - loss: 1.0782e-04 - val_loss: 1.1250e-04 - 62s/epoch - 42ms/step
Epoch 157/200
1493/1493 - 63s - loss: 1.0158e-04 - val_loss: 1.1723e-04 - 63s/epoch - 42ms/step
Epoch 158/200
1493/1493 - 63s - loss: 1.0318e-04 - val_loss: 1.2249e-04 - 63s/epoch - 42ms/step
Epoch 159/200
1493/1493 - 62s - loss: 1.0157e-04 - val_loss: 1.0541e-04 - 62s/epoch - 42ms/step
Epoch 160/200
1493/1493 - 62s - loss: 1.0097e-04 - val_loss: 1.1309e-04 - 62s/epoch - 42ms/step
Epoch 161/200
1493/1493 - 62s - loss: 1.0629e-04 - val_loss: 1.1601e-04 - 62s/epoch - 42ms/step
Epoch 162/200
1493/1493 - 62s - loss: 1.0013e-04 - val_loss: 9.5637e-05 - 62s/epoch - 42ms/step
Epoch 163/200
1493/1493 - 62s - loss: 1.0048e-04 - val_loss: 1.5635e-04 - 62s/epoch - 42ms/step
Epoch 164/200
1493/1493 - 62s - loss: 1.0769e-04 - val_loss: 1.0698e-04 - 62s/epoch - 42ms/step
Epoch 165/200
1493/1493 - 62s - loss: 1.2616e-04 - val_loss: 1.0270e-04 - 62s/epoch - 42ms/step
Epoch 166/200
1493/1493 - 62s - loss: 1.0149e-04 - val_loss: 1.0829e-04 - 62s/epoch - 42ms/step
Epoch 167/200
1493/1493 - 62s - loss: 1.0034e-04 - val_loss: 9.2938e-05 - 62s/epoch - 42ms/step
Epoch 168/200
1493/1493 - 62s - loss: 9.9423e-05 - val_loss: 1.0229e-04 - 62s/epoch - 42ms/step
Epoch 169/200
1493/1493 - 62s - loss: 9.9424e-05 - val_loss: 1.1918e-04 - 62s/epoch - 42ms/step
Epoch 170/200
1493/1493 - 62s - loss: 1.0073e-04 - val_loss: 3.0752e-04 - 62s/epoch - 42ms/step
Epoch 171/200
1493/1493 - 62s - loss: 1.4489e-04 - val_loss: 5.4486e-04 - 62s/epoch - 42ms/step
Epoch 172/200
1493/1493 - 62s - loss: 1.6909e-04 - val_loss: 1.5962e-04 - 62s/epoch - 42ms/step
Epoch 173/200
1493/1493 - 62s - loss: 1.1452e-04 - val_loss: 1.0038e-04 - 62s/epoch - 42ms/step
Epoch 174/200
1493/1493 - 62s - loss: 1.0336e-04 - val_loss: 9.3160e-05 - 62s/epoch - 42ms/step
Epoch 175/200
1493/1493 - 62s - loss: 1.0243e-04 - val_loss: 1.0948e-04 - 62s/epoch - 42ms/step
Epoch 176/200
1493/1493 - 62s - loss: 1.0296e-04 - val_loss: 1.1168e-04 - 62s/epoch - 42ms/step
Epoch 177/200
1493/1493 - 62s - loss: 1.0076e-04 - val_loss: 9.4144e-05 - 62s/epoch - 42ms/step
Epoch 178/200
1493/1493 - 62s - loss: 9.8956e-05 - val_loss: 1.0393e-04 - 62s/epoch - 42ms/step
Epoch 179/200
1493/1493 - 62s - loss: 9.7146e-05 - val_loss: 1.1357e-04 - 62s/epoch - 42ms/step
Epoch 180/200
1493/1493 - 62s - loss: 9.8150e-05 - val_loss: 1.7770e-04 - 62s/epoch - 42ms/step
Epoch 181/200
1493/1493 - 62s - loss: 1.1140e-04 - val_loss: 1.0211e-04 - 62s/epoch - 42ms/step
Epoch 182/200
1493/1493 - 62s - loss: 9.9683e-05 - val_loss: 1.2693e-04 - 62s/epoch - 42ms/step
Epoch 183/200
1493/1493 - 62s - loss: 9.9625e-05 - val_loss: 1.1075e-04 - 62s/epoch - 42ms/step
Epoch 184/200
1493/1493 - 62s - loss: 9.7833e-05 - val_loss: 9.7919e-05 - 62s/epoch - 42ms/step
Epoch 185/200
1493/1493 - 62s - loss: 9.4789e-05 - val_loss: 1.4327e-04 - 62s/epoch - 42ms/step
Epoch 186/200
1493/1493 - 62s - loss: 9.7683e-05 - val_loss: 1.0075e-04 - 62s/epoch - 42ms/step
Epoch 187/200
1493/1493 - 62s - loss: 9.5202e-05 - val_loss: 1.4011e-04 - 62s/epoch - 42ms/step
Epoch 188/200
1493/1493 - 62s - loss: 1.1187e-04 - val_loss: 1.1738e-04 - 62s/epoch - 42ms/step
Epoch 189/200
1493/1493 - 62s - loss: 9.8601e-05 - val_loss: 1.0364e-04 - 62s/epoch - 42ms/step
Epoch 190/200
1493/1493 - 62s - loss: 9.6511e-05 - val_loss: 1.3170e-04 - 62s/epoch - 42ms/step
Epoch 191/200
1493/1493 - 62s - loss: 9.8233e-05 - val_loss: 1.3223e-04 - 62s/epoch - 42ms/step
Epoch 192/200
1493/1493 - 62s - loss: 9.7193e-05 - val_loss: 1.1456e-04 - 62s/epoch - 41ms/step
Epoch 193/200
1493/1493 - 62s - loss: 9.8546e-05 - val_loss: 3.1620e-04 - 62s/epoch - 42ms/step
Epoch 194/200
1493/1493 - 62s - loss: 1.4093e-04 - val_loss: 6.3997e-04 - 62s/epoch - 42ms/step
Epoch 195/200
1493/1493 - 62s - loss: 1.4634e-04 - val_loss: 2.6426e-04 - 62s/epoch - 42ms/step
Epoch 196/200
1493/1493 - 62s - loss: 1.2692e-04 - val_loss: 9.1834e-05 - 62s/epoch - 42ms/step
Epoch 197/200
1493/1493 - 62s - loss: 1.0056e-04 - val_loss: 1.2345e-04 - 62s/epoch - 42ms/step
Epoch 198/200
1493/1493 - 62s - loss: 1.0600e-04 - val_loss: 1.4477e-04 - 62s/epoch - 42ms/step
Epoch 199/200
1493/1493 - 62s - loss: 1.0424e-04 - val_loss: 8.3480e-05 - 62s/epoch - 42ms/step
Epoch 200/200
1493/1493 - 62s - loss: 9.5953e-05 - val_loss: 9.4950e-05 - 62s/epoch - 42ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 9.495028643868864e-05
  1/332 [..............................] - ETA: 38s  7/332 [..............................] - ETA: 2s  14/332 [>.............................] - ETA: 2s 22/332 [>.............................] - ETA: 2s 30/332 [=>............................] - ETA: 2s 39/332 [==>...........................] - ETA: 2s 48/332 [===>..........................] - ETA: 1s 57/332 [====>.........................] - ETA: 1s 66/332 [====>.........................] - ETA: 1s 74/332 [=====>........................] - ETA: 1s 83/332 [======>.......................] - ETA: 1s 92/332 [=======>......................] - ETA: 1s100/332 [========>.....................] - ETA: 1s109/332 [========>.....................] - ETA: 1s118/332 [=========>....................] - ETA: 1s127/332 [==========>...................] - ETA: 1s135/332 [===========>..................] - ETA: 1s144/332 [============>.................] - ETA: 1s153/332 [============>.................] - ETA: 1s162/332 [=============>................] - ETA: 1s170/332 [==============>...............] - ETA: 1s179/332 [===============>..............] - ETA: 0s188/332 [===============>..............] - ETA: 0s196/332 [================>.............] - ETA: 0s205/332 [=================>............] - ETA: 0s213/332 [==================>...........] - ETA: 0s222/332 [===================>..........] - ETA: 0s231/332 [===================>..........] - ETA: 0s240/332 [====================>.........] - ETA: 0s249/332 [=====================>........] - ETA: 0s258/332 [======================>.......] - ETA: 0s267/332 [=======================>......] - ETA: 0s276/332 [=======================>......] - ETA: 0s285/332 [========================>.....] - ETA: 0s294/332 [=========================>....] - ETA: 0s303/332 [==========================>...] - ETA: 0s312/332 [===========================>..] - ETA: 0s321/332 [============================>.] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 6ms/step
correlation 0.0010788419192303722
cosine 0.0008500624873527616
MAE: 0.005396405
RMSE: 0.009744238
r2: 0.9938417195227113
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       multiple                  0         
                                                                 
 dense_21 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_21 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1757592   
                                                                 
 batch_normalization_22 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 632)               0         
                                                                 
 dense_22 (Dense)            (None, 2780)              1759740   
                                                                 
 batch_normalization_23 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2780)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 10,573,984
Trainable params: 10,561,600
Non-trainable params: 12,384
_________________________________________________________________
Encoder
Model: "model_22"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_23 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_22 (InputLayer)       multiple                  0         
                                                                 
 dense_21 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_21 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1757592   
                                                                 
=================================================================
Total params: 5,285,412
Trainable params: 5,279,852
Non-trainable params: 5,560
_________________________________________________________________
Decoder
Model: "model_23"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_24 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_22 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 632)               0         
                                                                 
 dense_22 (Dense)            (None, 2780)              1759740   
                                                                 
 batch_normalization_23 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2780)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 5,288,572
Trainable params: 5,281,748
Non-trainable params: 6,824
_________________________________________________________________
['2.2custom_n_b', 'mse', 64, 200, 0.0005, 0.5, 632, 9.595313895260915e-05, 9.495028643868864e-05, 0.0010788419192303722, 0.0008500624873527616, 0.005396404769271612, 0.00974423810839653, 0.9938417195227113, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats2_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_24"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_25 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_24 (Dense)            (None, 2907)              3677355   
                                                                 
 batch_normalization_24 (Bat  (None, 2907)             11628     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2907)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1837856   
                                                                 
 batch_normalization_25 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 632)               0         
                                                                 
 dense_25 (Dense)            (None, 2907)              1840131   
                                                                 
 batch_normalization_26 (Bat  (None, 2907)             11628     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2907)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3675712   
                                                                 
=================================================================
Total params: 11,056,838
Trainable params: 11,043,946
Non-trainable params: 12,892
_________________________________________________________________
Epoch 1/200
1493/1493 - 67s - loss: 0.0094 - val_loss: 0.0042 - 67s/epoch - 45ms/step
Epoch 2/200
1493/1493 - 67s - loss: 0.0030 - val_loss: 0.0041 - 67s/epoch - 45ms/step
Epoch 3/200
1493/1493 - 67s - loss: 0.0022 - val_loss: 0.0017 - 67s/epoch - 45ms/step
Epoch 4/200
1493/1493 - 67s - loss: 0.0017 - val_loss: 0.0027 - 67s/epoch - 45ms/step
Epoch 5/200
1493/1493 - 67s - loss: 0.0017 - val_loss: 0.0014 - 67s/epoch - 45ms/step
Epoch 6/200
1493/1493 - 67s - loss: 0.0014 - val_loss: 0.0014 - 67s/epoch - 45ms/step
Epoch 7/200
1493/1493 - 67s - loss: 0.0014 - val_loss: 0.0020 - 67s/epoch - 45ms/step
Epoch 8/200
1493/1493 - 67s - loss: 0.0012 - val_loss: 0.0016 - 67s/epoch - 45ms/step
Epoch 9/200
1493/1493 - 67s - loss: 0.0011 - val_loss: 0.0010 - 67s/epoch - 45ms/step
Epoch 10/200
1493/1493 - 67s - loss: 9.5247e-04 - val_loss: 9.1379e-04 - 67s/epoch - 45ms/step
Epoch 11/200
1493/1493 - 67s - loss: 8.8449e-04 - val_loss: 8.0706e-04 - 67s/epoch - 45ms/step
Epoch 12/200
1493/1493 - 67s - loss: 7.5905e-04 - val_loss: 9.3400e-04 - 67s/epoch - 45ms/step
Epoch 13/200
1493/1493 - 67s - loss: 7.3547e-04 - val_loss: 0.0010 - 67s/epoch - 45ms/step
Epoch 14/200
1493/1493 - 67s - loss: 7.1109e-04 - val_loss: 5.3713e-04 - 67s/epoch - 45ms/step
Epoch 15/200
1493/1493 - 67s - loss: 6.2175e-04 - val_loss: 5.7405e-04 - 67s/epoch - 45ms/step
Epoch 16/200
1493/1493 - 67s - loss: 5.6331e-04 - val_loss: 5.0435e-04 - 67s/epoch - 45ms/step
Epoch 17/200
1493/1493 - 67s - loss: 5.2522e-04 - val_loss: 6.5726e-04 - 67s/epoch - 45ms/step
Epoch 18/200
1493/1493 - 67s - loss: 4.9764e-04 - val_loss: 9.0581e-04 - 67s/epoch - 45ms/step
Epoch 19/200
1493/1493 - 67s - loss: 5.0336e-04 - val_loss: 4.5695e-04 - 67s/epoch - 45ms/step
Epoch 20/200
1493/1493 - 67s - loss: 4.4831e-04 - val_loss: 4.1223e-04 - 67s/epoch - 45ms/step
Epoch 21/200
1493/1493 - 67s - loss: 4.1956e-04 - val_loss: 6.2034e-04 - 67s/epoch - 45ms/step
Epoch 22/200
1493/1493 - 67s - loss: 4.1285e-04 - val_loss: 4.5394e-04 - 67s/epoch - 45ms/step
Epoch 23/200
1493/1493 - 67s - loss: 4.0525e-04 - val_loss: 6.5405e-04 - 67s/epoch - 45ms/step
Epoch 24/200
1493/1493 - 67s - loss: 3.9660e-04 - val_loss: 3.5205e-04 - 67s/epoch - 45ms/step
Epoch 25/200
1493/1493 - 67s - loss: 3.6401e-04 - val_loss: 3.3648e-04 - 67s/epoch - 45ms/step
Epoch 26/200
1493/1493 - 67s - loss: 3.3933e-04 - val_loss: 4.2894e-04 - 67s/epoch - 45ms/step
Epoch 27/200
1493/1493 - 67s - loss: 3.2691e-04 - val_loss: 3.4158e-04 - 67s/epoch - 45ms/step
Epoch 28/200
1493/1493 - 67s - loss: 3.0839e-04 - val_loss: 9.3223e-04 - 67s/epoch - 45ms/step
Epoch 29/200
1493/1493 - 67s - loss: 3.2019e-04 - val_loss: 2.9161e-04 - 67s/epoch - 45ms/step
Epoch 30/200
1493/1493 - 67s - loss: 2.9061e-04 - val_loss: 3.5630e-04 - 67s/epoch - 45ms/step
Epoch 31/200
1493/1493 - 67s - loss: 2.8412e-04 - val_loss: 4.5979e-04 - 67s/epoch - 45ms/step
Epoch 32/200
1493/1493 - 67s - loss: 3.4429e-04 - val_loss: 5.6770e-04 - 67s/epoch - 45ms/step
Epoch 33/200
1493/1493 - 67s - loss: 3.1957e-04 - val_loss: 2.7230e-04 - 67s/epoch - 45ms/step
Epoch 34/200
1493/1493 - 67s - loss: 2.7168e-04 - val_loss: 2.4709e-04 - 67s/epoch - 45ms/step
Epoch 35/200
1493/1493 - 67s - loss: 2.5894e-04 - val_loss: 3.9247e-04 - 67s/epoch - 45ms/step
Epoch 36/200
1493/1493 - 67s - loss: 2.6633e-04 - val_loss: 3.8537e-04 - 67s/epoch - 45ms/step
Epoch 37/200
1493/1493 - 67s - loss: 2.5136e-04 - val_loss: 4.9634e-04 - 67s/epoch - 45ms/step
Epoch 38/200
1493/1493 - 67s - loss: 2.7328e-04 - val_loss: 2.2755e-04 - 67s/epoch - 45ms/step
Epoch 39/200
1493/1493 - 67s - loss: 2.3806e-04 - val_loss: 2.3129e-04 - 67s/epoch - 45ms/step
Epoch 40/200
1493/1493 - 67s - loss: 2.3207e-04 - val_loss: 3.0820e-04 - 67s/epoch - 45ms/step
Epoch 41/200
1493/1493 - 67s - loss: 2.3753e-04 - val_loss: 2.1664e-04 - 67s/epoch - 45ms/step
Epoch 42/200
1493/1493 - 67s - loss: 2.2390e-04 - val_loss: 2.2308e-04 - 67s/epoch - 45ms/step
Epoch 43/200
1493/1493 - 67s - loss: 2.2310e-04 - val_loss: 3.9915e-04 - 67s/epoch - 45ms/step
Epoch 44/200
1493/1493 - 67s - loss: 2.3773e-04 - val_loss: 3.5581e-04 - 67s/epoch - 45ms/step
Epoch 45/200
1493/1493 - 67s - loss: 2.2853e-04 - val_loss: 1.8788e-04 - 67s/epoch - 45ms/step
Epoch 46/200
1493/1493 - 67s - loss: 2.1046e-04 - val_loss: 2.0769e-04 - 67s/epoch - 45ms/step
Epoch 47/200
1493/1493 - 67s - loss: 2.0791e-04 - val_loss: 2.9211e-04 - 67s/epoch - 45ms/step
Epoch 48/200
1493/1493 - 67s - loss: 2.0755e-04 - val_loss: 2.5105e-04 - 67s/epoch - 45ms/step
Epoch 49/200
1493/1493 - 67s - loss: 2.0328e-04 - val_loss: 7.1966e-04 - 67s/epoch - 45ms/step
Epoch 50/200
1493/1493 - 67s - loss: 2.5664e-04 - val_loss: 0.0011 - 67s/epoch - 45ms/step
Epoch 51/200
1493/1493 - 67s - loss: 2.7899e-04 - val_loss: 2.0259e-04 - 67s/epoch - 45ms/step
Epoch 52/200
1493/1493 - 67s - loss: 2.0064e-04 - val_loss: 2.0243e-04 - 67s/epoch - 45ms/step
Epoch 53/200
1493/1493 - 67s - loss: 1.9807e-04 - val_loss: 1.7499e-04 - 67s/epoch - 45ms/step
Epoch 54/200
1493/1493 - 67s - loss: 1.9336e-04 - val_loss: 2.2574e-04 - 67s/epoch - 45ms/step
Epoch 55/200
1493/1493 - 67s - loss: 1.8520e-04 - val_loss: 1.9031e-04 - 67s/epoch - 45ms/step
Epoch 56/200
1493/1493 - 67s - loss: 1.8457e-04 - val_loss: 2.2874e-04 - 67s/epoch - 45ms/step
Epoch 57/200
1493/1493 - 67s - loss: 1.8043e-04 - val_loss: 2.0057e-04 - 67s/epoch - 45ms/step
Epoch 58/200
1493/1493 - 67s - loss: 1.7877e-04 - val_loss: 1.6940e-04 - 67s/epoch - 45ms/step
Epoch 59/200
1493/1493 - 67s - loss: 1.7452e-04 - val_loss: 1.8876e-04 - 67s/epoch - 45ms/step
Epoch 60/200
1493/1493 - 67s - loss: 1.7488e-04 - val_loss: 2.7228e-04 - 67s/epoch - 45ms/step
Epoch 61/200
1493/1493 - 67s - loss: 1.9768e-04 - val_loss: 1.7506e-04 - 67s/epoch - 45ms/step
Epoch 62/200
1493/1493 - 67s - loss: 1.7136e-04 - val_loss: 1.9354e-04 - 67s/epoch - 45ms/step
Epoch 63/200
1493/1493 - 67s - loss: 1.6797e-04 - val_loss: 2.2269e-04 - 67s/epoch - 45ms/step
Epoch 64/200
1493/1493 - 67s - loss: 1.7388e-04 - val_loss: 0.0022 - 67s/epoch - 45ms/step
Epoch 65/200
1493/1493 - 67s - loss: 3.3828e-04 - val_loss: 5.4068e-04 - 67s/epoch - 45ms/step
Epoch 66/200
1493/1493 - 67s - loss: 2.1280e-04 - val_loss: 1.7993e-04 - 67s/epoch - 45ms/step
Epoch 67/200
1493/1493 - 67s - loss: 1.7561e-04 - val_loss: 1.5938e-04 - 67s/epoch - 45ms/step
Epoch 68/200
1493/1493 - 67s - loss: 1.6883e-04 - val_loss: 2.4875e-04 - 67s/epoch - 45ms/step
Epoch 69/200
1493/1493 - 67s - loss: 1.8241e-04 - val_loss: 1.8275e-04 - 67s/epoch - 45ms/step
Epoch 70/200
1493/1493 - 67s - loss: 1.6406e-04 - val_loss: 2.2029e-04 - 67s/epoch - 45ms/step
Epoch 71/200
1493/1493 - 67s - loss: 1.6841e-04 - val_loss: 1.6639e-04 - 67s/epoch - 45ms/step
Epoch 72/200
1493/1493 - 67s - loss: 1.6274e-04 - val_loss: 2.0264e-04 - 67s/epoch - 45ms/step
Epoch 73/200
1493/1493 - 67s - loss: 1.6424e-04 - val_loss: 1.8990e-04 - 67s/epoch - 45ms/step
Epoch 74/200
1493/1493 - 67s - loss: 1.6439e-04 - val_loss: 1.5026e-04 - 67s/epoch - 45ms/step
Epoch 75/200
1493/1493 - 67s - loss: 1.5390e-04 - val_loss: 1.6119e-04 - 67s/epoch - 45ms/step
Epoch 76/200
1493/1493 - 67s - loss: 1.5175e-04 - val_loss: 1.6806e-04 - 67s/epoch - 45ms/step
Epoch 77/200
1493/1493 - 67s - loss: 1.4801e-04 - val_loss: 3.2878e-04 - 67s/epoch - 45ms/step
Epoch 78/200
1493/1493 - 67s - loss: 1.5566e-04 - val_loss: 1.8609e-04 - 67s/epoch - 45ms/step
Epoch 79/200
1493/1493 - 67s - loss: 1.5043e-04 - val_loss: 2.3451e-04 - 67s/epoch - 45ms/step
Epoch 80/200
1493/1493 - 67s - loss: 1.6036e-04 - val_loss: 1.4416e-04 - 67s/epoch - 45ms/step
Epoch 81/200
1493/1493 - 67s - loss: 1.4671e-04 - val_loss: 3.3758e-04 - 67s/epoch - 45ms/step
Epoch 82/200
1493/1493 - 67s - loss: 1.4921e-04 - val_loss: 1.5313e-04 - 67s/epoch - 45ms/step
Epoch 83/200
1493/1493 - 67s - loss: 1.4208e-04 - val_loss: 1.6843e-04 - 67s/epoch - 45ms/step
Epoch 84/200
1493/1493 - 67s - loss: 1.4103e-04 - val_loss: 1.5861e-04 - 67s/epoch - 45ms/step
Epoch 85/200
1493/1493 - 67s - loss: 1.4700e-04 - val_loss: 1.6871e-04 - 67s/epoch - 45ms/step
Epoch 86/200
1493/1493 - 67s - loss: 1.4256e-04 - val_loss: 1.5262e-04 - 67s/epoch - 45ms/step
Epoch 87/200
1493/1493 - 67s - loss: 1.3699e-04 - val_loss: 1.3584e-04 - 67s/epoch - 45ms/step
Epoch 88/200
1493/1493 - 67s - loss: 1.3599e-04 - val_loss: 1.6489e-04 - 67s/epoch - 45ms/step
Epoch 89/200
1493/1493 - 67s - loss: 1.4040e-04 - val_loss: 1.4547e-04 - 67s/epoch - 45ms/step
Epoch 90/200
1493/1493 - 67s - loss: 1.3512e-04 - val_loss: 1.5709e-04 - 67s/epoch - 45ms/step
Epoch 91/200
1493/1493 - 67s - loss: 1.3403e-04 - val_loss: 1.5757e-04 - 67s/epoch - 45ms/step
Epoch 92/200
1493/1493 - 67s - loss: 1.3359e-04 - val_loss: 4.5153e-04 - 67s/epoch - 45ms/step
Epoch 93/200
1493/1493 - 67s - loss: 1.6979e-04 - val_loss: 1.7885e-04 - 67s/epoch - 45ms/step
Epoch 94/200
1493/1493 - 67s - loss: 1.4062e-04 - val_loss: 1.1815e-04 - 67s/epoch - 45ms/step
Epoch 95/200
1493/1493 - 67s - loss: 1.3321e-04 - val_loss: 1.5971e-04 - 67s/epoch - 45ms/step
Epoch 96/200
1493/1493 - 67s - loss: 1.3254e-04 - val_loss: 1.5808e-04 - 67s/epoch - 45ms/step
Epoch 97/200
1493/1493 - 67s - loss: 1.3077e-04 - val_loss: 1.3482e-04 - 67s/epoch - 45ms/step
Epoch 98/200
1493/1493 - 67s - loss: 1.3611e-04 - val_loss: 5.6160e-04 - 67s/epoch - 45ms/step
Epoch 99/200
1493/1493 - 67s - loss: 2.1798e-04 - val_loss: 1.1789e-04 - 67s/epoch - 45ms/step
Epoch 100/200
1493/1493 - 67s - loss: 1.5704e-04 - val_loss: 1.2126e-04 - 67s/epoch - 45ms/step
Epoch 101/200
1493/1493 - 67s - loss: 1.3443e-04 - val_loss: 1.4198e-04 - 67s/epoch - 45ms/step
Epoch 102/200
1493/1493 - 67s - loss: 1.3426e-04 - val_loss: 2.0949e-04 - 67s/epoch - 45ms/step
Epoch 103/200
1493/1493 - 67s - loss: 1.3611e-04 - val_loss: 1.2869e-04 - 67s/epoch - 45ms/step
Epoch 104/200
1493/1493 - 67s - loss: 1.2838e-04 - val_loss: 2.5088e-04 - 67s/epoch - 45ms/step
Epoch 105/200
1493/1493 - 66s - loss: 1.4109e-04 - val_loss: 1.2236e-04 - 66s/epoch - 44ms/step
Epoch 106/200
1493/1493 - 66s - loss: 1.2655e-04 - val_loss: 1.4031e-04 - 66s/epoch - 44ms/step
Epoch 107/200
1493/1493 - 66s - loss: 1.2542e-04 - val_loss: 1.4211e-04 - 66s/epoch - 44ms/step
Epoch 108/200
1493/1493 - 66s - loss: 1.2940e-04 - val_loss: 1.3792e-04 - 66s/epoch - 44ms/step
Epoch 109/200
1493/1493 - 66s - loss: 1.2444e-04 - val_loss: 1.5112e-04 - 66s/epoch - 44ms/step
Epoch 110/200
1493/1493 - 66s - loss: 1.2225e-04 - val_loss: 1.4245e-04 - 66s/epoch - 44ms/step
Epoch 111/200
1493/1493 - 66s - loss: 1.2206e-04 - val_loss: 1.5311e-04 - 66s/epoch - 44ms/step
Epoch 112/200
1493/1493 - 66s - loss: 1.2901e-04 - val_loss: 1.2061e-04 - 66s/epoch - 44ms/step
Epoch 113/200
1493/1493 - 66s - loss: 1.1960e-04 - val_loss: 2.7641e-04 - 66s/epoch - 44ms/step
Epoch 114/200
1493/1493 - 66s - loss: 1.4161e-04 - val_loss: 1.1174e-04 - 66s/epoch - 44ms/step
Epoch 115/200
1493/1493 - 66s - loss: 1.2268e-04 - val_loss: 2.0299e-04 - 66s/epoch - 44ms/step
Epoch 116/200
1493/1493 - 66s - loss: 1.2431e-04 - val_loss: 1.4917e-04 - 66s/epoch - 44ms/step
Epoch 117/200
1493/1493 - 66s - loss: 1.1877e-04 - val_loss: 1.7502e-04 - 66s/epoch - 44ms/step
Epoch 118/200
1493/1493 - 65s - loss: 1.2383e-04 - val_loss: 1.3659e-04 - 65s/epoch - 44ms/step
Epoch 119/200
1493/1493 - 66s - loss: 1.1894e-04 - val_loss: 1.1947e-04 - 66s/epoch - 44ms/step
Epoch 120/200
1493/1493 - 66s - loss: 1.1690e-04 - val_loss: 1.2664e-04 - 66s/epoch - 44ms/step
Epoch 121/200
1493/1493 - 65s - loss: 1.1649e-04 - val_loss: 1.8393e-04 - 65s/epoch - 44ms/step
Epoch 122/200
1493/1493 - 66s - loss: 1.1708e-04 - val_loss: 1.1175e-04 - 66s/epoch - 44ms/step
Epoch 123/200
1493/1493 - 66s - loss: 1.1508e-04 - val_loss: 1.3183e-04 - 66s/epoch - 44ms/step
Epoch 124/200
1493/1493 - 65s - loss: 1.1693e-04 - val_loss: 4.0049e-04 - 65s/epoch - 44ms/step
Epoch 125/200
1493/1493 - 65s - loss: 1.7965e-04 - val_loss: 1.2579e-04 - 65s/epoch - 44ms/step
Epoch 126/200
1493/1493 - 66s - loss: 1.2119e-04 - val_loss: 1.5041e-04 - 66s/epoch - 44ms/step
Epoch 127/200
1493/1493 - 65s - loss: 1.2124e-04 - val_loss: 7.0006e-04 - 65s/epoch - 44ms/step
Epoch 128/200
1493/1493 - 66s - loss: 1.7395e-04 - val_loss: 1.1177e-04 - 66s/epoch - 44ms/step
Epoch 129/200
1493/1493 - 65s - loss: 1.2154e-04 - val_loss: 1.1326e-04 - 65s/epoch - 44ms/step
Epoch 130/200
1493/1493 - 65s - loss: 1.1618e-04 - val_loss: 1.1041e-04 - 65s/epoch - 44ms/step
Epoch 131/200
1493/1493 - 65s - loss: 1.1674e-04 - val_loss: 2.0082e-04 - 65s/epoch - 44ms/step
Epoch 132/200
1493/1493 - 65s - loss: 1.2809e-04 - val_loss: 1.1348e-04 - 65s/epoch - 44ms/step
Epoch 133/200
1493/1493 - 65s - loss: 1.1372e-04 - val_loss: 1.4563e-04 - 65s/epoch - 44ms/step
Epoch 134/200
1493/1493 - 65s - loss: 1.1690e-04 - val_loss: 1.2351e-04 - 65s/epoch - 44ms/step
Epoch 135/200
1493/1493 - 65s - loss: 1.1144e-04 - val_loss: 1.0734e-04 - 65s/epoch - 44ms/step
Epoch 136/200
1493/1493 - 65s - loss: 1.1132e-04 - val_loss: 2.0473e-04 - 65s/epoch - 44ms/step
Epoch 137/200
1493/1493 - 65s - loss: 1.3131e-04 - val_loss: 1.5592e-04 - 65s/epoch - 44ms/step
Epoch 138/200
1493/1493 - 65s - loss: 1.1919e-04 - val_loss: 1.4773e-04 - 65s/epoch - 44ms/step
Epoch 139/200
1493/1493 - 65s - loss: 1.2419e-04 - val_loss: 2.4033e-04 - 65s/epoch - 44ms/step
Epoch 140/200
1493/1493 - 65s - loss: 1.4236e-04 - val_loss: 1.0540e-04 - 65s/epoch - 44ms/step
Epoch 141/200
1493/1493 - 65s - loss: 1.1215e-04 - val_loss: 1.3687e-04 - 65s/epoch - 44ms/step
Epoch 142/200
1493/1493 - 65s - loss: 1.1439e-04 - val_loss: 1.1301e-04 - 65s/epoch - 44ms/step
Epoch 143/200
1493/1493 - 65s - loss: 1.0894e-04 - val_loss: 1.3451e-04 - 65s/epoch - 44ms/step
Epoch 144/200
1493/1493 - 65s - loss: 1.1345e-04 - val_loss: 1.0560e-04 - 65s/epoch - 44ms/step
Epoch 145/200
1493/1493 - 65s - loss: 1.0766e-04 - val_loss: 1.0230e-04 - 65s/epoch - 44ms/step
Epoch 146/200
1493/1493 - 65s - loss: 1.0746e-04 - val_loss: 1.0801e-04 - 65s/epoch - 44ms/step
Epoch 147/200
1493/1493 - 65s - loss: 1.0561e-04 - val_loss: 1.2043e-04 - 65s/epoch - 44ms/step
Epoch 148/200
1493/1493 - 65s - loss: 1.0549e-04 - val_loss: 1.7220e-04 - 65s/epoch - 44ms/step
Epoch 149/200
1493/1493 - 65s - loss: 1.0506e-04 - val_loss: 1.0645e-04 - 65s/epoch - 44ms/step
Epoch 150/200
1493/1493 - 65s - loss: 1.0389e-04 - val_loss: 1.0103e-04 - 65s/epoch - 44ms/step
Epoch 151/200
1493/1493 - 65s - loss: 1.0650e-04 - val_loss: 1.4336e-04 - 65s/epoch - 44ms/step
Epoch 152/200
1493/1493 - 65s - loss: 1.0609e-04 - val_loss: 1.6643e-04 - 65s/epoch - 44ms/step
Epoch 153/200
1493/1493 - 65s - loss: 1.0939e-04 - val_loss: 1.0558e-04 - 65s/epoch - 44ms/step
Epoch 154/200
1493/1493 - 65s - loss: 1.0248e-04 - val_loss: 1.2078e-04 - 65s/epoch - 44ms/step
Epoch 155/200
1493/1493 - 65s - loss: 1.0252e-04 - val_loss: 1.1192e-04 - 65s/epoch - 44ms/step
Epoch 156/200
1493/1493 - 65s - loss: 1.0875e-04 - val_loss: 1.1440e-04 - 65s/epoch - 44ms/step
Epoch 157/200
1493/1493 - 65s - loss: 1.0164e-04 - val_loss: 1.2214e-04 - 65s/epoch - 44ms/step
Epoch 158/200
1493/1493 - 65s - loss: 1.0199e-04 - val_loss: 1.1672e-04 - 65s/epoch - 44ms/step
Epoch 159/200
1493/1493 - 65s - loss: 1.0094e-04 - val_loss: 9.7384e-05 - 65s/epoch - 44ms/step
Epoch 160/200
1493/1493 - 65s - loss: 1.0062e-04 - val_loss: 1.0775e-04 - 65s/epoch - 44ms/step
Epoch 161/200
1493/1493 - 65s - loss: 1.0507e-04 - val_loss: 1.0109e-04 - 65s/epoch - 44ms/step
Epoch 162/200
1493/1493 - 65s - loss: 1.0051e-04 - val_loss: 1.0382e-04 - 65s/epoch - 44ms/step
Epoch 163/200
1493/1493 - 65s - loss: 9.9925e-05 - val_loss: 1.2375e-04 - 65s/epoch - 44ms/step
Epoch 164/200
1493/1493 - 65s - loss: 1.0285e-04 - val_loss: 1.0830e-04 - 65s/epoch - 44ms/step
Epoch 165/200
1493/1493 - 65s - loss: 1.1578e-04 - val_loss: 1.0600e-04 - 65s/epoch - 44ms/step
Epoch 166/200
1493/1493 - 65s - loss: 1.0093e-04 - val_loss: 1.1711e-04 - 65s/epoch - 44ms/step
Epoch 167/200
1493/1493 - 65s - loss: 9.9735e-05 - val_loss: 9.4796e-05 - 65s/epoch - 44ms/step
Epoch 168/200
1493/1493 - 65s - loss: 9.9041e-05 - val_loss: 1.0341e-04 - 65s/epoch - 44ms/step
Epoch 169/200
1493/1493 - 65s - loss: 9.8970e-05 - val_loss: 1.1706e-04 - 65s/epoch - 44ms/step
Epoch 170/200
1493/1493 - 65s - loss: 9.9871e-05 - val_loss: 2.3136e-04 - 65s/epoch - 44ms/step
Epoch 171/200
1493/1493 - 65s - loss: 1.3081e-04 - val_loss: 3.8381e-04 - 65s/epoch - 44ms/step
Epoch 172/200
1493/1493 - 65s - loss: 1.3703e-04 - val_loss: 1.2350e-04 - 65s/epoch - 44ms/step
Epoch 173/200
1493/1493 - 65s - loss: 1.0491e-04 - val_loss: 9.8958e-05 - 65s/epoch - 44ms/step
Epoch 174/200
1493/1493 - 65s - loss: 9.9192e-05 - val_loss: 1.0284e-04 - 65s/epoch - 44ms/step
Epoch 175/200
1493/1493 - 65s - loss: 1.0018e-04 - val_loss: 1.1862e-04 - 65s/epoch - 44ms/step
Epoch 176/200
1493/1493 - 65s - loss: 9.9998e-05 - val_loss: 1.0668e-04 - 65s/epoch - 44ms/step
Epoch 177/200
1493/1493 - 65s - loss: 1.0095e-04 - val_loss: 9.2809e-05 - 65s/epoch - 44ms/step
Epoch 178/200
1493/1493 - 65s - loss: 9.8392e-05 - val_loss: 9.7591e-05 - 65s/epoch - 44ms/step
Epoch 179/200
1493/1493 - 65s - loss: 9.6542e-05 - val_loss: 1.1334e-04 - 65s/epoch - 44ms/step
Epoch 180/200
1493/1493 - 65s - loss: 9.8033e-05 - val_loss: 1.2469e-04 - 65s/epoch - 44ms/step
Epoch 181/200
1493/1493 - 65s - loss: 9.9517e-05 - val_loss: 1.1003e-04 - 65s/epoch - 44ms/step
Epoch 182/200
1493/1493 - 65s - loss: 9.6331e-05 - val_loss: 1.3086e-04 - 65s/epoch - 44ms/step
Epoch 183/200
1493/1493 - 65s - loss: 9.5900e-05 - val_loss: 1.1264e-04 - 65s/epoch - 44ms/step
Epoch 184/200
1493/1493 - 65s - loss: 9.5958e-05 - val_loss: 9.6210e-05 - 65s/epoch - 44ms/step
Epoch 185/200
1493/1493 - 65s - loss: 9.3726e-05 - val_loss: 1.5256e-04 - 65s/epoch - 44ms/step
Epoch 186/200
1493/1493 - 65s - loss: 1.0190e-04 - val_loss: 9.6966e-05 - 65s/epoch - 44ms/step
Epoch 187/200
1493/1493 - 65s - loss: 9.4167e-05 - val_loss: 1.2448e-04 - 65s/epoch - 44ms/step
Epoch 188/200
1493/1493 - 65s - loss: 1.0583e-04 - val_loss: 1.1157e-04 - 65s/epoch - 44ms/step
Epoch 189/200
1493/1493 - 65s - loss: 9.5746e-05 - val_loss: 1.1468e-04 - 65s/epoch - 44ms/step
Epoch 190/200
1493/1493 - 65s - loss: 9.5925e-05 - val_loss: 1.0555e-04 - 65s/epoch - 44ms/step
Epoch 191/200
1493/1493 - 65s - loss: 9.7673e-05 - val_loss: 1.4206e-04 - 65s/epoch - 44ms/step
Epoch 192/200
1493/1493 - 65s - loss: 9.7780e-05 - val_loss: 1.1882e-04 - 65s/epoch - 44ms/step
Epoch 193/200
1493/1493 - 65s - loss: 9.6814e-05 - val_loss: 2.4771e-04 - 65s/epoch - 44ms/step
Epoch 194/200
1493/1493 - 65s - loss: 1.2441e-04 - val_loss: 5.1219e-04 - 65s/epoch - 44ms/step
Epoch 195/200
1493/1493 - 65s - loss: 1.8351e-04 - val_loss: 2.4549e-04 - 65s/epoch - 44ms/step
Epoch 196/200
1493/1493 - 65s - loss: 1.3199e-04 - val_loss: 8.7055e-05 - 65s/epoch - 44ms/step
Epoch 197/200
1493/1493 - 65s - loss: 1.0352e-04 - val_loss: 1.2384e-04 - 65s/epoch - 44ms/step
Epoch 198/200
1493/1493 - 65s - loss: 1.0598e-04 - val_loss: 1.3287e-04 - 65s/epoch - 44ms/step
Epoch 199/200
1493/1493 - 66s - loss: 1.0108e-04 - val_loss: 8.5522e-05 - 66s/epoch - 44ms/step
Epoch 200/200
1493/1493 - 65s - loss: 9.6116e-05 - val_loss: 9.7773e-05 - 65s/epoch - 44ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 9.777311788639054e-05
  1/332 [..............................] - ETA: 52s  7/332 [..............................] - ETA: 2s  14/332 [>.............................] - ETA: 2s 22/332 [>.............................] - ETA: 2s 30/332 [=>............................] - ETA: 2s 38/332 [==>...........................] - ETA: 2s 46/332 [===>..........................] - ETA: 2s 54/332 [===>..........................] - ETA: 1s 62/332 [====>.........................] - ETA: 1s 70/332 [=====>........................] - ETA: 1s 78/332 [======>.......................] - ETA: 1s 86/332 [======>.......................] - ETA: 1s 93/332 [=======>......................] - ETA: 1s101/332 [========>.....................] - ETA: 1s109/332 [========>.....................] - ETA: 1s117/332 [=========>....................] - ETA: 1s125/332 [==========>...................] - ETA: 1s133/332 [===========>..................] - ETA: 1s141/332 [===========>..................] - ETA: 1s149/332 [============>.................] - ETA: 1s157/332 [=============>................] - ETA: 1s165/332 [=============>................] - ETA: 1s173/332 [==============>...............] - ETA: 1s181/332 [===============>..............] - ETA: 1s189/332 [================>.............] - ETA: 0s197/332 [================>.............] - ETA: 0s205/332 [=================>............] - ETA: 0s213/332 [==================>...........] - ETA: 0s221/332 [==================>...........] - ETA: 0s229/332 [===================>..........] - ETA: 0s237/332 [====================>.........] - ETA: 0s245/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s261/332 [======================>.......] - ETA: 0s269/332 [=======================>......] - ETA: 0s277/332 [========================>.....] - ETA: 0s285/332 [========================>.....] - ETA: 0s293/332 [=========================>....] - ETA: 0s301/332 [==========================>...] - ETA: 0s309/332 [==========================>...] - ETA: 0s317/332 [===========================>..] - ETA: 0s325/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 7ms/step
correlation 0.001110716575817751
cosine 0.0008752623617177274
MAE: 0.0055040726
RMSE: 0.009888025
r2: 0.9936583771181708
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_24"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_25 (InputLayer)       multiple                  0         
                                                                 
 dense_24 (Dense)            (None, 2907)              3677355   
                                                                 
 batch_normalization_24 (Bat  (None, 2907)             11628     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2907)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1837856   
                                                                 
 batch_normalization_25 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 632)               0         
                                                                 
 dense_25 (Dense)            (None, 2907)              1840131   
                                                                 
 batch_normalization_26 (Bat  (None, 2907)             11628     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2907)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3675712   
                                                                 
=================================================================
Total params: 11,056,838
Trainable params: 11,043,946
Non-trainable params: 12,892
_________________________________________________________________
Encoder
Model: "model_25"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_26 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_25 (InputLayer)       multiple                  0         
                                                                 
 dense_24 (Dense)            (None, 2907)              3677355   
                                                                 
 batch_normalization_24 (Bat  (None, 2907)             11628     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2907)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1837856   
                                                                 
=================================================================
Total params: 5,526,839
Trainable params: 5,521,025
Non-trainable params: 5,814
_________________________________________________________________
Decoder
Model: "model_26"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_27 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_25 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 632)               0         
                                                                 
 dense_25 (Dense)            (None, 2907)              1840131   
                                                                 
 batch_normalization_26 (Bat  (None, 2907)             11628     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2907)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3675712   
                                                                 
=================================================================
Total params: 5,529,999
Trainable params: 5,522,921
Non-trainable params: 7,078
_________________________________________________________________
['2.3custom_n_b', 'mse', 64, 200, 0.0005, 0.5, 632, 9.611560381017625e-05, 9.777311788639054e-05, 0.001110716575817751, 0.0008752623617177274, 0.005504072643816471, 0.009888025000691414, 0.9936583771181708, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats2_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_27"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_27 (Dense)            (None, 3033)              3836745   
                                                                 
 batch_normalization_27 (Bat  (None, 3033)             12132     
 chNormalization)                                                
                                                                 
 re_lu_27 (ReLU)             (None, 3033)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1917488   
                                                                 
 batch_normalization_28 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_28 (ReLU)             (None, 632)               0         
                                                                 
 dense_28 (Dense)            (None, 3033)              1919889   
                                                                 
 batch_normalization_29 (Bat  (None, 3033)             12132     
 chNormalization)                                                
                                                                 
 re_lu_29 (ReLU)             (None, 3033)              0         
                                                                 
 dense_29 (Dense)            (None, 1264)              3834976   
                                                                 
=================================================================
Total params: 11,535,890
Trainable params: 11,522,494
Non-trainable params: 13,396
_________________________________________________________________
Epoch 1/200
1493/1493 - 79s - loss: 0.0094 - val_loss: 0.0038 - 79s/epoch - 53ms/step
Epoch 2/200
1493/1493 - 78s - loss: 0.0030 - val_loss: 0.0038 - 78s/epoch - 52ms/step
Epoch 3/200
1493/1493 - 78s - loss: 0.0021 - val_loss: 0.0017 - 78s/epoch - 52ms/step
Epoch 4/200
1493/1493 - 78s - loss: 0.0018 - val_loss: 0.0058 - 78s/epoch - 52ms/step
Epoch 5/200
1493/1493 - 78s - loss: 0.0018 - val_loss: 0.0015 - 78s/epoch - 52ms/step
Epoch 6/200
1493/1493 - 78s - loss: 0.0015 - val_loss: 0.0014 - 78s/epoch - 52ms/step
Epoch 7/200
1493/1493 - 78s - loss: 0.0014 - val_loss: 0.0024 - 78s/epoch - 52ms/step
Epoch 8/200
1493/1493 - 78s - loss: 0.0013 - val_loss: 0.0011 - 78s/epoch - 52ms/step
Epoch 9/200
1493/1493 - 78s - loss: 0.0011 - val_loss: 0.0010 - 78s/epoch - 52ms/step
Epoch 10/200
1493/1493 - 78s - loss: 9.7193e-04 - val_loss: 8.9478e-04 - 78s/epoch - 52ms/step
Epoch 11/200
1493/1493 - 78s - loss: 8.8093e-04 - val_loss: 9.1231e-04 - 78s/epoch - 52ms/step
Epoch 12/200
1493/1493 - 78s - loss: 7.3903e-04 - val_loss: 8.1967e-04 - 78s/epoch - 52ms/step
Epoch 13/200
1493/1493 - 78s - loss: 7.0275e-04 - val_loss: 0.0012 - 78s/epoch - 52ms/step
Epoch 14/200
1493/1493 - 78s - loss: 7.1550e-04 - val_loss: 5.8185e-04 - 78s/epoch - 52ms/step
Epoch 15/200
1493/1493 - 78s - loss: 6.2411e-04 - val_loss: 0.0012 - 78s/epoch - 52ms/step
Epoch 16/200
1493/1493 - 78s - loss: 5.8127e-04 - val_loss: 5.1826e-04 - 78s/epoch - 52ms/step
Epoch 17/200
1493/1493 - 78s - loss: 5.1241e-04 - val_loss: 6.0533e-04 - 78s/epoch - 52ms/step
Epoch 18/200
1493/1493 - 78s - loss: 4.9506e-04 - val_loss: 6.9600e-04 - 78s/epoch - 52ms/step
Epoch 19/200
1493/1493 - 78s - loss: 4.7568e-04 - val_loss: 5.0647e-04 - 78s/epoch - 52ms/step
Epoch 20/200
1493/1493 - 78s - loss: 4.4214e-04 - val_loss: 3.9959e-04 - 78s/epoch - 52ms/step
Epoch 21/200
1493/1493 - 78s - loss: 4.0823e-04 - val_loss: 5.3681e-04 - 78s/epoch - 52ms/step
Epoch 22/200
1493/1493 - 78s - loss: 3.9983e-04 - val_loss: 5.3458e-04 - 78s/epoch - 52ms/step
Epoch 23/200
1493/1493 - 78s - loss: 4.0936e-04 - val_loss: 5.8229e-04 - 78s/epoch - 52ms/step
Epoch 24/200
1493/1493 - 78s - loss: 3.8636e-04 - val_loss: 3.3304e-04 - 78s/epoch - 52ms/step
Epoch 25/200
1493/1493 - 78s - loss: 3.4251e-04 - val_loss: 3.2471e-04 - 78s/epoch - 52ms/step
Epoch 26/200
1493/1493 - 78s - loss: 3.3271e-04 - val_loss: 3.9118e-04 - 78s/epoch - 52ms/step
Epoch 27/200
1493/1493 - 78s - loss: 3.1461e-04 - val_loss: 3.3211e-04 - 78s/epoch - 52ms/step
Epoch 28/200
1493/1493 - 78s - loss: 3.0288e-04 - val_loss: 6.0570e-04 - 78s/epoch - 52ms/step
Epoch 29/200
1493/1493 - 78s - loss: 2.9449e-04 - val_loss: 4.6783e-04 - 78s/epoch - 52ms/step
Epoch 30/200
1493/1493 - 78s - loss: 3.1119e-04 - val_loss: 3.3188e-04 - 78s/epoch - 53ms/step
Epoch 31/200
1493/1493 - 78s - loss: 2.7865e-04 - val_loss: 5.4149e-04 - 78s/epoch - 53ms/step
Epoch 32/200
1493/1493 - 78s - loss: 2.9486e-04 - val_loss: 3.1823e-04 - 78s/epoch - 52ms/step
Epoch 33/200
1493/1493 - 78s - loss: 2.7295e-04 - val_loss: 2.7388e-04 - 78s/epoch - 52ms/step
Epoch 34/200
1493/1493 - 78s - loss: 2.5873e-04 - val_loss: 2.5292e-04 - 78s/epoch - 52ms/step
Epoch 35/200
1493/1493 - 78s - loss: 2.4928e-04 - val_loss: 9.5732e-04 - 78s/epoch - 52ms/step
Epoch 36/200
1493/1493 - 78s - loss: 3.0317e-04 - val_loss: 2.9955e-04 - 78s/epoch - 52ms/step
Epoch 37/200
1493/1493 - 78s - loss: 2.4666e-04 - val_loss: 2.8966e-04 - 78s/epoch - 52ms/step
Epoch 38/200
1493/1493 - 78s - loss: 2.5036e-04 - val_loss: 2.3146e-04 - 78s/epoch - 52ms/step
Epoch 39/200
1493/1493 - 78s - loss: 2.3159e-04 - val_loss: 2.3247e-04 - 78s/epoch - 52ms/step
Epoch 40/200
1493/1493 - 78s - loss: 2.2519e-04 - val_loss: 3.1032e-04 - 78s/epoch - 52ms/step
Epoch 41/200
1493/1493 - 78s - loss: 2.2818e-04 - val_loss: 2.1066e-04 - 78s/epoch - 52ms/step
Epoch 42/200
1493/1493 - 78s - loss: 2.1890e-04 - val_loss: 2.1954e-04 - 78s/epoch - 52ms/step
Epoch 43/200
1493/1493 - 78s - loss: 2.2098e-04 - val_loss: 4.9563e-04 - 78s/epoch - 52ms/step
Epoch 44/200
1493/1493 - 78s - loss: 2.3795e-04 - val_loss: 4.7390e-04 - 78s/epoch - 52ms/step
Epoch 45/200
1493/1493 - 78s - loss: 2.3692e-04 - val_loss: 1.7846e-04 - 78s/epoch - 52ms/step
Epoch 46/200
1493/1493 - 78s - loss: 2.0543e-04 - val_loss: 2.0324e-04 - 78s/epoch - 52ms/step
Epoch 47/200
1493/1493 - 78s - loss: 2.0249e-04 - val_loss: 2.8637e-04 - 78s/epoch - 52ms/step
Epoch 48/200
1493/1493 - 78s - loss: 2.0362e-04 - val_loss: 2.3884e-04 - 78s/epoch - 52ms/step
Epoch 49/200
1493/1493 - 78s - loss: 1.9909e-04 - val_loss: 5.1417e-04 - 78s/epoch - 52ms/step
Epoch 50/200
1493/1493 - 78s - loss: 2.5163e-04 - val_loss: 5.0870e-04 - 78s/epoch - 52ms/step
Epoch 51/200
1493/1493 - 78s - loss: 2.2381e-04 - val_loss: 1.9534e-04 - 78s/epoch - 52ms/step
Epoch 52/200
1493/1493 - 78s - loss: 1.9209e-04 - val_loss: 2.1633e-04 - 78s/epoch - 52ms/step
Epoch 53/200
1493/1493 - 78s - loss: 1.9079e-04 - val_loss: 1.9063e-04 - 78s/epoch - 52ms/step
Epoch 54/200
1493/1493 - 78s - loss: 1.8462e-04 - val_loss: 2.3386e-04 - 78s/epoch - 52ms/step
Epoch 55/200
1493/1493 - 78s - loss: 1.7997e-04 - val_loss: 1.7775e-04 - 78s/epoch - 52ms/step
Epoch 56/200
1493/1493 - 78s - loss: 1.7859e-04 - val_loss: 2.0319e-04 - 78s/epoch - 52ms/step
Epoch 57/200
1493/1493 - 78s - loss: 1.7620e-04 - val_loss: 1.9815e-04 - 78s/epoch - 52ms/step
Epoch 58/200
1493/1493 - 78s - loss: 1.7409e-04 - val_loss: 1.6574e-04 - 78s/epoch - 52ms/step
Epoch 59/200
1493/1493 - 78s - loss: 1.7012e-04 - val_loss: 1.9062e-04 - 78s/epoch - 52ms/step
Epoch 60/200
1493/1493 - 78s - loss: 1.6836e-04 - val_loss: 4.0009e-04 - 78s/epoch - 52ms/step
Epoch 61/200
1493/1493 - 78s - loss: 2.6959e-04 - val_loss: 1.6686e-04 - 78s/epoch - 52ms/step
Epoch 62/200
1493/1493 - 78s - loss: 1.7278e-04 - val_loss: 1.9504e-04 - 78s/epoch - 52ms/step
Epoch 63/200
1493/1493 - 78s - loss: 1.6695e-04 - val_loss: 2.1754e-04 - 78s/epoch - 52ms/step
Epoch 64/200
1493/1493 - 78s - loss: 1.6820e-04 - val_loss: 0.0016 - 78s/epoch - 52ms/step
Epoch 65/200
1493/1493 - 78s - loss: 2.7978e-04 - val_loss: 3.1342e-04 - 78s/epoch - 52ms/step
Epoch 66/200
1493/1493 - 78s - loss: 1.8811e-04 - val_loss: 2.1423e-04 - 78s/epoch - 52ms/step
Epoch 67/200
1493/1493 - 78s - loss: 1.6942e-04 - val_loss: 1.5654e-04 - 78s/epoch - 52ms/step
Epoch 68/200
1493/1493 - 78s - loss: 1.6331e-04 - val_loss: 2.7302e-04 - 78s/epoch - 52ms/step
Epoch 69/200
1493/1493 - 78s - loss: 1.8398e-04 - val_loss: 1.9427e-04 - 78s/epoch - 52ms/step
Epoch 70/200
1493/1493 - 78s - loss: 1.5920e-04 - val_loss: 1.8375e-04 - 78s/epoch - 52ms/step
Epoch 71/200
1493/1493 - 78s - loss: 1.5745e-04 - val_loss: 1.5203e-04 - 78s/epoch - 52ms/step
Epoch 72/200
1493/1493 - 78s - loss: 1.5651e-04 - val_loss: 2.4579e-04 - 78s/epoch - 52ms/step
Epoch 73/200
1493/1493 - 78s - loss: 1.5816e-04 - val_loss: 1.6753e-04 - 78s/epoch - 52ms/step
Epoch 74/200
1493/1493 - 78s - loss: 1.5440e-04 - val_loss: 1.5541e-04 - 78s/epoch - 52ms/step
Epoch 75/200
1493/1493 - 78s - loss: 1.4797e-04 - val_loss: 1.4628e-04 - 78s/epoch - 52ms/step
Epoch 76/200
1493/1493 - 78s - loss: 1.4648e-04 - val_loss: 1.6268e-04 - 78s/epoch - 52ms/step
Epoch 77/200
1493/1493 - 78s - loss: 1.4311e-04 - val_loss: 4.7416e-04 - 78s/epoch - 52ms/step
Epoch 78/200
1493/1493 - 78s - loss: 1.6077e-04 - val_loss: 1.8548e-04 - 78s/epoch - 52ms/step
Epoch 79/200
1493/1493 - 78s - loss: 1.4824e-04 - val_loss: 3.0751e-04 - 78s/epoch - 52ms/step
Epoch 80/200
1493/1493 - 78s - loss: 1.7412e-04 - val_loss: 1.5966e-04 - 78s/epoch - 52ms/step
Epoch 81/200
1493/1493 - 78s - loss: 1.4527e-04 - val_loss: 2.2767e-04 - 78s/epoch - 52ms/step
Epoch 82/200
1493/1493 - 78s - loss: 1.4375e-04 - val_loss: 1.4545e-04 - 78s/epoch - 52ms/step
Epoch 83/200
1493/1493 - 78s - loss: 1.3887e-04 - val_loss: 1.6599e-04 - 78s/epoch - 52ms/step
Epoch 84/200
1493/1493 - 78s - loss: 1.3753e-04 - val_loss: 1.5713e-04 - 78s/epoch - 52ms/step
Epoch 85/200
1493/1493 - 78s - loss: 1.3766e-04 - val_loss: 1.7509e-04 - 78s/epoch - 52ms/step
Epoch 86/200
1493/1493 - 78s - loss: 1.3741e-04 - val_loss: 1.6621e-04 - 78s/epoch - 52ms/step
Epoch 87/200
1493/1493 - 78s - loss: 1.3366e-04 - val_loss: 1.4149e-04 - 78s/epoch - 52ms/step
Epoch 88/200
1493/1493 - 78s - loss: 1.3281e-04 - val_loss: 1.5976e-04 - 78s/epoch - 52ms/step
Epoch 89/200
1493/1493 - 78s - loss: 1.3919e-04 - val_loss: 1.5370e-04 - 78s/epoch - 52ms/step
Epoch 90/200
1493/1493 - 78s - loss: 1.3174e-04 - val_loss: 1.6773e-04 - 78s/epoch - 52ms/step
Epoch 91/200
1493/1493 - 78s - loss: 1.3004e-04 - val_loss: 1.6031e-04 - 78s/epoch - 52ms/step
Epoch 92/200
1493/1493 - 78s - loss: 1.3021e-04 - val_loss: 4.2985e-04 - 78s/epoch - 52ms/step
Epoch 93/200
1493/1493 - 78s - loss: 1.6796e-04 - val_loss: 2.6631e-04 - 78s/epoch - 52ms/step
Epoch 94/200
1493/1493 - 78s - loss: 1.4511e-04 - val_loss: 1.1621e-04 - 78s/epoch - 52ms/step
Epoch 95/200
1493/1493 - 78s - loss: 1.2982e-04 - val_loss: 2.3539e-04 - 78s/epoch - 52ms/step
Epoch 96/200
1493/1493 - 78s - loss: 1.3970e-04 - val_loss: 1.4896e-04 - 78s/epoch - 52ms/step
Epoch 97/200
1493/1493 - 78s - loss: 1.2925e-04 - val_loss: 1.2266e-04 - 78s/epoch - 52ms/step
Epoch 98/200
1493/1493 - 78s - loss: 1.2974e-04 - val_loss: 6.5297e-04 - 78s/epoch - 52ms/step
Epoch 99/200
1493/1493 - 78s - loss: 1.9598e-04 - val_loss: 1.1951e-04 - 78s/epoch - 52ms/step
Epoch 100/200
1493/1493 - 78s - loss: 1.4165e-04 - val_loss: 1.2115e-04 - 78s/epoch - 52ms/step
Epoch 101/200
1493/1493 - 78s - loss: 1.2915e-04 - val_loss: 2.4963e-04 - 78s/epoch - 52ms/step
Epoch 102/200
1493/1493 - 78s - loss: 1.4344e-04 - val_loss: 1.7432e-04 - 78s/epoch - 52ms/step
Epoch 103/200
1493/1493 - 78s - loss: 1.3684e-04 - val_loss: 1.2562e-04 - 78s/epoch - 53ms/step
Epoch 104/200
1493/1493 - 78s - loss: 1.2544e-04 - val_loss: 3.7413e-04 - 78s/epoch - 52ms/step
Epoch 105/200
1493/1493 - 78s - loss: 1.4661e-04 - val_loss: 1.1769e-04 - 78s/epoch - 52ms/step
Epoch 106/200
1493/1493 - 78s - loss: 1.2378e-04 - val_loss: 1.3545e-04 - 78s/epoch - 52ms/step
Epoch 107/200
1493/1493 - 79s - loss: 1.2286e-04 - val_loss: 1.4133e-04 - 79s/epoch - 53ms/step
Epoch 108/200
1493/1493 - 78s - loss: 1.2448e-04 - val_loss: 1.3347e-04 - 78s/epoch - 52ms/step
Epoch 109/200
1493/1493 - 78s - loss: 1.2218e-04 - val_loss: 1.4650e-04 - 78s/epoch - 52ms/step
Epoch 110/200
1493/1493 - 78s - loss: 1.1981e-04 - val_loss: 1.3901e-04 - 78s/epoch - 52ms/step
Epoch 111/200
1493/1493 - 78s - loss: 1.1886e-04 - val_loss: 1.3486e-04 - 78s/epoch - 52ms/step
Epoch 112/200
1493/1493 - 78s - loss: 1.1880e-04 - val_loss: 1.2051e-04 - 78s/epoch - 52ms/step
Epoch 113/200
1493/1493 - 78s - loss: 1.1642e-04 - val_loss: 3.4292e-04 - 78s/epoch - 52ms/step
Epoch 114/200
1493/1493 - 78s - loss: 1.4357e-04 - val_loss: 1.0810e-04 - 78s/epoch - 52ms/step
Epoch 115/200
1493/1493 - 78s - loss: 1.2259e-04 - val_loss: 1.9545e-04 - 78s/epoch - 52ms/step
Epoch 116/200
1493/1493 - 78s - loss: 1.1885e-04 - val_loss: 1.3216e-04 - 78s/epoch - 52ms/step
Epoch 117/200
1493/1493 - 78s - loss: 1.1569e-04 - val_loss: 3.1453e-04 - 78s/epoch - 52ms/step
Epoch 118/200
1493/1493 - 78s - loss: 1.3188e-04 - val_loss: 1.2098e-04 - 78s/epoch - 52ms/step
Epoch 119/200
1493/1493 - 78s - loss: 1.1658e-04 - val_loss: 1.2209e-04 - 78s/epoch - 52ms/step
Epoch 120/200
1493/1493 - 78s - loss: 1.1414e-04 - val_loss: 1.2202e-04 - 78s/epoch - 52ms/step
Epoch 121/200
1493/1493 - 78s - loss: 1.1416e-04 - val_loss: 1.6498e-04 - 78s/epoch - 52ms/step
Epoch 122/200
1493/1493 - 78s - loss: 1.1530e-04 - val_loss: 1.0508e-04 - 78s/epoch - 52ms/step
Epoch 123/200
1493/1493 - 78s - loss: 1.1230e-04 - val_loss: 1.2466e-04 - 78s/epoch - 52ms/step
Epoch 124/200
1493/1493 - 78s - loss: 1.1232e-04 - val_loss: 3.1739e-04 - 78s/epoch - 52ms/step
Epoch 125/200
1493/1493 - 78s - loss: 1.4895e-04 - val_loss: 1.6745e-04 - 78s/epoch - 52ms/step
Epoch 126/200
1493/1493 - 78s - loss: 1.1850e-04 - val_loss: 2.4091e-04 - 78s/epoch - 52ms/step
Epoch 127/200
1493/1493 - 78s - loss: 1.2492e-04 - val_loss: 3.2463e-04 - 78s/epoch - 52ms/step
Epoch 128/200
1493/1493 - 78s - loss: 1.3452e-04 - val_loss: 1.0337e-04 - 78s/epoch - 52ms/step
Epoch 129/200
1493/1493 - 78s - loss: 1.1264e-04 - val_loss: 1.1423e-04 - 78s/epoch - 52ms/step
Epoch 130/200
1493/1493 - 78s - loss: 1.1073e-04 - val_loss: 1.1594e-04 - 78s/epoch - 52ms/step
Epoch 131/200
1493/1493 - 78s - loss: 1.1279e-04 - val_loss: 3.0894e-04 - 78s/epoch - 52ms/step
Epoch 132/200
1493/1493 - 78s - loss: 1.4389e-04 - val_loss: 1.1023e-04 - 78s/epoch - 52ms/step
Epoch 133/200
1493/1493 - 78s - loss: 1.1228e-04 - val_loss: 1.1068e-04 - 78s/epoch - 52ms/step
Epoch 134/200
1493/1493 - 78s - loss: 1.0863e-04 - val_loss: 1.2311e-04 - 78s/epoch - 52ms/step
Epoch 135/200
1493/1493 - 78s - loss: 1.0885e-04 - val_loss: 1.0500e-04 - 78s/epoch - 52ms/step
Epoch 136/200
1493/1493 - 78s - loss: 1.0790e-04 - val_loss: 1.2930e-04 - 78s/epoch - 52ms/step
Epoch 137/200
1493/1493 - 78s - loss: 1.1565e-04 - val_loss: 1.3006e-04 - 78s/epoch - 52ms/step
Epoch 138/200
1493/1493 - 78s - loss: 1.1053e-04 - val_loss: 1.2138e-04 - 78s/epoch - 52ms/step
Epoch 139/200
1493/1493 - 78s - loss: 1.1066e-04 - val_loss: 1.4406e-04 - 78s/epoch - 52ms/step
Epoch 140/200
1493/1493 - 78s - loss: 1.1766e-04 - val_loss: 1.0142e-04 - 78s/epoch - 52ms/step
Epoch 141/200
1493/1493 - 78s - loss: 1.0639e-04 - val_loss: 1.2719e-04 - 78s/epoch - 52ms/step
Epoch 142/200
1493/1493 - 78s - loss: 1.0881e-04 - val_loss: 1.1886e-04 - 78s/epoch - 52ms/step
Epoch 143/200
1493/1493 - 78s - loss: 1.0459e-04 - val_loss: 1.7365e-04 - 78s/epoch - 52ms/step
Epoch 144/200
1493/1493 - 78s - loss: 1.0771e-04 - val_loss: 1.2115e-04 - 78s/epoch - 52ms/step
Epoch 145/200
1493/1493 - 78s - loss: 1.0462e-04 - val_loss: 1.0819e-04 - 78s/epoch - 52ms/step
Epoch 146/200
1493/1493 - 78s - loss: 1.0297e-04 - val_loss: 1.0040e-04 - 78s/epoch - 52ms/step
Epoch 147/200
1493/1493 - 78s - loss: 1.0166e-04 - val_loss: 1.2227e-04 - 78s/epoch - 52ms/step
Epoch 148/200
1493/1493 - 78s - loss: 1.0402e-04 - val_loss: 1.8295e-04 - 78s/epoch - 52ms/step
Epoch 149/200
1493/1493 - 78s - loss: 1.0166e-04 - val_loss: 1.1887e-04 - 78s/epoch - 52ms/step
Epoch 150/200
1493/1493 - 78s - loss: 1.0110e-04 - val_loss: 1.0189e-04 - 78s/epoch - 52ms/step
Epoch 151/200
1493/1493 - 78s - loss: 1.0255e-04 - val_loss: 1.2962e-04 - 78s/epoch - 52ms/step
Epoch 152/200
1493/1493 - 78s - loss: 1.0213e-04 - val_loss: 1.9349e-04 - 78s/epoch - 52ms/step
Epoch 153/200
1493/1493 - 78s - loss: 1.1174e-04 - val_loss: 1.1342e-04 - 78s/epoch - 52ms/step
Epoch 154/200
1493/1493 - 78s - loss: 1.0108e-04 - val_loss: 1.3331e-04 - 78s/epoch - 52ms/step
Epoch 155/200
1493/1493 - 78s - loss: 1.1089e-04 - val_loss: 9.9929e-05 - 78s/epoch - 52ms/step
Epoch 156/200
1493/1493 - 78s - loss: 1.0360e-04 - val_loss: 1.1362e-04 - 78s/epoch - 52ms/step
Epoch 157/200
1493/1493 - 78s - loss: 9.9280e-05 - val_loss: 1.1765e-04 - 78s/epoch - 52ms/step
Epoch 158/200
1493/1493 - 78s - loss: 1.0072e-04 - val_loss: 1.1387e-04 - 78s/epoch - 52ms/step
Epoch 159/200
1493/1493 - 78s - loss: 9.8915e-05 - val_loss: 1.0284e-04 - 78s/epoch - 52ms/step
Epoch 160/200
1493/1493 - 78s - loss: 1.0085e-04 - val_loss: 1.9096e-04 - 78s/epoch - 52ms/step
Epoch 161/200
1493/1493 - 78s - loss: 1.8872e-04 - val_loss: 1.0144e-04 - 78s/epoch - 52ms/step
Epoch 162/200
1493/1493 - 78s - loss: 1.0825e-04 - val_loss: 9.7114e-05 - 78s/epoch - 52ms/step
Epoch 163/200
1493/1493 - 78s - loss: 1.0348e-04 - val_loss: 2.2474e-04 - 78s/epoch - 52ms/step
Epoch 164/200
1493/1493 - 78s - loss: 1.3211e-04 - val_loss: 1.0233e-04 - 78s/epoch - 52ms/step
Epoch 165/200
1493/1493 - 78s - loss: 1.2774e-04 - val_loss: 1.0016e-04 - 78s/epoch - 52ms/step
Epoch 166/200
1493/1493 - 78s - loss: 1.0272e-04 - val_loss: 1.1119e-04 - 78s/epoch - 52ms/step
Epoch 167/200
1493/1493 - 78s - loss: 1.0048e-04 - val_loss: 9.4102e-05 - 78s/epoch - 52ms/step
Epoch 168/200
1493/1493 - 78s - loss: 9.8738e-05 - val_loss: 1.0539e-04 - 78s/epoch - 52ms/step
Epoch 169/200
1493/1493 - 78s - loss: 9.8655e-05 - val_loss: 1.2275e-04 - 78s/epoch - 52ms/step
Epoch 170/200
1493/1493 - 78s - loss: 9.7337e-05 - val_loss: 1.5420e-04 - 78s/epoch - 52ms/step
Epoch 171/200
1493/1493 - 78s - loss: 1.0786e-04 - val_loss: 3.1434e-04 - 78s/epoch - 52ms/step
Epoch 172/200
1493/1493 - 78s - loss: 1.4216e-04 - val_loss: 1.0146e-04 - 78s/epoch - 52ms/step
Epoch 173/200
1493/1493 - 78s - loss: 1.0139e-04 - val_loss: 9.9480e-05 - 78s/epoch - 52ms/step
Epoch 174/200
1493/1493 - 78s - loss: 9.7605e-05 - val_loss: 1.0139e-04 - 78s/epoch - 52ms/step
Epoch 175/200
1493/1493 - 78s - loss: 9.7763e-05 - val_loss: 1.1784e-04 - 78s/epoch - 52ms/step
Epoch 176/200
1493/1493 - 78s - loss: 9.9096e-05 - val_loss: 1.0748e-04 - 78s/epoch - 52ms/step
Epoch 177/200
1493/1493 - 78s - loss: 9.7939e-05 - val_loss: 9.2995e-05 - 78s/epoch - 52ms/step
Epoch 178/200
1493/1493 - 78s - loss: 9.5406e-05 - val_loss: 1.0220e-04 - 78s/epoch - 52ms/step
Epoch 179/200
1493/1493 - 78s - loss: 9.4333e-05 - val_loss: 1.0613e-04 - 78s/epoch - 52ms/step
Epoch 180/200
1493/1493 - 78s - loss: 9.5887e-05 - val_loss: 1.1605e-04 - 78s/epoch - 52ms/step
Epoch 181/200
1493/1493 - 78s - loss: 9.7536e-05 - val_loss: 1.0259e-04 - 78s/epoch - 52ms/step
Epoch 182/200
1493/1493 - 78s - loss: 9.5307e-05 - val_loss: 1.1498e-04 - 78s/epoch - 52ms/step
Epoch 183/200
1493/1493 - 78s - loss: 9.4544e-05 - val_loss: 1.0290e-04 - 78s/epoch - 52ms/step
Epoch 184/200
1493/1493 - 78s - loss: 9.3989e-05 - val_loss: 9.6195e-05 - 78s/epoch - 52ms/step
Epoch 185/200
1493/1493 - 78s - loss: 9.1892e-05 - val_loss: 1.2091e-04 - 78s/epoch - 52ms/step
Epoch 186/200
1493/1493 - 78s - loss: 9.5249e-05 - val_loss: 9.8400e-05 - 78s/epoch - 52ms/step
Epoch 187/200
1493/1493 - 78s - loss: 9.2651e-05 - val_loss: 1.7585e-04 - 78s/epoch - 52ms/step
Epoch 188/200
1493/1493 - 78s - loss: 1.1121e-04 - val_loss: 1.1517e-04 - 78s/epoch - 52ms/step
Epoch 189/200
1493/1493 - 78s - loss: 9.8045e-05 - val_loss: 1.1117e-04 - 78s/epoch - 52ms/step
Epoch 190/200
1493/1493 - 78s - loss: 9.5299e-05 - val_loss: 1.1167e-04 - 78s/epoch - 52ms/step
Epoch 191/200
1493/1493 - 78s - loss: 9.6846e-05 - val_loss: 2.8463e-04 - 78s/epoch - 52ms/step
Epoch 192/200
1493/1493 - 78s - loss: 1.2737e-04 - val_loss: 9.2894e-05 - 78s/epoch - 52ms/step
Epoch 193/200
1493/1493 - 78s - loss: 1.0121e-04 - val_loss: 3.1475e-04 - 78s/epoch - 52ms/step
Epoch 194/200
1493/1493 - 78s - loss: 1.3891e-04 - val_loss: 9.4950e-04 - 78s/epoch - 52ms/step
Epoch 195/200
1493/1493 - 78s - loss: 1.9793e-04 - val_loss: 1.7152e-04 - 78s/epoch - 52ms/step
Epoch 196/200
1493/1493 - 78s - loss: 1.2146e-04 - val_loss: 8.9903e-05 - 78s/epoch - 52ms/step
Epoch 197/200
1493/1493 - 78s - loss: 1.0313e-04 - val_loss: 1.3224e-04 - 78s/epoch - 52ms/step
Epoch 198/200
1493/1493 - 78s - loss: 1.0811e-04 - val_loss: 6.0768e-04 - 78s/epoch - 52ms/step
Epoch 199/200
1493/1493 - 78s - loss: 1.4617e-04 - val_loss: 9.1782e-05 - 78s/epoch - 52ms/step
Epoch 200/200
1493/1493 - 78s - loss: 1.0277e-04 - val_loss: 9.5495e-05 - 78s/epoch - 52ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 9.549500828143209e-05
  1/332 [..............................] - ETA: 56s  7/332 [..............................] - ETA: 3s  13/332 [>.............................] - ETA: 2s 19/332 [>.............................] - ETA: 2s 25/332 [=>............................] - ETA: 2s 31/332 [=>............................] - ETA: 2s 37/332 [==>...........................] - ETA: 2s 43/332 [==>...........................] - ETA: 2s 49/332 [===>..........................] - ETA: 2s 55/332 [===>..........................] - ETA: 2s 61/332 [====>.........................] - ETA: 2s 67/332 [=====>........................] - ETA: 2s 73/332 [=====>........................] - ETA: 2s 79/332 [======>.......................] - ETA: 2s 85/332 [======>.......................] - ETA: 2s 91/332 [=======>......................] - ETA: 2s 97/332 [=======>......................] - ETA: 2s103/332 [========>.....................] - ETA: 2s109/332 [========>.....................] - ETA: 1s115/332 [=========>....................] - ETA: 1s121/332 [=========>....................] - ETA: 1s125/332 [==========>...................] - ETA: 1s131/332 [==========>...................] - ETA: 1s137/332 [===========>..................] - ETA: 1s143/332 [===========>..................] - ETA: 1s149/332 [============>.................] - ETA: 1s155/332 [=============>................] - ETA: 1s161/332 [=============>................] - ETA: 1s167/332 [==============>...............] - ETA: 1s173/332 [==============>...............] - ETA: 1s179/332 [===============>..............] - ETA: 1s185/332 [===============>..............] - ETA: 1s191/332 [================>.............] - ETA: 1s197/332 [================>.............] - ETA: 1s203/332 [=================>............] - ETA: 1s209/332 [=================>............] - ETA: 1s215/332 [==================>...........] - ETA: 1s221/332 [==================>...........] - ETA: 1s225/332 [===================>..........] - ETA: 0s231/332 [===================>..........] - ETA: 0s237/332 [====================>.........] - ETA: 0s243/332 [====================>.........] - ETA: 0s249/332 [=====================>........] - ETA: 0s255/332 [======================>.......] - ETA: 0s261/332 [======================>.......] - ETA: 0s267/332 [=======================>......] - ETA: 0s273/332 [=======================>......] - ETA: 0s279/332 [========================>.....] - ETA: 0s285/332 [========================>.....] - ETA: 0s291/332 [=========================>....] - ETA: 0s297/332 [=========================>....] - ETA: 0s303/332 [==========================>...] - ETA: 0s309/332 [==========================>...] - ETA: 0s315/332 [===========================>..] - ETA: 0s321/332 [============================>.] - ETA: 0s327/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 9ms/step
correlation 0.0010858197010915952
cosine 0.0008556382457490152
MAE: 0.005471497
RMSE: 0.009772151
r2: 0.9938063303622444
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_27"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       multiple                  0         
                                                                 
 dense_27 (Dense)            (None, 3033)              3836745   
                                                                 
 batch_normalization_27 (Bat  (None, 3033)             12132     
 chNormalization)                                                
                                                                 
 re_lu_27 (ReLU)             (None, 3033)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1917488   
                                                                 
 batch_normalization_28 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_28 (ReLU)             (None, 632)               0         
                                                                 
 dense_28 (Dense)            (None, 3033)              1919889   
                                                                 
 batch_normalization_29 (Bat  (None, 3033)             12132     
 chNormalization)                                                
                                                                 
 re_lu_29 (ReLU)             (None, 3033)              0         
                                                                 
 dense_29 (Dense)            (None, 1264)              3834976   
                                                                 
=================================================================
Total params: 11,535,890
Trainable params: 11,522,494
Non-trainable params: 13,396
_________________________________________________________________
Encoder
Model: "model_28"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_29 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_28 (InputLayer)       multiple                  0         
                                                                 
 dense_27 (Dense)            (None, 3033)              3836745   
                                                                 
 batch_normalization_27 (Bat  (None, 3033)             12132     
 chNormalization)                                                
                                                                 
 re_lu_27 (ReLU)             (None, 3033)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1917488   
                                                                 
=================================================================
Total params: 5,766,365
Trainable params: 5,760,299
Non-trainable params: 6,066
_________________________________________________________________
Decoder
Model: "model_29"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_30 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_28 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_28 (ReLU)             (None, 632)               0         
                                                                 
 dense_28 (Dense)            (None, 3033)              1919889   
                                                                 
 batch_normalization_29 (Bat  (None, 3033)             12132     
 chNormalization)                                                
                                                                 
 re_lu_29 (ReLU)             (None, 3033)              0         
                                                                 
 dense_29 (Dense)            (None, 1264)              3834976   
                                                                 
=================================================================
Total params: 5,769,525
Trainable params: 5,762,195
Non-trainable params: 7,330
_________________________________________________________________
['2.4custom_n_b', 'mse', 64, 200, 0.0005, 0.5, 632, 0.00010277072578901425, 9.549500828143209e-05, 0.0010858197010915952, 0.0008556382457490152, 0.005471496842801571, 0.00977215077728033, 0.9938063303622444, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats2_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_30"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_31 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_30 (Dense)            (None, 3160)              3997400   
                                                                 
 batch_normalization_30 (Bat  (None, 3160)             12640     
 chNormalization)                                                
                                                                 
 re_lu_30 (ReLU)             (None, 3160)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1997752   
                                                                 
 batch_normalization_31 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_31 (ReLU)             (None, 632)               0         
                                                                 
 dense_31 (Dense)            (None, 3160)              2000280   
                                                                 
 batch_normalization_32 (Bat  (None, 3160)             12640     
 chNormalization)                                                
                                                                 
 re_lu_32 (ReLU)             (None, 3160)              0         
                                                                 
 dense_32 (Dense)            (None, 1264)              3995504   
                                                                 
=================================================================
Total params: 12,018,744
Trainable params: 12,004,840
Non-trainable params: 13,904
_________________________________________________________________
Epoch 1/200
1493/1493 - 87s - loss: 0.0095 - val_loss: 0.0038 - 87s/epoch - 58ms/step
Epoch 2/200
1493/1493 - 86s - loss: 0.0030 - val_loss: 0.0054 - 86s/epoch - 58ms/step
Epoch 3/200
1493/1493 - 87s - loss: 0.0023 - val_loss: 0.0016 - 87s/epoch - 58ms/step
Epoch 4/200
1493/1493 - 87s - loss: 0.0018 - val_loss: 0.0039 - 87s/epoch - 58ms/step
Epoch 5/200
1493/1493 - 87s - loss: 0.0017 - val_loss: 0.0016 - 87s/epoch - 58ms/step
Epoch 6/200
1493/1493 - 87s - loss: 0.0015 - val_loss: 0.0013 - 87s/epoch - 58ms/step
Epoch 7/200
1493/1493 - 87s - loss: 0.0013 - val_loss: 0.0017 - 87s/epoch - 58ms/step
Epoch 8/200
1493/1493 - 87s - loss: 0.0012 - val_loss: 0.0011 - 87s/epoch - 58ms/step
Epoch 9/200
1493/1493 - 87s - loss: 0.0011 - val_loss: 0.0036 - 87s/epoch - 58ms/step
Epoch 10/200
1493/1493 - 87s - loss: 0.0016 - val_loss: 7.4887e-04 - 87s/epoch - 58ms/step
Epoch 11/200
1493/1493 - 87s - loss: 8.7589e-04 - val_loss: 7.9329e-04 - 87s/epoch - 58ms/step
Epoch 12/200
1493/1493 - 87s - loss: 7.5423e-04 - val_loss: 9.2264e-04 - 87s/epoch - 58ms/step
Epoch 13/200
1493/1493 - 87s - loss: 7.0851e-04 - val_loss: 0.0014 - 87s/epoch - 58ms/step
Epoch 14/200
1493/1493 - 87s - loss: 7.3699e-04 - val_loss: 6.3776e-04 - 87s/epoch - 58ms/step
Epoch 15/200
1493/1493 - 87s - loss: 6.2542e-04 - val_loss: 0.0013 - 87s/epoch - 58ms/step
Epoch 16/200
1493/1493 - 87s - loss: 6.4869e-04 - val_loss: 4.7356e-04 - 87s/epoch - 58ms/step
Epoch 17/200
1493/1493 - 87s - loss: 5.1147e-04 - val_loss: 8.4616e-04 - 87s/epoch - 58ms/step
Epoch 18/200
1493/1493 - 87s - loss: 5.0906e-04 - val_loss: 8.0818e-04 - 87s/epoch - 58ms/step
Epoch 19/200
1493/1493 - 87s - loss: 4.9345e-04 - val_loss: 6.2467e-04 - 87s/epoch - 58ms/step
Epoch 20/200
1493/1493 - 87s - loss: 4.4767e-04 - val_loss: 3.8770e-04 - 87s/epoch - 58ms/step
Epoch 21/200
1493/1493 - 87s - loss: 4.0910e-04 - val_loss: 5.7385e-04 - 87s/epoch - 58ms/step
Epoch 22/200
1493/1493 - 87s - loss: 4.0160e-04 - val_loss: 4.7537e-04 - 87s/epoch - 58ms/step
Epoch 23/200
1493/1493 - 86s - loss: 3.8910e-04 - val_loss: 7.0891e-04 - 86s/epoch - 58ms/step
Epoch 24/200
1493/1493 - 86s - loss: 3.8534e-04 - val_loss: 3.4187e-04 - 86s/epoch - 58ms/step
Epoch 25/200
1493/1493 - 86s - loss: 3.4606e-04 - val_loss: 3.3565e-04 - 86s/epoch - 58ms/step
Epoch 26/200
1493/1493 - 87s - loss: 3.3021e-04 - val_loss: 4.5323e-04 - 87s/epoch - 58ms/step
Epoch 27/200
1493/1493 - 86s - loss: 3.1535e-04 - val_loss: 3.3865e-04 - 86s/epoch - 58ms/step
Epoch 28/200
1493/1493 - 87s - loss: 2.9873e-04 - val_loss: 6.2913e-04 - 87s/epoch - 58ms/step
Epoch 29/200
1493/1493 - 87s - loss: 3.0682e-04 - val_loss: 3.0018e-04 - 87s/epoch - 58ms/step
Epoch 30/200
1493/1493 - 86s - loss: 2.8425e-04 - val_loss: 3.0770e-04 - 86s/epoch - 58ms/step
Epoch 31/200
1493/1493 - 86s - loss: 2.7839e-04 - val_loss: 4.3914e-04 - 86s/epoch - 58ms/step
Epoch 32/200
1493/1493 - 86s - loss: 3.1394e-04 - val_loss: 3.4637e-04 - 86s/epoch - 58ms/step
Epoch 33/200
1493/1493 - 86s - loss: 2.7906e-04 - val_loss: 2.6431e-04 - 86s/epoch - 58ms/step
Epoch 34/200
1493/1493 - 86s - loss: 2.5989e-04 - val_loss: 2.4134e-04 - 86s/epoch - 58ms/step
Epoch 35/200
1493/1493 - 86s - loss: 2.4932e-04 - val_loss: 5.9563e-04 - 86s/epoch - 58ms/step
Epoch 36/200
1493/1493 - 86s - loss: 2.8006e-04 - val_loss: 2.8373e-04 - 86s/epoch - 58ms/step
Epoch 37/200
1493/1493 - 86s - loss: 2.4722e-04 - val_loss: 2.7407e-04 - 86s/epoch - 58ms/step
Epoch 38/200
1493/1493 - 86s - loss: 2.4618e-04 - val_loss: 2.1777e-04 - 86s/epoch - 58ms/step
Epoch 39/200
1493/1493 - 86s - loss: 2.3169e-04 - val_loss: 2.2527e-04 - 86s/epoch - 58ms/step
Epoch 40/200
1493/1493 - 86s - loss: 2.2221e-04 - val_loss: 3.0535e-04 - 86s/epoch - 58ms/step
Epoch 41/200
1493/1493 - 86s - loss: 2.2395e-04 - val_loss: 2.1613e-04 - 86s/epoch - 58ms/step
Epoch 42/200
1493/1493 - 86s - loss: 2.1689e-04 - val_loss: 2.2249e-04 - 86s/epoch - 58ms/step
Epoch 43/200
1493/1493 - 86s - loss: 2.1671e-04 - val_loss: 4.1394e-04 - 86s/epoch - 58ms/step
Epoch 44/200
1493/1493 - 86s - loss: 2.3987e-04 - val_loss: 3.2795e-04 - 86s/epoch - 58ms/step
Epoch 45/200
1493/1493 - 86s - loss: 2.2224e-04 - val_loss: 1.7998e-04 - 86s/epoch - 58ms/step
Epoch 46/200
1493/1493 - 86s - loss: 2.0524e-04 - val_loss: 1.9447e-04 - 86s/epoch - 58ms/step
Epoch 47/200
1493/1493 - 86s - loss: 2.0504e-04 - val_loss: 2.5280e-04 - 86s/epoch - 58ms/step
Epoch 48/200
1493/1493 - 86s - loss: 2.0006e-04 - val_loss: 2.3610e-04 - 86s/epoch - 58ms/step
Epoch 49/200
1493/1493 - 86s - loss: 1.9784e-04 - val_loss: 6.9579e-04 - 86s/epoch - 58ms/step
Epoch 50/200
1493/1493 - 86s - loss: 2.4672e-04 - val_loss: 3.8703e-04 - 86s/epoch - 58ms/step
Epoch 51/200
1493/1493 - 86s - loss: 2.1745e-04 - val_loss: 1.8355e-04 - 86s/epoch - 58ms/step
Epoch 52/200
1493/1493 - 86s - loss: 1.8757e-04 - val_loss: 2.0141e-04 - 86s/epoch - 58ms/step
Epoch 53/200
1493/1493 - 86s - loss: 1.8961e-04 - val_loss: 1.7332e-04 - 86s/epoch - 58ms/step
Epoch 54/200
1493/1493 - 86s - loss: 1.8410e-04 - val_loss: 2.1668e-04 - 86s/epoch - 58ms/step
Epoch 55/200
1493/1493 - 86s - loss: 1.7974e-04 - val_loss: 1.8055e-04 - 86s/epoch - 58ms/step
Epoch 56/200
1493/1493 - 86s - loss: 1.7828e-04 - val_loss: 2.1976e-04 - 86s/epoch - 58ms/step
Epoch 57/200
1493/1493 - 86s - loss: 1.7482e-04 - val_loss: 1.9069e-04 - 86s/epoch - 58ms/step
Epoch 58/200
1493/1493 - 86s - loss: 1.7265e-04 - val_loss: 1.6302e-04 - 86s/epoch - 58ms/step
Epoch 59/200
1493/1493 - 86s - loss: 1.6941e-04 - val_loss: 1.8438e-04 - 86s/epoch - 58ms/step
Epoch 60/200
1493/1493 - 86s - loss: 1.7349e-04 - val_loss: 7.4806e-04 - 86s/epoch - 58ms/step
Epoch 61/200
1493/1493 - 86s - loss: 6.3791e-04 - val_loss: 1.9704e-04 - 86s/epoch - 58ms/step
Epoch 62/200
1493/1493 - 86s - loss: 2.1519e-04 - val_loss: 1.9567e-04 - 86s/epoch - 58ms/step
Epoch 63/200
1493/1493 - 87s - loss: 1.8803e-04 - val_loss: 1.9406e-04 - 87s/epoch - 58ms/step
Epoch 64/200
1493/1493 - 86s - loss: 1.8485e-04 - val_loss: 0.0015 - 86s/epoch - 58ms/step
Epoch 65/200
1493/1493 - 86s - loss: 3.2770e-04 - val_loss: 5.0057e-04 - 86s/epoch - 58ms/step
Epoch 66/200
1493/1493 - 86s - loss: 2.1193e-04 - val_loss: 1.8023e-04 - 86s/epoch - 57ms/step
Epoch 67/200
1493/1493 - 85s - loss: 1.7775e-04 - val_loss: 1.6140e-04 - 85s/epoch - 57ms/step
Epoch 68/200
1493/1493 - 85s - loss: 1.7125e-04 - val_loss: 3.8226e-04 - 85s/epoch - 57ms/step
Epoch 69/200
1493/1493 - 85s - loss: 1.9351e-04 - val_loss: 1.7661e-04 - 85s/epoch - 57ms/step
Epoch 70/200
1493/1493 - 86s - loss: 1.6596e-04 - val_loss: 2.7733e-04 - 86s/epoch - 57ms/step
Epoch 71/200
1493/1493 - 85s - loss: 1.8347e-04 - val_loss: 1.4916e-04 - 85s/epoch - 57ms/step
Epoch 72/200
1493/1493 - 81s - loss: 1.6869e-04 - val_loss: 1.9932e-04 - 81s/epoch - 54ms/step
Epoch 73/200
1493/1493 - 80s - loss: 1.6326e-04 - val_loss: 1.8056e-04 - 80s/epoch - 53ms/step
Epoch 74/200
1493/1493 - 79s - loss: 1.5862e-04 - val_loss: 1.6295e-04 - 79s/epoch - 53ms/step
Epoch 75/200
1493/1493 - 79s - loss: 1.5379e-04 - val_loss: 1.5229e-04 - 79s/epoch - 53ms/step
Epoch 76/200
1493/1493 - 79s - loss: 1.5025e-04 - val_loss: 1.4831e-04 - 79s/epoch - 53ms/step
Epoch 77/200
1493/1493 - 79s - loss: 1.5010e-04 - val_loss: 6.2498e-04 - 79s/epoch - 53ms/step
Epoch 78/200
1493/1493 - 79s - loss: 1.7688e-04 - val_loss: 1.8163e-04 - 79s/epoch - 53ms/step
Epoch 79/200
1493/1493 - 80s - loss: 1.5803e-04 - val_loss: 2.3990e-04 - 80s/epoch - 53ms/step
Epoch 80/200
1493/1493 - 80s - loss: 1.6693e-04 - val_loss: 1.6598e-04 - 80s/epoch - 53ms/step
Epoch 81/200
1493/1493 - 79s - loss: 1.4926e-04 - val_loss: 3.0119e-04 - 79s/epoch - 53ms/step
Epoch 82/200
1493/1493 - 80s - loss: 1.4676e-04 - val_loss: 1.4050e-04 - 80s/epoch - 54ms/step
Epoch 83/200
1493/1493 - 79s - loss: 1.4201e-04 - val_loss: 1.5147e-04 - 79s/epoch - 53ms/step
Epoch 84/200
1493/1493 - 79s - loss: 1.4051e-04 - val_loss: 1.6283e-04 - 79s/epoch - 53ms/step
Epoch 85/200
1493/1493 - 78s - loss: 1.5267e-04 - val_loss: 1.5580e-04 - 78s/epoch - 53ms/step
Epoch 86/200
1493/1493 - 79s - loss: 1.4577e-04 - val_loss: 1.4769e-04 - 79s/epoch - 53ms/step
Epoch 87/200
1493/1493 - 80s - loss: 1.3736e-04 - val_loss: 1.3272e-04 - 80s/epoch - 53ms/step
Epoch 88/200
1493/1493 - 79s - loss: 1.3652e-04 - val_loss: 1.8125e-04 - 79s/epoch - 53ms/step
Epoch 89/200
1493/1493 - 79s - loss: 1.4352e-04 - val_loss: 1.4157e-04 - 79s/epoch - 53ms/step
Epoch 90/200
1493/1493 - 79s - loss: 1.3493e-04 - val_loss: 1.5418e-04 - 79s/epoch - 53ms/step
Epoch 91/200
1493/1493 - 80s - loss: 1.3295e-04 - val_loss: 1.5785e-04 - 80s/epoch - 54ms/step
Epoch 92/200
1493/1493 - 80s - loss: 1.3157e-04 - val_loss: 2.5377e-04 - 80s/epoch - 54ms/step
Epoch 93/200
1493/1493 - 79s - loss: 1.5198e-04 - val_loss: 1.9139e-04 - 79s/epoch - 53ms/step
Epoch 94/200
1493/1493 - 79s - loss: 1.4368e-04 - val_loss: 1.2091e-04 - 79s/epoch - 53ms/step
Epoch 95/200
1493/1493 - 79s - loss: 1.3178e-04 - val_loss: 1.4185e-04 - 79s/epoch - 53ms/step
Epoch 96/200
1493/1493 - 79s - loss: 1.3111e-04 - val_loss: 1.5077e-04 - 79s/epoch - 53ms/step
Epoch 97/200
1493/1493 - 79s - loss: 1.2924e-04 - val_loss: 1.2495e-04 - 79s/epoch - 53ms/step
Epoch 98/200
1493/1493 - 79s - loss: 1.3325e-04 - val_loss: 6.0831e-04 - 79s/epoch - 53ms/step
Epoch 99/200
1493/1493 - 79s - loss: 2.1884e-04 - val_loss: 1.2385e-04 - 79s/epoch - 53ms/step
Epoch 100/200
1493/1493 - 79s - loss: 1.4821e-04 - val_loss: 1.2517e-04 - 79s/epoch - 53ms/step
Epoch 101/200
1493/1493 - 79s - loss: 1.3633e-04 - val_loss: 2.1458e-04 - 79s/epoch - 53ms/step
Epoch 102/200
1493/1493 - 79s - loss: 1.5283e-04 - val_loss: 1.7629e-04 - 79s/epoch - 53ms/step
Epoch 103/200
1493/1493 - 79s - loss: 1.3767e-04 - val_loss: 1.2778e-04 - 79s/epoch - 53ms/step
Epoch 104/200
1493/1493 - 79s - loss: 1.2883e-04 - val_loss: 2.4596e-04 - 79s/epoch - 53ms/step
Epoch 105/200
1493/1493 - 79s - loss: 1.4135e-04 - val_loss: 1.2045e-04 - 79s/epoch - 53ms/step
Epoch 106/200
1493/1493 - 78s - loss: 1.2629e-04 - val_loss: 1.4259e-04 - 78s/epoch - 52ms/step
Epoch 107/200
1493/1493 - 79s - loss: 1.2509e-04 - val_loss: 1.4107e-04 - 79s/epoch - 53ms/step
Epoch 108/200
1493/1493 - 79s - loss: 1.2568e-04 - val_loss: 1.3460e-04 - 79s/epoch - 53ms/step
Epoch 109/200
1493/1493 - 78s - loss: 1.2410e-04 - val_loss: 1.3526e-04 - 78s/epoch - 53ms/step
Epoch 110/200
1493/1493 - 79s - loss: 1.2188e-04 - val_loss: 1.3664e-04 - 79s/epoch - 53ms/step
Epoch 111/200
1493/1493 - 79s - loss: 1.2113e-04 - val_loss: 1.2086e-04 - 79s/epoch - 53ms/step
Epoch 112/200
1493/1493 - 79s - loss: 1.2011e-04 - val_loss: 1.2528e-04 - 79s/epoch - 53ms/step
Epoch 113/200
1493/1493 - 79s - loss: 1.1847e-04 - val_loss: 2.5038e-04 - 79s/epoch - 53ms/step
Epoch 114/200
1493/1493 - 79s - loss: 1.4621e-04 - val_loss: 1.1689e-04 - 79s/epoch - 53ms/step
Epoch 115/200
1493/1493 - 79s - loss: 1.2732e-04 - val_loss: 2.1968e-04 - 79s/epoch - 53ms/step
Epoch 116/200
1493/1493 - 79s - loss: 1.2093e-04 - val_loss: 1.4552e-04 - 79s/epoch - 53ms/step
Epoch 117/200
1493/1493 - 79s - loss: 1.1751e-04 - val_loss: 2.0081e-04 - 79s/epoch - 53ms/step
Epoch 118/200
1493/1493 - 80s - loss: 1.2717e-04 - val_loss: 1.2154e-04 - 80s/epoch - 53ms/step
Epoch 119/200
1493/1493 - 79s - loss: 1.1764e-04 - val_loss: 1.2680e-04 - 79s/epoch - 53ms/step
Epoch 120/200
1493/1493 - 79s - loss: 1.1619e-04 - val_loss: 1.2750e-04 - 79s/epoch - 53ms/step
Epoch 121/200
1493/1493 - 78s - loss: 1.1610e-04 - val_loss: 1.9277e-04 - 78s/epoch - 52ms/step
Epoch 122/200
1493/1493 - 78s - loss: 1.1792e-04 - val_loss: 1.1580e-04 - 78s/epoch - 52ms/step
Epoch 123/200
1493/1493 - 78s - loss: 1.1338e-04 - val_loss: 1.1427e-04 - 78s/epoch - 52ms/step
Epoch 124/200
1493/1493 - 78s - loss: 1.1514e-04 - val_loss: 3.2499e-04 - 78s/epoch - 52ms/step
Epoch 125/200
1493/1493 - 77s - loss: 1.5187e-04 - val_loss: 3.1536e-04 - 77s/epoch - 51ms/step
Epoch 126/200
1493/1493 - 77s - loss: 1.4170e-04 - val_loss: 1.9792e-04 - 77s/epoch - 52ms/step
Epoch 127/200
1493/1493 - 77s - loss: 1.2515e-04 - val_loss: 1.1392e-04 - 77s/epoch - 51ms/step
Epoch 128/200
1493/1493 - 77s - loss: 1.1400e-04 - val_loss: 1.0976e-04 - 77s/epoch - 51ms/step
Epoch 129/200
1493/1493 - 77s - loss: 1.1303e-04 - val_loss: 1.1393e-04 - 77s/epoch - 51ms/step
Epoch 130/200
1493/1493 - 78s - loss: 1.1229e-04 - val_loss: 1.1231e-04 - 78s/epoch - 52ms/step
Epoch 131/200
1493/1493 - 77s - loss: 1.1209e-04 - val_loss: 2.0000e-04 - 77s/epoch - 52ms/step
Epoch 132/200
1493/1493 - 77s - loss: 1.2586e-04 - val_loss: 1.0763e-04 - 77s/epoch - 52ms/step
Epoch 133/200
1493/1493 - 77s - loss: 1.1122e-04 - val_loss: 1.1242e-04 - 77s/epoch - 52ms/step
Epoch 134/200
1493/1493 - 78s - loss: 1.0873e-04 - val_loss: 1.2203e-04 - 78s/epoch - 52ms/step
Epoch 135/200
1493/1493 - 77s - loss: 1.0834e-04 - val_loss: 1.0936e-04 - 77s/epoch - 52ms/step
Epoch 136/200
1493/1493 - 76s - loss: 1.0865e-04 - val_loss: 1.2820e-04 - 76s/epoch - 51ms/step
Epoch 137/200
1493/1493 - 76s - loss: 1.1746e-04 - val_loss: 1.9910e-04 - 76s/epoch - 51ms/step
Epoch 138/200
1493/1493 - 76s - loss: 1.3134e-04 - val_loss: 1.0898e-04 - 76s/epoch - 51ms/step
Epoch 139/200
1493/1493 - 76s - loss: 1.1311e-04 - val_loss: 2.4162e-04 - 76s/epoch - 51ms/step
Epoch 140/200
1493/1493 - 76s - loss: 1.3852e-04 - val_loss: 1.0057e-04 - 76s/epoch - 51ms/step
Epoch 141/200
1493/1493 - 78s - loss: 1.1109e-04 - val_loss: 1.4514e-04 - 78s/epoch - 52ms/step
Epoch 142/200
1493/1493 - 77s - loss: 1.1600e-04 - val_loss: 1.2133e-04 - 77s/epoch - 52ms/step
Epoch 143/200
1493/1493 - 77s - loss: 1.0780e-04 - val_loss: 1.3181e-04 - 77s/epoch - 52ms/step
Epoch 144/200
1493/1493 - 78s - loss: 1.1243e-04 - val_loss: 1.1788e-04 - 78s/epoch - 52ms/step
Epoch 145/200
1493/1493 - 77s - loss: 1.0682e-04 - val_loss: 1.0485e-04 - 77s/epoch - 51ms/step
Epoch 146/200
1493/1493 - 77s - loss: 1.0607e-04 - val_loss: 1.0836e-04 - 77s/epoch - 52ms/step
Epoch 147/200
1493/1493 - 77s - loss: 1.0465e-04 - val_loss: 1.1599e-04 - 77s/epoch - 51ms/step
Epoch 148/200
1493/1493 - 78s - loss: 1.0448e-04 - val_loss: 1.7071e-04 - 78s/epoch - 52ms/step
Epoch 149/200
1493/1493 - 78s - loss: 1.0392e-04 - val_loss: 1.1308e-04 - 78s/epoch - 52ms/step
Epoch 150/200
1493/1493 - 76s - loss: 1.0271e-04 - val_loss: 1.0010e-04 - 76s/epoch - 51ms/step
Epoch 151/200
1493/1493 - 76s - loss: 1.0412e-04 - val_loss: 1.4035e-04 - 76s/epoch - 51ms/step
Epoch 152/200
1493/1493 - 76s - loss: 1.0785e-04 - val_loss: 1.6431e-04 - 76s/epoch - 51ms/step
Epoch 153/200
1493/1493 - 77s - loss: 1.0939e-04 - val_loss: 1.1512e-04 - 77s/epoch - 52ms/step
Epoch 154/200
1493/1493 - 77s - loss: 1.0203e-04 - val_loss: 1.2917e-04 - 77s/epoch - 51ms/step
Epoch 155/200
1493/1493 - 75s - loss: 1.0457e-04 - val_loss: 1.0781e-04 - 75s/epoch - 51ms/step
Epoch 156/200
1493/1493 - 73s - loss: 1.0216e-04 - val_loss: 1.1007e-04 - 73s/epoch - 49ms/step
Epoch 157/200
1493/1493 - 74s - loss: 9.9970e-05 - val_loss: 1.0880e-04 - 74s/epoch - 49ms/step
Epoch 158/200
1493/1493 - 72s - loss: 1.0324e-04 - val_loss: 1.1698e-04 - 72s/epoch - 49ms/step
Epoch 159/200
1493/1493 - 74s - loss: 1.0016e-04 - val_loss: 1.0377e-04 - 74s/epoch - 50ms/step
Epoch 160/200
1493/1493 - 73s - loss: 1.0035e-04 - val_loss: 1.4247e-04 - 73s/epoch - 49ms/step
Epoch 161/200
1493/1493 - 73s - loss: 1.2666e-04 - val_loss: 9.5297e-05 - 73s/epoch - 49ms/step
Epoch 162/200
1493/1493 - 73s - loss: 1.0222e-04 - val_loss: 1.0345e-04 - 73s/epoch - 49ms/step
Epoch 163/200
1493/1493 - 73s - loss: 1.0012e-04 - val_loss: 1.1004e-04 - 73s/epoch - 49ms/step
Epoch 164/200
1493/1493 - 74s - loss: 1.0248e-04 - val_loss: 1.0944e-04 - 74s/epoch - 49ms/step
Epoch 165/200
1493/1493 - 75s - loss: 1.2838e-04 - val_loss: 9.9262e-05 - 75s/epoch - 50ms/step
Epoch 166/200
1493/1493 - 74s - loss: 1.0132e-04 - val_loss: 1.0865e-04 - 74s/epoch - 50ms/step
Epoch 167/200
1493/1493 - 74s - loss: 1.0020e-04 - val_loss: 9.1906e-05 - 74s/epoch - 50ms/step
Epoch 168/200
1493/1493 - 74s - loss: 9.8904e-05 - val_loss: 9.7797e-05 - 74s/epoch - 50ms/step
Epoch 169/200
1493/1493 - 75s - loss: 9.8819e-05 - val_loss: 1.1853e-04 - 75s/epoch - 50ms/step
Epoch 170/200
1493/1493 - 74s - loss: 9.9457e-05 - val_loss: 2.6080e-04 - 74s/epoch - 50ms/step
Epoch 171/200
1493/1493 - 74s - loss: 1.2691e-04 - val_loss: 2.6683e-04 - 74s/epoch - 49ms/step
Epoch 172/200
1493/1493 - 74s - loss: 1.2803e-04 - val_loss: 1.0399e-04 - 74s/epoch - 50ms/step
Epoch 173/200
1493/1493 - 75s - loss: 1.0200e-04 - val_loss: 1.0022e-04 - 75s/epoch - 50ms/step
Epoch 174/200
1493/1493 - 74s - loss: 9.9115e-05 - val_loss: 9.8031e-05 - 74s/epoch - 50ms/step
Epoch 175/200
1493/1493 - 74s - loss: 9.9513e-05 - val_loss: 1.0775e-04 - 74s/epoch - 49ms/step
Epoch 176/200
1493/1493 - 75s - loss: 9.9244e-05 - val_loss: 1.0076e-04 - 75s/epoch - 50ms/step
Epoch 177/200
1493/1493 - 74s - loss: 9.8573e-05 - val_loss: 8.8219e-05 - 74s/epoch - 50ms/step
Epoch 178/200
1493/1493 - 74s - loss: 9.6303e-05 - val_loss: 9.8942e-05 - 74s/epoch - 50ms/step
Epoch 179/200
1493/1493 - 74s - loss: 9.5420e-05 - val_loss: 1.1501e-04 - 74s/epoch - 49ms/step
Epoch 180/200
1493/1493 - 75s - loss: 9.6656e-05 - val_loss: 1.0838e-04 - 75s/epoch - 50ms/step
Epoch 181/200
1493/1493 - 74s - loss: 9.7684e-05 - val_loss: 1.2370e-04 - 74s/epoch - 50ms/step
Epoch 182/200
1493/1493 - 75s - loss: 9.8018e-05 - val_loss: 1.3314e-04 - 75s/epoch - 50ms/step
Epoch 183/200
1493/1493 - 74s - loss: 9.6197e-05 - val_loss: 1.1390e-04 - 74s/epoch - 49ms/step
Epoch 184/200
1493/1493 - 73s - loss: 9.6449e-05 - val_loss: 9.6762e-05 - 73s/epoch - 49ms/step
Epoch 185/200
1493/1493 - 74s - loss: 9.3449e-05 - val_loss: 1.4003e-04 - 74s/epoch - 50ms/step
Epoch 186/200
1493/1493 - 74s - loss: 1.0046e-04 - val_loss: 9.5384e-05 - 74s/epoch - 49ms/step
Epoch 187/200
1493/1493 - 74s - loss: 9.4180e-05 - val_loss: 1.1409e-04 - 74s/epoch - 49ms/step
Epoch 188/200
1493/1493 - 73s - loss: 9.9638e-05 - val_loss: 1.0896e-04 - 73s/epoch - 49ms/step
Epoch 189/200
1493/1493 - 74s - loss: 9.3735e-05 - val_loss: 1.0324e-04 - 74s/epoch - 50ms/step
Epoch 190/200
1493/1493 - 74s - loss: 9.5610e-05 - val_loss: 1.1446e-04 - 74s/epoch - 50ms/step
Epoch 191/200
1493/1493 - 73s - loss: 9.7228e-05 - val_loss: 3.1402e-04 - 73s/epoch - 49ms/step
Epoch 192/200
1493/1493 - 73s - loss: 1.2764e-04 - val_loss: 1.0267e-04 - 73s/epoch - 49ms/step
Epoch 193/200
1493/1493 - 74s - loss: 1.0077e-04 - val_loss: 3.0362e-04 - 74s/epoch - 50ms/step
Epoch 194/200
1493/1493 - 74s - loss: 1.2270e-04 - val_loss: 4.8846e-04 - 74s/epoch - 50ms/step
Epoch 195/200
1493/1493 - 74s - loss: 1.7555e-04 - val_loss: 4.3235e-04 - 74s/epoch - 50ms/step
Epoch 196/200
1493/1493 - 74s - loss: 1.6257e-04 - val_loss: 9.3453e-05 - 74s/epoch - 50ms/step
Epoch 197/200
1493/1493 - 75s - loss: 1.0721e-04 - val_loss: 1.4245e-04 - 75s/epoch - 50ms/step
Epoch 198/200
1493/1493 - 75s - loss: 1.1694e-04 - val_loss: 1.2243e-04 - 75s/epoch - 50ms/step
Epoch 199/200
1493/1493 - 75s - loss: 1.0360e-04 - val_loss: 8.5172e-05 - 75s/epoch - 50ms/step
Epoch 200/200
1493/1493 - 73s - loss: 9.7877e-05 - val_loss: 1.0054e-04 - 73s/epoch - 49ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00010054231097456068
  1/332 [..............................] - ETA: 42s  7/332 [..............................] - ETA: 3s  13/332 [>.............................] - ETA: 2s 20/332 [>.............................] - ETA: 2s 27/332 [=>............................] - ETA: 2s 35/332 [==>...........................] - ETA: 2s 43/332 [==>...........................] - ETA: 2s 51/332 [===>..........................] - ETA: 2s 59/332 [====>.........................] - ETA: 2s 67/332 [=====>........................] - ETA: 1s 75/332 [=====>........................] - ETA: 1s 82/332 [======>.......................] - ETA: 1s 90/332 [=======>......................] - ETA: 1s 97/332 [=======>......................] - ETA: 1s104/332 [========>.....................] - ETA: 1s112/332 [=========>....................] - ETA: 1s119/332 [=========>....................] - ETA: 1s127/332 [==========>...................] - ETA: 1s134/332 [===========>..................] - ETA: 1s142/332 [===========>..................] - ETA: 1s149/332 [============>.................] - ETA: 1s156/332 [=============>................] - ETA: 1s164/332 [=============>................] - ETA: 1s171/332 [==============>...............] - ETA: 1s177/332 [==============>...............] - ETA: 1s185/332 [===============>..............] - ETA: 1s193/332 [================>.............] - ETA: 1s201/332 [=================>............] - ETA: 0s209/332 [=================>............] - ETA: 0s216/332 [==================>...........] - ETA: 0s224/332 [===================>..........] - ETA: 0s232/332 [===================>..........] - ETA: 0s240/332 [====================>.........] - ETA: 0s248/332 [=====================>........] - ETA: 0s256/332 [======================>.......] - ETA: 0s264/332 [======================>.......] - ETA: 0s272/332 [=======================>......] - ETA: 0s279/332 [========================>.....] - ETA: 0s287/332 [========================>.....] - ETA: 0s295/332 [=========================>....] - ETA: 0s302/332 [==========================>...] - ETA: 0s309/332 [==========================>...] - ETA: 0s317/332 [===========================>..] - ETA: 0s325/332 [============================>.] - ETA: 0s332/332 [==============================] - ETA: 0s332/332 [==============================] - 3s 7ms/step
correlation 0.0011378336056106458
cosine 0.000896258148390349
MAE: 0.005581746
RMSE: 0.010027074
r2: 0.9934785588458314
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_30"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_31 (InputLayer)       multiple                  0         
                                                                 
 dense_30 (Dense)            (None, 3160)              3997400   
                                                                 
 batch_normalization_30 (Bat  (None, 3160)             12640     
 chNormalization)                                                
                                                                 
 re_lu_30 (ReLU)             (None, 3160)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1997752   
                                                                 
 batch_normalization_31 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_31 (ReLU)             (None, 632)               0         
                                                                 
 dense_31 (Dense)            (None, 3160)              2000280   
                                                                 
 batch_normalization_32 (Bat  (None, 3160)             12640     
 chNormalization)                                                
                                                                 
 re_lu_32 (ReLU)             (None, 3160)              0         
                                                                 
 dense_32 (Dense)            (None, 1264)              3995504   
                                                                 
=================================================================
Total params: 12,018,744
Trainable params: 12,004,840
Non-trainable params: 13,904
_________________________________________________________________
Encoder
Model: "model_31"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_32 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_31 (InputLayer)       multiple                  0         
                                                                 
 dense_30 (Dense)            (None, 3160)              3997400   
                                                                 
 batch_normalization_30 (Bat  (None, 3160)             12640     
 chNormalization)                                                
                                                                 
 re_lu_30 (ReLU)             (None, 3160)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1997752   
                                                                 
=================================================================
Total params: 6,007,792
Trainable params: 6,001,472
Non-trainable params: 6,320
_________________________________________________________________
Decoder
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_31 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_31 (ReLU)             (None, 632)               0         
                                                                 
 dense_31 (Dense)            (None, 3160)              2000280   
                                                                 
 batch_normalization_32 (Bat  (None, 3160)             12640     
 chNormalization)                                                
                                                                 
 re_lu_32 (ReLU)             (None, 3160)              0         
                                                                 
 dense_32 (Dense)            (None, 1264)              3995504   
                                                                 
=================================================================
Total params: 6,010,952
Trainable params: 6,003,368
Non-trainable params: 7,584
_________________________________________________________________
['2.5custom_n_b', 'mse', 64, 200, 0.0005, 0.5, 632, 9.78766183834523e-05, 0.00010054231097456068, 0.0011378336056106458, 0.000896258148390349, 0.005581745877861977, 0.010027074255049229, 0.9934785588458314, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Fri Dec 30 02:51:05 CET 2022
done
