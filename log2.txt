start
Mon Dec 26 21:18:47 CET 2022
2022-12-26 21:18:48.749975: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-26 21:18:48.825515: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-12-26 21:19:19,237 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7faebf6ad790> object, created with modnet version 0.1.12
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
Shape of dataset to encode: (106113, 1264)
2022-12-26 21:19:20.971777: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense (Dense)               (None, 3792)              4796880   
                                                                 
 batch_normalization (BatchN  (None, 3792)             15168     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_2 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 3792)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Epoch 1/100
5969/5969 - 282s - loss: 0.0089 - val_loss: 0.0032 - 282s/epoch - 47ms/step
Epoch 2/100
5969/5969 - 282s - loss: 0.0037 - val_loss: 0.0020 - 282s/epoch - 47ms/step
Epoch 3/100
5969/5969 - 282s - loss: 0.0021 - val_loss: 0.0015 - 282s/epoch - 47ms/step
Epoch 4/100
5969/5969 - 282s - loss: 0.0015 - val_loss: 0.0013 - 282s/epoch - 47ms/step
Epoch 5/100
5969/5969 - 305s - loss: 0.0013 - val_loss: 9.6929e-04 - 305s/epoch - 51ms/step
Epoch 6/100
5969/5969 - 309s - loss: 0.0011 - val_loss: 8.9972e-04 - 309s/epoch - 52ms/step
Epoch 7/100
5969/5969 - 309s - loss: 9.6602e-04 - val_loss: 7.9110e-04 - 309s/epoch - 52ms/step
Epoch 8/100
5969/5969 - 309s - loss: 8.8786e-04 - val_loss: 6.9186e-04 - 309s/epoch - 52ms/step
Epoch 9/100
5969/5969 - 309s - loss: 8.2633e-04 - val_loss: 6.7769e-04 - 309s/epoch - 52ms/step
Epoch 10/100
5969/5969 - 309s - loss: 7.7098e-04 - val_loss: 6.5438e-04 - 309s/epoch - 52ms/step
Epoch 11/100
5969/5969 - 309s - loss: 7.4217e-04 - val_loss: 6.1963e-04 - 309s/epoch - 52ms/step
Epoch 12/100
5969/5969 - 309s - loss: 7.1062e-04 - val_loss: 6.1179e-04 - 309s/epoch - 52ms/step
Epoch 13/100
5969/5969 - 309s - loss: 6.7589e-04 - val_loss: 5.8041e-04 - 309s/epoch - 52ms/step
Epoch 14/100
5969/5969 - 309s - loss: 6.5307e-04 - val_loss: 5.5433e-04 - 309s/epoch - 52ms/step
Epoch 15/100
5969/5969 - 309s - loss: 6.3579e-04 - val_loss: 5.4985e-04 - 309s/epoch - 52ms/step
Epoch 16/100
5969/5969 - 309s - loss: 6.2288e-04 - val_loss: 5.1712e-04 - 309s/epoch - 52ms/step
Epoch 17/100
5969/5969 - 309s - loss: 5.9491e-04 - val_loss: 5.2203e-04 - 309s/epoch - 52ms/step
Epoch 18/100
5969/5969 - 309s - loss: 5.6987e-04 - val_loss: 4.9532e-04 - 309s/epoch - 52ms/step
Epoch 19/100
5969/5969 - 309s - loss: 5.5886e-04 - val_loss: 5.0508e-04 - 309s/epoch - 52ms/step
Epoch 20/100
5969/5969 - 309s - loss: 5.5078e-04 - val_loss: 7.1462e-04 - 309s/epoch - 52ms/step
Epoch 21/100
5969/5969 - 309s - loss: 5.4086e-04 - val_loss: 4.5194e-04 - 309s/epoch - 52ms/step
Epoch 22/100
5969/5969 - 309s - loss: 5.2555e-04 - val_loss: 4.5926e-04 - 309s/epoch - 52ms/step
Epoch 23/100
5969/5969 - 309s - loss: 5.1118e-04 - val_loss: 4.5110e-04 - 309s/epoch - 52ms/step
Epoch 24/100
5969/5969 - 309s - loss: 5.0623e-04 - val_loss: 4.6248e-04 - 309s/epoch - 52ms/step
Epoch 25/100
5969/5969 - 309s - loss: 4.9475e-04 - val_loss: 4.3863e-04 - 309s/epoch - 52ms/step
Epoch 26/100
5969/5969 - 309s - loss: 4.7997e-04 - val_loss: 4.2517e-04 - 309s/epoch - 52ms/step
Epoch 27/100
5969/5969 - 309s - loss: 4.7184e-04 - val_loss: 4.0071e-04 - 309s/epoch - 52ms/step
Epoch 28/100
5969/5969 - 309s - loss: 4.6405e-04 - val_loss: 4.0990e-04 - 309s/epoch - 52ms/step
Epoch 29/100
5969/5969 - 309s - loss: 4.7409e-04 - val_loss: 4.9171e-04 - 309s/epoch - 52ms/step
Epoch 30/100
5969/5969 - 309s - loss: 4.5290e-04 - val_loss: 3.9874e-04 - 309s/epoch - 52ms/step
Epoch 31/100
5969/5969 - 309s - loss: 4.4757e-04 - val_loss: 3.7393e-04 - 309s/epoch - 52ms/step
Epoch 32/100
5969/5969 - 309s - loss: 4.4248e-04 - val_loss: 3.8824e-04 - 309s/epoch - 52ms/step
Epoch 33/100
5969/5969 - 309s - loss: 4.3113e-04 - val_loss: 3.9854e-04 - 309s/epoch - 52ms/step
Epoch 34/100
5969/5969 - 309s - loss: 4.3204e-04 - val_loss: 3.7403e-04 - 309s/epoch - 52ms/step
Epoch 35/100
5969/5969 - 309s - loss: 4.2312e-04 - val_loss: 3.7810e-04 - 309s/epoch - 52ms/step
Epoch 36/100
5969/5969 - 309s - loss: 4.2100e-04 - val_loss: 3.9638e-04 - 309s/epoch - 52ms/step
Epoch 37/100
5969/5969 - 309s - loss: 4.1814e-04 - val_loss: 3.7072e-04 - 309s/epoch - 52ms/step
Epoch 38/100
5969/5969 - 309s - loss: 4.2737e-04 - val_loss: 3.6972e-04 - 309s/epoch - 52ms/step
Epoch 39/100
5969/5969 - 310s - loss: 4.1242e-04 - val_loss: 3.7900e-04 - 310s/epoch - 52ms/step
Epoch 40/100
5969/5969 - 310s - loss: 4.0459e-04 - val_loss: 3.5453e-04 - 310s/epoch - 52ms/step
Epoch 41/100
5969/5969 - 310s - loss: 3.9952e-04 - val_loss: 3.5342e-04 - 310s/epoch - 52ms/step
Epoch 42/100
5969/5969 - 310s - loss: 3.9450e-04 - val_loss: 3.3903e-04 - 310s/epoch - 52ms/step
Epoch 43/100
5969/5969 - 310s - loss: 3.9496e-04 - val_loss: 3.3732e-04 - 310s/epoch - 52ms/step
Epoch 44/100
5969/5969 - 310s - loss: 3.8773e-04 - val_loss: 3.4127e-04 - 310s/epoch - 52ms/step
Epoch 45/100
5969/5969 - 310s - loss: 3.8866e-04 - val_loss: 3.1163e-04 - 310s/epoch - 52ms/step
Epoch 46/100
5969/5969 - 310s - loss: 3.7921e-04 - val_loss: 3.3316e-04 - 310s/epoch - 52ms/step
Epoch 47/100
5969/5969 - 310s - loss: 3.8290e-04 - val_loss: 3.2774e-04 - 310s/epoch - 52ms/step
Epoch 48/100
5969/5969 - 310s - loss: 3.7690e-04 - val_loss: 3.0449e-04 - 310s/epoch - 52ms/step
Epoch 49/100
5969/5969 - 310s - loss: 3.8732e-04 - val_loss: 3.3278e-04 - 310s/epoch - 52ms/step
Epoch 50/100
5969/5969 - 310s - loss: 3.7478e-04 - val_loss: 3.2974e-04 - 310s/epoch - 52ms/step
Epoch 51/100
5969/5969 - 310s - loss: 3.9690e-04 - val_loss: 3.3142e-04 - 310s/epoch - 52ms/step
Epoch 52/100
5969/5969 - 310s - loss: 3.7389e-04 - val_loss: 3.3830e-04 - 310s/epoch - 52ms/step
Epoch 53/100
5969/5969 - 310s - loss: 3.6560e-04 - val_loss: 3.2748e-04 - 310s/epoch - 52ms/step
Epoch 54/100
5969/5969 - 310s - loss: 3.6329e-04 - val_loss: 3.2026e-04 - 310s/epoch - 52ms/step
Epoch 55/100
5969/5969 - 310s - loss: 3.5727e-04 - val_loss: 3.2067e-04 - 310s/epoch - 52ms/step
Epoch 56/100
5969/5969 - 310s - loss: 3.5737e-04 - val_loss: 3.1192e-04 - 310s/epoch - 52ms/step
Epoch 57/100
5969/5969 - 310s - loss: 3.5860e-04 - val_loss: 3.1517e-04 - 310s/epoch - 52ms/step
Epoch 58/100
5969/5969 - 310s - loss: 3.5094e-04 - val_loss: 3.1914e-04 - 310s/epoch - 52ms/step
Epoch 59/100
5969/5969 - 310s - loss: 3.4704e-04 - val_loss: 3.1152e-04 - 310s/epoch - 52ms/step
Epoch 60/100
5969/5969 - 310s - loss: 3.5680e-04 - val_loss: 3.0183e-04 - 310s/epoch - 52ms/step
Epoch 61/100
5969/5969 - 310s - loss: 3.5017e-04 - val_loss: 3.0940e-04 - 310s/epoch - 52ms/step
Epoch 62/100
5969/5969 - 310s - loss: 3.4438e-04 - val_loss: 3.2795e-04 - 310s/epoch - 52ms/step
Epoch 63/100
5969/5969 - 310s - loss: 3.4430e-04 - val_loss: 3.0166e-04 - 310s/epoch - 52ms/step
Epoch 64/100
5969/5969 - 310s - loss: 3.5324e-04 - val_loss: 3.0249e-04 - 310s/epoch - 52ms/step
Epoch 65/100
5969/5969 - 310s - loss: 3.4108e-04 - val_loss: 3.2472e-04 - 310s/epoch - 52ms/step
Epoch 66/100
5969/5969 - 310s - loss: 3.3663e-04 - val_loss: 2.9431e-04 - 310s/epoch - 52ms/step
Epoch 67/100
5969/5969 - 310s - loss: 3.3986e-04 - val_loss: 2.8871e-04 - 310s/epoch - 52ms/step
Epoch 68/100
5969/5969 - 310s - loss: 3.3989e-04 - val_loss: 2.9707e-04 - 310s/epoch - 52ms/step
Epoch 69/100
5969/5969 - 310s - loss: 3.3600e-04 - val_loss: 3.0310e-04 - 310s/epoch - 52ms/step
Epoch 70/100
5969/5969 - 310s - loss: 3.3269e-04 - val_loss: 2.8535e-04 - 310s/epoch - 52ms/step
Epoch 71/100
5969/5969 - 309s - loss: 3.4090e-04 - val_loss: 2.9794e-04 - 309s/epoch - 52ms/step
Epoch 72/100
5969/5969 - 310s - loss: 3.4033e-04 - val_loss: 3.0386e-04 - 310s/epoch - 52ms/step
Epoch 73/100
5969/5969 - 309s - loss: 3.2841e-04 - val_loss: 2.8749e-04 - 309s/epoch - 52ms/step
Epoch 74/100
5969/5969 - 309s - loss: 3.2666e-04 - val_loss: 3.0022e-04 - 309s/epoch - 52ms/step
Epoch 75/100
5969/5969 - 309s - loss: 3.2579e-04 - val_loss: 3.0378e-04 - 309s/epoch - 52ms/step
Epoch 76/100
5969/5969 - 309s - loss: 3.1970e-04 - val_loss: 2.7785e-04 - 309s/epoch - 52ms/step
Epoch 77/100
5969/5969 - 309s - loss: 3.5651e-04 - val_loss: 2.8270e-04 - 309s/epoch - 52ms/step
Epoch 78/100
5969/5969 - 309s - loss: 3.3569e-04 - val_loss: 3.1915e-04 - 309s/epoch - 52ms/step
Epoch 79/100
5969/5969 - 309s - loss: 3.2282e-04 - val_loss: 3.1950e-04 - 309s/epoch - 52ms/step
Epoch 80/100
5969/5969 - 309s - loss: 3.1977e-04 - val_loss: 2.9845e-04 - 309s/epoch - 52ms/step
Epoch 81/100
5969/5969 - 309s - loss: 3.2185e-04 - val_loss: 2.7679e-04 - 309s/epoch - 52ms/step
Epoch 82/100
5969/5969 - 309s - loss: 3.1827e-04 - val_loss: 2.7305e-04 - 309s/epoch - 52ms/step
Epoch 83/100
5969/5969 - 309s - loss: 3.2268e-04 - val_loss: 2.9254e-04 - 309s/epoch - 52ms/step
Epoch 84/100
5969/5969 - 309s - loss: 3.1249e-04 - val_loss: 3.0258e-04 - 309s/epoch - 52ms/step
Epoch 85/100
5969/5969 - 309s - loss: 3.1884e-04 - val_loss: 3.5074e-04 - 309s/epoch - 52ms/step
Epoch 86/100
5969/5969 - 309s - loss: 3.2111e-04 - val_loss: 3.0353e-04 - 309s/epoch - 52ms/step
Epoch 87/100
5969/5969 - 309s - loss: 3.2057e-04 - val_loss: 2.9288e-04 - 309s/epoch - 52ms/step
Epoch 88/100
5969/5969 - 309s - loss: 3.1698e-04 - val_loss: 2.8023e-04 - 309s/epoch - 52ms/step
Epoch 89/100
5969/5969 - 309s - loss: 3.1472e-04 - val_loss: 2.7972e-04 - 309s/epoch - 52ms/step
Epoch 90/100
5969/5969 - 309s - loss: 3.0859e-04 - val_loss: 2.7393e-04 - 309s/epoch - 52ms/step
Epoch 91/100
5969/5969 - 309s - loss: 3.2199e-04 - val_loss: 2.8617e-04 - 309s/epoch - 52ms/step
Epoch 92/100
5969/5969 - 309s - loss: 3.1203e-04 - val_loss: 3.0519e-04 - 309s/epoch - 52ms/step
Epoch 93/100
5969/5969 - 309s - loss: 3.1946e-04 - val_loss: 2.9261e-04 - 309s/epoch - 52ms/step
Epoch 94/100
5969/5969 - 309s - loss: 3.0483e-04 - val_loss: 2.9457e-04 - 309s/epoch - 52ms/step
Epoch 95/100
5969/5969 - 309s - loss: 3.0863e-04 - val_loss: 2.9678e-04 - 309s/epoch - 52ms/step
Epoch 96/100
5969/5969 - 309s - loss: 3.0665e-04 - val_loss: 3.0794e-04 - 309s/epoch - 52ms/step
Epoch 97/100
5969/5969 - 309s - loss: 3.0503e-04 - val_loss: 3.5456e-04 - 309s/epoch - 52ms/step
Epoch 98/100
5969/5969 - 309s - loss: 3.0291e-04 - val_loss: 2.9301e-04 - 309s/epoch - 52ms/step
Epoch 99/100
5969/5969 - 309s - loss: 3.0225e-04 - val_loss: 2.7883e-04 - 309s/epoch - 52ms/step
Epoch 100/100
5969/5969 - 309s - loss: 3.0014e-04 - val_loss: 2.8474e-04 - 309s/epoch - 52ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00028474058490246534
  1/332 [..............................] - ETA: 33s  7/332 [..............................] - ETA: 3s  13/332 [>.............................] - ETA: 3s 19/332 [>.............................] - ETA: 3s 25/332 [=>............................] - ETA: 2s 31/332 [=>............................] - ETA: 2s 37/332 [==>...........................] - ETA: 2s 43/332 [==>...........................] - ETA: 2s 49/332 [===>..........................] - ETA: 2s 55/332 [===>..........................] - ETA: 2s 61/332 [====>.........................] - ETA: 2s 67/332 [=====>........................] - ETA: 2s 73/332 [=====>........................] - ETA: 2s 79/332 [======>.......................] - ETA: 2s 85/332 [======>.......................] - ETA: 2s 91/332 [=======>......................] - ETA: 2s 97/332 [=======>......................] - ETA: 2s103/332 [========>.....................] - ETA: 2s109/332 [========>.....................] - ETA: 2s115/332 [=========>....................] - ETA: 2s121/332 [=========>....................] - ETA: 2s127/332 [==========>...................] - ETA: 1s133/332 [===========>..................] - ETA: 1s139/332 [===========>..................] - ETA: 1s145/332 [============>.................] - ETA: 1s151/332 [============>.................] - ETA: 1s157/332 [=============>................] - ETA: 1s163/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s175/332 [==============>...............] - ETA: 1s181/332 [===============>..............] - ETA: 1s187/332 [===============>..............] - ETA: 1s193/332 [================>.............] - ETA: 1s199/332 [================>.............] - ETA: 1s205/332 [=================>............] - ETA: 1s211/332 [==================>...........] - ETA: 1s217/332 [==================>...........] - ETA: 1s223/332 [===================>..........] - ETA: 1s229/332 [===================>..........] - ETA: 0s235/332 [====================>.........] - ETA: 0s241/332 [====================>.........] - ETA: 0s247/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s259/332 [======================>.......] - ETA: 0s265/332 [======================>.......] - ETA: 0s271/332 [=======================>......] - ETA: 0s277/332 [========================>.....] - ETA: 0s283/332 [========================>.....] - ETA: 0s289/332 [=========================>....] - ETA: 0s295/332 [=========================>....] - ETA: 0s301/332 [==========================>...] - ETA: 0s307/332 [==========================>...] - ETA: 0s313/332 [===========================>..] - ETA: 0s319/332 [===========================>..] - ETA: 0s325/332 [============================>.] - ETA: 0s331/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 10ms/step
correlation 0.003009073832239064
cosine 0.0024379701141227293
MAE: 0.008221232
RMSE: 0.016874244
r2: 0.9815289556695497
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 3792)              4796880   
                                                                 
 batch_normalization (BatchN  (None, 3792)             15168     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_2 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 3792)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Encoder
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 3792)              4796880   
                                                                 
 batch_normalization (BatchN  (None, 3792)             15168     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
=================================================================
Total params: 7,209,224
Trainable params: 7,201,640
Non-trainable params: 7,584
_________________________________________________________________
Decoder
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_2 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 3792)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 7,212,384
Trainable params: 7,203,536
Non-trainable params: 8,848
_________________________________________________________________
['3n_b', 'mse', 16, 100, 0.0005, 0.5, 632, 0.00030013732612133026, 0.00028474058490246534, 0.003009073832239064, 0.0024379701141227293, 0.008221232332289219, 0.016874244436621666, 0.9815289556695497, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_3 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_3 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_5 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 3792)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Epoch 1/100
5969/5969 - 318s - loss: 0.0086 - val_loss: 0.0026 - 318s/epoch - 53ms/step
Epoch 2/100
5969/5969 - 317s - loss: 0.0026 - val_loss: 0.0018 - 317s/epoch - 53ms/step
Epoch 3/100
5969/5969 - 317s - loss: 0.0018 - val_loss: 0.0014 - 317s/epoch - 53ms/step
Epoch 4/100
5969/5969 - 317s - loss: 0.0014 - val_loss: 0.0011 - 317s/epoch - 53ms/step
Epoch 5/100
5969/5969 - 317s - loss: 0.0012 - val_loss: 0.0010 - 317s/epoch - 53ms/step
Epoch 6/100
5969/5969 - 317s - loss: 0.0011 - val_loss: 8.7356e-04 - 317s/epoch - 53ms/step
Epoch 7/100
5969/5969 - 317s - loss: 0.0010 - val_loss: 8.0614e-04 - 317s/epoch - 53ms/step
Epoch 8/100
5969/5969 - 317s - loss: 9.4431e-04 - val_loss: 7.6002e-04 - 317s/epoch - 53ms/step
Epoch 9/100
5969/5969 - 317s - loss: 8.9400e-04 - val_loss: 7.2427e-04 - 317s/epoch - 53ms/step
Epoch 10/100
5969/5969 - 317s - loss: 8.5339e-04 - val_loss: 7.0838e-04 - 317s/epoch - 53ms/step
Epoch 11/100
5969/5969 - 317s - loss: 8.1928e-04 - val_loss: 6.6040e-04 - 317s/epoch - 53ms/step
Epoch 12/100
5969/5969 - 317s - loss: 7.8089e-04 - val_loss: 6.4144e-04 - 317s/epoch - 53ms/step
Epoch 13/100
5969/5969 - 317s - loss: 7.6116e-04 - val_loss: 5.9732e-04 - 317s/epoch - 53ms/step
Epoch 14/100
5969/5969 - 317s - loss: 7.4139e-04 - val_loss: 5.8848e-04 - 317s/epoch - 53ms/step
Epoch 15/100
5969/5969 - 317s - loss: 7.1233e-04 - val_loss: 5.7604e-04 - 317s/epoch - 53ms/step
Epoch 16/100
5969/5969 - 317s - loss: 6.9412e-04 - val_loss: 6.3228e-04 - 317s/epoch - 53ms/step
Epoch 17/100
5969/5969 - 317s - loss: 6.7834e-04 - val_loss: 5.4611e-04 - 317s/epoch - 53ms/step
Epoch 18/100
5969/5969 - 317s - loss: 6.6932e-04 - val_loss: 5.3094e-04 - 317s/epoch - 53ms/step
Epoch 19/100
5969/5969 - 317s - loss: 6.6483e-04 - val_loss: 5.4108e-04 - 317s/epoch - 53ms/step
Epoch 20/100
5969/5969 - 317s - loss: 6.3601e-04 - val_loss: 5.0371e-04 - 317s/epoch - 53ms/step
Epoch 21/100
5969/5969 - 318s - loss: 6.2460e-04 - val_loss: 4.8081e-04 - 318s/epoch - 53ms/step
Epoch 22/100
5969/5969 - 317s - loss: 6.2284e-04 - val_loss: 4.9457e-04 - 317s/epoch - 53ms/step
Epoch 23/100
5969/5969 - 317s - loss: 5.9998e-04 - val_loss: 4.7836e-04 - 317s/epoch - 53ms/step
Epoch 24/100
5969/5969 - 317s - loss: 5.9563e-04 - val_loss: 4.8439e-04 - 317s/epoch - 53ms/step
Epoch 25/100
5969/5969 - 318s - loss: 5.8402e-04 - val_loss: 4.5748e-04 - 318s/epoch - 53ms/step
Epoch 26/100
5969/5969 - 318s - loss: 5.8217e-04 - val_loss: 4.5079e-04 - 318s/epoch - 53ms/step
Epoch 27/100
5969/5969 - 318s - loss: 5.7475e-04 - val_loss: 4.4822e-04 - 318s/epoch - 53ms/step
Epoch 28/100
5969/5969 - 318s - loss: 5.6566e-04 - val_loss: 4.3949e-04 - 318s/epoch - 53ms/step
Epoch 29/100
5969/5969 - 317s - loss: 5.6206e-04 - val_loss: 4.5262e-04 - 317s/epoch - 53ms/step
Epoch 30/100
5969/5969 - 317s - loss: 5.4391e-04 - val_loss: 4.2359e-04 - 317s/epoch - 53ms/step
Epoch 31/100
5969/5969 - 318s - loss: 5.7140e-04 - val_loss: 4.2526e-04 - 318s/epoch - 53ms/step
Epoch 32/100
5969/5969 - 318s - loss: 5.3727e-04 - val_loss: 4.4573e-04 - 318s/epoch - 53ms/step
Epoch 33/100
5969/5969 - 318s - loss: 5.2738e-04 - val_loss: 4.0662e-04 - 318s/epoch - 53ms/step
Epoch 34/100
5969/5969 - 318s - loss: 5.3135e-04 - val_loss: 4.0373e-04 - 318s/epoch - 53ms/step
Epoch 35/100
5969/5969 - 317s - loss: 5.1898e-04 - val_loss: 4.2364e-04 - 317s/epoch - 53ms/step
Epoch 36/100
5969/5969 - 318s - loss: 5.1281e-04 - val_loss: 4.3935e-04 - 318s/epoch - 53ms/step
Epoch 37/100
5969/5969 - 317s - loss: 5.1575e-04 - val_loss: 4.2415e-04 - 317s/epoch - 53ms/step
Epoch 38/100
5969/5969 - 317s - loss: 5.0530e-04 - val_loss: 4.5949e-04 - 317s/epoch - 53ms/step
Epoch 39/100
5969/5969 - 317s - loss: 5.1990e-04 - val_loss: 3.9801e-04 - 317s/epoch - 53ms/step
Epoch 40/100
5969/5969 - 317s - loss: 4.9543e-04 - val_loss: 4.0162e-04 - 317s/epoch - 53ms/step
Epoch 41/100
5969/5969 - 317s - loss: 4.9871e-04 - val_loss: 3.9857e-04 - 317s/epoch - 53ms/step
Epoch 42/100
5969/5969 - 318s - loss: 4.9475e-04 - val_loss: 3.9108e-04 - 318s/epoch - 53ms/step
Epoch 43/100
5969/5969 - 317s - loss: 4.8274e-04 - val_loss: 3.9928e-04 - 317s/epoch - 53ms/step
Epoch 44/100
5969/5969 - 318s - loss: 4.8014e-04 - val_loss: 3.9102e-04 - 318s/epoch - 53ms/step
Epoch 45/100
5969/5969 - 317s - loss: 4.8531e-04 - val_loss: 3.7763e-04 - 317s/epoch - 53ms/step
Epoch 46/100
5969/5969 - 318s - loss: 4.8200e-04 - val_loss: 4.0534e-04 - 318s/epoch - 53ms/step
Epoch 47/100
5969/5969 - 318s - loss: 4.8572e-04 - val_loss: 3.7985e-04 - 318s/epoch - 53ms/step
Epoch 48/100
5969/5969 - 318s - loss: 4.8245e-04 - val_loss: 3.7935e-04 - 318s/epoch - 53ms/step
Epoch 49/100
5969/5969 - 318s - loss: 5.0064e-04 - val_loss: 3.7749e-04 - 318s/epoch - 53ms/step
Epoch 50/100
5969/5969 - 318s - loss: 4.6646e-04 - val_loss: 3.6451e-04 - 318s/epoch - 53ms/step
Epoch 51/100
5969/5969 - 317s - loss: 4.8419e-04 - val_loss: 3.7958e-04 - 317s/epoch - 53ms/step
Epoch 52/100
5969/5969 - 318s - loss: 4.6172e-04 - val_loss: 3.6444e-04 - 318s/epoch - 53ms/step
Epoch 53/100
5969/5969 - 317s - loss: 4.5636e-04 - val_loss: 3.5883e-04 - 317s/epoch - 53ms/step
Epoch 54/100
5969/5969 - 318s - loss: 4.6003e-04 - val_loss: 3.8706e-04 - 318s/epoch - 53ms/step
Epoch 55/100
5969/5969 - 317s - loss: 4.5672e-04 - val_loss: 3.5236e-04 - 317s/epoch - 53ms/step
Epoch 56/100
5969/5969 - 318s - loss: 4.4703e-04 - val_loss: 3.6254e-04 - 318s/epoch - 53ms/step
Epoch 57/100
5969/5969 - 318s - loss: 4.5333e-04 - val_loss: 3.4523e-04 - 318s/epoch - 53ms/step
Epoch 58/100
5969/5969 - 318s - loss: 4.5037e-04 - val_loss: 3.5818e-04 - 318s/epoch - 53ms/step
Epoch 59/100
5969/5969 - 318s - loss: 4.3735e-04 - val_loss: 3.5149e-04 - 318s/epoch - 53ms/step
Epoch 60/100
5969/5969 - 318s - loss: 4.4284e-04 - val_loss: 3.3643e-04 - 318s/epoch - 53ms/step
Epoch 61/100
5969/5969 - 318s - loss: 4.3728e-04 - val_loss: 3.5057e-04 - 318s/epoch - 53ms/step
Epoch 62/100
5969/5969 - 318s - loss: 4.3907e-04 - val_loss: 4.3838e-04 - 318s/epoch - 53ms/step
Epoch 63/100
5969/5969 - 318s - loss: 4.3984e-04 - val_loss: 3.5713e-04 - 318s/epoch - 53ms/step
Epoch 64/100
5969/5969 - 318s - loss: 4.4369e-04 - val_loss: 3.6802e-04 - 318s/epoch - 53ms/step
Epoch 65/100
5969/5969 - 318s - loss: 4.3770e-04 - val_loss: 3.6275e-04 - 318s/epoch - 53ms/step
Epoch 66/100
5969/5969 - 318s - loss: 4.3576e-04 - val_loss: 3.4043e-04 - 318s/epoch - 53ms/step
Epoch 67/100
5969/5969 - 318s - loss: 4.4168e-04 - val_loss: 3.5559e-04 - 318s/epoch - 53ms/step
Epoch 68/100
5969/5969 - 318s - loss: 4.2692e-04 - val_loss: 3.4026e-04 - 318s/epoch - 53ms/step
Epoch 69/100
5969/5969 - 318s - loss: 4.2940e-04 - val_loss: 3.5034e-04 - 318s/epoch - 53ms/step
Epoch 70/100
5969/5969 - 318s - loss: 4.2840e-04 - val_loss: 3.4470e-04 - 318s/epoch - 53ms/step
Epoch 71/100
5969/5969 - 318s - loss: 4.2332e-04 - val_loss: 3.3113e-04 - 318s/epoch - 53ms/step
Epoch 72/100
5969/5969 - 318s - loss: 4.2392e-04 - val_loss: 3.5042e-04 - 318s/epoch - 53ms/step
Epoch 73/100
5969/5969 - 317s - loss: 4.3281e-04 - val_loss: 3.3938e-04 - 317s/epoch - 53ms/step
Epoch 74/100
5969/5969 - 317s - loss: 4.2579e-04 - val_loss: 3.4208e-04 - 317s/epoch - 53ms/step
Epoch 75/100
5969/5969 - 317s - loss: 4.1653e-04 - val_loss: 3.5028e-04 - 317s/epoch - 53ms/step
Epoch 76/100
5969/5969 - 317s - loss: 4.2038e-04 - val_loss: 3.3073e-04 - 317s/epoch - 53ms/step
Epoch 77/100
5969/5969 - 317s - loss: 4.1683e-04 - val_loss: 3.9621e-04 - 317s/epoch - 53ms/step
Epoch 78/100
5969/5969 - 317s - loss: 4.1431e-04 - val_loss: 3.4646e-04 - 317s/epoch - 53ms/step
Epoch 79/100
5969/5969 - 317s - loss: 4.2719e-04 - val_loss: 3.6190e-04 - 317s/epoch - 53ms/step
Epoch 80/100
5969/5969 - 317s - loss: 4.1062e-04 - val_loss: 3.2992e-04 - 317s/epoch - 53ms/step
Epoch 81/100
5969/5969 - 317s - loss: 4.2848e-04 - val_loss: 4.0500e-04 - 317s/epoch - 53ms/step
Epoch 82/100
5969/5969 - 317s - loss: 4.2778e-04 - val_loss: 3.2864e-04 - 317s/epoch - 53ms/step
Epoch 83/100
5969/5969 - 317s - loss: 4.6704e-04 - val_loss: 4.2663e-04 - 317s/epoch - 53ms/step
Epoch 84/100
5969/5969 - 317s - loss: 4.1533e-04 - val_loss: 3.5196e-04 - 317s/epoch - 53ms/step
Epoch 85/100
5969/5969 - 317s - loss: 4.1348e-04 - val_loss: 3.5940e-04 - 317s/epoch - 53ms/step
Epoch 86/100
5969/5969 - 317s - loss: 4.0435e-04 - val_loss: 3.4052e-04 - 317s/epoch - 53ms/step
Epoch 87/100
5969/5969 - 317s - loss: 4.1283e-04 - val_loss: 3.7240e-04 - 317s/epoch - 53ms/step
Epoch 88/100
5969/5969 - 317s - loss: 4.2707e-04 - val_loss: 3.6718e-04 - 317s/epoch - 53ms/step
Epoch 89/100
5969/5969 - 316s - loss: 4.0678e-04 - val_loss: 3.5502e-04 - 316s/epoch - 53ms/step
Epoch 90/100
5969/5969 - 317s - loss: 4.0632e-04 - val_loss: 3.3039e-04 - 317s/epoch - 53ms/step
Epoch 91/100
5969/5969 - 317s - loss: 4.0664e-04 - val_loss: 3.5023e-04 - 317s/epoch - 53ms/step
Epoch 92/100
5969/5969 - 317s - loss: 4.2206e-04 - val_loss: 3.3514e-04 - 317s/epoch - 53ms/step
Epoch 93/100
5969/5969 - 317s - loss: 4.1099e-04 - val_loss: 4.0208e-04 - 317s/epoch - 53ms/step
Epoch 94/100
5969/5969 - 317s - loss: 4.0020e-04 - val_loss: 3.3214e-04 - 317s/epoch - 53ms/step
Epoch 95/100
5969/5969 - 316s - loss: 4.0973e-04 - val_loss: 3.4149e-04 - 316s/epoch - 53ms/step
Epoch 96/100
5969/5969 - 317s - loss: 4.0079e-04 - val_loss: 3.3744e-04 - 317s/epoch - 53ms/step
Epoch 97/100
5969/5969 - 317s - loss: 4.1821e-04 - val_loss: 3.7838e-04 - 317s/epoch - 53ms/step
Epoch 98/100
5969/5969 - 317s - loss: 3.9706e-04 - val_loss: 3.5093e-04 - 317s/epoch - 53ms/step
Epoch 99/100
5969/5969 - 317s - loss: 3.9092e-04 - val_loss: 3.3774e-04 - 317s/epoch - 53ms/step
Epoch 100/100
5969/5969 - 317s - loss: 3.9956e-04 - val_loss: 3.3519e-04 - 317s/epoch - 53ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0003351875930093229
  1/332 [..............................] - ETA: 28s  7/332 [..............................] - ETA: 3s  13/332 [>.............................] - ETA: 3s 19/332 [>.............................] - ETA: 3s 25/332 [=>............................] - ETA: 2s 31/332 [=>............................] - ETA: 2s 37/332 [==>...........................] - ETA: 2s 43/332 [==>...........................] - ETA: 2s 49/332 [===>..........................] - ETA: 2s 55/332 [===>..........................] - ETA: 2s 61/332 [====>.........................] - ETA: 2s 67/332 [=====>........................] - ETA: 2s 73/332 [=====>........................] - ETA: 2s 79/332 [======>.......................] - ETA: 2s 85/332 [======>.......................] - ETA: 2s 91/332 [=======>......................] - ETA: 2s 97/332 [=======>......................] - ETA: 2s103/332 [========>.....................] - ETA: 2s109/332 [========>.....................] - ETA: 2s115/332 [=========>....................] - ETA: 2s121/332 [=========>....................] - ETA: 2s127/332 [==========>...................] - ETA: 1s133/332 [===========>..................] - ETA: 1s139/332 [===========>..................] - ETA: 1s145/332 [============>.................] - ETA: 1s151/332 [============>.................] - ETA: 1s157/332 [=============>................] - ETA: 1s163/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s175/332 [==============>...............] - ETA: 1s181/332 [===============>..............] - ETA: 1s187/332 [===============>..............] - ETA: 1s193/332 [================>.............] - ETA: 1s199/332 [================>.............] - ETA: 1s205/332 [=================>............] - ETA: 1s211/332 [==================>...........] - ETA: 1s217/332 [==================>...........] - ETA: 1s223/332 [===================>..........] - ETA: 1s229/332 [===================>..........] - ETA: 0s235/332 [====================>.........] - ETA: 0s241/332 [====================>.........] - ETA: 0s247/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s259/332 [======================>.......] - ETA: 0s265/332 [======================>.......] - ETA: 0s271/332 [=======================>......] - ETA: 0s277/332 [========================>.....] - ETA: 0s283/332 [========================>.....] - ETA: 0s289/332 [=========================>....] - ETA: 0s295/332 [=========================>....] - ETA: 0s301/332 [==========================>...] - ETA: 0s307/332 [==========================>...] - ETA: 0s313/332 [===========================>..] - ETA: 0s319/332 [===========================>..] - ETA: 0s325/332 [============================>.] - ETA: 0s331/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 10ms/step
correlation 0.0037487930580664707
cosine 0.0030153366672826453
MAE: 0.009680711
RMSE: 0.018308122
r2: 0.9782562501118331
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_3 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_5 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 3792)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Encoder
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_3 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
=================================================================
Total params: 7,209,224
Trainable params: 7,201,640
Non-trainable params: 7,584
_________________________________________________________________
Decoder
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_5 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 3792)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 7,212,384
Trainable params: 7,203,536
Non-trainable params: 8,848
_________________________________________________________________
['3n_b', 'mse', 16, 100, 0.001, 0.5, 632, 0.00039955726242624223, 0.0003351875930093229, 0.0037487930580664707, 0.0030153366672826453, 0.009680710732936859, 0.0183081217110157, 0.9782562501118331, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_6 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_6 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_8 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 3792)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Epoch 1/100
5969/5969 - 313s - loss: 0.0103 - val_loss: 0.0026 - 313s/epoch - 52ms/step
Epoch 2/100
5969/5969 - 313s - loss: 0.0026 - val_loss: 0.0021 - 313s/epoch - 52ms/step
Epoch 3/100
5969/5969 - 313s - loss: 0.0019 - val_loss: 0.0014 - 313s/epoch - 52ms/step
Epoch 4/100
5969/5969 - 313s - loss: 0.0015 - val_loss: 0.0011 - 313s/epoch - 52ms/step
Epoch 5/100
5969/5969 - 313s - loss: 0.0014 - val_loss: 0.0011 - 313s/epoch - 52ms/step
Epoch 6/100
5969/5969 - 313s - loss: 0.0012 - val_loss: 0.0010 - 313s/epoch - 52ms/step
Epoch 7/100
5969/5969 - 313s - loss: 0.0012 - val_loss: 9.4370e-04 - 313s/epoch - 52ms/step
Epoch 8/100
5969/5969 - 313s - loss: 0.0011 - val_loss: 8.7568e-04 - 313s/epoch - 52ms/step
Epoch 9/100
5969/5969 - 313s - loss: 0.0011 - val_loss: 8.8430e-04 - 313s/epoch - 52ms/step
Epoch 10/100
5969/5969 - 313s - loss: 0.0010 - val_loss: 8.0077e-04 - 313s/epoch - 52ms/step
Epoch 11/100
5969/5969 - 313s - loss: 9.7872e-04 - val_loss: 8.0651e-04 - 313s/epoch - 52ms/step
Epoch 12/100
5969/5969 - 313s - loss: 9.4739e-04 - val_loss: 7.8886e-04 - 313s/epoch - 52ms/step
Epoch 13/100
5969/5969 - 313s - loss: 9.2888e-04 - val_loss: 7.4545e-04 - 313s/epoch - 52ms/step
Epoch 14/100
5969/5969 - 313s - loss: 8.9242e-04 - val_loss: 7.2609e-04 - 313s/epoch - 52ms/step
Epoch 15/100
5969/5969 - 313s - loss: 9.1137e-04 - val_loss: 7.2873e-04 - 313s/epoch - 52ms/step
Epoch 16/100
5969/5969 - 313s - loss: 8.6657e-04 - val_loss: 8.1502e-04 - 313s/epoch - 52ms/step
Epoch 17/100
5969/5969 - 313s - loss: 8.6429e-04 - val_loss: 6.8706e-04 - 313s/epoch - 52ms/step
Epoch 18/100
5969/5969 - 313s - loss: 8.3777e-04 - val_loss: 6.6810e-04 - 313s/epoch - 52ms/step
Epoch 19/100
5969/5969 - 312s - loss: 8.2578e-04 - val_loss: 7.1557e-04 - 312s/epoch - 52ms/step
Epoch 20/100
5969/5969 - 313s - loss: 8.1626e-04 - val_loss: 6.3299e-04 - 313s/epoch - 52ms/step
Epoch 21/100
5969/5969 - 313s - loss: 8.0315e-04 - val_loss: 6.4107e-04 - 313s/epoch - 52ms/step
Epoch 22/100
5969/5969 - 313s - loss: 7.8774e-04 - val_loss: 6.2042e-04 - 313s/epoch - 52ms/step
Epoch 23/100
5969/5969 - 313s - loss: 7.6720e-04 - val_loss: 6.0774e-04 - 313s/epoch - 52ms/step
Epoch 24/100
5969/5969 - 313s - loss: 7.7483e-04 - val_loss: 6.6431e-04 - 313s/epoch - 52ms/step
Epoch 25/100
5969/5969 - 313s - loss: 8.1111e-04 - val_loss: 6.5998e-04 - 313s/epoch - 52ms/step
Epoch 26/100
5969/5969 - 313s - loss: 7.6146e-04 - val_loss: 6.1664e-04 - 313s/epoch - 52ms/step
Epoch 27/100
5969/5969 - 313s - loss: 8.8210e-04 - val_loss: 7.2202e-04 - 313s/epoch - 52ms/step
Epoch 28/100
5969/5969 - 313s - loss: 7.7771e-04 - val_loss: 6.5125e-04 - 313s/epoch - 52ms/step
Epoch 29/100
5969/5969 - 313s - loss: 7.9091e-04 - val_loss: 6.8116e-04 - 313s/epoch - 52ms/step
Epoch 30/100
5969/5969 - 313s - loss: 7.4740e-04 - val_loss: 5.7382e-04 - 313s/epoch - 52ms/step
Epoch 31/100
5969/5969 - 313s - loss: 7.3398e-04 - val_loss: 5.5460e-04 - 313s/epoch - 52ms/step
Epoch 32/100
5969/5969 - 313s - loss: 7.6073e-04 - val_loss: 6.5443e-04 - 313s/epoch - 52ms/step
Epoch 33/100
5969/5969 - 313s - loss: 7.4542e-04 - val_loss: 6.2768e-04 - 313s/epoch - 52ms/step
Epoch 34/100
5969/5969 - 313s - loss: 7.4238e-04 - val_loss: 5.9793e-04 - 313s/epoch - 52ms/step
Epoch 35/100
5969/5969 - 313s - loss: 7.2067e-04 - val_loss: 5.9205e-04 - 313s/epoch - 52ms/step
Epoch 36/100
5969/5969 - 313s - loss: 7.1034e-04 - val_loss: 5.8709e-04 - 313s/epoch - 52ms/step
Epoch 37/100
5969/5969 - 313s - loss: 7.0885e-04 - val_loss: 5.7461e-04 - 313s/epoch - 52ms/step
Epoch 38/100
5969/5969 - 313s - loss: 7.0534e-04 - val_loss: 7.2565e-04 - 313s/epoch - 52ms/step
Epoch 39/100
5969/5969 - 313s - loss: 6.9869e-04 - val_loss: 5.3692e-04 - 313s/epoch - 52ms/step
Epoch 40/100
5969/5969 - 313s - loss: 6.7825e-04 - val_loss: 5.5170e-04 - 313s/epoch - 52ms/step
Epoch 41/100
5969/5969 - 313s - loss: 6.9735e-04 - val_loss: 5.3402e-04 - 313s/epoch - 52ms/step
Epoch 42/100
5969/5969 - 313s - loss: 6.8469e-04 - val_loss: 5.2594e-04 - 313s/epoch - 52ms/step
Epoch 43/100
5969/5969 - 312s - loss: 6.8393e-04 - val_loss: 5.7359e-04 - 312s/epoch - 52ms/step
Epoch 44/100
5969/5969 - 312s - loss: 6.6422e-04 - val_loss: 5.5279e-04 - 312s/epoch - 52ms/step
Epoch 45/100
5969/5969 - 311s - loss: 6.7440e-04 - val_loss: 5.6774e-04 - 311s/epoch - 52ms/step
Epoch 46/100
5969/5969 - 311s - loss: 6.6501e-04 - val_loss: 5.1664e-04 - 311s/epoch - 52ms/step
Epoch 47/100
5969/5969 - 311s - loss: 6.7815e-04 - val_loss: 5.3510e-04 - 311s/epoch - 52ms/step
Epoch 48/100
5969/5969 - 311s - loss: 6.6207e-04 - val_loss: 5.2525e-04 - 311s/epoch - 52ms/step
Epoch 49/100
5969/5969 - 312s - loss: 6.7607e-04 - val_loss: 5.2952e-04 - 312s/epoch - 52ms/step
Epoch 50/100
5969/5969 - 311s - loss: 6.5007e-04 - val_loss: 5.5571e-04 - 311s/epoch - 52ms/step
Epoch 51/100
5969/5969 - 312s - loss: 6.5875e-04 - val_loss: 5.1357e-04 - 312s/epoch - 52ms/step
Epoch 52/100
5969/5969 - 312s - loss: 6.3909e-04 - val_loss: 5.3252e-04 - 312s/epoch - 52ms/step
Epoch 53/100
5969/5969 - 311s - loss: 6.3856e-04 - val_loss: 5.1801e-04 - 311s/epoch - 52ms/step
Epoch 54/100
5969/5969 - 312s - loss: 6.6579e-04 - val_loss: 6.5492e-04 - 312s/epoch - 52ms/step
Epoch 55/100
5969/5969 - 312s - loss: 6.4799e-04 - val_loss: 4.9795e-04 - 312s/epoch - 52ms/step
Epoch 56/100
5969/5969 - 311s - loss: 6.3509e-04 - val_loss: 5.1905e-04 - 311s/epoch - 52ms/step
Epoch 57/100
5969/5969 - 312s - loss: 6.2755e-04 - val_loss: 5.0581e-04 - 312s/epoch - 52ms/step
Epoch 58/100
5969/5969 - 311s - loss: 6.3692e-04 - val_loss: 5.3803e-04 - 311s/epoch - 52ms/step
Epoch 59/100
5969/5969 - 312s - loss: 6.3628e-04 - val_loss: 5.3819e-04 - 312s/epoch - 52ms/step
Epoch 60/100
5969/5969 - 312s - loss: 6.5490e-04 - val_loss: 5.0455e-04 - 312s/epoch - 52ms/step
Epoch 61/100
5969/5969 - 311s - loss: 6.3880e-04 - val_loss: 5.2589e-04 - 311s/epoch - 52ms/step
Epoch 62/100
5969/5969 - 312s - loss: 6.2614e-04 - val_loss: 5.7912e-04 - 312s/epoch - 52ms/step
Epoch 63/100
5969/5969 - 312s - loss: 6.3273e-04 - val_loss: 5.3372e-04 - 312s/epoch - 52ms/step
Epoch 64/100
5969/5969 - 311s - loss: 6.1725e-04 - val_loss: 5.0816e-04 - 311s/epoch - 52ms/step
Epoch 65/100
5969/5969 - 312s - loss: 6.2804e-04 - val_loss: 5.7349e-04 - 312s/epoch - 52ms/step
Epoch 66/100
5969/5969 - 312s - loss: 6.4251e-04 - val_loss: 4.8913e-04 - 312s/epoch - 52ms/step
Epoch 67/100
5969/5969 - 312s - loss: 6.2272e-04 - val_loss: 5.1479e-04 - 312s/epoch - 52ms/step
Epoch 68/100
5969/5969 - 312s - loss: 6.0951e-04 - val_loss: 5.6863e-04 - 312s/epoch - 52ms/step
Epoch 69/100
5969/5969 - 311s - loss: 6.0721e-04 - val_loss: 5.6838e-04 - 311s/epoch - 52ms/step
Epoch 70/100
5969/5969 - 312s - loss: 5.9752e-04 - val_loss: 5.2362e-04 - 312s/epoch - 52ms/step
Epoch 71/100
5969/5969 - 311s - loss: 6.0813e-04 - val_loss: 5.0316e-04 - 311s/epoch - 52ms/step
Epoch 72/100
5969/5969 - 312s - loss: 6.0963e-04 - val_loss: 5.6941e-04 - 312s/epoch - 52ms/step
Epoch 73/100
5969/5969 - 311s - loss: 5.9740e-04 - val_loss: 5.1459e-04 - 311s/epoch - 52ms/step
Epoch 74/100
5969/5969 - 312s - loss: 6.1329e-04 - val_loss: 5.0220e-04 - 312s/epoch - 52ms/step
Epoch 75/100
5969/5969 - 312s - loss: 5.9896e-04 - val_loss: 5.4804e-04 - 312s/epoch - 52ms/step
Epoch 76/100
5969/5969 - 312s - loss: 5.8937e-04 - val_loss: 5.1517e-04 - 312s/epoch - 52ms/step
Epoch 77/100
5969/5969 - 311s - loss: 5.9222e-04 - val_loss: 5.2507e-04 - 311s/epoch - 52ms/step
Epoch 78/100
5969/5969 - 312s - loss: 6.1884e-04 - val_loss: 5.3291e-04 - 312s/epoch - 52ms/step
Epoch 79/100
5969/5969 - 312s - loss: 5.8653e-04 - val_loss: 5.0124e-04 - 312s/epoch - 52ms/step
Epoch 80/100
5969/5969 - 312s - loss: 5.9448e-04 - val_loss: 5.0424e-04 - 312s/epoch - 52ms/step
Epoch 81/100
5969/5969 - 312s - loss: 5.9119e-04 - val_loss: 5.4411e-04 - 312s/epoch - 52ms/step
Epoch 82/100
5969/5969 - 312s - loss: 6.0641e-04 - val_loss: 4.5917e-04 - 312s/epoch - 52ms/step
Epoch 83/100
5969/5969 - 312s - loss: 5.9728e-04 - val_loss: 5.4116e-04 - 312s/epoch - 52ms/step
Epoch 84/100
5969/5969 - 312s - loss: 5.7942e-04 - val_loss: 5.2458e-04 - 312s/epoch - 52ms/step
Epoch 85/100
5969/5969 - 312s - loss: 5.9425e-04 - val_loss: 4.9453e-04 - 312s/epoch - 52ms/step
Epoch 86/100
5969/5969 - 312s - loss: 6.1175e-04 - val_loss: 6.2009e-04 - 312s/epoch - 52ms/step
Epoch 87/100
5969/5969 - 312s - loss: 5.8431e-04 - val_loss: 5.7895e-04 - 312s/epoch - 52ms/step
Epoch 88/100
5969/5969 - 312s - loss: 5.7523e-04 - val_loss: 4.8518e-04 - 312s/epoch - 52ms/step
Epoch 89/100
5969/5969 - 312s - loss: 5.8576e-04 - val_loss: 5.2154e-04 - 312s/epoch - 52ms/step
Epoch 90/100
5969/5969 - 312s - loss: 5.7175e-04 - val_loss: 5.6489e-04 - 312s/epoch - 52ms/step
Epoch 91/100
5969/5969 - 312s - loss: 5.7922e-04 - val_loss: 4.9137e-04 - 312s/epoch - 52ms/step
Epoch 92/100
5969/5969 - 312s - loss: 5.7716e-04 - val_loss: 6.9507e-04 - 312s/epoch - 52ms/step
Epoch 93/100
5969/5969 - 312s - loss: 5.7447e-04 - val_loss: 5.2262e-04 - 312s/epoch - 52ms/step
Epoch 94/100
5969/5969 - 312s - loss: 5.8037e-04 - val_loss: 5.0135e-04 - 312s/epoch - 52ms/step
Epoch 95/100
5969/5969 - 312s - loss: 5.7546e-04 - val_loss: 5.0971e-04 - 312s/epoch - 52ms/step
Epoch 96/100
5969/5969 - 312s - loss: 5.7452e-04 - val_loss: 5.9298e-04 - 312s/epoch - 52ms/step
Epoch 97/100
5969/5969 - 312s - loss: 5.8250e-04 - val_loss: 0.0010 - 312s/epoch - 52ms/step
Epoch 98/100
5969/5969 - 312s - loss: 5.8922e-04 - val_loss: 8.9148e-04 - 312s/epoch - 52ms/step
Epoch 99/100
5969/5969 - 312s - loss: 5.9902e-04 - val_loss: 6.4149e-04 - 312s/epoch - 52ms/step
Epoch 100/100
5969/5969 - 312s - loss: 5.6623e-04 - val_loss: 6.5171e-04 - 312s/epoch - 52ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0006517143337987363
  1/332 [..............................] - ETA: 27s  7/332 [..............................] - ETA: 3s  13/332 [>.............................] - ETA: 3s 19/332 [>.............................] - ETA: 3s 25/332 [=>............................] - ETA: 2s 31/332 [=>............................] - ETA: 2s 37/332 [==>...........................] - ETA: 2s 43/332 [==>...........................] - ETA: 2s 49/332 [===>..........................] - ETA: 2s 55/332 [===>..........................] - ETA: 2s 61/332 [====>.........................] - ETA: 2s 67/332 [=====>........................] - ETA: 2s 73/332 [=====>........................] - ETA: 2s 79/332 [======>.......................] - ETA: 2s 85/332 [======>.......................] - ETA: 2s 91/332 [=======>......................] - ETA: 2s 97/332 [=======>......................] - ETA: 2s103/332 [========>.....................] - ETA: 2s109/332 [========>.....................] - ETA: 2s115/332 [=========>....................] - ETA: 2s121/332 [=========>....................] - ETA: 2s127/332 [==========>...................] - ETA: 1s133/332 [===========>..................] - ETA: 1s139/332 [===========>..................] - ETA: 1s145/332 [============>.................] - ETA: 1s151/332 [============>.................] - ETA: 1s157/332 [=============>................] - ETA: 1s163/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s175/332 [==============>...............] - ETA: 1s181/332 [===============>..............] - ETA: 1s187/332 [===============>..............] - ETA: 1s193/332 [================>.............] - ETA: 1s199/332 [================>.............] - ETA: 1s205/332 [=================>............] - ETA: 1s211/332 [==================>...........] - ETA: 1s217/332 [==================>...........] - ETA: 1s223/332 [===================>..........] - ETA: 1s229/332 [===================>..........] - ETA: 0s235/332 [====================>.........] - ETA: 0s241/332 [====================>.........] - ETA: 0s247/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s259/332 [======================>.......] - ETA: 0s265/332 [======================>.......] - ETA: 0s271/332 [=======================>......] - ETA: 0s277/332 [========================>.....] - ETA: 0s283/332 [========================>.....] - ETA: 0s289/332 [=========================>....] - ETA: 0s295/332 [=========================>....] - ETA: 0s301/332 [==========================>...] - ETA: 0s307/332 [==========================>...] - ETA: 0s313/332 [===========================>..] - ETA: 0s319/332 [===========================>..] - ETA: 0s325/332 [============================>.] - ETA: 0s331/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 10ms/step
correlation 0.007198509234686505
cosine 0.005886723410620404
MAE: 0.012330529
RMSE: 0.025528684
r2: 0.957721930855765
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_6 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_8 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 3792)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Encoder
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_6 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
=================================================================
Total params: 7,209,224
Trainable params: 7,201,640
Non-trainable params: 7,584
_________________________________________________________________
Decoder
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 3792)              2400336   
                                                                 
 batch_normalization_8 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 3792)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 7,212,384
Trainable params: 7,203,536
Non-trainable params: 8,848
_________________________________________________________________
['3n_b', 'mse', 16, 100, 0.002, 0.5, 632, 0.0005662282928824425, 0.0006517143337987363, 0.007198509234686505, 0.005886723410620404, 0.012330529280006886, 0.025528684258461, 0.957721930855765, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_3n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_9 (Dense)             (None, 3792)              4796880   
                                                                 
 batch_normalization_9 (Batc  (None, 3792)             15168     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 3792)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               2397176   
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 3792)              2400336   
                                                                 
 batch_normalization_11 (Bat  (None, 3792)             15168     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 3792)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              4794352   
                                                                 
=================================================================
Total params: 14,421,608
Trainable params: 14,405,176
Non-trainable params: 16,432
_________________________________________________________________
Epoch 1/200
5969/5969 - 319s - loss: 0.0089 - val_loss: 0.0037 - 319s/epoch - 53ms/step
Epoch 2/200
5969/5969 - 319s - loss: 0.0037 - val_loss: 0.0020 - 319s/epoch - 53ms/step
Epoch 3/200
5969/5969 - 319s - loss: 0.0022 - val_loss: 0.0014 - 319s/epoch - 53ms/step
Epoch 4/200
5969/5969 - 319s - loss: 0.0016 - val_loss: 0.0011 - 319s/epoch - 53ms/step
Epoch 5/200
5969/5969 - 319s - loss: 0.0013 - val_loss: 9.1255e-04 - 319s/epoch - 53ms/step
Epoch 6/200
5969/5969 - 319s - loss: 0.0011 - val_loss: 9.4692e-04 - 319s/epoch - 53ms/step
Epoch 7/200
5969/5969 - 319s - loss: 9.6611e-04 - val_loss: 8.0662e-04 - 319s/epoch - 54ms/step
Epoch 8/200
5969/5969 - 319s - loss: 8.9444e-04 - val_loss: 7.1367e-04 - 319s/epoch - 53ms/step
Epoch 9/200
5969/5969 - 319s - loss: 8.2797e-04 - val_loss: 6.6203e-04 - 319s/epoch - 53ms/step
Epoch 10/200
5969/5969 - 318s - loss: 7.9298e-04 - val_loss: 6.3912e-04 - 318s/epoch - 53ms/step
Epoch 11/200
5969/5969 - 318s - loss: 7.5770e-04 - val_loss: 6.0825e-04 - 318s/epoch - 53ms/step
Epoch 12/200
5969/5969 - 318s - loss: 7.2105e-04 - val_loss: 6.1710e-04 - 318s/epoch - 53ms/step
Epoch 13/200
5969/5969 - 318s - loss: 6.8905e-04 - val_loss: 5.5847e-04 - 318s/epoch - 53ms/step
Epoch 14/200
5969/5969 - 318s - loss: 6.5336e-04 - val_loss: 5.2985e-04 - 318s/epoch - 53ms/step
Epoch 15/200
5969/5969 - 318s - loss: 6.4248e-04 - val_loss: 5.4911e-04 - 318s/epoch - 53ms/step
Epoch 16/200
5969/5969 - 318s - loss: 6.0945e-04 - val_loss: 5.5477e-04 - 318s/epoch - 53ms/step
Epoch 17/200
5969/5969 - 318s - loss: 5.9684e-04 - val_loss: 5.0148e-04 - 318s/epoch - 53ms/step
Epoch 18/200
5969/5969 - 318s - loss: 5.8252e-04 - val_loss: 5.0021e-04 - 318s/epoch - 53ms/step
Epoch 19/200
5969/5969 - 318s - loss: 5.6404e-04 - val_loss: 5.0026e-04 - 318s/epoch - 53ms/step
Epoch 20/200
5969/5969 - 318s - loss: 5.5493e-04 - val_loss: 4.6063e-04 - 318s/epoch - 53ms/step
Epoch 21/200
5969/5969 - 318s - loss: 5.4279e-04 - val_loss: 4.4139e-04 - 318s/epoch - 53ms/step
Epoch 22/200
5969/5969 - 320s - loss: 5.4459e-04 - val_loss: 4.5221e-04 - 320s/epoch - 54ms/step
Epoch 23/200
5969/5969 - 325s - loss: 5.2199e-04 - val_loss: 4.3554e-04 - 325s/epoch - 54ms/step
Epoch 24/200
5969/5969 - 335s - loss: 5.1260e-04 - val_loss: 4.5595e-04 - 335s/epoch - 56ms/step
Epoch 25/200
5969/5969 - 335s - loss: 5.0228e-04 - val_loss: 4.2169e-04 - 335s/epoch - 56ms/step
Epoch 26/200
5969/5969 - 335s - loss: 4.9485e-04 - val_loss: 3.9750e-04 - 335s/epoch - 56ms/step
Epoch 27/200
5969/5969 - 335s - loss: 4.8277e-04 - val_loss: 3.8894e-04 - 335s/epoch - 56ms/step
Epoch 28/200
5969/5969 - 335s - loss: 4.7801e-04 - val_loss: 3.8984e-04 - 335s/epoch - 56ms/step
Epoch 29/200
5969/5969 - 335s - loss: 4.7793e-04 - val_loss: 3.9843e-04 - 335s/epoch - 56ms/step
Epoch 30/200
5969/5969 - 335s - loss: 4.6515e-04 - val_loss: 3.7555e-04 - 335s/epoch - 56ms/step
Epoch 31/200
5969/5969 - 335s - loss: 4.5783e-04 - val_loss: 3.6674e-04 - 335s/epoch - 56ms/step
Epoch 32/200
5969/5969 - 335s - loss: 4.4948e-04 - val_loss: 3.8660e-04 - 335s/epoch - 56ms/step
Epoch 33/200
5969/5969 - 335s - loss: 4.4544e-04 - val_loss: 3.7533e-04 - 335s/epoch - 56ms/step
Epoch 34/200
5969/5969 - 335s - loss: 4.4078e-04 - val_loss: 3.4725e-04 - 335s/epoch - 56ms/step
Epoch 35/200
5969/5969 - 335s - loss: 4.3244e-04 - val_loss: 3.4624e-04 - 335s/epoch - 56ms/step
Epoch 36/200
5969/5969 - 335s - loss: 4.4085e-04 - val_loss: 3.5033e-04 - 335s/epoch - 56ms/step
Epoch 37/200
5969/5969 - 335s - loss: 4.2670e-04 - val_loss: 3.6176e-04 - 335s/epoch - 56ms/step
Epoch 38/200
5969/5969 - 335s - loss: 4.2247e-04 - val_loss: 3.4029e-04 - 335s/epoch - 56ms/step
Epoch 39/200
5969/5969 - 335s - loss: 4.2059e-04 - val_loss: 3.4321e-04 - 335s/epoch - 56ms/step
Epoch 40/200
5969/5969 - 335s - loss: 4.1112e-04 - val_loss: 3.4775e-04 - 335s/epoch - 56ms/step
Epoch 41/200
5969/5969 - 335s - loss: 4.1221e-04 - val_loss: 3.2797e-04 - 335s/epoch - 56ms/step
Epoch 42/200
5969/5969 - 335s - loss: 4.0536e-04 - val_loss: 3.2776e-04 - 335s/epoch - 56ms/step
Epoch 43/200
5969/5969 - 335s - loss: 4.1894e-04 - val_loss: 3.2809e-04 - 335s/epoch - 56ms/step
Epoch 44/200
5969/5969 - 335s - loss: 3.9852e-04 - val_loss: 3.4501e-04 - 335s/epoch - 56ms/step
Epoch 45/200
5969/5969 - 335s - loss: 4.0092e-04 - val_loss: 3.1549e-04 - 335s/epoch - 56ms/step
Epoch 46/200
5969/5969 - 335s - loss: 3.8847e-04 - val_loss: 3.1122e-04 - 335s/epoch - 56ms/step
Epoch 47/200
5969/5969 - 335s - loss: 3.9470e-04 - val_loss: 3.2137e-04 - 335s/epoch - 56ms/step
Epoch 48/200
5969/5969 - 335s - loss: 3.9924e-04 - val_loss: 3.1507e-04 - 335s/epoch - 56ms/step
Epoch 49/200
5969/5969 - 335s - loss: 3.8621e-04 - val_loss: 3.1782e-04 - 335s/epoch - 56ms/step
Epoch 50/200
5969/5969 - 335s - loss: 3.7963e-04 - val_loss: 3.1200e-04 - 335s/epoch - 56ms/step
Epoch 51/200
5969/5969 - 335s - loss: 3.8603e-04 - val_loss: 3.1352e-04 - 335s/epoch - 56ms/step
Epoch 52/200
5969/5969 - 335s - loss: 3.7624e-04 - val_loss: 3.0198e-04 - 335s/epoch - 56ms/step
Epoch 53/200
5969/5969 - 335s - loss: 3.7686e-04 - val_loss: 3.4391e-04 - 335s/epoch - 56ms/step
Epoch 54/200
5969/5969 - 335s - loss: 3.7432e-04 - val_loss: 3.7001e-04 - 335s/epoch - 56ms/step
Epoch 55/200
5969/5969 - 335s - loss: 3.6747e-04 - val_loss: 2.9292e-04 - 335s/epoch - 56ms/step
Epoch 56/200
5969/5969 - 335s - loss: 3.6978e-04 - val_loss: 2.9496e-04 - 335s/epoch - 56ms/step
Epoch 57/200
5969/5969 - 335s - loss: 3.6511e-04 - val_loss: 2.8308e-04 - 335s/epoch - 56ms/step
Epoch 58/200
5969/5969 - 335s - loss: 3.6686e-04 - val_loss: 3.0117e-04 - 335s/epoch - 56ms/step
Epoch 59/200
5969/5969 - 335s - loss: 3.5870e-04 - val_loss: 3.1282e-04 - 335s/epoch - 56ms/step
Epoch 60/200
5969/5969 - 335s - loss: 3.5871e-04 - val_loss: 2.9340e-04 - 335s/epoch - 56ms/step
Epoch 61/200
5969/5969 - 335s - loss: 3.5609e-04 - val_loss: 3.0409e-04 - 335s/epoch - 56ms/step
Epoch 62/200
5969/5969 - 335s - loss: 3.5107e-04 - val_loss: 3.1889e-04 - 335s/epoch - 56ms/step
Epoch 63/200
5969/5969 - 335s - loss: 3.6410e-04 - val_loss: 2.9159e-04 - 335s/epoch - 56ms/step
Epoch 64/200
5969/5969 - 335s - loss: 3.4645e-04 - val_loss: 2.9880e-04 - 335s/epoch - 56ms/step
Epoch 65/200
5969/5969 - 335s - loss: 3.6085e-04 - val_loss: 3.0445e-04 - 335s/epoch - 56ms/step
Epoch 66/200
5969/5969 - 335s - loss: 3.5415e-04 - val_loss: 2.8484e-04 - 335s/epoch - 56ms/step
Epoch 67/200
5969/5969 - 335s - loss: 3.5346e-04 - val_loss: 2.8741e-04 - 335s/epoch - 56ms/step
Epoch 68/200
5969/5969 - 335s - loss: 3.4668e-04 - val_loss: 2.8500e-04 - 335s/epoch - 56ms/step
Epoch 69/200
5969/5969 - 335s - loss: 3.3997e-04 - val_loss: 2.7510e-04 - 335s/epoch - 56ms/step
Epoch 70/200
5969/5969 - 335s - loss: 3.3858e-04 - val_loss: 2.8386e-04 - 335s/epoch - 56ms/step
Epoch 71/200
5969/5969 - 335s - loss: 3.4565e-04 - val_loss: 2.9076e-04 - 335s/epoch - 56ms/step
Epoch 72/200
5969/5969 - 335s - loss: 3.4960e-04 - val_loss: 2.9159e-04 - 335s/epoch - 56ms/step
Epoch 73/200
5969/5969 - 335s - loss: 3.3820e-04 - val_loss: 2.8242e-04 - 335s/epoch - 56ms/step
Epoch 74/200
5969/5969 - 335s - loss: 3.3520e-04 - val_loss: 2.8239e-04 - 335s/epoch - 56ms/step
Epoch 75/200
5969/5969 - 335s - loss: 3.3540e-04 - val_loss: 2.9007e-04 - 335s/epoch - 56ms/step
Epoch 76/200
5969/5969 - 335s - loss: 3.3076e-04 - val_loss: 2.7902e-04 - 335s/epoch - 56ms/step
Epoch 77/200
5969/5969 - 335s - loss: 3.3547e-04 - val_loss: 2.7178e-04 - 335s/epoch - 56ms/step
Epoch 78/200
5969/5969 - 335s - loss: 3.2867e-04 - val_loss: 2.8969e-04 - 335s/epoch - 56ms/step
Epoch 79/200
5969/5969 - 335s - loss: 3.2755e-04 - val_loss: 2.8286e-04 - 335s/epoch - 56ms/step
Epoch 80/200
5969/5969 - 335s - loss: 3.2768e-04 - val_loss: 2.6880e-04 - 335s/epoch - 56ms/step
Epoch 81/200
5969/5969 - 335s - loss: 3.2778e-04 - val_loss: 2.8081e-04 - 335s/epoch - 56ms/step
Epoch 82/200
5969/5969 - 335s - loss: 3.2725e-04 - val_loss: 2.6485e-04 - 335s/epoch - 56ms/step
Epoch 83/200
5969/5969 - 335s - loss: 3.3226e-04 - val_loss: 3.1310e-04 - 335s/epoch - 56ms/step
Epoch 84/200
5969/5969 - 335s - loss: 3.2362e-04 - val_loss: 2.7558e-04 - 335s/epoch - 56ms/step
Epoch 85/200
5969/5969 - 335s - loss: 3.2845e-04 - val_loss: 3.0500e-04 - 335s/epoch - 56ms/step
Epoch 86/200
5969/5969 - 335s - loss: 3.2563e-04 - val_loss: 2.9544e-04 - 335s/epoch - 56ms/step
Epoch 87/200
5969/5969 - 335s - loss: 3.2390e-04 - val_loss: 2.9502e-04 - 335s/epoch - 56ms/step
Epoch 88/200
5969/5969 - 335s - loss: 3.1947e-04 - val_loss: 2.5989e-04 - 335s/epoch - 56ms/step
Epoch 89/200
5969/5969 - 335s - loss: 3.1828e-04 - val_loss: 2.9313e-04 - 335s/epoch - 56ms/step
Epoch 90/200
5969/5969 - 335s - loss: 3.2223e-04 - val_loss: 2.7152e-04 - 335s/epoch - 56ms/step
Epoch 91/200
5969/5969 - 335s - loss: 3.2112e-04 - val_loss: 2.7483e-04 - 335s/epoch - 56ms/step
Epoch 92/200
5969/5969 - 335s - loss: 3.1502e-04 - val_loss: 2.9444e-04 - 335s/epoch - 56ms/step
Epoch 93/200
5969/5969 - 335s - loss: 3.1196e-04 - val_loss: 2.6424e-04 - 335s/epoch - 56ms/step
Epoch 94/200
5969/5969 - 335s - loss: 3.1155e-04 - val_loss: 2.8785e-04 - 335s/epoch - 56ms/step
Epoch 95/200
5969/5969 - 335s - loss: 3.1661e-04 - val_loss: 2.7283e-04 - 335s/epoch - 56ms/step
Epoch 96/200
5969/5969 - 335s - loss: 3.1295e-04 - val_loss: 2.9305e-04 - 335s/epoch - 56ms/step
Epoch 97/200
5969/5969 - 335s - loss: 3.2016e-04 - val_loss: 2.7830e-04 - 335s/epoch - 56ms/step
Epoch 98/200
5969/5969 - 335s - loss: 3.1901e-04 - val_loss: 2.6876e-04 - 335s/epoch - 56ms/step
Epoch 99/200
5969/5969 - 335s - loss: 3.0956e-04 - val_loss: 2.6564e-04 - 335s/epoch - 56ms/step
Epoch 100/200
5969/5969 - 335s - loss: 3.0483e-04 - val_loss: 2.5770e-04 - 335s/epoch - 56ms/step
Epoch 101/200
5969/5969 - 335s - loss: 3.0163e-04 - val_loss: 2.5293e-04 - 335s/epoch - 56ms/step
Epoch 102/200
5969/5969 - 335s - loss: 3.1511e-04 - val_loss: 2.6135e-04 - 335s/epoch - 56ms/step
Epoch 103/200
5969/5969 - 335s - loss: 3.0526e-04 - val_loss: 2.5923e-04 - 335s/epoch - 56ms/step
Epoch 104/200
5969/5969 - 335s - loss: 3.0613e-04 - val_loss: 2.6307e-04 - 335s/epoch - 56ms/step
Epoch 105/200
5969/5969 - 335s - loss: 3.0157e-04 - val_loss: 2.7451e-04 - 335s/epoch - 56ms/step
Epoch 106/200
5969/5969 - 335s - loss: 3.0711e-04 - val_loss: 2.6659e-04 - 335s/epoch - 56ms/step
Epoch 107/200
5969/5969 - 335s - loss: 3.0325e-04 - val_loss: 3.0147e-04 - 335s/epoch - 56ms/step
Epoch 108/200
5969/5969 - 335s - loss: 3.0014e-04 - val_loss: 2.5563e-04 - 335s/epoch - 56ms/step
Epoch 109/200
5969/5969 - 335s - loss: 2.9593e-04 - val_loss: 2.7618e-04 - 335s/epoch - 56ms/step
Epoch 110/200
5969/5969 - 334s - loss: 3.0270e-04 - val_loss: 2.6618e-04 - 334s/epoch - 56ms/step
Epoch 111/200
5969/5969 - 334s - loss: 2.9860e-04 - val_loss: 2.6118e-04 - 334s/epoch - 56ms/step
Epoch 112/200
5969/5969 - 334s - loss: 3.0302e-04 - val_loss: 2.6488e-04 - 334s/epoch - 56ms/step
Epoch 113/200
5969/5969 - 334s - loss: 2.9375e-04 - val_loss: 2.5644e-04 - 334s/epoch - 56ms/step
Epoch 114/200
5969/5969 - 335s - loss: 2.9605e-04 - val_loss: 2.6485e-04 - 335s/epoch - 56ms/step
Epoch 115/200
5969/5969 - 334s - loss: 2.9540e-04 - val_loss: 2.4270e-04 - 334s/epoch - 56ms/step
Epoch 116/200
5969/5969 - 334s - loss: 2.9313e-04 - val_loss: 2.4782e-04 - 334s/epoch - 56ms/step
Epoch 117/200
5969/5969 - 334s - loss: 2.9299e-04 - val_loss: 2.5229e-04 - 334s/epoch - 56ms/step
Epoch 118/200
5969/5969 - 334s - loss: 3.0279e-04 - val_loss: 2.6141e-04 - 334s/epoch - 56ms/step
Epoch 119/200
5969/5969 - 335s - loss: 2.9776e-04 - val_loss: 2.4936e-04 - 335s/epoch - 56ms/step
Epoch 120/200
5969/5969 - 335s - loss: 2.9337e-04 - val_loss: 2.5479e-04 - 335s/epoch - 56ms/step
Epoch 121/200
5969/5969 - 335s - loss: 2.9250e-04 - val_loss: 2.6295e-04 - 335s/epoch - 56ms/step
Epoch 122/200
5969/5969 - 335s - loss: 2.9505e-04 - val_loss: 2.3302e-04 - 335s/epoch - 56ms/step
Epoch 123/200
5969/5969 - 335s - loss: 2.9306e-04 - val_loss: 2.4997e-04 - 335s/epoch - 56ms/step
Epoch 124/200
5969/5969 - 335s - loss: 2.8959e-04 - val_loss: 2.5009e-04 - 335s/epoch - 56ms/step
Epoch 125/200
5969/5969 - 335s - loss: 2.9143e-04 - val_loss: 2.4728e-04 - 335s/epoch - 56ms/step
Epoch 126/200
5969/5969 - 335s - loss: 2.8669e-04 - val_loss: 3.2860e-04 - 335s/epoch - 56ms/step
Epoch 127/200
5969/5969 - 335s - loss: 2.8972e-04 - val_loss: 3.9873e-04 - 335s/epoch - 56ms/step
Epoch 128/200
5969/5969 - 335s - loss: 2.9046e-04 - val_loss: 2.9083e-04 - 335s/epoch - 56ms/step
Epoch 129/200
5969/5969 - 335s - loss: 2.8951e-04 - val_loss: 2.4186e-04 - 335s/epoch - 56ms/step
Epoch 130/200
5969/5969 - 335s - loss: 2.8689e-04 - val_loss: 2.4892e-04 - 335s/epoch - 56ms/step
Epoch 131/200
5969/5969 - 335s - loss: 2.8469e-04 - val_loss: 3.0582e-04 - 335s/epoch - 56ms/step
Epoch 132/200
5969/5969 - 335s - loss: 3.0168e-04 - val_loss: 3.9641e-04 - 335s/epoch - 56ms/step
Epoch 133/200
5969/5969 - 335s - loss: 2.8876e-04 - val_loss: 3.0551e-04 - 335s/epoch - 56ms/step
Epoch 134/200
5969/5969 - 335s - loss: 2.8501e-04 - val_loss: 2.6182e-04 - 335s/epoch - 56ms/step
Epoch 135/200
5969/5969 - 335s - loss: 2.8395e-04 - val_loss: 2.7161e-04 - 335s/epoch - 56ms/step
Epoch 136/200
5969/5969 - 335s - loss: 2.8271e-04 - val_loss: 2.5152e-04 - 335s/epoch - 56ms/step
Epoch 137/200
5969/5969 - 335s - loss: 2.9300e-04 - val_loss: 2.5279e-04 - 335s/epoch - 56ms/step
Epoch 138/200
5969/5969 - 335s - loss: 2.8188e-04 - val_loss: 2.5985e-04 - 335s/epoch - 56ms/step
Epoch 139/200
5969/5969 - 335s - loss: 2.8119e-04 - val_loss: 2.4088e-04 - 335s/epoch - 56ms/step
Epoch 140/200
5969/5969 - 335s - loss: 2.8048e-04 - val_loss: 2.5157e-04 - 335s/epoch - 56ms/step
Epoch 141/200
5969/5969 - 335s - loss: 2.8523e-04 - val_loss: 2.2435e-04 - 335s/epoch - 56ms/step
Epoch 142/200
5969/5969 - 335s - loss: 2.7742e-04 - val_loss: 2.8780e-04 - 335s/epoch - 56ms/step
Epoch 143/200
5969/5969 - 335s - loss: 2.8038e-04 - val_loss: 2.5501e-04 - 335s/epoch - 56ms/step
Epoch 144/200
5969/5969 - 335s - loss: 2.7772e-04 - val_loss: 2.3478e-04 - 335s/epoch - 56ms/step
Epoch 145/200
5969/5969 - 335s - loss: 2.8225e-04 - val_loss: 2.5702e-04 - 335s/epoch - 56ms/step
Epoch 146/200
5969/5969 - 335s - loss: 2.8326e-04 - val_loss: 2.5233e-04 - 335s/epoch - 56ms/step
Epoch 147/200
5969/5969 - 335s - loss: 2.7897e-04 - val_loss: 2.7420e-04 - 335s/epoch - 56ms/step
Epoch 148/200
5969/5969 - 335s - loss: 2.7630e-04 - val_loss: 2.8387e-04 - 335s/epoch - 56ms/step
Epoch 149/200
5969/5969 - 335s - loss: 2.7676e-04 - val_loss: 2.8330e-04 - 335s/epoch - 56ms/step
Epoch 150/200
5969/5969 - 335s - loss: 2.7764e-04 - val_loss: 2.6216e-04 - 335s/epoch - 56ms/step
Epoch 151/200
5969/5969 - 334s - loss: 2.7490e-04 - val_loss: 2.4555e-04 - 334s/epoch - 56ms/step
Epoch 152/200
5969/5969 - 335s - loss: 2.7416e-04 - val_loss: 2.5345e-04 - 335s/epoch - 56ms/step
Epoch 153/200
5969/5969 - 334s - loss: 2.7328e-04 - val_loss: 2.6852e-04 - 334s/epoch - 56ms/step
Epoch 154/200
5969/5969 - 334s - loss: 2.9023e-04 - val_loss: 2.3368e-04 - 334s/epoch - 56ms/step
Epoch 155/200
5969/5969 - 334s - loss: 2.7915e-04 - val_loss: 2.3211e-04 - 334s/epoch - 56ms/step
Epoch 156/200
5969/5969 - 334s - loss: 2.7598e-04 - val_loss: 2.8770e-04 - 334s/epoch - 56ms/step
Epoch 157/200
5969/5969 - 334s - loss: 2.7284e-04 - val_loss: 2.4076e-04 - 334s/epoch - 56ms/step
Epoch 158/200
5969/5969 - 334s - loss: 2.7553e-04 - val_loss: 2.5290e-04 - 334s/epoch - 56ms/step
Epoch 159/200
5969/5969 - 334s - loss: 2.8855e-04 - val_loss: 2.2668e-04 - 334s/epoch - 56ms/step
Epoch 160/200
5969/5969 - 334s - loss: 2.7422e-04 - val_loss: 2.2766e-04 - 334s/epoch - 56ms/step
Epoch 161/200
5969/5969 - 334s - loss: 2.8764e-04 - val_loss: 2.4754e-04 - 334s/epoch - 56ms/step
Epoch 162/200
5969/5969 - 334s - loss: 2.7206e-04 - val_loss: 2.5335e-04 - 334s/epoch - 56ms/step
Epoch 163/200
5969/5969 - 334s - loss: 2.7011e-04 - val_loss: 2.4755e-04 - 334s/epoch - 56ms/step
Epoch 164/200
5969/5969 - 334s - loss: 2.8422e-04 - val_loss: 2.4494e-04 - 334s/epoch - 56ms/step
Epoch 165/200
5969/5969 - 334s - loss: 2.7447e-04 - val_loss: 2.4302e-04 - 334s/epoch - 56ms/step
Epoch 166/200
5969/5969 - 334s - loss: 2.6984e-04 - val_loss: 2.5133e-04 - 334s/epoch - 56ms/step
Epoch 167/200
5969/5969 - 334s - loss: 2.6697e-04 - val_loss: 2.2634e-04 - 334s/epoch - 56ms/step
Epoch 168/200
5969/5969 - 334s - loss: 2.6855e-04 - val_loss: 2.2842e-04 - 334s/epoch - 56ms/step
Epoch 169/200
5969/5969 - 334s - loss: 2.7722e-04 - val_loss: 2.3228e-04 - 334s/epoch - 56ms/step
Epoch 170/200
5969/5969 - 334s - loss: 2.7177e-04 - val_loss: 2.7244e-04 - 334s/epoch - 56ms/step
Epoch 171/200
5969/5969 - 334s - loss: 2.7281e-04 - val_loss: 3.2801e-04 - 334s/epoch - 56ms/step
Epoch 172/200
5969/5969 - 334s - loss: 2.6739e-04 - val_loss: 3.5561e-04 - 334s/epoch - 56ms/step
Epoch 173/200
5969/5969 - 334s - loss: 2.6508e-04 - val_loss: 2.5022e-04 - 334s/epoch - 56ms/step
Epoch 174/200
5969/5969 - 334s - loss: 2.6171e-04 - val_loss: 2.5019e-04 - 334s/epoch - 56ms/step
Epoch 175/200
5969/5969 - 334s - loss: 2.6447e-04 - val_loss: 2.3890e-04 - 334s/epoch - 56ms/step
Epoch 176/200
5969/5969 - 334s - loss: 2.6656e-04 - val_loss: 3.5405e-04 - 334s/epoch - 56ms/step
Epoch 177/200
5969/5969 - 334s - loss: 2.6968e-04 - val_loss: 2.2902e-04 - 334s/epoch - 56ms/step
Epoch 178/200
5969/5969 - 334s - loss: 2.7115e-04 - val_loss: 2.3457e-04 - 334s/epoch - 56ms/step
Epoch 179/200
5969/5969 - 334s - loss: 2.6189e-04 - val_loss: 2.4445e-04 - 334s/epoch - 56ms/step
Epoch 180/200
5969/5969 - 334s - loss: 2.6533e-04 - val_loss: 2.4409e-04 - 334s/epoch - 56ms/step
Epoch 181/200
5969/5969 - 334s - loss: 2.6183e-04 - val_loss: 2.4447e-04 - 334s/epoch - 56ms/step
Epoch 182/200
5969/5969 - 334s - loss: 2.6343e-04 - val_loss: 2.4913e-04 - 334s/epoch - 56ms/step
Epoch 183/200
5969/5969 - 334s - loss: 2.6232e-04 - val_loss: 2.2347e-04 - 334s/epoch - 56ms/step
Epoch 184/200
5969/5969 - 334s - loss: 2.6771e-04 - val_loss: 2.5514e-04 - 334s/epoch - 56ms/step
Epoch 185/200
5969/5969 - 334s - loss: 2.5921e-04 - val_loss: 2.3351e-04 - 334s/epoch - 56ms/step
Epoch 186/200
5969/5969 - 334s - loss: 2.6621e-04 - val_loss: 2.2672e-04 - 334s/epoch - 56ms/step
Epoch 187/200
5969/5969 - 334s - loss: 2.6141e-04 - val_loss: 2.6040e-04 - 334s/epoch - 56ms/step
Epoch 188/200
5969/5969 - 334s - loss: 2.6311e-04 - val_loss: 2.5641e-04 - 334s/epoch - 56ms/step
Epoch 189/200
5969/5969 - 334s - loss: 2.6524e-04 - val_loss: 2.3547e-04 - 334s/epoch - 56ms/step
Epoch 190/200
5969/5969 - 334s - loss: 2.5840e-04 - val_loss: 2.5565e-04 - 334s/epoch - 56ms/step
Epoch 191/200
