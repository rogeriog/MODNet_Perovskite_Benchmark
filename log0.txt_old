start
Sun Dec 25 23:52:21 CET 2022
2022-12-25 23:52:24.267568: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-25 23:52:25.177872: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-12-25 23:52:25.551181: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-12-25 23:53:07,977 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fe5b77da8b0> object, created with modnet version 0.1.12
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
Shape of dataset to encode: (106113, 1264)
2022-12-25 23:53:10.076017: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-25 23:53:11.901045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20637 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:0f:00.0, compute capability: 8.6
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense (Dense)               (None, 2528)              3197920   
                                                                 
 batch_normalization (BatchN  (None, 2528)             10112     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_2 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 2528)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/50
2022-12-25 23:53:17.141449: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
5969/5969 - 20s - loss: 0.0081 - val_loss: 0.0028 - 20s/epoch - 3ms/step
Epoch 2/50
5969/5969 - 16s - loss: 0.0036 - val_loss: 0.0026 - 16s/epoch - 3ms/step
Epoch 3/50
5969/5969 - 16s - loss: 0.0022 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 4/50
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0012 - 16s/epoch - 3ms/step
Epoch 5/50
5969/5969 - 16s - loss: 0.0013 - val_loss: 9.3648e-04 - 16s/epoch - 3ms/step
Epoch 6/50
5969/5969 - 16s - loss: 0.0011 - val_loss: 9.0268e-04 - 16s/epoch - 3ms/step
Epoch 7/50
5969/5969 - 16s - loss: 0.0010 - val_loss: 8.0109e-04 - 16s/epoch - 3ms/step
Epoch 8/50
5969/5969 - 16s - loss: 9.2053e-04 - val_loss: 7.3421e-04 - 16s/epoch - 3ms/step
Epoch 9/50
5969/5969 - 16s - loss: 8.7103e-04 - val_loss: 7.1555e-04 - 16s/epoch - 3ms/step
Epoch 10/50
5969/5969 - 16s - loss: 8.1094e-04 - val_loss: 6.6394e-04 - 16s/epoch - 3ms/step
Epoch 11/50
5969/5969 - 16s - loss: 7.6539e-04 - val_loss: 6.3207e-04 - 16s/epoch - 3ms/step
Epoch 12/50
5969/5969 - 16s - loss: 7.3918e-04 - val_loss: 6.0798e-04 - 16s/epoch - 3ms/step
Epoch 13/50
5969/5969 - 16s - loss: 7.0154e-04 - val_loss: 5.6909e-04 - 16s/epoch - 3ms/step
Epoch 14/50
5969/5969 - 16s - loss: 6.7718e-04 - val_loss: 5.4641e-04 - 16s/epoch - 3ms/step
Epoch 15/50
5969/5969 - 16s - loss: 6.5390e-04 - val_loss: 5.2758e-04 - 16s/epoch - 3ms/step
Epoch 16/50
5969/5969 - 16s - loss: 6.5086e-04 - val_loss: 6.4770e-04 - 16s/epoch - 3ms/step
Epoch 17/50
5969/5969 - 16s - loss: 6.1850e-04 - val_loss: 5.2900e-04 - 16s/epoch - 3ms/step
Epoch 18/50
5969/5969 - 16s - loss: 6.0602e-04 - val_loss: 5.3846e-04 - 16s/epoch - 3ms/step
Epoch 19/50
5969/5969 - 16s - loss: 5.8336e-04 - val_loss: 4.8838e-04 - 16s/epoch - 3ms/step
Epoch 20/50
5969/5969 - 16s - loss: 5.8313e-04 - val_loss: 4.8491e-04 - 16s/epoch - 3ms/step
Epoch 21/50
5969/5969 - 16s - loss: 5.5998e-04 - val_loss: 4.5263e-04 - 16s/epoch - 3ms/step
Epoch 22/50
5969/5969 - 16s - loss: 5.6328e-04 - val_loss: 4.8310e-04 - 16s/epoch - 3ms/step
Epoch 23/50
5969/5969 - 16s - loss: 5.3623e-04 - val_loss: 4.5128e-04 - 16s/epoch - 3ms/step
Epoch 24/50
5969/5969 - 16s - loss: 5.2695e-04 - val_loss: 4.4819e-04 - 16s/epoch - 3ms/step
Epoch 25/50
5969/5969 - 16s - loss: 5.2822e-04 - val_loss: 4.3238e-04 - 16s/epoch - 3ms/step
Epoch 26/50
5969/5969 - 16s - loss: 5.1950e-04 - val_loss: 4.2494e-04 - 16s/epoch - 3ms/step
Epoch 27/50
5969/5969 - 16s - loss: 5.0158e-04 - val_loss: 4.1457e-04 - 16s/epoch - 3ms/step
Epoch 28/50
5969/5969 - 16s - loss: 4.9802e-04 - val_loss: 4.1042e-04 - 16s/epoch - 3ms/step
Epoch 29/50
5969/5969 - 16s - loss: 4.9356e-04 - val_loss: 4.3328e-04 - 16s/epoch - 3ms/step
Epoch 30/50
5969/5969 - 16s - loss: 4.8232e-04 - val_loss: 3.8868e-04 - 16s/epoch - 3ms/step
Epoch 31/50
5969/5969 - 16s - loss: 4.7678e-04 - val_loss: 4.0161e-04 - 16s/epoch - 3ms/step
Epoch 32/50
5969/5969 - 16s - loss: 4.6855e-04 - val_loss: 3.8678e-04 - 16s/epoch - 3ms/step
Epoch 33/50
5969/5969 - 16s - loss: 4.6733e-04 - val_loss: 3.9223e-04 - 16s/epoch - 3ms/step
Epoch 34/50
5969/5969 - 16s - loss: 4.5755e-04 - val_loss: 3.7625e-04 - 16s/epoch - 3ms/step
Epoch 35/50
5969/5969 - 16s - loss: 4.5558e-04 - val_loss: 3.8626e-04 - 16s/epoch - 3ms/step
Epoch 36/50
5969/5969 - 16s - loss: 4.4933e-04 - val_loss: 3.9471e-04 - 16s/epoch - 3ms/step
Epoch 37/50
5969/5969 - 16s - loss: 4.5008e-04 - val_loss: 3.9337e-04 - 16s/epoch - 3ms/step
Epoch 38/50
5969/5969 - 16s - loss: 4.3718e-04 - val_loss: 3.6464e-04 - 16s/epoch - 3ms/step
Epoch 39/50
5969/5969 - 16s - loss: 4.3937e-04 - val_loss: 4.0583e-04 - 16s/epoch - 3ms/step
Epoch 40/50
5969/5969 - 16s - loss: 4.2988e-04 - val_loss: 3.6967e-04 - 16s/epoch - 3ms/step
Epoch 41/50
5969/5969 - 16s - loss: 4.2778e-04 - val_loss: 3.5555e-04 - 16s/epoch - 3ms/step
Epoch 42/50
5969/5969 - 16s - loss: 4.2604e-04 - val_loss: 3.3891e-04 - 16s/epoch - 3ms/step
Epoch 43/50
5969/5969 - 16s - loss: 4.1688e-04 - val_loss: 3.5172e-04 - 16s/epoch - 3ms/step
Epoch 44/50
5969/5969 - 16s - loss: 4.1629e-04 - val_loss: 3.6770e-04 - 16s/epoch - 3ms/step
Epoch 45/50
5969/5969 - 16s - loss: 4.2012e-04 - val_loss: 3.4796e-04 - 16s/epoch - 3ms/step
Epoch 46/50
5969/5969 - 16s - loss: 4.1460e-04 - val_loss: 3.5296e-04 - 16s/epoch - 3ms/step
Epoch 47/50
5969/5969 - 16s - loss: 4.1385e-04 - val_loss: 3.5234e-04 - 16s/epoch - 3ms/step
Epoch 48/50
5969/5969 - 16s - loss: 4.0785e-04 - val_loss: 3.3165e-04 - 16s/epoch - 3ms/step
Epoch 49/50
5969/5969 - 16s - loss: 4.0985e-04 - val_loss: 3.4328e-04 - 16s/epoch - 3ms/step
Epoch 50/50
5969/5969 - 16s - loss: 3.9813e-04 - val_loss: 3.4082e-04 - 16s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0003408200282137841
  1/332 [..............................] - ETA: 33s 51/332 [===>..........................] - ETA: 0s 102/332 [========>.....................] - ETA: 0s153/332 [============>.................] - ETA: 0s204/332 [=================>............] - ETA: 0s255/332 [======================>.......] - ETA: 0s306/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 993us/step
correlation 0.003709340624615151
cosine 0.0029864273473895636
MAE: 0.009747916
RMSE: 0.0184613
r2: 0.9778905582167768
RMSE zero-vector: 0.23411466903540806
['default', 16, 50, 0.0005, 'mse', 0.5, 632, 0.00039812715840525925, 0.0003408200282137841, 0.003709340624615151, 0.0029864273473895636, 0.009747915901243687, 0.018461300060153008, 0.9778905582167768, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 2528)              3197920   
                                                                 
 batch_normalization (BatchN  (None, 2528)             10112     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_2 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 2528)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 2528)              3197920   
                                                                 
 batch_normalization (BatchN  (None, 2528)             10112     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_2 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 2528)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_3 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_3 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_5 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2528)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/50
5969/5969 - 16s - loss: 0.0075 - val_loss: 0.0026 - 16s/epoch - 3ms/step
Epoch 2/50
5969/5969 - 16s - loss: 0.0027 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 3/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 4/50
5969/5969 - 16s - loss: 0.0014 - val_loss: 0.0011 - 16s/epoch - 3ms/step
Epoch 5/50
5969/5969 - 16s - loss: 0.0012 - val_loss: 9.5042e-04 - 16s/epoch - 3ms/step
Epoch 6/50
5969/5969 - 16s - loss: 0.0011 - val_loss: 8.6349e-04 - 16s/epoch - 3ms/step
Epoch 7/50
5969/5969 - 16s - loss: 0.0010 - val_loss: 8.3816e-04 - 16s/epoch - 3ms/step
Epoch 8/50
5969/5969 - 16s - loss: 9.4978e-04 - val_loss: 7.3937e-04 - 16s/epoch - 3ms/step
Epoch 9/50
5969/5969 - 16s - loss: 9.2773e-04 - val_loss: 7.0601e-04 - 16s/epoch - 3ms/step
Epoch 10/50
5969/5969 - 16s - loss: 8.8104e-04 - val_loss: 7.2540e-04 - 16s/epoch - 3ms/step
Epoch 11/50
5969/5969 - 16s - loss: 8.3990e-04 - val_loss: 6.9484e-04 - 16s/epoch - 3ms/step
Epoch 12/50
5969/5969 - 16s - loss: 7.9736e-04 - val_loss: 6.2365e-04 - 16s/epoch - 3ms/step
Epoch 13/50
5969/5969 - 16s - loss: 7.6575e-04 - val_loss: 6.3221e-04 - 16s/epoch - 3ms/step
Epoch 14/50
5969/5969 - 16s - loss: 7.4501e-04 - val_loss: 6.1191e-04 - 16s/epoch - 3ms/step
Epoch 15/50
5969/5969 - 16s - loss: 7.2642e-04 - val_loss: 6.1049e-04 - 16s/epoch - 3ms/step
Epoch 16/50
5969/5969 - 16s - loss: 7.1407e-04 - val_loss: 6.1059e-04 - 16s/epoch - 3ms/step
Epoch 17/50
5969/5969 - 16s - loss: 6.8967e-04 - val_loss: 5.6407e-04 - 16s/epoch - 3ms/step
Epoch 18/50
5969/5969 - 16s - loss: 6.8446e-04 - val_loss: 5.5783e-04 - 16s/epoch - 3ms/step
Epoch 19/50
5969/5969 - 16s - loss: 6.7639e-04 - val_loss: 5.7728e-04 - 16s/epoch - 3ms/step
Epoch 20/50
5969/5969 - 16s - loss: 6.5454e-04 - val_loss: 5.1403e-04 - 16s/epoch - 3ms/step
Epoch 21/50
5969/5969 - 16s - loss: 6.5045e-04 - val_loss: 5.0638e-04 - 16s/epoch - 3ms/step
Epoch 22/50
5969/5969 - 16s - loss: 6.4115e-04 - val_loss: 5.1275e-04 - 16s/epoch - 3ms/step
Epoch 23/50
5969/5969 - 16s - loss: 6.2152e-04 - val_loss: 5.2139e-04 - 16s/epoch - 3ms/step
Epoch 24/50
5969/5969 - 16s - loss: 6.0497e-04 - val_loss: 5.0526e-04 - 16s/epoch - 3ms/step
Epoch 25/50
5969/5969 - 16s - loss: 6.0260e-04 - val_loss: 4.9456e-04 - 16s/epoch - 3ms/step
Epoch 26/50
5969/5969 - 16s - loss: 5.9798e-04 - val_loss: 4.6397e-04 - 16s/epoch - 3ms/step
Epoch 27/50
5969/5969 - 16s - loss: 5.8866e-04 - val_loss: 4.6766e-04 - 16s/epoch - 3ms/step
Epoch 28/50
5969/5969 - 16s - loss: 5.8495e-04 - val_loss: 4.5712e-04 - 16s/epoch - 3ms/step
Epoch 29/50
5969/5969 - 16s - loss: 5.8572e-04 - val_loss: 4.7213e-04 - 16s/epoch - 3ms/step
Epoch 30/50
5969/5969 - 16s - loss: 5.8740e-04 - val_loss: 4.6547e-04 - 16s/epoch - 3ms/step
Epoch 31/50
5969/5969 - 16s - loss: 5.6227e-04 - val_loss: 4.3930e-04 - 16s/epoch - 3ms/step
Epoch 32/50
5969/5969 - 16s - loss: 5.5484e-04 - val_loss: 4.5781e-04 - 16s/epoch - 3ms/step
Epoch 33/50
5969/5969 - 16s - loss: 5.4675e-04 - val_loss: 4.3673e-04 - 16s/epoch - 3ms/step
Epoch 34/50
5969/5969 - 16s - loss: 5.3888e-04 - val_loss: 4.2363e-04 - 16s/epoch - 3ms/step
Epoch 35/50
5969/5969 - 16s - loss: 5.4140e-04 - val_loss: 4.4295e-04 - 16s/epoch - 3ms/step
Epoch 36/50
5969/5969 - 16s - loss: 5.3520e-04 - val_loss: 4.3403e-04 - 16s/epoch - 3ms/step
Epoch 37/50
5969/5969 - 16s - loss: 5.3300e-04 - val_loss: 4.6698e-04 - 16s/epoch - 3ms/step
Epoch 38/50
5969/5969 - 16s - loss: 5.2982e-04 - val_loss: 4.5071e-04 - 16s/epoch - 3ms/step
Epoch 39/50
5969/5969 - 16s - loss: 5.2508e-04 - val_loss: 4.3131e-04 - 16s/epoch - 3ms/step
Epoch 40/50
5969/5969 - 16s - loss: 5.1051e-04 - val_loss: 4.3223e-04 - 16s/epoch - 3ms/step
Epoch 41/50
5969/5969 - 16s - loss: 5.1629e-04 - val_loss: 4.1780e-04 - 16s/epoch - 3ms/step
Epoch 42/50
5969/5969 - 16s - loss: 5.1375e-04 - val_loss: 4.7630e-04 - 16s/epoch - 3ms/step
Epoch 43/50
5969/5969 - 16s - loss: 4.9823e-04 - val_loss: 4.2506e-04 - 16s/epoch - 3ms/step
Epoch 44/50
5969/5969 - 16s - loss: 4.9407e-04 - val_loss: 4.1192e-04 - 16s/epoch - 3ms/step
Epoch 45/50
5969/5969 - 16s - loss: 5.0795e-04 - val_loss: 3.8502e-04 - 16s/epoch - 3ms/step
Epoch 46/50
5969/5969 - 16s - loss: 5.0354e-04 - val_loss: 3.8849e-04 - 16s/epoch - 3ms/step
Epoch 47/50
5969/5969 - 16s - loss: 4.8911e-04 - val_loss: 3.9317e-04 - 16s/epoch - 3ms/step
Epoch 48/50
5969/5969 - 16s - loss: 4.8947e-04 - val_loss: 4.1271e-04 - 16s/epoch - 3ms/step
Epoch 49/50
5969/5969 - 16s - loss: 4.8891e-04 - val_loss: 3.9472e-04 - 16s/epoch - 3ms/step
Epoch 50/50
5969/5969 - 16s - loss: 4.7871e-04 - val_loss: 4.0781e-04 - 16s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00040780557901598513
  1/332 [..............................] - ETA: 27s 51/332 [===>..........................] - ETA: 0s 102/332 [========>.....................] - ETA: 0s153/332 [============>.................] - ETA: 0s204/332 [=================>............] - ETA: 0s254/332 [=====================>........] - ETA: 0s304/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 997us/step
correlation 0.004482212356023864
cosine 0.0036068593348101584
MAE: 0.010553221
RMSE: 0.020194182
r2: 0.9735449299291979
RMSE zero-vector: 0.23411466903540806
['default', 16, 50, 0.001, 'mse', 0.5, 632, 0.0004787099314853549, 0.00040780557901598513, 0.004482212356023864, 0.0036068593348101584, 0.010553221218287945, 0.02019418217241764, 0.9735449299291979, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_3 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_5 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2528)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_3 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_5 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2528)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_6 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_6 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_8 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2528)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/50
5969/5969 - 16s - loss: 0.0161 - val_loss: 0.0033 - 16s/epoch - 3ms/step
Epoch 2/50
5969/5969 - 16s - loss: 0.0033 - val_loss: 0.0030 - 16s/epoch - 3ms/step
Epoch 3/50
5969/5969 - 16s - loss: 0.0027 - val_loss: 0.0022 - 16s/epoch - 3ms/step
Epoch 4/50
5969/5969 - 16s - loss: 0.0023 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 5/50
5969/5969 - 16s - loss: 0.0021 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 6/50
5969/5969 - 16s - loss: 0.0020 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 7/50
5969/5969 - 16s - loss: 0.0019 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 8/50
5969/5969 - 16s - loss: 0.0019 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 9/50
5969/5969 - 16s - loss: 0.0019 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 10/50
5969/5969 - 16s - loss: 0.0019 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 11/50
5969/5969 - 16s - loss: 0.0019 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 12/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 13/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 14/50
5969/5969 - 16s - loss: 0.0019 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 15/50
5969/5969 - 16s - loss: 0.0019 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 16/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 17/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 18/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 19/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 20/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 21/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 22/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 23/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 24/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 25/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 26/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0023 - 16s/epoch - 3ms/step
Epoch 27/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 28/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 29/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 30/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 31/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 32/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0024 - 16s/epoch - 3ms/step
Epoch 33/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 34/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 35/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 36/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0022 - 16s/epoch - 3ms/step
Epoch 37/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 38/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0021 - 16s/epoch - 3ms/step
Epoch 39/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0025 - 16s/epoch - 3ms/step
Epoch 40/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0021 - 16s/epoch - 3ms/step
Epoch 41/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 42/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 43/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 44/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 45/50
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 46/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 47/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 48/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 49/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 50/50
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0015 - 16s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0015130876563489437
  1/332 [..............................] - ETA: 28s 51/332 [===>..........................] - ETA: 0s 102/332 [========>.....................] - ETA: 0s153/332 [============>.................] - ETA: 0s204/332 [=================>............] - ETA: 0s254/332 [=====================>........] - ETA: 0s305/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.01745712541990176
cosine 0.014035178868670846
MAE: 0.021351147
RMSE: 0.038898397
r2: 0.9018425272795882
RMSE zero-vector: 0.23411466903540806
['default', 16, 50, 0.005, 'mse', 0.5, 632, 0.0017054904019460082, 0.0015130876563489437, 0.01745712541990176, 0.014035178868670846, 0.021351147443056107, 0.038898397237062454, 0.9018425272795882, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_6 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_8 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2528)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_6 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_8 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2528)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_9 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_9 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_11 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2528)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
5969/5969 - 17s - loss: 0.0080 - val_loss: 0.0027 - 17s/epoch - 3ms/step
Epoch 2/100
5969/5969 - 16s - loss: 0.0036 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 3/100
5969/5969 - 16s - loss: 0.0022 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 4/100
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0013 - 16s/epoch - 3ms/step
Epoch 5/100
5969/5969 - 16s - loss: 0.0013 - val_loss: 0.0011 - 16s/epoch - 3ms/step
Epoch 6/100
5969/5969 - 16s - loss: 0.0011 - val_loss: 0.0011 - 16s/epoch - 3ms/step
Epoch 7/100
5969/5969 - 16s - loss: 9.9438e-04 - val_loss: 9.2041e-04 - 16s/epoch - 3ms/step
Epoch 8/100
5969/5969 - 16s - loss: 9.0399e-04 - val_loss: 8.1803e-04 - 16s/epoch - 3ms/step
Epoch 9/100
5969/5969 - 16s - loss: 8.4489e-04 - val_loss: 7.9678e-04 - 16s/epoch - 3ms/step
Epoch 10/100
5969/5969 - 16s - loss: 8.0077e-04 - val_loss: 7.7023e-04 - 16s/epoch - 3ms/step
Epoch 11/100
5969/5969 - 16s - loss: 7.5856e-04 - val_loss: 6.5053e-04 - 16s/epoch - 3ms/step
Epoch 12/100
5969/5969 - 16s - loss: 7.2938e-04 - val_loss: 7.2661e-04 - 16s/epoch - 3ms/step
Epoch 13/100
5969/5969 - 16s - loss: 7.0866e-04 - val_loss: 6.5086e-04 - 16s/epoch - 3ms/step
Epoch 14/100
5969/5969 - 16s - loss: 6.7857e-04 - val_loss: 6.5381e-04 - 16s/epoch - 3ms/step
Epoch 15/100
5969/5969 - 16s - loss: 6.4811e-04 - val_loss: 6.5166e-04 - 16s/epoch - 3ms/step
Epoch 16/100
5969/5969 - 16s - loss: 6.3544e-04 - val_loss: 6.1808e-04 - 16s/epoch - 3ms/step
Epoch 17/100
5969/5969 - 16s - loss: 6.1314e-04 - val_loss: 6.0271e-04 - 16s/epoch - 3ms/step
Epoch 18/100
5969/5969 - 16s - loss: 5.9125e-04 - val_loss: 5.7698e-04 - 16s/epoch - 3ms/step
Epoch 19/100
5969/5969 - 16s - loss: 5.7618e-04 - val_loss: 6.0170e-04 - 16s/epoch - 3ms/step
Epoch 20/100
5969/5969 - 16s - loss: 5.6127e-04 - val_loss: 5.6311e-04 - 16s/epoch - 3ms/step
Epoch 21/100
5969/5969 - 16s - loss: 5.6151e-04 - val_loss: 5.1343e-04 - 16s/epoch - 3ms/step
Epoch 22/100
5969/5969 - 16s - loss: 5.5288e-04 - val_loss: 5.2750e-04 - 16s/epoch - 3ms/step
Epoch 23/100
5969/5969 - 16s - loss: 5.3117e-04 - val_loss: 5.3031e-04 - 16s/epoch - 3ms/step
Epoch 24/100
5969/5969 - 16s - loss: 5.2667e-04 - val_loss: 5.4733e-04 - 16s/epoch - 3ms/step
Epoch 25/100
5969/5969 - 16s - loss: 5.1812e-04 - val_loss: 5.1537e-04 - 16s/epoch - 3ms/step
Epoch 26/100
5969/5969 - 16s - loss: 5.0898e-04 - val_loss: 4.9901e-04 - 16s/epoch - 3ms/step
Epoch 27/100
5969/5969 - 16s - loss: 4.9835e-04 - val_loss: 4.7328e-04 - 16s/epoch - 3ms/step
Epoch 28/100
5969/5969 - 16s - loss: 4.8794e-04 - val_loss: 4.7147e-04 - 16s/epoch - 3ms/step
Epoch 29/100
5969/5969 - 16s - loss: 4.8207e-04 - val_loss: 4.9610e-04 - 16s/epoch - 3ms/step
Epoch 30/100
5969/5969 - 16s - loss: 4.7635e-04 - val_loss: 4.4713e-04 - 16s/epoch - 3ms/step
Epoch 31/100
5969/5969 - 16s - loss: 4.7052e-04 - val_loss: 4.4515e-04 - 16s/epoch - 3ms/step
Epoch 32/100
5969/5969 - 16s - loss: 4.6273e-04 - val_loss: 5.6530e-04 - 16s/epoch - 3ms/step
Epoch 33/100
5969/5969 - 16s - loss: 4.5615e-04 - val_loss: 4.7806e-04 - 16s/epoch - 3ms/step
Epoch 34/100
5969/5969 - 16s - loss: 4.5905e-04 - val_loss: 4.3392e-04 - 16s/epoch - 3ms/step
Epoch 35/100
5969/5969 - 16s - loss: 4.6299e-04 - val_loss: 4.5855e-04 - 16s/epoch - 3ms/step
Epoch 36/100
5969/5969 - 16s - loss: 4.4209e-04 - val_loss: 4.6559e-04 - 16s/epoch - 3ms/step
Epoch 37/100
5969/5969 - 16s - loss: 4.5667e-04 - val_loss: 5.3570e-04 - 16s/epoch - 3ms/step
Epoch 38/100
5969/5969 - 16s - loss: 4.3403e-04 - val_loss: 4.4636e-04 - 16s/epoch - 3ms/step
Epoch 39/100
5969/5969 - 16s - loss: 4.2994e-04 - val_loss: 3.9826e-04 - 16s/epoch - 3ms/step
Epoch 40/100
5969/5969 - 16s - loss: 4.2460e-04 - val_loss: 4.2420e-04 - 16s/epoch - 3ms/step
Epoch 41/100
5969/5969 - 16s - loss: 4.3170e-04 - val_loss: 4.3287e-04 - 16s/epoch - 3ms/step
Epoch 42/100
5969/5969 - 16s - loss: 4.2334e-04 - val_loss: 4.3444e-04 - 16s/epoch - 3ms/step
Epoch 43/100
5969/5969 - 16s - loss: 4.1556e-04 - val_loss: 4.2931e-04 - 16s/epoch - 3ms/step
Epoch 44/100
5969/5969 - 16s - loss: 4.2530e-04 - val_loss: 4.2970e-04 - 16s/epoch - 3ms/step
Epoch 45/100
5969/5969 - 16s - loss: 4.0982e-04 - val_loss: 6.8635e-04 - 16s/epoch - 3ms/step
Epoch 46/100
5969/5969 - 16s - loss: 4.0339e-04 - val_loss: 4.1171e-04 - 16s/epoch - 3ms/step
Epoch 47/100
5969/5969 - 16s - loss: 4.0508e-04 - val_loss: 4.6639e-04 - 16s/epoch - 3ms/step
Epoch 48/100
5969/5969 - 16s - loss: 4.0849e-04 - val_loss: 4.7029e-04 - 16s/epoch - 3ms/step
Epoch 49/100
5969/5969 - 16s - loss: 4.0683e-04 - val_loss: 4.1666e-04 - 16s/epoch - 3ms/step
Epoch 50/100
5969/5969 - 16s - loss: 3.9648e-04 - val_loss: 4.4537e-04 - 16s/epoch - 3ms/step
Epoch 51/100
5969/5969 - 16s - loss: 3.9771e-04 - val_loss: 4.4136e-04 - 16s/epoch - 3ms/step
Epoch 52/100
5969/5969 - 16s - loss: 3.8993e-04 - val_loss: 4.4670e-04 - 16s/epoch - 3ms/step
Epoch 53/100
5969/5969 - 16s - loss: 3.9110e-04 - val_loss: 4.6203e-04 - 16s/epoch - 3ms/step
Epoch 54/100
5969/5969 - 16s - loss: 3.8139e-04 - val_loss: 4.9759e-04 - 16s/epoch - 3ms/step
Epoch 55/100
5969/5969 - 16s - loss: 3.8282e-04 - val_loss: 3.9823e-04 - 16s/epoch - 3ms/step
Epoch 56/100
5969/5969 - 16s - loss: 3.8338e-04 - val_loss: 4.4554e-04 - 16s/epoch - 3ms/step
Epoch 57/100
5969/5969 - 16s - loss: 3.7960e-04 - val_loss: 4.6451e-04 - 16s/epoch - 3ms/step
Epoch 58/100
5969/5969 - 16s - loss: 3.7326e-04 - val_loss: 4.8116e-04 - 16s/epoch - 3ms/step
Epoch 59/100
5969/5969 - 16s - loss: 3.7077e-04 - val_loss: 4.4299e-04 - 16s/epoch - 3ms/step
Epoch 60/100
5969/5969 - 16s - loss: 3.7329e-04 - val_loss: 4.7128e-04 - 16s/epoch - 3ms/step
Epoch 61/100
5969/5969 - 16s - loss: 3.6743e-04 - val_loss: 4.2768e-04 - 16s/epoch - 3ms/step
Epoch 62/100
5969/5969 - 16s - loss: 3.6408e-04 - val_loss: 5.4265e-04 - 16s/epoch - 3ms/step
Epoch 63/100
5969/5969 - 16s - loss: 3.6804e-04 - val_loss: 4.2353e-04 - 16s/epoch - 3ms/step
Epoch 64/100
5969/5969 - 16s - loss: 3.6197e-04 - val_loss: 4.5081e-04 - 16s/epoch - 3ms/step
Epoch 65/100
5969/5969 - 16s - loss: 3.6573e-04 - val_loss: 4.8493e-04 - 16s/epoch - 3ms/step
Epoch 66/100
5969/5969 - 16s - loss: 3.5870e-04 - val_loss: 4.7240e-04 - 16s/epoch - 3ms/step
Epoch 67/100
5969/5969 - 16s - loss: 3.5634e-04 - val_loss: 4.5998e-04 - 16s/epoch - 3ms/step
Epoch 68/100
5969/5969 - 16s - loss: 3.5644e-04 - val_loss: 4.2901e-04 - 16s/epoch - 3ms/step
Epoch 69/100
5969/5969 - 16s - loss: 3.5457e-04 - val_loss: 3.9736e-04 - 16s/epoch - 3ms/step
Epoch 70/100
5969/5969 - 16s - loss: 3.5179e-04 - val_loss: 3.6515e-04 - 16s/epoch - 3ms/step
Epoch 71/100
5969/5969 - 16s - loss: 3.5105e-04 - val_loss: 4.3908e-04 - 16s/epoch - 3ms/step
Epoch 72/100
5969/5969 - 16s - loss: 3.4986e-04 - val_loss: 4.6879e-04 - 16s/epoch - 3ms/step
Epoch 73/100
5969/5969 - 16s - loss: 3.4836e-04 - val_loss: 4.2372e-04 - 16s/epoch - 3ms/step
Epoch 74/100
5969/5969 - 16s - loss: 3.5668e-04 - val_loss: 3.8328e-04 - 16s/epoch - 3ms/step
Epoch 75/100
5969/5969 - 16s - loss: 3.4782e-04 - val_loss: 4.8314e-04 - 16s/epoch - 3ms/step
Epoch 76/100
5969/5969 - 16s - loss: 3.4467e-04 - val_loss: 5.2284e-04 - 16s/epoch - 3ms/step
Epoch 77/100
5969/5969 - 16s - loss: 3.4412e-04 - val_loss: 3.4301e-04 - 16s/epoch - 3ms/step
Epoch 78/100
5969/5969 - 16s - loss: 3.4514e-04 - val_loss: 4.6210e-04 - 16s/epoch - 3ms/step
Epoch 79/100
5969/5969 - 16s - loss: 3.4418e-04 - val_loss: 3.9371e-04 - 16s/epoch - 3ms/step
Epoch 80/100
5969/5969 - 16s - loss: 3.3874e-04 - val_loss: 3.9934e-04 - 16s/epoch - 3ms/step
Epoch 81/100
5969/5969 - 16s - loss: 3.4194e-04 - val_loss: 3.8477e-04 - 16s/epoch - 3ms/step
Epoch 82/100
5969/5969 - 16s - loss: 3.4132e-04 - val_loss: 3.6457e-04 - 16s/epoch - 3ms/step
Epoch 83/100
5969/5969 - 16s - loss: 3.4143e-04 - val_loss: 4.2897e-04 - 16s/epoch - 3ms/step
Epoch 84/100
5969/5969 - 16s - loss: 3.3557e-04 - val_loss: 4.3381e-04 - 16s/epoch - 3ms/step
Epoch 85/100
5969/5969 - 16s - loss: 3.4779e-04 - val_loss: 4.2045e-04 - 16s/epoch - 3ms/step
Epoch 86/100
5969/5969 - 16s - loss: 3.3493e-04 - val_loss: 4.3129e-04 - 16s/epoch - 3ms/step
Epoch 87/100
5969/5969 - 16s - loss: 3.3772e-04 - val_loss: 4.8749e-04 - 16s/epoch - 3ms/step
Epoch 88/100
5969/5969 - 16s - loss: 3.3688e-04 - val_loss: 3.4532e-04 - 16s/epoch - 3ms/step
Epoch 89/100
5969/5969 - 16s - loss: 3.3819e-04 - val_loss: 4.3058e-04 - 16s/epoch - 3ms/step
Epoch 90/100
5969/5969 - 16s - loss: 3.3303e-04 - val_loss: 3.7637e-04 - 16s/epoch - 3ms/step
Epoch 91/100
5969/5969 - 16s - loss: 3.3428e-04 - val_loss: 6.0688e-04 - 16s/epoch - 3ms/step
Epoch 92/100
5969/5969 - 16s - loss: 3.3250e-04 - val_loss: 5.4475e-04 - 16s/epoch - 3ms/step
Epoch 93/100
5969/5969 - 16s - loss: 3.2607e-04 - val_loss: 4.8852e-04 - 16s/epoch - 3ms/step
Epoch 94/100
5969/5969 - 16s - loss: 3.2274e-04 - val_loss: 3.9115e-04 - 16s/epoch - 3ms/step
Epoch 95/100
5969/5969 - 16s - loss: 3.2621e-04 - val_loss: 5.0271e-04 - 16s/epoch - 3ms/step
Epoch 96/100
5969/5969 - 16s - loss: 3.2755e-04 - val_loss: 4.6957e-04 - 16s/epoch - 3ms/step
Epoch 97/100
5969/5969 - 16s - loss: 3.2948e-04 - val_loss: 3.9678e-04 - 16s/epoch - 3ms/step
Epoch 98/100
5969/5969 - 16s - loss: 3.2726e-04 - val_loss: 4.2690e-04 - 16s/epoch - 3ms/step
Epoch 99/100
5969/5969 - 16s - loss: 3.2067e-04 - val_loss: 4.6141e-04 - 16s/epoch - 3ms/step
Epoch 100/100
5969/5969 - 16s - loss: 3.2212e-04 - val_loss: 4.0060e-04 - 16s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00040060284663923085
  1/332 [..............................] - ETA: 28s 51/332 [===>..........................] - ETA: 0s 101/332 [========>.....................] - ETA: 0s152/332 [============>.................] - ETA: 0s203/332 [=================>............] - ETA: 0s254/332 [=====================>........] - ETA: 0s304/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.004298123438718676
cosine 0.003505446800321082
MAE: 0.009118854
RMSE: 0.020015035
r2: 0.9740124953650888
RMSE zero-vector: 0.23411466903540806
['default', 16, 100, 0.0005, 'mse', 0.5, 632, 0.0003221249207854271, 0.00040060284663923085, 0.004298123438718676, 0.003505446800321082, 0.009118854068219662, 0.020015034824609756, 0.9740124953650888, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_9 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_11 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2528)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_9 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_11 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2528)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_12 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_12 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_14 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2528)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
5969/5969 - 16s - loss: 0.0076 - val_loss: 0.0027 - 16s/epoch - 3ms/step
Epoch 2/100
5969/5969 - 16s - loss: 0.0027 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 3/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0013 - 16s/epoch - 3ms/step
Epoch 4/100
5969/5969 - 16s - loss: 0.0014 - val_loss: 0.0011 - 16s/epoch - 3ms/step
Epoch 5/100
5969/5969 - 16s - loss: 0.0012 - val_loss: 9.7605e-04 - 16s/epoch - 3ms/step
Epoch 6/100
5969/5969 - 16s - loss: 0.0011 - val_loss: 8.7830e-04 - 16s/epoch - 3ms/step
Epoch 7/100
5969/5969 - 16s - loss: 0.0010 - val_loss: 8.1759e-04 - 16s/epoch - 3ms/step
Epoch 8/100
5969/5969 - 16s - loss: 9.5410e-04 - val_loss: 7.6616e-04 - 16s/epoch - 3ms/step
Epoch 9/100
5969/5969 - 16s - loss: 8.9210e-04 - val_loss: 7.0881e-04 - 16s/epoch - 3ms/step
Epoch 10/100
5969/5969 - 16s - loss: 8.5694e-04 - val_loss: 7.1466e-04 - 16s/epoch - 3ms/step
Epoch 11/100
5969/5969 - 16s - loss: 8.1737e-04 - val_loss: 6.8560e-04 - 16s/epoch - 3ms/step
Epoch 12/100
5969/5969 - 16s - loss: 7.8317e-04 - val_loss: 6.4577e-04 - 16s/epoch - 3ms/step
Epoch 13/100
5969/5969 - 16s - loss: 7.5742e-04 - val_loss: 5.9112e-04 - 16s/epoch - 3ms/step
Epoch 14/100
5969/5969 - 16s - loss: 7.3532e-04 - val_loss: 5.9348e-04 - 16s/epoch - 3ms/step
Epoch 15/100
5969/5969 - 16s - loss: 7.1648e-04 - val_loss: 5.9581e-04 - 16s/epoch - 3ms/step
Epoch 16/100
5969/5969 - 16s - loss: 7.1002e-04 - val_loss: 6.1182e-04 - 16s/epoch - 3ms/step
Epoch 17/100
5969/5969 - 16s - loss: 6.9399e-04 - val_loss: 5.5514e-04 - 16s/epoch - 3ms/step
Epoch 18/100
5969/5969 - 16s - loss: 6.7344e-04 - val_loss: 5.4772e-04 - 16s/epoch - 3ms/step
Epoch 19/100
5969/5969 - 16s - loss: 6.5204e-04 - val_loss: 5.2870e-04 - 16s/epoch - 3ms/step
Epoch 20/100
5969/5969 - 16s - loss: 6.4550e-04 - val_loss: 5.2705e-04 - 16s/epoch - 3ms/step
Epoch 21/100
5969/5969 - 16s - loss: 6.3713e-04 - val_loss: 5.2486e-04 - 16s/epoch - 3ms/step
Epoch 22/100
5969/5969 - 16s - loss: 6.2801e-04 - val_loss: 5.0230e-04 - 16s/epoch - 3ms/step
Epoch 23/100
5969/5969 - 16s - loss: 6.1566e-04 - val_loss: 5.2395e-04 - 16s/epoch - 3ms/step
Epoch 24/100
5969/5969 - 16s - loss: 6.0859e-04 - val_loss: 5.1036e-04 - 16s/epoch - 3ms/step
Epoch 25/100
5969/5969 - 16s - loss: 6.0172e-04 - val_loss: 5.0216e-04 - 16s/epoch - 3ms/step
Epoch 26/100
5969/5969 - 16s - loss: 5.9147e-04 - val_loss: 4.7502e-04 - 16s/epoch - 3ms/step
Epoch 27/100
5969/5969 - 16s - loss: 5.8412e-04 - val_loss: 4.7641e-04 - 16s/epoch - 3ms/step
Epoch 28/100
5969/5969 - 16s - loss: 5.6595e-04 - val_loss: 4.7727e-04 - 16s/epoch - 3ms/step
Epoch 29/100
5969/5969 - 16s - loss: 5.8464e-04 - val_loss: 4.7913e-04 - 16s/epoch - 3ms/step
Epoch 30/100
5969/5969 - 16s - loss: 5.6338e-04 - val_loss: 4.5218e-04 - 16s/epoch - 3ms/step
Epoch 31/100
5969/5969 - 16s - loss: 5.5641e-04 - val_loss: 4.3853e-04 - 16s/epoch - 3ms/step
Epoch 32/100
5969/5969 - 16s - loss: 5.4704e-04 - val_loss: 4.9407e-04 - 16s/epoch - 3ms/step
Epoch 33/100
5969/5969 - 16s - loss: 5.5634e-04 - val_loss: 4.7417e-04 - 16s/epoch - 3ms/step
Epoch 34/100
5969/5969 - 16s - loss: 5.3166e-04 - val_loss: 4.3993e-04 - 16s/epoch - 3ms/step
Epoch 35/100
5969/5969 - 16s - loss: 5.3358e-04 - val_loss: 4.3501e-04 - 16s/epoch - 3ms/step
Epoch 36/100
5969/5969 - 16s - loss: 5.2395e-04 - val_loss: 4.8610e-04 - 16s/epoch - 3ms/step
Epoch 37/100
5969/5969 - 16s - loss: 5.4258e-04 - val_loss: 4.5443e-04 - 16s/epoch - 3ms/step
Epoch 38/100
5969/5969 - 16s - loss: 5.2487e-04 - val_loss: 4.2866e-04 - 16s/epoch - 3ms/step
Epoch 39/100
5969/5969 - 16s - loss: 5.1403e-04 - val_loss: 4.2223e-04 - 16s/epoch - 3ms/step
Epoch 40/100
5969/5969 - 16s - loss: 5.1105e-04 - val_loss: 4.2024e-04 - 16s/epoch - 3ms/step
Epoch 41/100
5969/5969 - 16s - loss: 5.1683e-04 - val_loss: 4.1264e-04 - 16s/epoch - 3ms/step
Epoch 42/100
5969/5969 - 16s - loss: 5.0318e-04 - val_loss: 3.9054e-04 - 16s/epoch - 3ms/step
Epoch 43/100
5969/5969 - 16s - loss: 4.9455e-04 - val_loss: 4.0678e-04 - 16s/epoch - 3ms/step
Epoch 44/100
5969/5969 - 16s - loss: 4.9516e-04 - val_loss: 4.1181e-04 - 16s/epoch - 3ms/step
Epoch 45/100
5969/5969 - 16s - loss: 4.9548e-04 - val_loss: 4.0672e-04 - 16s/epoch - 3ms/step
Epoch 46/100
5969/5969 - 16s - loss: 4.9864e-04 - val_loss: 4.2037e-04 - 16s/epoch - 3ms/step
Epoch 47/100
5969/5969 - 16s - loss: 4.9144e-04 - val_loss: 3.9306e-04 - 16s/epoch - 3ms/step
Epoch 48/100
5969/5969 - 16s - loss: 4.8622e-04 - val_loss: 3.9753e-04 - 16s/epoch - 3ms/step
Epoch 49/100
5969/5969 - 16s - loss: 4.9433e-04 - val_loss: 4.1366e-04 - 16s/epoch - 3ms/step
Epoch 50/100
5969/5969 - 16s - loss: 4.8153e-04 - val_loss: 4.0473e-04 - 16s/epoch - 3ms/step
Epoch 51/100
5969/5969 - 16s - loss: 4.7530e-04 - val_loss: 3.9337e-04 - 16s/epoch - 3ms/step
Epoch 52/100
5969/5969 - 16s - loss: 4.8753e-04 - val_loss: 3.8789e-04 - 16s/epoch - 3ms/step
Epoch 53/100
5969/5969 - 16s - loss: 4.7185e-04 - val_loss: 3.7989e-04 - 16s/epoch - 3ms/step
Epoch 54/100
5969/5969 - 16s - loss: 4.7149e-04 - val_loss: 4.0513e-04 - 16s/epoch - 3ms/step
Epoch 55/100
5969/5969 - 16s - loss: 4.6311e-04 - val_loss: 3.7792e-04 - 16s/epoch - 3ms/step
Epoch 56/100
5969/5969 - 16s - loss: 4.6756e-04 - val_loss: 4.8050e-04 - 16s/epoch - 3ms/step
Epoch 57/100
5969/5969 - 16s - loss: 4.6426e-04 - val_loss: 3.8582e-04 - 16s/epoch - 3ms/step
Epoch 58/100
5969/5969 - 16s - loss: 4.5267e-04 - val_loss: 3.8848e-04 - 16s/epoch - 3ms/step
Epoch 59/100
5969/5969 - 16s - loss: 4.5819e-04 - val_loss: 3.6101e-04 - 16s/epoch - 3ms/step
Epoch 60/100
5969/5969 - 16s - loss: 4.5347e-04 - val_loss: 3.7395e-04 - 16s/epoch - 3ms/step
Epoch 61/100
5969/5969 - 16s - loss: 4.6584e-04 - val_loss: 3.7727e-04 - 16s/epoch - 3ms/step
Epoch 62/100
5969/5969 - 16s - loss: 4.5227e-04 - val_loss: 4.0863e-04 - 16s/epoch - 3ms/step
Epoch 63/100
5969/5969 - 16s - loss: 4.4665e-04 - val_loss: 3.7053e-04 - 16s/epoch - 3ms/step
Epoch 64/100
5969/5969 - 16s - loss: 4.5077e-04 - val_loss: 3.8230e-04 - 16s/epoch - 3ms/step
Epoch 65/100
5969/5969 - 16s - loss: 4.4968e-04 - val_loss: 3.9541e-04 - 16s/epoch - 3ms/step
Epoch 66/100
5969/5969 - 16s - loss: 4.4963e-04 - val_loss: 3.5456e-04 - 16s/epoch - 3ms/step
Epoch 67/100
5969/5969 - 16s - loss: 4.4249e-04 - val_loss: 3.7629e-04 - 16s/epoch - 3ms/step
Epoch 68/100
5969/5969 - 16s - loss: 4.4263e-04 - val_loss: 3.6846e-04 - 16s/epoch - 3ms/step
Epoch 69/100
5969/5969 - 16s - loss: 4.3757e-04 - val_loss: 3.5322e-04 - 16s/epoch - 3ms/step
Epoch 70/100
5969/5969 - 16s - loss: 4.3140e-04 - val_loss: 3.6298e-04 - 16s/epoch - 3ms/step
Epoch 71/100
5969/5969 - 16s - loss: 4.7047e-04 - val_loss: 4.0014e-04 - 16s/epoch - 3ms/step
Epoch 72/100
5969/5969 - 16s - loss: 4.3375e-04 - val_loss: 3.6926e-04 - 16s/epoch - 3ms/step
Epoch 73/100
5969/5969 - 16s - loss: 4.6383e-04 - val_loss: 3.6428e-04 - 16s/epoch - 3ms/step
Epoch 74/100
5969/5969 - 16s - loss: 4.3480e-04 - val_loss: 3.7420e-04 - 16s/epoch - 3ms/step
Epoch 75/100
5969/5969 - 16s - loss: 4.3159e-04 - val_loss: 3.7470e-04 - 16s/epoch - 3ms/step
Epoch 76/100
5969/5969 - 16s - loss: 4.2323e-04 - val_loss: 3.5120e-04 - 16s/epoch - 3ms/step
Epoch 77/100
5969/5969 - 16s - loss: 4.2260e-04 - val_loss: 3.4531e-04 - 16s/epoch - 3ms/step
Epoch 78/100
5969/5969 - 16s - loss: 4.4168e-04 - val_loss: 3.5949e-04 - 16s/epoch - 3ms/step
Epoch 79/100
5969/5969 - 16s - loss: 4.2116e-04 - val_loss: 3.9517e-04 - 16s/epoch - 3ms/step
Epoch 80/100
5969/5969 - 16s - loss: 4.4389e-04 - val_loss: 3.3203e-04 - 16s/epoch - 3ms/step
Epoch 81/100
5969/5969 - 16s - loss: 4.2270e-04 - val_loss: 3.4755e-04 - 16s/epoch - 3ms/step
Epoch 82/100
5969/5969 - 16s - loss: 4.3007e-04 - val_loss: 3.3805e-04 - 16s/epoch - 3ms/step
Epoch 83/100
5969/5969 - 16s - loss: 4.2366e-04 - val_loss: 3.7252e-04 - 16s/epoch - 3ms/step
Epoch 84/100
5969/5969 - 16s - loss: 4.2476e-04 - val_loss: 3.6272e-04 - 16s/epoch - 3ms/step
Epoch 85/100
5969/5969 - 16s - loss: 4.1397e-04 - val_loss: 3.6219e-04 - 16s/epoch - 3ms/step
Epoch 86/100
5969/5969 - 16s - loss: 4.2831e-04 - val_loss: 3.6866e-04 - 16s/epoch - 3ms/step
Epoch 87/100
5969/5969 - 16s - loss: 4.2811e-04 - val_loss: 3.6613e-04 - 16s/epoch - 3ms/step
Epoch 88/100
5969/5969 - 16s - loss: 4.2153e-04 - val_loss: 3.2984e-04 - 16s/epoch - 3ms/step
Epoch 89/100
5969/5969 - 16s - loss: 4.1585e-04 - val_loss: 3.5229e-04 - 16s/epoch - 3ms/step
Epoch 90/100
5969/5969 - 16s - loss: 4.1732e-04 - val_loss: 3.5907e-04 - 16s/epoch - 3ms/step
Epoch 91/100
5969/5969 - 16s - loss: 4.1119e-04 - val_loss: 3.3612e-04 - 16s/epoch - 3ms/step
Epoch 92/100
5969/5969 - 16s - loss: 4.1455e-04 - val_loss: 4.7733e-04 - 16s/epoch - 3ms/step
Epoch 93/100
5969/5969 - 16s - loss: 4.0805e-04 - val_loss: 3.4038e-04 - 16s/epoch - 3ms/step
Epoch 94/100
5969/5969 - 16s - loss: 4.0600e-04 - val_loss: 3.3526e-04 - 16s/epoch - 3ms/step
Epoch 95/100
5969/5969 - 16s - loss: 4.0610e-04 - val_loss: 3.5893e-04 - 16s/epoch - 3ms/step
Epoch 96/100
5969/5969 - 16s - loss: 4.1325e-04 - val_loss: 3.5352e-04 - 16s/epoch - 3ms/step
Epoch 97/100
5969/5969 - 16s - loss: 4.0645e-04 - val_loss: 3.3640e-04 - 16s/epoch - 3ms/step
Epoch 98/100
5969/5969 - 16s - loss: 4.0437e-04 - val_loss: 3.6671e-04 - 16s/epoch - 3ms/step
Epoch 99/100
5969/5969 - 16s - loss: 4.0281e-04 - val_loss: 3.5179e-04 - 16s/epoch - 3ms/step
Epoch 100/100
5969/5969 - 16s - loss: 3.9650e-04 - val_loss: 3.4341e-04 - 16s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0003434142272453755
  1/332 [..............................] - ETA: 29s 50/332 [===>..........................] - ETA: 0s 100/332 [========>.....................] - ETA: 0s150/332 [============>.................] - ETA: 0s200/332 [=================>............] - ETA: 0s250/332 [=====================>........] - ETA: 0s300/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.00380655750211032
cosine 0.003075316194494041
MAE: 0.009670808
RMSE: 0.018531429
r2: 0.9777223514998782
RMSE zero-vector: 0.23411466903540806
['default', 16, 100, 0.001, 'mse', 0.5, 632, 0.0003964980423916131, 0.0003434142272453755, 0.00380655750211032, 0.003075316194494041, 0.009670807980000973, 0.018531428650021553, 0.9777223514998782, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_12 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_14 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2528)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_13"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_14 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_12 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_14"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_15 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_14 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2528)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
5969/5969 - 17s - loss: 0.0161 - val_loss: 0.0033 - 17s/epoch - 3ms/step
Epoch 2/100
5969/5969 - 16s - loss: 0.0033 - val_loss: 0.0029 - 16s/epoch - 3ms/step
Epoch 3/100
5969/5969 - 16s - loss: 0.0027 - val_loss: 0.0023 - 16s/epoch - 3ms/step
Epoch 4/100
5969/5969 - 16s - loss: 0.0023 - val_loss: 0.0253 - 16s/epoch - 3ms/step
Epoch 5/100
5969/5969 - 16s - loss: 0.0022 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 6/100
5969/5969 - 16s - loss: 0.0021 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 7/100
5969/5969 - 16s - loss: 0.0020 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 8/100
5969/5969 - 16s - loss: 0.0019 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 9/100
5969/5969 - 16s - loss: 0.0019 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 10/100
5969/5969 - 16s - loss: 0.0019 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 11/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 12/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 13/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 14/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 15/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 16/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 17/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 18/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 19/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 20/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 21/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 22/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 23/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 24/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 25/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 26/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 27/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 28/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 29/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 30/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 31/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 32/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 33/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 34/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 35/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 36/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 37/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 38/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 39/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 40/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 41/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 42/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0023 - 16s/epoch - 3ms/step
Epoch 43/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 44/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 45/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 46/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 47/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 48/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0021 - 16s/epoch - 3ms/step
Epoch 49/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 50/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 51/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 52/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 53/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 54/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 55/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 56/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0021 - 16s/epoch - 3ms/step
Epoch 57/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0023 - 16s/epoch - 3ms/step
Epoch 58/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 59/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 60/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0022 - 16s/epoch - 3ms/step
Epoch 61/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0024 - 16s/epoch - 3ms/step
Epoch 62/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0038 - 16s/epoch - 3ms/step
Epoch 63/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 64/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 65/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0038 - 16s/epoch - 3ms/step
Epoch 66/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 67/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0021 - 16s/epoch - 3ms/step
Epoch 68/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0030 - 16s/epoch - 3ms/step
Epoch 69/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 70/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0024 - 16s/epoch - 3ms/step
Epoch 71/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0025 - 16s/epoch - 3ms/step
Epoch 72/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0036 - 16s/epoch - 3ms/step
Epoch 73/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0026 - 16s/epoch - 3ms/step
Epoch 74/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 75/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 76/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0026 - 16s/epoch - 3ms/step
Epoch 77/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 78/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0023 - 16s/epoch - 3ms/step
Epoch 79/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 80/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0028 - 16s/epoch - 3ms/step
Epoch 81/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0022 - 16s/epoch - 3ms/step
Epoch 82/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 83/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0024 - 16s/epoch - 3ms/step
Epoch 84/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0022 - 16s/epoch - 3ms/step
Epoch 85/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0023 - 16s/epoch - 3ms/step
Epoch 86/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 87/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0026 - 16s/epoch - 3ms/step
Epoch 88/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 89/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 90/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 91/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 92/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0038 - 16s/epoch - 3ms/step
Epoch 93/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0021 - 16s/epoch - 3ms/step
Epoch 94/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 95/100
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 96/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0025 - 16s/epoch - 3ms/step
Epoch 97/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0022 - 16s/epoch - 3ms/step
Epoch 98/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0025 - 16s/epoch - 3ms/step
Epoch 99/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0021 - 16s/epoch - 3ms/step
Epoch 100/100
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0020 - 16s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0019863226916640997
  1/332 [..............................] - ETA: 30s 51/332 [===>..........................] - ETA: 0s 101/332 [========>.....................] - ETA: 0s152/332 [============>.................] - ETA: 0s202/332 [=================>............] - ETA: 0s253/332 [=====================>........] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.020508553547720795
cosine 0.01660465274133909
MAE: 0.023129147
RMSE: 0.044568148
r2: 0.8711430807142386
RMSE zero-vector: 0.23411466903540806
['default', 16, 100, 0.005, 'mse', 0.5, 632, 0.0017627108609303832, 0.0019863226916640997, 0.020508553547720795, 0.01660465274133909, 0.023129146546125412, 0.044568147510290146, 0.8711430807142386, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_16"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_17 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_17"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_18 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_18 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_18 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_20 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2528)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
5969/5969 - 16s - loss: 0.0080 - val_loss: 0.0032 - 16s/epoch - 3ms/step
Epoch 2/200
5969/5969 - 16s - loss: 0.0036 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 3/200
5969/5969 - 16s - loss: 0.0022 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 4/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0012 - 16s/epoch - 3ms/step
Epoch 5/200
5969/5969 - 16s - loss: 0.0013 - val_loss: 9.5593e-04 - 16s/epoch - 3ms/step
Epoch 6/200
5969/5969 - 16s - loss: 0.0011 - val_loss: 8.5272e-04 - 16s/epoch - 3ms/step
Epoch 7/200
5969/5969 - 16s - loss: 9.9421e-04 - val_loss: 8.0509e-04 - 16s/epoch - 3ms/step
Epoch 8/200
5969/5969 - 16s - loss: 9.3655e-04 - val_loss: 7.5008e-04 - 16s/epoch - 3ms/step
Epoch 9/200
5969/5969 - 16s - loss: 8.7878e-04 - val_loss: 6.8316e-04 - 16s/epoch - 3ms/step
Epoch 10/200
5969/5969 - 16s - loss: 8.2332e-04 - val_loss: 6.8536e-04 - 16s/epoch - 3ms/step
Epoch 11/200
5969/5969 - 16s - loss: 7.8073e-04 - val_loss: 6.4072e-04 - 16s/epoch - 3ms/step
Epoch 12/200
5969/5969 - 16s - loss: 7.4951e-04 - val_loss: 6.1277e-04 - 16s/epoch - 3ms/step
Epoch 13/200
5969/5969 - 16s - loss: 7.0296e-04 - val_loss: 5.7498e-04 - 16s/epoch - 3ms/step
Epoch 14/200
5969/5969 - 16s - loss: 6.8315e-04 - val_loss: 5.4767e-04 - 16s/epoch - 3ms/step
Epoch 15/200
5969/5969 - 16s - loss: 6.6090e-04 - val_loss: 5.5551e-04 - 16s/epoch - 3ms/step
Epoch 16/200
5969/5969 - 16s - loss: 6.5029e-04 - val_loss: 5.3461e-04 - 16s/epoch - 3ms/step
Epoch 17/200
5969/5969 - 16s - loss: 6.2820e-04 - val_loss: 5.2697e-04 - 16s/epoch - 3ms/step
Epoch 18/200
5969/5969 - 16s - loss: 6.0513e-04 - val_loss: 5.3959e-04 - 16s/epoch - 3ms/step
Epoch 19/200
5969/5969 - 16s - loss: 5.8824e-04 - val_loss: 5.0059e-04 - 16s/epoch - 3ms/step
Epoch 20/200
5969/5969 - 16s - loss: 5.8112e-04 - val_loss: 4.7689e-04 - 16s/epoch - 3ms/step
Epoch 21/200
5969/5969 - 16s - loss: 5.6659e-04 - val_loss: 5.0049e-04 - 16s/epoch - 3ms/step
Epoch 22/200
5969/5969 - 16s - loss: 5.5492e-04 - val_loss: 4.5694e-04 - 16s/epoch - 3ms/step
Epoch 23/200
5969/5969 - 16s - loss: 5.4518e-04 - val_loss: 4.6531e-04 - 16s/epoch - 3ms/step
Epoch 24/200
5969/5969 - 16s - loss: 5.3136e-04 - val_loss: 4.6299e-04 - 16s/epoch - 3ms/step
Epoch 25/200
5969/5969 - 16s - loss: 5.2564e-04 - val_loss: 4.5808e-04 - 16s/epoch - 3ms/step
Epoch 26/200
5969/5969 - 16s - loss: 5.1023e-04 - val_loss: 4.0568e-04 - 16s/epoch - 3ms/step
Epoch 27/200
5969/5969 - 16s - loss: 5.1048e-04 - val_loss: 4.3073e-04 - 16s/epoch - 3ms/step
Epoch 28/200
5969/5969 - 16s - loss: 4.9924e-04 - val_loss: 4.1133e-04 - 16s/epoch - 3ms/step
Epoch 29/200
5969/5969 - 16s - loss: 4.9730e-04 - val_loss: 4.0904e-04 - 16s/epoch - 3ms/step
Epoch 30/200
5969/5969 - 16s - loss: 4.7847e-04 - val_loss: 3.7993e-04 - 16s/epoch - 3ms/step
Epoch 31/200
5969/5969 - 16s - loss: 4.8163e-04 - val_loss: 3.8622e-04 - 16s/epoch - 3ms/step
Epoch 32/200
5969/5969 - 16s - loss: 4.7649e-04 - val_loss: 3.9487e-04 - 16s/epoch - 3ms/step
Epoch 33/200
5969/5969 - 16s - loss: 4.6332e-04 - val_loss: 4.0849e-04 - 16s/epoch - 3ms/step
Epoch 34/200
5969/5969 - 16s - loss: 4.6066e-04 - val_loss: 3.8773e-04 - 16s/epoch - 3ms/step
Epoch 35/200
5969/5969 - 16s - loss: 4.5032e-04 - val_loss: 3.8316e-04 - 16s/epoch - 3ms/step
Epoch 36/200
5969/5969 - 16s - loss: 4.5168e-04 - val_loss: 3.6679e-04 - 16s/epoch - 3ms/step
Epoch 37/200
5969/5969 - 16s - loss: 4.4538e-04 - val_loss: 3.7634e-04 - 16s/epoch - 3ms/step
Epoch 38/200
5969/5969 - 16s - loss: 4.3912e-04 - val_loss: 3.8594e-04 - 16s/epoch - 3ms/step
Epoch 39/200
5969/5969 - 16s - loss: 4.3910e-04 - val_loss: 4.7034e-04 - 16s/epoch - 3ms/step
Epoch 40/200
5969/5969 - 16s - loss: 4.3097e-04 - val_loss: 3.5529e-04 - 16s/epoch - 3ms/step
Epoch 41/200
5969/5969 - 16s - loss: 4.3482e-04 - val_loss: 3.5820e-04 - 16s/epoch - 3ms/step
Epoch 42/200
5969/5969 - 16s - loss: 4.2837e-04 - val_loss: 3.4157e-04 - 16s/epoch - 3ms/step
Epoch 43/200
5969/5969 - 16s - loss: 4.2349e-04 - val_loss: 3.4766e-04 - 16s/epoch - 3ms/step
Epoch 44/200
5969/5969 - 16s - loss: 4.2135e-04 - val_loss: 3.5484e-04 - 16s/epoch - 3ms/step
Epoch 45/200
5969/5969 - 16s - loss: 4.1525e-04 - val_loss: 3.3848e-04 - 16s/epoch - 3ms/step
Epoch 46/200
5969/5969 - 16s - loss: 4.0985e-04 - val_loss: 3.3941e-04 - 16s/epoch - 3ms/step
Epoch 47/200
5969/5969 - 16s - loss: 4.0991e-04 - val_loss: 3.4109e-04 - 16s/epoch - 3ms/step
Epoch 48/200
5969/5969 - 16s - loss: 4.0176e-04 - val_loss: 3.3638e-04 - 16s/epoch - 3ms/step
Epoch 49/200
5969/5969 - 16s - loss: 4.0956e-04 - val_loss: 3.3949e-04 - 16s/epoch - 3ms/step
Epoch 50/200
5969/5969 - 16s - loss: 4.1214e-04 - val_loss: 3.4572e-04 - 16s/epoch - 3ms/step
Epoch 51/200
5969/5969 - 16s - loss: 3.9747e-04 - val_loss: 3.2342e-04 - 16s/epoch - 3ms/step
Epoch 52/200
5969/5969 - 16s - loss: 3.9635e-04 - val_loss: 3.2502e-04 - 16s/epoch - 3ms/step
Epoch 53/200
5969/5969 - 16s - loss: 3.9549e-04 - val_loss: 3.3195e-04 - 16s/epoch - 3ms/step
Epoch 54/200
5969/5969 - 16s - loss: 3.8908e-04 - val_loss: 3.5322e-04 - 16s/epoch - 3ms/step
Epoch 55/200
5969/5969 - 16s - loss: 3.8572e-04 - val_loss: 3.2367e-04 - 16s/epoch - 3ms/step
Epoch 56/200
5969/5969 - 16s - loss: 3.8594e-04 - val_loss: 3.0491e-04 - 16s/epoch - 3ms/step
Epoch 57/200
5969/5969 - 16s - loss: 3.7850e-04 - val_loss: 3.0958e-04 - 16s/epoch - 3ms/step
Epoch 58/200
5969/5969 - 16s - loss: 3.7892e-04 - val_loss: 3.2686e-04 - 16s/epoch - 3ms/step
Epoch 59/200
5969/5969 - 16s - loss: 3.7626e-04 - val_loss: 3.0333e-04 - 16s/epoch - 3ms/step
Epoch 60/200
5969/5969 - 16s - loss: 3.8363e-04 - val_loss: 3.0789e-04 - 16s/epoch - 3ms/step
Epoch 61/200
5969/5969 - 16s - loss: 3.7659e-04 - val_loss: 3.1320e-04 - 16s/epoch - 3ms/step
Epoch 62/200
5969/5969 - 16s - loss: 3.7712e-04 - val_loss: 3.2703e-04 - 16s/epoch - 3ms/step
Epoch 63/200
5969/5969 - 16s - loss: 3.7412e-04 - val_loss: 3.0271e-04 - 16s/epoch - 3ms/step
Epoch 64/200
5969/5969 - 16s - loss: 3.6564e-04 - val_loss: 3.0589e-04 - 16s/epoch - 3ms/step
Epoch 65/200
5969/5969 - 16s - loss: 3.7365e-04 - val_loss: 3.0620e-04 - 16s/epoch - 3ms/step
Epoch 66/200
5969/5969 - 16s - loss: 3.7478e-04 - val_loss: 3.1145e-04 - 16s/epoch - 3ms/step
Epoch 67/200
5969/5969 - 16s - loss: 3.6490e-04 - val_loss: 2.9987e-04 - 16s/epoch - 3ms/step
Epoch 68/200
5969/5969 - 16s - loss: 3.6403e-04 - val_loss: 3.1039e-04 - 16s/epoch - 3ms/step
Epoch 69/200
5969/5969 - 16s - loss: 3.6697e-04 - val_loss: 2.9940e-04 - 16s/epoch - 3ms/step
Epoch 70/200
5969/5969 - 16s - loss: 3.5950e-04 - val_loss: 2.9569e-04 - 16s/epoch - 3ms/step
Epoch 71/200
5969/5969 - 16s - loss: 3.5436e-04 - val_loss: 3.1017e-04 - 16s/epoch - 3ms/step
Epoch 72/200
5969/5969 - 16s - loss: 3.5759e-04 - val_loss: 3.0783e-04 - 16s/epoch - 3ms/step
Epoch 73/200
5969/5969 - 16s - loss: 3.6270e-04 - val_loss: 3.1393e-04 - 16s/epoch - 3ms/step
Epoch 74/200
5969/5969 - 16s - loss: 3.5552e-04 - val_loss: 3.0357e-04 - 16s/epoch - 3ms/step
Epoch 75/200
5969/5969 - 16s - loss: 3.5437e-04 - val_loss: 3.0725e-04 - 16s/epoch - 3ms/step
Epoch 76/200
5969/5969 - 16s - loss: 3.4776e-04 - val_loss: 2.7888e-04 - 16s/epoch - 3ms/step
Epoch 77/200
5969/5969 - 16s - loss: 3.4823e-04 - val_loss: 2.8672e-04 - 16s/epoch - 3ms/step
Epoch 78/200
5969/5969 - 16s - loss: 3.5602e-04 - val_loss: 2.9190e-04 - 16s/epoch - 3ms/step
Epoch 79/200
5969/5969 - 16s - loss: 3.4504e-04 - val_loss: 2.8610e-04 - 16s/epoch - 3ms/step
Epoch 80/200
5969/5969 - 16s - loss: 3.4548e-04 - val_loss: 2.8203e-04 - 16s/epoch - 3ms/step
Epoch 81/200
5969/5969 - 16s - loss: 3.5421e-04 - val_loss: 2.9239e-04 - 16s/epoch - 3ms/step
Epoch 82/200
5969/5969 - 16s - loss: 3.4364e-04 - val_loss: 2.7115e-04 - 16s/epoch - 3ms/step
Epoch 83/200
5969/5969 - 16s - loss: 3.4234e-04 - val_loss: 3.0997e-04 - 16s/epoch - 3ms/step
Epoch 84/200
5969/5969 - 16s - loss: 3.4023e-04 - val_loss: 3.0451e-04 - 16s/epoch - 3ms/step
Epoch 85/200
5969/5969 - 16s - loss: 3.4851e-04 - val_loss: 2.8211e-04 - 16s/epoch - 3ms/step
Epoch 86/200
5969/5969 - 16s - loss: 3.4100e-04 - val_loss: 3.1322e-04 - 16s/epoch - 3ms/step
Epoch 87/200
5969/5969 - 16s - loss: 3.3978e-04 - val_loss: 3.0279e-04 - 16s/epoch - 3ms/step
Epoch 88/200
5969/5969 - 16s - loss: 3.3771e-04 - val_loss: 2.7020e-04 - 16s/epoch - 3ms/step
Epoch 89/200
5969/5969 - 16s - loss: 3.3511e-04 - val_loss: 2.8227e-04 - 16s/epoch - 3ms/step
Epoch 90/200
5969/5969 - 16s - loss: 3.3684e-04 - val_loss: 2.7418e-04 - 16s/epoch - 3ms/step
Epoch 91/200
5969/5969 - 16s - loss: 3.3217e-04 - val_loss: 3.3414e-04 - 16s/epoch - 3ms/step
Epoch 92/200
5969/5969 - 16s - loss: 3.3139e-04 - val_loss: 2.8906e-04 - 16s/epoch - 3ms/step
Epoch 93/200
5969/5969 - 16s - loss: 3.2821e-04 - val_loss: 2.9475e-04 - 16s/epoch - 3ms/step
Epoch 94/200
5969/5969 - 16s - loss: 3.3898e-04 - val_loss: 2.7595e-04 - 16s/epoch - 3ms/step
Epoch 95/200
5969/5969 - 16s - loss: 3.3271e-04 - val_loss: 2.9083e-04 - 16s/epoch - 3ms/step
Epoch 96/200
5969/5969 - 16s - loss: 3.3151e-04 - val_loss: 2.9245e-04 - 16s/epoch - 3ms/step
Epoch 97/200
5969/5969 - 16s - loss: 3.3381e-04 - val_loss: 2.7175e-04 - 16s/epoch - 3ms/step
Epoch 98/200
5969/5969 - 16s - loss: 3.3313e-04 - val_loss: 2.8051e-04 - 16s/epoch - 3ms/step
Epoch 99/200
5969/5969 - 16s - loss: 3.2672e-04 - val_loss: 2.7303e-04 - 16s/epoch - 3ms/step
Epoch 100/200
5969/5969 - 16s - loss: 3.2566e-04 - val_loss: 2.8047e-04 - 16s/epoch - 3ms/step
Epoch 101/200
5969/5969 - 16s - loss: 3.2738e-04 - val_loss: 2.8739e-04 - 16s/epoch - 3ms/step
Epoch 102/200
5969/5969 - 16s - loss: 3.2670e-04 - val_loss: 2.8560e-04 - 16s/epoch - 3ms/step
Epoch 103/200
5969/5969 - 16s - loss: 3.3334e-04 - val_loss: 2.7844e-04 - 16s/epoch - 3ms/step
Epoch 104/200
5969/5969 - 16s - loss: 3.2619e-04 - val_loss: 2.7534e-04 - 16s/epoch - 3ms/step
Epoch 105/200
5969/5969 - 16s - loss: 3.1971e-04 - val_loss: 2.7442e-04 - 16s/epoch - 3ms/step
Epoch 106/200
5969/5969 - 16s - loss: 3.2154e-04 - val_loss: 2.7561e-04 - 16s/epoch - 3ms/step
Epoch 107/200
5969/5969 - 16s - loss: 3.2096e-04 - val_loss: 2.9218e-04 - 16s/epoch - 3ms/step
Epoch 108/200
5969/5969 - 16s - loss: 3.2034e-04 - val_loss: 2.7168e-04 - 16s/epoch - 3ms/step
Epoch 109/200
5969/5969 - 16s - loss: 3.1947e-04 - val_loss: 2.7565e-04 - 16s/epoch - 3ms/step
Epoch 110/200
5969/5969 - 16s - loss: 3.1444e-04 - val_loss: 3.0776e-04 - 16s/epoch - 3ms/step
Epoch 111/200
5969/5969 - 16s - loss: 3.2064e-04 - val_loss: 2.8569e-04 - 16s/epoch - 3ms/step
Epoch 112/200
5969/5969 - 16s - loss: 3.2677e-04 - val_loss: 2.8331e-04 - 16s/epoch - 3ms/step
Epoch 113/200
5969/5969 - 16s - loss: 3.1246e-04 - val_loss: 2.7484e-04 - 16s/epoch - 3ms/step
Epoch 114/200
5969/5969 - 16s - loss: 3.1117e-04 - val_loss: 2.6903e-04 - 16s/epoch - 3ms/step
Epoch 115/200
5969/5969 - 16s - loss: 3.1859e-04 - val_loss: 2.5825e-04 - 16s/epoch - 3ms/step
Epoch 116/200
5969/5969 - 16s - loss: 3.1435e-04 - val_loss: 2.7761e-04 - 16s/epoch - 3ms/step
Epoch 117/200
5969/5969 - 16s - loss: 3.1407e-04 - val_loss: 2.6563e-04 - 16s/epoch - 3ms/step
Epoch 118/200
5969/5969 - 16s - loss: 3.1823e-04 - val_loss: 2.8979e-04 - 16s/epoch - 3ms/step
Epoch 119/200
5969/5969 - 16s - loss: 3.1250e-04 - val_loss: 2.7744e-04 - 16s/epoch - 3ms/step
Epoch 120/200
5969/5969 - 16s - loss: 3.1153e-04 - val_loss: 2.7000e-04 - 16s/epoch - 3ms/step
Epoch 121/200
5969/5969 - 16s - loss: 3.1128e-04 - val_loss: 2.7999e-04 - 16s/epoch - 3ms/step
Epoch 122/200
5969/5969 - 16s - loss: 3.1203e-04 - val_loss: 2.5809e-04 - 16s/epoch - 3ms/step
Epoch 123/200
5969/5969 - 16s - loss: 3.0526e-04 - val_loss: 2.8556e-04 - 16s/epoch - 3ms/step
Epoch 124/200
5969/5969 - 16s - loss: 3.0750e-04 - val_loss: 2.8453e-04 - 16s/epoch - 3ms/step
Epoch 125/200
5969/5969 - 16s - loss: 3.1036e-04 - val_loss: 2.8113e-04 - 16s/epoch - 3ms/step
Epoch 126/200
5969/5969 - 16s - loss: 3.0884e-04 - val_loss: 2.8232e-04 - 16s/epoch - 3ms/step
Epoch 127/200
5969/5969 - 16s - loss: 3.1241e-04 - val_loss: 2.6446e-04 - 16s/epoch - 3ms/step
Epoch 128/200
5969/5969 - 16s - loss: 3.0552e-04 - val_loss: 2.6630e-04 - 16s/epoch - 3ms/step
Epoch 129/200
5969/5969 - 16s - loss: 3.1035e-04 - val_loss: 2.8290e-04 - 16s/epoch - 3ms/step
Epoch 130/200
5969/5969 - 16s - loss: 3.0484e-04 - val_loss: 2.8488e-04 - 16s/epoch - 3ms/step
Epoch 131/200
5969/5969 - 16s - loss: 3.1329e-04 - val_loss: 2.7654e-04 - 16s/epoch - 3ms/step
Epoch 132/200
5969/5969 - 16s - loss: 3.0458e-04 - val_loss: 2.7413e-04 - 16s/epoch - 3ms/step
Epoch 133/200
5969/5969 - 16s - loss: 3.0351e-04 - val_loss: 2.6723e-04 - 16s/epoch - 3ms/step
Epoch 134/200
5969/5969 - 16s - loss: 3.0366e-04 - val_loss: 2.6908e-04 - 16s/epoch - 3ms/step
Epoch 135/200
5969/5969 - 16s - loss: 3.1170e-04 - val_loss: 2.8065e-04 - 16s/epoch - 3ms/step
Epoch 136/200
5969/5969 - 16s - loss: 2.9998e-04 - val_loss: 2.5518e-04 - 16s/epoch - 3ms/step
Epoch 137/200
5969/5969 - 16s - loss: 3.0159e-04 - val_loss: 2.7183e-04 - 16s/epoch - 3ms/step
Epoch 138/200
5969/5969 - 16s - loss: 3.0288e-04 - val_loss: 3.0820e-04 - 16s/epoch - 3ms/step
Epoch 139/200
5969/5969 - 16s - loss: 3.0121e-04 - val_loss: 2.8688e-04 - 16s/epoch - 3ms/step
Epoch 140/200
5969/5969 - 16s - loss: 3.0196e-04 - val_loss: 2.6351e-04 - 16s/epoch - 3ms/step
Epoch 141/200
5969/5969 - 16s - loss: 2.9964e-04 - val_loss: 2.5304e-04 - 16s/epoch - 3ms/step
Epoch 142/200
5969/5969 - 16s - loss: 3.0088e-04 - val_loss: 2.8696e-04 - 16s/epoch - 3ms/step
Epoch 143/200
5969/5969 - 16s - loss: 2.9689e-04 - val_loss: 2.7651e-04 - 16s/epoch - 3ms/step
Epoch 144/200
5969/5969 - 16s - loss: 2.9805e-04 - val_loss: 2.5735e-04 - 16s/epoch - 3ms/step
Epoch 145/200
5969/5969 - 16s - loss: 3.0160e-04 - val_loss: 2.5151e-04 - 16s/epoch - 3ms/step
Epoch 146/200
5969/5969 - 16s - loss: 2.9900e-04 - val_loss: 2.6845e-04 - 16s/epoch - 3ms/step
Epoch 147/200
5969/5969 - 16s - loss: 2.9507e-04 - val_loss: 2.5638e-04 - 16s/epoch - 3ms/step
Epoch 148/200
5969/5969 - 16s - loss: 3.0293e-04 - val_loss: 2.8730e-04 - 16s/epoch - 3ms/step
Epoch 149/200
5969/5969 - 16s - loss: 2.9758e-04 - val_loss: 2.5648e-04 - 16s/epoch - 3ms/step
Epoch 150/200
5969/5969 - 16s - loss: 2.9599e-04 - val_loss: 2.6101e-04 - 16s/epoch - 3ms/step
Epoch 151/200
5969/5969 - 16s - loss: 2.9189e-04 - val_loss: 2.5583e-04 - 16s/epoch - 3ms/step
Epoch 152/200
5969/5969 - 16s - loss: 2.9562e-04 - val_loss: 2.5658e-04 - 16s/epoch - 3ms/step
Epoch 153/200
5969/5969 - 16s - loss: 2.9986e-04 - val_loss: 2.6477e-04 - 16s/epoch - 3ms/step
Epoch 154/200
5969/5969 - 16s - loss: 2.9965e-04 - val_loss: 2.6020e-04 - 16s/epoch - 3ms/step
Epoch 155/200
5969/5969 - 16s - loss: 2.9110e-04 - val_loss: 2.4300e-04 - 16s/epoch - 3ms/step
Epoch 156/200
5969/5969 - 16s - loss: 2.8985e-04 - val_loss: 2.8039e-04 - 16s/epoch - 3ms/step
Epoch 157/200
5969/5969 - 16s - loss: 2.8938e-04 - val_loss: 2.4380e-04 - 16s/epoch - 3ms/step
Epoch 158/200
5969/5969 - 16s - loss: 2.9026e-04 - val_loss: 2.6646e-04 - 16s/epoch - 3ms/step
Epoch 159/200
5969/5969 - 16s - loss: 2.9296e-04 - val_loss: 2.5610e-04 - 16s/epoch - 3ms/step
Epoch 160/200
5969/5969 - 16s - loss: 2.8887e-04 - val_loss: 3.0066e-04 - 16s/epoch - 3ms/step
Epoch 161/200
5969/5969 - 16s - loss: 2.9656e-04 - val_loss: 2.9550e-04 - 16s/epoch - 3ms/step
Epoch 162/200
5969/5969 - 16s - loss: 2.8925e-04 - val_loss: 3.1702e-04 - 16s/epoch - 3ms/step
Epoch 163/200
5969/5969 - 16s - loss: 2.9454e-04 - val_loss: 2.6852e-04 - 16s/epoch - 3ms/step
Epoch 164/200
5969/5969 - 16s - loss: 2.9006e-04 - val_loss: 2.5337e-04 - 16s/epoch - 3ms/step
Epoch 165/200
5969/5969 - 16s - loss: 2.8774e-04 - val_loss: 2.5305e-04 - 16s/epoch - 3ms/step
Epoch 166/200
5969/5969 - 16s - loss: 2.8644e-04 - val_loss: 2.4851e-04 - 16s/epoch - 3ms/step
Epoch 167/200
5969/5969 - 16s - loss: 2.9064e-04 - val_loss: 4.3738e-04 - 16s/epoch - 3ms/step
Epoch 168/200
5969/5969 - 16s - loss: 2.9208e-04 - val_loss: 2.3906e-04 - 16s/epoch - 3ms/step
Epoch 169/200
5969/5969 - 16s - loss: 2.9055e-04 - val_loss: 2.4206e-04 - 16s/epoch - 3ms/step
Epoch 170/200
5969/5969 - 16s - loss: 2.8522e-04 - val_loss: 2.4718e-04 - 16s/epoch - 3ms/step
Epoch 171/200
5969/5969 - 16s - loss: 2.9206e-04 - val_loss: 2.6415e-04 - 16s/epoch - 3ms/step
Epoch 172/200
5969/5969 - 16s - loss: 2.8848e-04 - val_loss: 2.7303e-04 - 16s/epoch - 3ms/step
Epoch 173/200
5969/5969 - 16s - loss: 2.8314e-04 - val_loss: 2.5111e-04 - 16s/epoch - 3ms/step
Epoch 174/200
5969/5969 - 16s - loss: 2.8899e-04 - val_loss: 2.5296e-04 - 16s/epoch - 3ms/step
Epoch 175/200
5969/5969 - 16s - loss: 2.8872e-04 - val_loss: 2.7056e-04 - 16s/epoch - 3ms/step
Epoch 176/200
5969/5969 - 16s - loss: 2.8602e-04 - val_loss: 2.5499e-04 - 16s/epoch - 3ms/step
Epoch 177/200
5969/5969 - 16s - loss: 2.8635e-04 - val_loss: 2.6515e-04 - 16s/epoch - 3ms/step
Epoch 178/200
5969/5969 - 16s - loss: 2.9318e-04 - val_loss: 2.6517e-04 - 16s/epoch - 3ms/step
Epoch 179/200
5969/5969 - 16s - loss: 2.8642e-04 - val_loss: 2.5481e-04 - 16s/epoch - 3ms/step
Epoch 180/200
5969/5969 - 16s - loss: 2.8377e-04 - val_loss: 2.6852e-04 - 16s/epoch - 3ms/step
Epoch 181/200
5969/5969 - 16s - loss: 2.8299e-04 - val_loss: 2.8156e-04 - 16s/epoch - 3ms/step
Epoch 182/200
5969/5969 - 16s - loss: 2.8178e-04 - val_loss: 2.7039e-04 - 16s/epoch - 3ms/step
Epoch 183/200
5969/5969 - 16s - loss: 3.0028e-04 - val_loss: 2.4403e-04 - 16s/epoch - 3ms/step
Epoch 184/200
5969/5969 - 16s - loss: 2.8072e-04 - val_loss: 2.5780e-04 - 16s/epoch - 3ms/step
Epoch 185/200
5969/5969 - 16s - loss: 2.7861e-04 - val_loss: 2.4494e-04 - 16s/epoch - 3ms/step
Epoch 186/200
5969/5969 - 16s - loss: 2.9092e-04 - val_loss: 2.8664e-04 - 16s/epoch - 3ms/step
Epoch 187/200
5969/5969 - 16s - loss: 2.9139e-04 - val_loss: 3.2229e-04 - 16s/epoch - 3ms/step
Epoch 188/200
5969/5969 - 16s - loss: 2.8169e-04 - val_loss: 2.8779e-04 - 16s/epoch - 3ms/step
Epoch 189/200
5969/5969 - 16s - loss: 2.8008e-04 - val_loss: 2.6240e-04 - 16s/epoch - 3ms/step
Epoch 190/200
5969/5969 - 16s - loss: 2.7726e-04 - val_loss: 2.8587e-04 - 16s/epoch - 3ms/step
Epoch 191/200
5969/5969 - 16s - loss: 2.8228e-04 - val_loss: 2.7039e-04 - 16s/epoch - 3ms/step
Epoch 192/200
5969/5969 - 16s - loss: 2.7848e-04 - val_loss: 2.5480e-04 - 16s/epoch - 3ms/step
Epoch 193/200
5969/5969 - 16s - loss: 2.8517e-04 - val_loss: 2.9341e-04 - 16s/epoch - 3ms/step
Epoch 194/200
5969/5969 - 16s - loss: 2.7606e-04 - val_loss: 2.8462e-04 - 16s/epoch - 3ms/step
Epoch 195/200
5969/5969 - 16s - loss: 2.7806e-04 - val_loss: 2.7238e-04 - 16s/epoch - 3ms/step
Epoch 196/200
5969/5969 - 16s - loss: 2.7741e-04 - val_loss: 2.4367e-04 - 16s/epoch - 3ms/step
Epoch 197/200
5969/5969 - 16s - loss: 2.8720e-04 - val_loss: 2.6450e-04 - 16s/epoch - 3ms/step
Epoch 198/200
5969/5969 - 16s - loss: 2.7636e-04 - val_loss: 2.5913e-04 - 16s/epoch - 3ms/step
Epoch 199/200
5969/5969 - 16s - loss: 2.7458e-04 - val_loss: 2.6424e-04 - 16s/epoch - 3ms/step
Epoch 200/200
5969/5969 - 16s - loss: 2.7847e-04 - val_loss: 2.6265e-04 - 16s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0002626502828206867
  1/332 [..............................] - ETA: 28s 52/332 [===>..........................] - ETA: 0s 103/332 [========>.....................] - ETA: 0s155/332 [=============>................] - ETA: 0s206/332 [=================>............] - ETA: 0s258/332 [======================>.......] - ETA: 0s310/332 [===========================>..] - ETA: 0s332/332 [==============================] - 0s 981us/step
correlation 0.002822818978076335
cosine 0.002288801846214838
MAE: 0.007934987
RMSE: 0.01620647
r2: 0.9829620189249366
RMSE zero-vector: 0.23411466903540806
['default', 16, 200, 0.0005, 'mse', 0.5, 632, 0.00027847281307913363, 0.0002626502828206867, 0.002822818978076335, 0.002288801846214838, 0.007934986613690853, 0.016206469386816025, 0.9829620189249366, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_18 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_20 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2528)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_19"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_20 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_18 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_20"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_21 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_20 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2528)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_21 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_21 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_22 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 632)               0         
                                                                 
 dense_22 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_23 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2528)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
5969/5969 - 16s - loss: 0.0075 - val_loss: 0.0025 - 16s/epoch - 3ms/step
Epoch 2/200
5969/5969 - 16s - loss: 0.0027 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 3/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0014 - 16s/epoch - 3ms/step
Epoch 4/200
5969/5969 - 16s - loss: 0.0014 - val_loss: 0.0011 - 16s/epoch - 3ms/step
Epoch 5/200
5969/5969 - 16s - loss: 0.0012 - val_loss: 9.6768e-04 - 16s/epoch - 3ms/step
Epoch 6/200
5969/5969 - 16s - loss: 0.0011 - val_loss: 9.2682e-04 - 16s/epoch - 3ms/step
Epoch 7/200
5969/5969 - 16s - loss: 0.0010 - val_loss: 8.2754e-04 - 16s/epoch - 3ms/step
Epoch 8/200
5969/5969 - 16s - loss: 9.5634e-04 - val_loss: 7.6122e-04 - 16s/epoch - 3ms/step
Epoch 9/200
5969/5969 - 16s - loss: 9.1167e-04 - val_loss: 7.2185e-04 - 16s/epoch - 3ms/step
Epoch 10/200
5969/5969 - 16s - loss: 8.6147e-04 - val_loss: 7.5533e-04 - 16s/epoch - 3ms/step
Epoch 11/200
5969/5969 - 16s - loss: 8.1973e-04 - val_loss: 6.7141e-04 - 16s/epoch - 3ms/step
Epoch 12/200
5969/5969 - 16s - loss: 7.9327e-04 - val_loss: 6.2528e-04 - 16s/epoch - 3ms/step
Epoch 13/200
5969/5969 - 16s - loss: 7.6616e-04 - val_loss: 6.2229e-04 - 16s/epoch - 3ms/step
Epoch 14/200
5969/5969 - 16s - loss: 7.4068e-04 - val_loss: 5.8691e-04 - 16s/epoch - 3ms/step
Epoch 15/200
5969/5969 - 16s - loss: 7.1948e-04 - val_loss: 5.9654e-04 - 16s/epoch - 3ms/step
Epoch 16/200
5969/5969 - 16s - loss: 7.0673e-04 - val_loss: 5.9374e-04 - 16s/epoch - 3ms/step
Epoch 17/200
5969/5969 - 16s - loss: 6.8638e-04 - val_loss: 5.6503e-04 - 16s/epoch - 3ms/step
Epoch 18/200
5969/5969 - 16s - loss: 7.1103e-04 - val_loss: 5.9197e-04 - 16s/epoch - 3ms/step
Epoch 19/200
5969/5969 - 16s - loss: 6.6710e-04 - val_loss: 5.6154e-04 - 16s/epoch - 3ms/step
Epoch 20/200
5969/5969 - 16s - loss: 6.6031e-04 - val_loss: 6.6020e-04 - 16s/epoch - 3ms/step
Epoch 21/200
5969/5969 - 16s - loss: 6.3948e-04 - val_loss: 4.8966e-04 - 16s/epoch - 3ms/step
Epoch 22/200
5969/5969 - 16s - loss: 6.4108e-04 - val_loss: 5.1636e-04 - 16s/epoch - 3ms/step
Epoch 23/200
5969/5969 - 16s - loss: 6.1508e-04 - val_loss: 5.0963e-04 - 16s/epoch - 3ms/step
Epoch 24/200
5969/5969 - 16s - loss: 6.0805e-04 - val_loss: 4.9658e-04 - 16s/epoch - 3ms/step
Epoch 25/200
5969/5969 - 16s - loss: 6.0766e-04 - val_loss: 4.9707e-04 - 16s/epoch - 3ms/step
Epoch 26/200
5969/5969 - 16s - loss: 5.9004e-04 - val_loss: 4.6095e-04 - 16s/epoch - 3ms/step
Epoch 27/200
5969/5969 - 16s - loss: 5.8141e-04 - val_loss: 4.7476e-04 - 16s/epoch - 3ms/step
Epoch 28/200
5969/5969 - 16s - loss: 5.9060e-04 - val_loss: 4.5770e-04 - 16s/epoch - 3ms/step
Epoch 29/200
5969/5969 - 16s - loss: 5.7528e-04 - val_loss: 5.1861e-04 - 16s/epoch - 3ms/step
Epoch 30/200
5969/5969 - 16s - loss: 5.6134e-04 - val_loss: 4.3826e-04 - 16s/epoch - 3ms/step
Epoch 31/200
5969/5969 - 16s - loss: 5.5625e-04 - val_loss: 4.1805e-04 - 16s/epoch - 3ms/step
Epoch 32/200
5969/5969 - 16s - loss: 5.6218e-04 - val_loss: 4.7451e-04 - 16s/epoch - 3ms/step
Epoch 33/200
5969/5969 - 16s - loss: 5.4631e-04 - val_loss: 4.2639e-04 - 16s/epoch - 3ms/step
Epoch 34/200
5969/5969 - 16s - loss: 5.3818e-04 - val_loss: 4.0457e-04 - 16s/epoch - 3ms/step
Epoch 35/200
5969/5969 - 16s - loss: 5.3867e-04 - val_loss: 6.0422e-04 - 16s/epoch - 3ms/step
Epoch 36/200
5969/5969 - 16s - loss: 5.3131e-04 - val_loss: 4.3199e-04 - 16s/epoch - 3ms/step
Epoch 37/200
5969/5969 - 16s - loss: 5.4575e-04 - val_loss: 4.2060e-04 - 16s/epoch - 3ms/step
Epoch 38/200
5969/5969 - 16s - loss: 5.2086e-04 - val_loss: 4.4351e-04 - 16s/epoch - 3ms/step
Epoch 39/200
5969/5969 - 16s - loss: 5.2444e-04 - val_loss: 4.0352e-04 - 16s/epoch - 3ms/step
Epoch 40/200
5969/5969 - 16s - loss: 5.1248e-04 - val_loss: 4.2419e-04 - 16s/epoch - 3ms/step
Epoch 41/200
5969/5969 - 16s - loss: 5.1204e-04 - val_loss: 4.0337e-04 - 16s/epoch - 3ms/step
Epoch 42/200
5969/5969 - 16s - loss: 5.1585e-04 - val_loss: 4.3399e-04 - 16s/epoch - 3ms/step
Epoch 43/200
5969/5969 - 16s - loss: 5.0388e-04 - val_loss: 4.0149e-04 - 16s/epoch - 3ms/step
Epoch 44/200
5969/5969 - 16s - loss: 5.1126e-04 - val_loss: 4.0590e-04 - 16s/epoch - 3ms/step
Epoch 45/200
5969/5969 - 16s - loss: 5.0576e-04 - val_loss: 3.9708e-04 - 16s/epoch - 3ms/step
Epoch 46/200
5969/5969 - 16s - loss: 4.9613e-04 - val_loss: 3.9301e-04 - 16s/epoch - 3ms/step
Epoch 47/200
5969/5969 - 16s - loss: 4.9509e-04 - val_loss: 3.9185e-04 - 16s/epoch - 3ms/step
Epoch 48/200
5969/5969 - 16s - loss: 4.9046e-04 - val_loss: 3.9367e-04 - 16s/epoch - 3ms/step
Epoch 49/200
5969/5969 - 16s - loss: 5.0508e-04 - val_loss: 3.8830e-04 - 16s/epoch - 3ms/step
Epoch 50/200
5969/5969 - 16s - loss: 4.9736e-04 - val_loss: 3.8337e-04 - 16s/epoch - 3ms/step
Epoch 51/200
5969/5969 - 16s - loss: 4.8255e-04 - val_loss: 3.7068e-04 - 16s/epoch - 3ms/step
Epoch 52/200
5969/5969 - 16s - loss: 4.7474e-04 - val_loss: 3.6959e-04 - 16s/epoch - 3ms/step
Epoch 53/200
5969/5969 - 16s - loss: 4.8082e-04 - val_loss: 3.6996e-04 - 16s/epoch - 3ms/step
Epoch 54/200
5969/5969 - 16s - loss: 4.7419e-04 - val_loss: 3.7765e-04 - 16s/epoch - 3ms/step
Epoch 55/200
5969/5969 - 16s - loss: 4.6663e-04 - val_loss: 3.7012e-04 - 16s/epoch - 3ms/step
Epoch 56/200
5969/5969 - 16s - loss: 4.6610e-04 - val_loss: 3.7197e-04 - 16s/epoch - 3ms/step
Epoch 57/200
5969/5969 - 16s - loss: 4.6948e-04 - val_loss: 3.6586e-04 - 16s/epoch - 3ms/step
Epoch 58/200
5969/5969 - 16s - loss: 4.5702e-04 - val_loss: 3.7520e-04 - 16s/epoch - 3ms/step
Epoch 59/200
5969/5969 - 16s - loss: 4.5887e-04 - val_loss: 3.5813e-04 - 16s/epoch - 3ms/step
Epoch 60/200
5969/5969 - 16s - loss: 4.6263e-04 - val_loss: 3.5537e-04 - 16s/epoch - 3ms/step
Epoch 61/200
5969/5969 - 16s - loss: 4.5639e-04 - val_loss: 3.7899e-04 - 16s/epoch - 3ms/step
Epoch 62/200
5969/5969 - 16s - loss: 4.5091e-04 - val_loss: 3.9670e-04 - 16s/epoch - 3ms/step
Epoch 63/200
5969/5969 - 16s - loss: 4.5006e-04 - val_loss: 3.5514e-04 - 16s/epoch - 3ms/step
Epoch 64/200
5969/5969 - 16s - loss: 4.5752e-04 - val_loss: 3.7051e-04 - 16s/epoch - 3ms/step
Epoch 65/200
5969/5969 - 16s - loss: 4.4701e-04 - val_loss: 3.9615e-04 - 16s/epoch - 3ms/step
Epoch 66/200
5969/5969 - 16s - loss: 4.4693e-04 - val_loss: 3.6338e-04 - 16s/epoch - 3ms/step
Epoch 67/200
5969/5969 - 16s - loss: 4.5017e-04 - val_loss: 3.6501e-04 - 16s/epoch - 3ms/step
Epoch 68/200
5969/5969 - 16s - loss: 4.4508e-04 - val_loss: 3.6890e-04 - 16s/epoch - 3ms/step
Epoch 69/200
5969/5969 - 16s - loss: 4.4095e-04 - val_loss: 3.4488e-04 - 16s/epoch - 3ms/step
Epoch 70/200
5969/5969 - 16s - loss: 4.3477e-04 - val_loss: 3.6919e-04 - 16s/epoch - 3ms/step
Epoch 71/200
5969/5969 - 16s - loss: 4.4248e-04 - val_loss: 3.4114e-04 - 16s/epoch - 3ms/step
Epoch 72/200
5969/5969 - 16s - loss: 4.3685e-04 - val_loss: 3.5500e-04 - 16s/epoch - 3ms/step
Epoch 73/200
5969/5969 - 16s - loss: 4.3780e-04 - val_loss: 3.6259e-04 - 16s/epoch - 3ms/step
Epoch 74/200
5969/5969 - 16s - loss: 4.4796e-04 - val_loss: 3.5088e-04 - 16s/epoch - 3ms/step
Epoch 75/200
5969/5969 - 16s - loss: 4.3654e-04 - val_loss: 4.3628e-04 - 16s/epoch - 3ms/step
Epoch 76/200
5969/5969 - 16s - loss: 4.3606e-04 - val_loss: 3.3755e-04 - 16s/epoch - 3ms/step
Epoch 77/200
5969/5969 - 16s - loss: 4.3058e-04 - val_loss: 3.4814e-04 - 16s/epoch - 3ms/step
Epoch 78/200
5969/5969 - 16s - loss: 4.4810e-04 - val_loss: 3.6276e-04 - 16s/epoch - 3ms/step
Epoch 79/200
5969/5969 - 16s - loss: 4.3127e-04 - val_loss: 3.4193e-04 - 16s/epoch - 3ms/step
Epoch 80/200
5969/5969 - 16s - loss: 4.2825e-04 - val_loss: 3.1940e-04 - 16s/epoch - 3ms/step
Epoch 81/200
5969/5969 - 16s - loss: 4.2413e-04 - val_loss: 3.7896e-04 - 16s/epoch - 3ms/step
Epoch 82/200
5969/5969 - 16s - loss: 4.3040e-04 - val_loss: 3.2067e-04 - 16s/epoch - 3ms/step
Epoch 83/200
5969/5969 - 16s - loss: 4.2592e-04 - val_loss: 3.7334e-04 - 16s/epoch - 3ms/step
Epoch 84/200
5969/5969 - 16s - loss: 4.2921e-04 - val_loss: 3.7122e-04 - 16s/epoch - 3ms/step
Epoch 85/200
5969/5969 - 16s - loss: 4.1784e-04 - val_loss: 3.3896e-04 - 16s/epoch - 3ms/step
Epoch 86/200
5969/5969 - 16s - loss: 4.1968e-04 - val_loss: 3.5112e-04 - 16s/epoch - 3ms/step
Epoch 87/200
5969/5969 - 16s - loss: 4.1811e-04 - val_loss: 3.5086e-04 - 16s/epoch - 3ms/step
Epoch 88/200
5969/5969 - 16s - loss: 4.2255e-04 - val_loss: 3.1630e-04 - 16s/epoch - 3ms/step
Epoch 89/200
5969/5969 - 16s - loss: 4.2012e-04 - val_loss: 3.3182e-04 - 16s/epoch - 3ms/step
Epoch 90/200
5969/5969 - 16s - loss: 4.2424e-04 - val_loss: 3.2825e-04 - 16s/epoch - 3ms/step
Epoch 91/200
5969/5969 - 16s - loss: 4.1781e-04 - val_loss: 3.4936e-04 - 16s/epoch - 3ms/step
Epoch 92/200
5969/5969 - 16s - loss: 4.1594e-04 - val_loss: 3.4426e-04 - 16s/epoch - 3ms/step
Epoch 93/200
5969/5969 - 16s - loss: 4.1838e-04 - val_loss: 3.3943e-04 - 16s/epoch - 3ms/step
Epoch 94/200
5969/5969 - 16s - loss: 4.1439e-04 - val_loss: 3.1182e-04 - 16s/epoch - 3ms/step
Epoch 95/200
5969/5969 - 16s - loss: 4.0824e-04 - val_loss: 3.2070e-04 - 16s/epoch - 3ms/step
Epoch 96/200
5969/5969 - 16s - loss: 4.1377e-04 - val_loss: 3.3750e-04 - 16s/epoch - 3ms/step
Epoch 97/200
5969/5969 - 16s - loss: 4.1862e-04 - val_loss: 3.4990e-04 - 16s/epoch - 3ms/step
Epoch 98/200
5969/5969 - 16s - loss: 4.0652e-04 - val_loss: 3.4056e-04 - 16s/epoch - 3ms/step
Epoch 99/200
5969/5969 - 16s - loss: 4.0866e-04 - val_loss: 3.4145e-04 - 16s/epoch - 3ms/step
Epoch 100/200
5969/5969 - 16s - loss: 4.0581e-04 - val_loss: 3.2361e-04 - 16s/epoch - 3ms/step
Epoch 101/200
5969/5969 - 16s - loss: 3.9995e-04 - val_loss: 3.0863e-04 - 16s/epoch - 3ms/step
Epoch 102/200
5969/5969 - 16s - loss: 4.1584e-04 - val_loss: 4.2812e-04 - 16s/epoch - 3ms/step
Epoch 103/200
5969/5969 - 16s - loss: 4.1437e-04 - val_loss: 3.2981e-04 - 16s/epoch - 3ms/step
Epoch 104/200
5969/5969 - 16s - loss: 4.0606e-04 - val_loss: 3.4352e-04 - 16s/epoch - 3ms/step
Epoch 105/200
5969/5969 - 16s - loss: 4.2357e-04 - val_loss: 3.8876e-04 - 16s/epoch - 3ms/step
Epoch 106/200
5969/5969 - 16s - loss: 3.9907e-04 - val_loss: 3.4183e-04 - 16s/epoch - 3ms/step
Epoch 107/200
5969/5969 - 16s - loss: 4.0481e-04 - val_loss: 3.7709e-04 - 16s/epoch - 3ms/step
Epoch 108/200
5969/5969 - 16s - loss: 4.0426e-04 - val_loss: 3.3105e-04 - 16s/epoch - 3ms/step
Epoch 109/200
5969/5969 - 16s - loss: 3.9771e-04 - val_loss: 3.7791e-04 - 16s/epoch - 3ms/step
Epoch 110/200
5969/5969 - 16s - loss: 3.9708e-04 - val_loss: 3.6081e-04 - 16s/epoch - 3ms/step
Epoch 111/200
5969/5969 - 16s - loss: 3.9803e-04 - val_loss: 3.6658e-04 - 16s/epoch - 3ms/step
Epoch 112/200
5969/5969 - 16s - loss: 4.0496e-04 - val_loss: 3.2247e-04 - 16s/epoch - 3ms/step
Epoch 113/200
5969/5969 - 16s - loss: 3.9415e-04 - val_loss: 3.1131e-04 - 16s/epoch - 3ms/step
Epoch 114/200
5969/5969 - 16s - loss: 3.8858e-04 - val_loss: 5.5702e-04 - 16s/epoch - 3ms/step
Epoch 115/200
5969/5969 - 16s - loss: 3.8742e-04 - val_loss: 3.0420e-04 - 16s/epoch - 3ms/step
Epoch 116/200
5969/5969 - 16s - loss: 3.9714e-04 - val_loss: 3.1623e-04 - 16s/epoch - 3ms/step
Epoch 117/200
5969/5969 - 16s - loss: 3.9568e-04 - val_loss: 3.1950e-04 - 16s/epoch - 3ms/step
Epoch 118/200
5969/5969 - 16s - loss: 3.9066e-04 - val_loss: 3.4042e-04 - 16s/epoch - 3ms/step
Epoch 119/200
5969/5969 - 16s - loss: 4.2215e-04 - val_loss: 3.3875e-04 - 16s/epoch - 3ms/step
Epoch 120/200
5969/5969 - 16s - loss: 3.9546e-04 - val_loss: 3.6491e-04 - 16s/epoch - 3ms/step
Epoch 121/200
5969/5969 - 16s - loss: 3.9391e-04 - val_loss: 3.3813e-04 - 16s/epoch - 3ms/step
Epoch 122/200
5969/5969 - 16s - loss: 3.8668e-04 - val_loss: 2.9829e-04 - 16s/epoch - 3ms/step
Epoch 123/200
5969/5969 - 16s - loss: 3.8975e-04 - val_loss: 3.4366e-04 - 16s/epoch - 3ms/step
Epoch 124/200
5969/5969 - 16s - loss: 4.0248e-04 - val_loss: 3.4249e-04 - 16s/epoch - 3ms/step
Epoch 125/200
5969/5969 - 16s - loss: 3.8388e-04 - val_loss: 3.3604e-04 - 16s/epoch - 3ms/step
Epoch 126/200
5969/5969 - 16s - loss: 3.9968e-04 - val_loss: 3.7428e-04 - 16s/epoch - 3ms/step
Epoch 127/200
5969/5969 - 16s - loss: 3.9088e-04 - val_loss: 3.2534e-04 - 16s/epoch - 3ms/step
Epoch 128/200
5969/5969 - 16s - loss: 3.8177e-04 - val_loss: 3.2091e-04 - 16s/epoch - 3ms/step
Epoch 129/200
5969/5969 - 16s - loss: 3.8528e-04 - val_loss: 3.2470e-04 - 16s/epoch - 3ms/step
Epoch 130/200
5969/5969 - 16s - loss: 3.8411e-04 - val_loss: 3.6555e-04 - 16s/epoch - 3ms/step
Epoch 131/200
5969/5969 - 16s - loss: 3.8932e-04 - val_loss: 4.2239e-04 - 16s/epoch - 3ms/step
Epoch 132/200
5969/5969 - 16s - loss: 4.0315e-04 - val_loss: 3.7628e-04 - 16s/epoch - 3ms/step
Epoch 133/200
5969/5969 - 16s - loss: 3.7685e-04 - val_loss: 3.6835e-04 - 16s/epoch - 3ms/step
Epoch 134/200
5969/5969 - 16s - loss: 4.0310e-04 - val_loss: 3.3326e-04 - 16s/epoch - 3ms/step
Epoch 135/200
5969/5969 - 16s - loss: 3.8801e-04 - val_loss: 3.6586e-04 - 16s/epoch - 3ms/step
Epoch 136/200
5969/5969 - 16s - loss: 3.8396e-04 - val_loss: 3.4576e-04 - 16s/epoch - 3ms/step
Epoch 137/200
5969/5969 - 16s - loss: 3.9121e-04 - val_loss: 3.6779e-04 - 16s/epoch - 3ms/step
Epoch 138/200
5969/5969 - 16s - loss: 3.8227e-04 - val_loss: 3.5469e-04 - 16s/epoch - 3ms/step
Epoch 139/200
5969/5969 - 16s - loss: 3.8911e-04 - val_loss: 3.5300e-04 - 16s/epoch - 3ms/step
Epoch 140/200
5969/5969 - 16s - loss: 3.8171e-04 - val_loss: 3.9677e-04 - 16s/epoch - 3ms/step
Epoch 141/200
5969/5969 - 16s - loss: 3.7882e-04 - val_loss: 3.0895e-04 - 16s/epoch - 3ms/step
Epoch 142/200
5969/5969 - 16s - loss: 4.0329e-04 - val_loss: 4.0408e-04 - 16s/epoch - 3ms/step
Epoch 143/200
5969/5969 - 16s - loss: 3.8300e-04 - val_loss: 4.1374e-04 - 16s/epoch - 3ms/step
Epoch 144/200
5969/5969 - 16s - loss: 3.8041e-04 - val_loss: 3.5069e-04 - 16s/epoch - 3ms/step
Epoch 145/200
5969/5969 - 16s - loss: 3.7196e-04 - val_loss: 3.5011e-04 - 16s/epoch - 3ms/step
Epoch 146/200
5969/5969 - 16s - loss: 3.7253e-04 - val_loss: 3.8305e-04 - 16s/epoch - 3ms/step
Epoch 147/200
5969/5969 - 16s - loss: 3.7530e-04 - val_loss: 4.0024e-04 - 16s/epoch - 3ms/step
Epoch 148/200
5969/5969 - 16s - loss: 3.8960e-04 - val_loss: 4.5694e-04 - 16s/epoch - 3ms/step
Epoch 149/200
5969/5969 - 16s - loss: 3.7100e-04 - val_loss: 4.1707e-04 - 16s/epoch - 3ms/step
Epoch 150/200
5969/5969 - 16s - loss: 3.8783e-04 - val_loss: 3.9121e-04 - 16s/epoch - 3ms/step
Epoch 151/200
5969/5969 - 16s - loss: 3.8259e-04 - val_loss: 3.4136e-04 - 16s/epoch - 3ms/step
Epoch 152/200
5969/5969 - 16s - loss: 3.7555e-04 - val_loss: 3.6977e-04 - 16s/epoch - 3ms/step
Epoch 153/200
5969/5969 - 16s - loss: 3.7158e-04 - val_loss: 3.9968e-04 - 16s/epoch - 3ms/step
Epoch 154/200
5969/5969 - 16s - loss: 3.7912e-04 - val_loss: 4.4840e-04 - 16s/epoch - 3ms/step
Epoch 155/200
5969/5969 - 16s - loss: 3.6800e-04 - val_loss: 3.6853e-04 - 16s/epoch - 3ms/step
Epoch 156/200
5969/5969 - 16s - loss: 3.8583e-04 - val_loss: 8.8183e-04 - 16s/epoch - 3ms/step
Epoch 157/200
5969/5969 - 16s - loss: 3.7689e-04 - val_loss: 3.6738e-04 - 16s/epoch - 3ms/step
Epoch 158/200
5969/5969 - 16s - loss: 3.6616e-04 - val_loss: 4.7127e-04 - 16s/epoch - 3ms/step
Epoch 159/200
5969/5969 - 16s - loss: 3.7786e-04 - val_loss: 3.6064e-04 - 16s/epoch - 3ms/step
Epoch 160/200
5969/5969 - 16s - loss: 3.6737e-04 - val_loss: 4.8250e-04 - 16s/epoch - 3ms/step
Epoch 161/200
5969/5969 - 16s - loss: 3.7552e-04 - val_loss: 3.8798e-04 - 16s/epoch - 3ms/step
Epoch 162/200
5969/5969 - 16s - loss: 3.6152e-04 - val_loss: 4.6374e-04 - 16s/epoch - 3ms/step
Epoch 163/200
5969/5969 - 16s - loss: 3.6783e-04 - val_loss: 3.7080e-04 - 16s/epoch - 3ms/step
Epoch 164/200
5969/5969 - 16s - loss: 3.7051e-04 - val_loss: 4.6067e-04 - 16s/epoch - 3ms/step
Epoch 165/200
5969/5969 - 16s - loss: 3.6462e-04 - val_loss: 4.5407e-04 - 16s/epoch - 3ms/step
Epoch 166/200
5969/5969 - 16s - loss: 3.5775e-04 - val_loss: 4.3335e-04 - 16s/epoch - 3ms/step
Epoch 167/200
5969/5969 - 16s - loss: 3.5859e-04 - val_loss: 4.2712e-04 - 16s/epoch - 3ms/step
Epoch 168/200
5969/5969 - 16s - loss: 3.6052e-04 - val_loss: 3.9380e-04 - 16s/epoch - 3ms/step
Epoch 169/200
5969/5969 - 16s - loss: 3.7313e-04 - val_loss: 4.4753e-04 - 16s/epoch - 3ms/step
Epoch 170/200
5969/5969 - 16s - loss: 3.7555e-04 - val_loss: 4.3768e-04 - 16s/epoch - 3ms/step
Epoch 171/200
5969/5969 - 16s - loss: 3.6613e-04 - val_loss: 4.2480e-04 - 16s/epoch - 3ms/step
Epoch 172/200
5969/5969 - 16s - loss: 3.6201e-04 - val_loss: 5.6460e-04 - 16s/epoch - 3ms/step
Epoch 173/200
5969/5969 - 16s - loss: 3.7054e-04 - val_loss: 4.3033e-04 - 16s/epoch - 3ms/step
Epoch 174/200
5969/5969 - 16s - loss: 3.5961e-04 - val_loss: 4.7219e-04 - 16s/epoch - 3ms/step
Epoch 175/200
5969/5969 - 16s - loss: 3.6010e-04 - val_loss: 5.3131e-04 - 16s/epoch - 3ms/step
Epoch 176/200
5969/5969 - 16s - loss: 3.6009e-04 - val_loss: 4.7633e-04 - 16s/epoch - 3ms/step
Epoch 177/200
5969/5969 - 16s - loss: 3.6728e-04 - val_loss: 5.3432e-04 - 16s/epoch - 3ms/step
Epoch 178/200
5969/5969 - 16s - loss: 3.6707e-04 - val_loss: 4.0902e-04 - 16s/epoch - 3ms/step
Epoch 179/200
5969/5969 - 16s - loss: 3.6073e-04 - val_loss: 7.0913e-04 - 16s/epoch - 3ms/step
Epoch 180/200
5969/5969 - 16s - loss: 3.5851e-04 - val_loss: 4.7569e-04 - 16s/epoch - 3ms/step
Epoch 181/200
5969/5969 - 16s - loss: 3.5914e-04 - val_loss: 5.9541e-04 - 16s/epoch - 3ms/step
Epoch 182/200
5969/5969 - 16s - loss: 3.6244e-04 - val_loss: 5.9326e-04 - 16s/epoch - 3ms/step
Epoch 183/200
5969/5969 - 16s - loss: 3.5491e-04 - val_loss: 4.3139e-04 - 16s/epoch - 3ms/step
Epoch 184/200
5969/5969 - 16s - loss: 3.5869e-04 - val_loss: 7.2222e-04 - 16s/epoch - 3ms/step
Epoch 185/200
5969/5969 - 16s - loss: 3.5566e-04 - val_loss: 4.5265e-04 - 16s/epoch - 3ms/step
Epoch 186/200
5969/5969 - 16s - loss: 3.7255e-04 - val_loss: 5.1495e-04 - 16s/epoch - 3ms/step
Epoch 187/200
5969/5969 - 16s - loss: 3.5958e-04 - val_loss: 7.3289e-04 - 16s/epoch - 3ms/step
Epoch 188/200
5969/5969 - 16s - loss: 3.5371e-04 - val_loss: 7.3049e-04 - 16s/epoch - 3ms/step
Epoch 189/200
5969/5969 - 16s - loss: 3.5304e-04 - val_loss: 4.3505e-04 - 16s/epoch - 3ms/step
Epoch 190/200
5969/5969 - 16s - loss: 3.5332e-04 - val_loss: 5.5851e-04 - 16s/epoch - 3ms/step
Epoch 191/200
5969/5969 - 16s - loss: 3.6101e-04 - val_loss: 5.2591e-04 - 16s/epoch - 3ms/step
Epoch 192/200
5969/5969 - 16s - loss: 3.5482e-04 - val_loss: 4.3032e-04 - 16s/epoch - 3ms/step
Epoch 193/200
5969/5969 - 16s - loss: 3.5114e-04 - val_loss: 7.0975e-04 - 16s/epoch - 3ms/step
Epoch 194/200
5969/5969 - 16s - loss: 3.5433e-04 - val_loss: 7.1330e-04 - 16s/epoch - 3ms/step
Epoch 195/200
5969/5969 - 16s - loss: 3.5018e-04 - val_loss: 5.6787e-04 - 16s/epoch - 3ms/step
Epoch 196/200
5969/5969 - 16s - loss: 3.5212e-04 - val_loss: 8.0050e-04 - 16s/epoch - 3ms/step
Epoch 197/200
5969/5969 - 16s - loss: 3.6182e-04 - val_loss: 7.8652e-04 - 16s/epoch - 3ms/step
Epoch 198/200
5969/5969 - 16s - loss: 3.5369e-04 - val_loss: 6.2325e-04 - 16s/epoch - 3ms/step
Epoch 199/200
5969/5969 - 16s - loss: 3.5804e-04 - val_loss: 5.4436e-04 - 16s/epoch - 3ms/step
Epoch 200/200
5969/5969 - 16s - loss: 3.5905e-04 - val_loss: 7.9833e-04 - 16s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0007983346586115658
  1/332 [..............................] - ETA: 32s 52/332 [===>..........................] - ETA: 0s 103/332 [========>.....................] - ETA: 0s155/332 [=============>................] - ETA: 0s207/332 [=================>............] - ETA: 0s258/332 [======================>.......] - ETA: 0s309/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 989us/step
correlation 0.006720011082856645
cosine 0.0059297294255881055
MAE: 0.009985226
RMSE: 0.028254764
r2: 0.9482110935062072
RMSE zero-vector: 0.23411466903540806
['default', 16, 200, 0.001, 'mse', 0.5, 632, 0.00035904994001612067, 0.0007983346586115658, 0.006720011082856645, 0.0059297294255881055, 0.009985226206481457, 0.028254764154553413, 0.9482110935062072, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       multiple                  0         
                                                                 
 dense_21 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_21 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_22 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 632)               0         
                                                                 
 dense_22 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_23 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2528)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_22"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_23 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_22 (InputLayer)       multiple                  0         
                                                                 
 dense_21 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_21 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_23"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_24 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_22 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 632)               0         
                                                                 
 dense_22 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_23 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2528)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_24"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_25 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_24 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_24 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_25 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 632)               0         
                                                                 
 dense_25 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_26 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2528)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
5969/5969 - 17s - loss: 0.0159 - val_loss: 0.0030 - 17s/epoch - 3ms/step
Epoch 2/200
5969/5969 - 16s - loss: 0.0032 - val_loss: 0.0025 - 16s/epoch - 3ms/step
Epoch 3/200
5969/5969 - 16s - loss: 0.0025 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 4/200
5969/5969 - 16s - loss: 0.0022 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 5/200
5969/5969 - 16s - loss: 0.0020 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 6/200
5969/5969 - 16s - loss: 0.0019 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 7/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 8/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 9/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 10/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 11/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 12/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 13/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0014 - 16s/epoch - 3ms/step
Epoch 14/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0014 - 16s/epoch - 3ms/step
Epoch 15/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 16/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 17/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0014 - 16s/epoch - 3ms/step
Epoch 18/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0014 - 16s/epoch - 3ms/step
Epoch 19/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 20/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0014 - 16s/epoch - 3ms/step
Epoch 21/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0014 - 16s/epoch - 3ms/step
Epoch 22/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0014 - 16s/epoch - 3ms/step
Epoch 23/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0014 - 16s/epoch - 3ms/step
Epoch 24/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 25/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 26/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 27/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 28/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 29/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 30/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0014 - 16s/epoch - 3ms/step
Epoch 31/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0014 - 16s/epoch - 3ms/step
Epoch 32/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 33/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 34/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0014 - 16s/epoch - 3ms/step
Epoch 35/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 36/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 37/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 38/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 39/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 40/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 41/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 42/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 43/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 44/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 45/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 46/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 47/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 48/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 49/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 50/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0024 - 16s/epoch - 3ms/step
Epoch 51/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 52/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 53/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 54/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 55/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0015 - 16s/epoch - 3ms/step
Epoch 56/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 57/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 58/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 59/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0016 - 16s/epoch - 3ms/step
Epoch 60/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 61/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 62/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0022 - 16s/epoch - 3ms/step
Epoch 63/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0022 - 16s/epoch - 3ms/step
Epoch 64/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0017 - 16s/epoch - 3ms/step
Epoch 65/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0024 - 16s/epoch - 3ms/step
Epoch 66/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 67/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 68/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0022 - 16s/epoch - 3ms/step
Epoch 69/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 70/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0021 - 16s/epoch - 3ms/step
Epoch 71/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 72/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0021 - 16s/epoch - 3ms/step
Epoch 73/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0021 - 16s/epoch - 3ms/step
Epoch 74/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 75/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 76/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0026 - 16s/epoch - 3ms/step
Epoch 77/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 78/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0022 - 16s/epoch - 3ms/step
Epoch 79/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 80/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 81/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0022 - 16s/epoch - 3ms/step
Epoch 82/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 83/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0023 - 16s/epoch - 3ms/step
Epoch 84/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0025 - 16s/epoch - 3ms/step
Epoch 85/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0023 - 16s/epoch - 3ms/step
Epoch 86/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0023 - 16s/epoch - 3ms/step
Epoch 87/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0030 - 16s/epoch - 3ms/step
Epoch 88/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0019 - 16s/epoch - 3ms/step
Epoch 89/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0021 - 16s/epoch - 3ms/step
Epoch 90/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 91/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0018 - 16s/epoch - 3ms/step
Epoch 92/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0029 - 16s/epoch - 3ms/step
Epoch 93/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0024 - 16s/epoch - 3ms/step
Epoch 94/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0022 - 16s/epoch - 3ms/step
Epoch 95/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0027 - 16s/epoch - 3ms/step
Epoch 96/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0028 - 16s/epoch - 3ms/step
Epoch 97/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0021 - 16s/epoch - 3ms/step
Epoch 98/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0025 - 16s/epoch - 3ms/step
Epoch 99/200
5969/5969 - 16s - loss: 0.0016 - val_loss: 0.0022 - 16s/epoch - 3ms/step
Epoch 100/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0020 - 16s/epoch - 3ms/step
Epoch 101/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0023 - 16s/epoch - 3ms/step
Epoch 102/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0028 - 16s/epoch - 3ms/step
Epoch 103/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0025 - 16s/epoch - 3ms/step
Epoch 104/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0032 - 16s/epoch - 3ms/step
Epoch 105/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0027 - 16s/epoch - 3ms/step
Epoch 106/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0030 - 16s/epoch - 3ms/step
Epoch 107/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0041 - 16s/epoch - 3ms/step
Epoch 108/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0027 - 16s/epoch - 3ms/step
Epoch 109/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0026 - 16s/epoch - 3ms/step
Epoch 110/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0035 - 16s/epoch - 3ms/step
Epoch 111/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0036 - 16s/epoch - 3ms/step
Epoch 112/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0029 - 16s/epoch - 3ms/step
Epoch 113/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0032 - 16s/epoch - 3ms/step
Epoch 114/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0038 - 16s/epoch - 3ms/step
Epoch 115/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0030 - 16s/epoch - 3ms/step
Epoch 116/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0030 - 16s/epoch - 3ms/step
Epoch 117/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0031 - 16s/epoch - 3ms/step
Epoch 118/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0037 - 16s/epoch - 3ms/step
Epoch 119/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0036 - 16s/epoch - 3ms/step
Epoch 120/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0035 - 16s/epoch - 3ms/step
Epoch 121/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0031 - 16s/epoch - 3ms/step
Epoch 122/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0031 - 16s/epoch - 3ms/step
Epoch 123/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0036 - 16s/epoch - 3ms/step
Epoch 124/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0036 - 16s/epoch - 3ms/step
Epoch 125/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0033 - 16s/epoch - 3ms/step
Epoch 126/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0051 - 16s/epoch - 3ms/step
Epoch 127/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0041 - 16s/epoch - 3ms/step
Epoch 128/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0041 - 16s/epoch - 3ms/step
Epoch 129/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0032 - 16s/epoch - 3ms/step
Epoch 130/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0041 - 16s/epoch - 3ms/step
Epoch 131/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0039 - 16s/epoch - 3ms/step
Epoch 132/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0044 - 16s/epoch - 3ms/step
Epoch 133/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0036 - 16s/epoch - 3ms/step
Epoch 134/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0035 - 16s/epoch - 3ms/step
Epoch 135/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0037 - 16s/epoch - 3ms/step
Epoch 136/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0035 - 16s/epoch - 3ms/step
Epoch 137/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0040 - 16s/epoch - 3ms/step
Epoch 138/200
5969/5969 - 16s - loss: 0.0017 - val_loss: 0.0041 - 16s/epoch - 3ms/step
Epoch 139/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0039 - 16s/epoch - 3ms/step
Epoch 140/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0039 - 16s/epoch - 3ms/step
Epoch 141/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0030 - 16s/epoch - 3ms/step
Epoch 142/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0035 - 16s/epoch - 3ms/step
Epoch 143/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0042 - 16s/epoch - 3ms/step
Epoch 144/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0037 - 16s/epoch - 3ms/step
Epoch 145/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0037 - 16s/epoch - 3ms/step
Epoch 146/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0040 - 16s/epoch - 3ms/step
Epoch 147/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0038 - 16s/epoch - 3ms/step
Epoch 148/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0043 - 16s/epoch - 3ms/step
Epoch 149/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0041 - 16s/epoch - 3ms/step
Epoch 150/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0045 - 16s/epoch - 3ms/step
Epoch 151/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0035 - 16s/epoch - 3ms/step
Epoch 152/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0032 - 16s/epoch - 3ms/step
Epoch 153/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0033 - 16s/epoch - 3ms/step
Epoch 154/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0037 - 16s/epoch - 3ms/step
Epoch 155/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0032 - 16s/epoch - 3ms/step
Epoch 156/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0037 - 16s/epoch - 3ms/step
Epoch 157/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0036 - 16s/epoch - 3ms/step
Epoch 158/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0034 - 16s/epoch - 3ms/step
Epoch 159/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0031 - 16s/epoch - 3ms/step
Epoch 160/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0038 - 16s/epoch - 3ms/step
Epoch 161/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0032 - 16s/epoch - 3ms/step
Epoch 162/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0041 - 16s/epoch - 3ms/step
Epoch 163/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0038 - 16s/epoch - 3ms/step
Epoch 164/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0038 - 16s/epoch - 3ms/step
Epoch 165/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0039 - 16s/epoch - 3ms/step
Epoch 166/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0040 - 16s/epoch - 3ms/step
Epoch 167/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0036 - 16s/epoch - 3ms/step
Epoch 168/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0032 - 16s/epoch - 3ms/step
Epoch 169/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0034 - 16s/epoch - 3ms/step
Epoch 170/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0036 - 16s/epoch - 3ms/step
Epoch 171/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0048 - 16s/epoch - 3ms/step
Epoch 172/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0050 - 16s/epoch - 3ms/step
Epoch 173/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0030 - 16s/epoch - 3ms/step
Epoch 174/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0035 - 16s/epoch - 3ms/step
Epoch 175/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0039 - 16s/epoch - 3ms/step
Epoch 176/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0036 - 16s/epoch - 3ms/step
Epoch 177/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0039 - 16s/epoch - 3ms/step
Epoch 178/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0050 - 16s/epoch - 3ms/step
Epoch 179/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0042 - 16s/epoch - 3ms/step
Epoch 180/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0035 - 16s/epoch - 3ms/step
Epoch 181/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0039 - 16s/epoch - 3ms/step
Epoch 182/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0044 - 16s/epoch - 3ms/step
Epoch 183/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0031 - 16s/epoch - 3ms/step
Epoch 184/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0042 - 16s/epoch - 3ms/step
Epoch 185/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0034 - 16s/epoch - 3ms/step
Epoch 186/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0037 - 16s/epoch - 3ms/step
Epoch 187/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0050 - 16s/epoch - 3ms/step
Epoch 188/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0048 - 16s/epoch - 3ms/step
Epoch 189/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0030 - 16s/epoch - 3ms/step
Epoch 190/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0042 - 16s/epoch - 3ms/step
Epoch 191/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0044 - 16s/epoch - 3ms/step
Epoch 192/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0029 - 16s/epoch - 3ms/step
Epoch 193/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0048 - 16s/epoch - 3ms/step
Epoch 194/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0050 - 16s/epoch - 3ms/step
Epoch 195/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0038 - 16s/epoch - 3ms/step
Epoch 196/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0049 - 16s/epoch - 3ms/step
Epoch 197/200
5969/5969 - 16s - loss: 0.0019 - val_loss: 0.0044 - 16s/epoch - 3ms/step
Epoch 198/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0041 - 16s/epoch - 3ms/step
Epoch 199/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0044 - 16s/epoch - 3ms/step
Epoch 200/200
5969/5969 - 16s - loss: 0.0018 - val_loss: 0.0060 - 16s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.006017499603331089
  1/332 [..............................] - ETA: 33s 51/332 [===>..........................] - ETA: 0s 102/332 [========>.....................] - ETA: 0s154/332 [============>.................] - ETA: 0s205/332 [=================>............] - ETA: 0s256/332 [======================>.......] - ETA: 0s307/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 990us/step
correlation 0.03334523307189733
cosine 0.027902545460071197
MAE: 0.028952237
RMSE: 0.077572405
r2: 0.6096318232785809
RMSE zero-vector: 0.23411466903540806
['default', 16, 200, 0.005, 'mse', 0.5, 632, 0.0018180080223828554, 0.006017499603331089, 0.03334523307189733, 0.027902545460071197, 0.028952237218618393, 0.07757240533828735, 0.6096318232785809, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_24"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_25 (InputLayer)       multiple                  0         
                                                                 
 dense_24 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_24 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_25 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 632)               0         
                                                                 
 dense_25 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_26 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2528)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_25"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_26 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_25 (InputLayer)       multiple                  0         
                                                                 
 dense_24 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_24 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_26"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_27 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_25 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 632)               0         
                                                                 
 dense_25 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_26 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2528)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_27"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_27 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_27 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_27 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_28 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_28 (ReLU)             (None, 632)               0         
                                                                 
 dense_28 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_29 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_29 (ReLU)             (None, 2528)              0         
                                                                 
 dense_29 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/50
2985/2985 - 9s - loss: 0.0081 - val_loss: 0.0032 - 9s/epoch - 3ms/step
Epoch 2/50
2985/2985 - 8s - loss: 0.0030 - val_loss: 0.0022 - 8s/epoch - 3ms/step
Epoch 3/50
2985/2985 - 8s - loss: 0.0026 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 4/50
2985/2985 - 8s - loss: 0.0019 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 5/50
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0010 - 8s/epoch - 3ms/step
Epoch 6/50
2985/2985 - 8s - loss: 0.0012 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 7/50
2985/2985 - 8s - loss: 0.0011 - val_loss: 7.5278e-04 - 8s/epoch - 3ms/step
Epoch 8/50
2985/2985 - 8s - loss: 8.7645e-04 - val_loss: 7.5066e-04 - 8s/epoch - 3ms/step
Epoch 9/50
2985/2985 - 8s - loss: 7.7221e-04 - val_loss: 5.7514e-04 - 8s/epoch - 3ms/step
Epoch 10/50
2985/2985 - 8s - loss: 6.9015e-04 - val_loss: 5.5350e-04 - 8s/epoch - 3ms/step
Epoch 11/50
2985/2985 - 8s - loss: 6.3361e-04 - val_loss: 5.3559e-04 - 8s/epoch - 3ms/step
Epoch 12/50
2985/2985 - 8s - loss: 5.7636e-04 - val_loss: 5.8260e-04 - 8s/epoch - 3ms/step
Epoch 13/50
2985/2985 - 8s - loss: 5.3992e-04 - val_loss: 4.5022e-04 - 8s/epoch - 3ms/step
Epoch 14/50
2985/2985 - 8s - loss: 5.0838e-04 - val_loss: 4.1814e-04 - 8s/epoch - 3ms/step
Epoch 15/50
2985/2985 - 8s - loss: 4.9325e-04 - val_loss: 3.7861e-04 - 8s/epoch - 3ms/step
Epoch 16/50
2985/2985 - 8s - loss: 4.5244e-04 - val_loss: 4.1389e-04 - 8s/epoch - 3ms/step
Epoch 17/50
2985/2985 - 8s - loss: 4.3412e-04 - val_loss: 3.9436e-04 - 8s/epoch - 3ms/step
Epoch 18/50
2985/2985 - 8s - loss: 4.1470e-04 - val_loss: 3.9591e-04 - 8s/epoch - 3ms/step
Epoch 19/50
2985/2985 - 8s - loss: 3.9947e-04 - val_loss: 3.6869e-04 - 8s/epoch - 3ms/step
Epoch 20/50
2985/2985 - 8s - loss: 4.2003e-04 - val_loss: 3.1104e-04 - 8s/epoch - 3ms/step
Epoch 21/50
2985/2985 - 8s - loss: 3.8207e-04 - val_loss: 3.1602e-04 - 8s/epoch - 3ms/step
Epoch 22/50
2985/2985 - 8s - loss: 3.6701e-04 - val_loss: 3.3163e-04 - 8s/epoch - 3ms/step
Epoch 23/50
2985/2985 - 8s - loss: 3.6688e-04 - val_loss: 3.5435e-04 - 8s/epoch - 3ms/step
Epoch 24/50
2985/2985 - 8s - loss: 3.4568e-04 - val_loss: 3.0664e-04 - 8s/epoch - 3ms/step
Epoch 25/50
2985/2985 - 8s - loss: 3.4036e-04 - val_loss: 3.0149e-04 - 8s/epoch - 3ms/step
Epoch 26/50
2985/2985 - 8s - loss: 3.9243e-04 - val_loss: 2.6393e-04 - 8s/epoch - 3ms/step
Epoch 27/50
2985/2985 - 8s - loss: 3.3358e-04 - val_loss: 2.7106e-04 - 8s/epoch - 3ms/step
Epoch 28/50
2985/2985 - 8s - loss: 3.2030e-04 - val_loss: 2.5917e-04 - 8s/epoch - 3ms/step
Epoch 29/50
2985/2985 - 8s - loss: 3.0902e-04 - val_loss: 2.5798e-04 - 8s/epoch - 3ms/step
Epoch 30/50
2985/2985 - 8s - loss: 3.0315e-04 - val_loss: 2.6217e-04 - 8s/epoch - 3ms/step
Epoch 31/50
2985/2985 - 8s - loss: 2.9504e-04 - val_loss: 2.7128e-04 - 8s/epoch - 3ms/step
Epoch 32/50
2985/2985 - 8s - loss: 2.9594e-04 - val_loss: 2.3687e-04 - 8s/epoch - 3ms/step
Epoch 33/50
2985/2985 - 8s - loss: 2.9583e-04 - val_loss: 2.3851e-04 - 8s/epoch - 3ms/step
Epoch 34/50
2985/2985 - 8s - loss: 2.8371e-04 - val_loss: 2.3873e-04 - 8s/epoch - 3ms/step
Epoch 35/50
2985/2985 - 8s - loss: 2.8625e-04 - val_loss: 2.4164e-04 - 8s/epoch - 3ms/step
Epoch 36/50
2985/2985 - 8s - loss: 2.7333e-04 - val_loss: 2.3777e-04 - 8s/epoch - 3ms/step
Epoch 37/50
2985/2985 - 8s - loss: 2.7297e-04 - val_loss: 2.3879e-04 - 8s/epoch - 3ms/step
Epoch 38/50
2985/2985 - 8s - loss: 2.6928e-04 - val_loss: 2.2376e-04 - 8s/epoch - 3ms/step
Epoch 39/50
2985/2985 - 8s - loss: 2.6622e-04 - val_loss: 2.2392e-04 - 8s/epoch - 3ms/step
Epoch 40/50
2985/2985 - 8s - loss: 2.6824e-04 - val_loss: 3.3171e-04 - 8s/epoch - 3ms/step
Epoch 41/50
2985/2985 - 8s - loss: 2.6989e-04 - val_loss: 2.0472e-04 - 8s/epoch - 3ms/step
Epoch 42/50
2985/2985 - 8s - loss: 2.5267e-04 - val_loss: 2.1515e-04 - 8s/epoch - 3ms/step
Epoch 43/50
2985/2985 - 8s - loss: 2.4867e-04 - val_loss: 2.1994e-04 - 8s/epoch - 3ms/step
Epoch 44/50
2985/2985 - 8s - loss: 2.4858e-04 - val_loss: 2.2674e-04 - 8s/epoch - 3ms/step
Epoch 45/50
2985/2985 - 8s - loss: 2.4471e-04 - val_loss: 1.9822e-04 - 8s/epoch - 3ms/step
Epoch 46/50
2985/2985 - 8s - loss: 2.4293e-04 - val_loss: 2.0848e-04 - 8s/epoch - 3ms/step
Epoch 47/50
2985/2985 - 8s - loss: 2.4054e-04 - val_loss: 2.0213e-04 - 8s/epoch - 3ms/step
Epoch 48/50
2985/2985 - 8s - loss: 2.3959e-04 - val_loss: 1.9656e-04 - 8s/epoch - 3ms/step
Epoch 49/50
2985/2985 - 8s - loss: 2.3529e-04 - val_loss: 2.3119e-04 - 8s/epoch - 3ms/step
Epoch 50/50
2985/2985 - 8s - loss: 2.4263e-04 - val_loss: 4.3818e-04 - 8s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00043818019912578166
  1/332 [..............................] - ETA: 31s 51/332 [===>..........................] - ETA: 0s 102/332 [========>.....................] - ETA: 0s153/332 [============>.................] - ETA: 0s204/332 [=================>............] - ETA: 0s255/332 [======================>.......] - ETA: 0s306/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1000us/step
correlation 0.004277476759348732
cosine 0.0034797983128216597
MAE: 0.014428156
RMSE: 0.020932753
r2: 0.971580617808978
RMSE zero-vector: 0.23411466903540806
['default', 32, 50, 0.0005, 'mse', 0.5, 632, 0.00024262721126433462, 0.00043818019912578166, 0.004277476759348732, 0.0034797983128216597, 0.014428156428039074, 0.020932752639055252, 0.971580617808978, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_27"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       multiple                  0         
                                                                 
 dense_27 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_27 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_27 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_28 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_28 (ReLU)             (None, 632)               0         
                                                                 
 dense_28 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_29 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_29 (ReLU)             (None, 2528)              0         
                                                                 
 dense_29 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_28"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_29 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_28 (InputLayer)       multiple                  0         
                                                                 
 dense_27 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_27 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_27 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_29"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_30 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_28 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_28 (ReLU)             (None, 632)               0         
                                                                 
 dense_28 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_29 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_29 (ReLU)             (None, 2528)              0         
                                                                 
 dense_29 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_30"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_31 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_30 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_30 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_30 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_31 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_31 (ReLU)             (None, 632)               0         
                                                                 
 dense_31 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_32 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_32 (ReLU)             (None, 2528)              0         
                                                                 
 dense_32 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/50
2985/2985 - 9s - loss: 0.0076 - val_loss: 0.0026 - 9s/epoch - 3ms/step
Epoch 2/50
2985/2985 - 8s - loss: 0.0030 - val_loss: 0.0020 - 8s/epoch - 3ms/step
Epoch 3/50
2985/2985 - 8s - loss: 0.0020 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 4/50
2985/2985 - 8s - loss: 0.0014 - val_loss: 9.9463e-04 - 8s/epoch - 3ms/step
Epoch 5/50
2985/2985 - 8s - loss: 0.0011 - val_loss: 9.0471e-04 - 8s/epoch - 3ms/step
Epoch 6/50
2985/2985 - 8s - loss: 8.9001e-04 - val_loss: 7.0365e-04 - 8s/epoch - 3ms/step
Epoch 7/50
2985/2985 - 8s - loss: 7.7768e-04 - val_loss: 6.1326e-04 - 8s/epoch - 3ms/step
Epoch 8/50
2985/2985 - 8s - loss: 6.8303e-04 - val_loss: 5.8108e-04 - 8s/epoch - 3ms/step
Epoch 9/50
2985/2985 - 8s - loss: 6.2642e-04 - val_loss: 5.0040e-04 - 8s/epoch - 3ms/step
Epoch 10/50
2985/2985 - 8s - loss: 5.9346e-04 - val_loss: 4.6381e-04 - 8s/epoch - 3ms/step
Epoch 11/50
2985/2985 - 8s - loss: 5.5547e-04 - val_loss: 4.5662e-04 - 8s/epoch - 3ms/step
Epoch 12/50
2985/2985 - 8s - loss: 5.2071e-04 - val_loss: 7.3383e-04 - 8s/epoch - 3ms/step
Epoch 13/50
2985/2985 - 8s - loss: 5.4677e-04 - val_loss: 4.4178e-04 - 8s/epoch - 3ms/step
Epoch 14/50
2985/2985 - 8s - loss: 4.7579e-04 - val_loss: 3.9879e-04 - 8s/epoch - 3ms/step
Epoch 15/50
2985/2985 - 8s - loss: 4.5690e-04 - val_loss: 3.7018e-04 - 8s/epoch - 3ms/step
Epoch 16/50
2985/2985 - 8s - loss: 4.3251e-04 - val_loss: 3.7143e-04 - 8s/epoch - 3ms/step
Epoch 17/50
2985/2985 - 8s - loss: 4.4765e-04 - val_loss: 4.3502e-04 - 8s/epoch - 3ms/step
Epoch 18/50
2985/2985 - 8s - loss: 4.1883e-04 - val_loss: 3.4404e-04 - 8s/epoch - 3ms/step
Epoch 19/50
2985/2985 - 8s - loss: 3.9509e-04 - val_loss: 3.4229e-04 - 8s/epoch - 3ms/step
Epoch 20/50
2985/2985 - 8s - loss: 3.9464e-04 - val_loss: 3.2309e-04 - 8s/epoch - 3ms/step
Epoch 21/50
2985/2985 - 8s - loss: 3.8350e-04 - val_loss: 3.0465e-04 - 8s/epoch - 3ms/step
Epoch 22/50
2985/2985 - 8s - loss: 3.7443e-04 - val_loss: 3.3403e-04 - 8s/epoch - 3ms/step
Epoch 23/50
2985/2985 - 8s - loss: 3.7223e-04 - val_loss: 2.9486e-04 - 8s/epoch - 3ms/step
Epoch 24/50
2985/2985 - 8s - loss: 3.5605e-04 - val_loss: 3.0987e-04 - 8s/epoch - 3ms/step
Epoch 25/50
2985/2985 - 8s - loss: 3.5038e-04 - val_loss: 2.7509e-04 - 8s/epoch - 3ms/step
Epoch 26/50
2985/2985 - 8s - loss: 3.4097e-04 - val_loss: 2.7136e-04 - 8s/epoch - 3ms/step
Epoch 27/50
2985/2985 - 8s - loss: 3.5330e-04 - val_loss: 2.6559e-04 - 8s/epoch - 3ms/step
Epoch 28/50
2985/2985 - 8s - loss: 3.3005e-04 - val_loss: 2.6151e-04 - 8s/epoch - 3ms/step
Epoch 29/50
2985/2985 - 8s - loss: 3.3989e-04 - val_loss: 2.8177e-04 - 8s/epoch - 3ms/step
Epoch 30/50
2985/2985 - 8s - loss: 3.2644e-04 - val_loss: 2.7021e-04 - 8s/epoch - 3ms/step
Epoch 31/50
2985/2985 - 8s - loss: 3.1708e-04 - val_loss: 2.5926e-04 - 8s/epoch - 3ms/step
Epoch 32/50
2985/2985 - 8s - loss: 3.1932e-04 - val_loss: 2.8313e-04 - 8s/epoch - 3ms/step
Epoch 33/50
2985/2985 - 8s - loss: 3.1388e-04 - val_loss: 2.5130e-04 - 8s/epoch - 3ms/step
Epoch 34/50
2985/2985 - 8s - loss: 3.0382e-04 - val_loss: 2.3680e-04 - 8s/epoch - 3ms/step
Epoch 35/50
2985/2985 - 8s - loss: 2.9660e-04 - val_loss: 2.4406e-04 - 8s/epoch - 3ms/step
Epoch 36/50
2985/2985 - 8s - loss: 2.9454e-04 - val_loss: 2.3237e-04 - 8s/epoch - 3ms/step
Epoch 37/50
2985/2985 - 8s - loss: 3.6145e-04 - val_loss: 2.5171e-04 - 8s/epoch - 3ms/step
Epoch 38/50
2985/2985 - 8s - loss: 3.0113e-04 - val_loss: 2.3112e-04 - 8s/epoch - 3ms/step
Epoch 39/50
2985/2985 - 8s - loss: 2.9139e-04 - val_loss: 2.2541e-04 - 8s/epoch - 3ms/step
Epoch 40/50
2985/2985 - 8s - loss: 2.9879e-04 - val_loss: 2.3229e-04 - 8s/epoch - 3ms/step
Epoch 41/50
2985/2985 - 8s - loss: 2.8077e-04 - val_loss: 2.2156e-04 - 8s/epoch - 3ms/step
Epoch 42/50
2985/2985 - 8s - loss: 2.7832e-04 - val_loss: 2.4903e-04 - 8s/epoch - 3ms/step
Epoch 43/50
2985/2985 - 8s - loss: 2.9016e-04 - val_loss: 2.4614e-04 - 8s/epoch - 3ms/step
Epoch 44/50
2985/2985 - 8s - loss: 2.7778e-04 - val_loss: 2.6080e-04 - 8s/epoch - 3ms/step
Epoch 45/50
2985/2985 - 8s - loss: 2.7647e-04 - val_loss: 2.1243e-04 - 8s/epoch - 3ms/step
Epoch 46/50
2985/2985 - 8s - loss: 2.7024e-04 - val_loss: 2.1935e-04 - 8s/epoch - 3ms/step
Epoch 47/50
2985/2985 - 8s - loss: 2.7003e-04 - val_loss: 2.2189e-04 - 8s/epoch - 3ms/step
Epoch 48/50
2985/2985 - 8s - loss: 2.9305e-04 - val_loss: 2.0856e-04 - 8s/epoch - 3ms/step
Epoch 49/50
2985/2985 - 8s - loss: 2.6554e-04 - val_loss: 2.1016e-04 - 8s/epoch - 3ms/step
Epoch 50/50
2985/2985 - 8s - loss: 2.6297e-04 - val_loss: 3.4114e-04 - 8s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0003411401994526386
  1/332 [..............................] - ETA: 28s 51/332 [===>..........................] - ETA: 0s 102/332 [========>.....................] - ETA: 0s153/332 [============>.................] - ETA: 0s204/332 [=================>............] - ETA: 0s256/332 [======================>.......] - ETA: 0s308/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 989us/step
correlation 0.00382095390542516
cosine 0.003014069287856929
MAE: 0.01204304
RMSE: 0.018469976
r2: 0.9778711385358648
RMSE zero-vector: 0.23411466903540806
['default', 32, 50, 0.001, 'mse', 0.5, 632, 0.0002629706577863544, 0.0003411401994526386, 0.00382095390542516, 0.003014069287856929, 0.012043040245771408, 0.018469976261258125, 0.9778711385358648, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_30"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_31 (InputLayer)       multiple                  0         
                                                                 
 dense_30 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_30 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_30 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_31 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_31 (ReLU)             (None, 632)               0         
                                                                 
 dense_31 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_32 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_32 (ReLU)             (None, 2528)              0         
                                                                 
 dense_32 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_31"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_32 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_31 (InputLayer)       multiple                  0         
                                                                 
 dense_30 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_30 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_30 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_31 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_31 (ReLU)             (None, 632)               0         
                                                                 
 dense_31 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_32 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_32 (ReLU)             (None, 2528)              0         
                                                                 
 dense_32 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_33"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_34 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_33 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_33 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_33 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_34 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_34 (ReLU)             (None, 632)               0         
                                                                 
 dense_34 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_35 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_35 (ReLU)             (None, 2528)              0         
                                                                 
 dense_35 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/50
2985/2985 - 9s - loss: 0.0255 - val_loss: 0.0034 - 9s/epoch - 3ms/step
Epoch 2/50
2985/2985 - 8s - loss: 0.0032 - val_loss: 0.0020 - 8s/epoch - 3ms/step
Epoch 3/50
2985/2985 - 8s - loss: 0.0023 - val_loss: 0.0018 - 8s/epoch - 3ms/step
Epoch 4/50
2985/2985 - 8s - loss: 0.0017 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 5/50
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0016 - 8s/epoch - 3ms/step
Epoch 6/50
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0011 - 8s/epoch - 3ms/step
Epoch 7/50
2985/2985 - 8s - loss: 0.0012 - val_loss: 9.9455e-04 - 8s/epoch - 3ms/step
Epoch 8/50
2985/2985 - 8s - loss: 0.0011 - val_loss: 8.9268e-04 - 8s/epoch - 3ms/step
Epoch 9/50
2985/2985 - 8s - loss: 0.0010 - val_loss: 8.9926e-04 - 8s/epoch - 3ms/step
Epoch 10/50
2985/2985 - 8s - loss: 0.0010 - val_loss: 8.1638e-04 - 8s/epoch - 3ms/step
Epoch 11/50
2985/2985 - 8s - loss: 0.0010 - val_loss: 8.3949e-04 - 8s/epoch - 3ms/step
Epoch 12/50
2985/2985 - 8s - loss: 9.9964e-04 - val_loss: 8.1701e-04 - 8s/epoch - 3ms/step
Epoch 13/50
2985/2985 - 8s - loss: 9.5925e-04 - val_loss: 0.0011 - 8s/epoch - 3ms/step
Epoch 14/50
2985/2985 - 8s - loss: 9.7699e-04 - val_loss: 7.5207e-04 - 8s/epoch - 3ms/step
Epoch 15/50
2985/2985 - 8s - loss: 9.1986e-04 - val_loss: 7.5115e-04 - 8s/epoch - 3ms/step
Epoch 16/50
2985/2985 - 8s - loss: 8.7611e-04 - val_loss: 8.0861e-04 - 8s/epoch - 3ms/step
Epoch 17/50
2985/2985 - 8s - loss: 8.7198e-04 - val_loss: 7.6354e-04 - 8s/epoch - 3ms/step
Epoch 18/50
2985/2985 - 8s - loss: 8.5789e-04 - val_loss: 7.3591e-04 - 8s/epoch - 3ms/step
Epoch 19/50
2985/2985 - 8s - loss: 8.8540e-04 - val_loss: 7.3902e-04 - 8s/epoch - 3ms/step
Epoch 20/50
2985/2985 - 8s - loss: 8.4078e-04 - val_loss: 6.5151e-04 - 8s/epoch - 3ms/step
Epoch 21/50
2985/2985 - 8s - loss: 8.1405e-04 - val_loss: 6.3197e-04 - 8s/epoch - 3ms/step
Epoch 22/50
2985/2985 - 8s - loss: 9.2644e-04 - val_loss: 7.9119e-04 - 8s/epoch - 3ms/step
Epoch 23/50
2985/2985 - 8s - loss: 8.6810e-04 - val_loss: 6.8091e-04 - 8s/epoch - 3ms/step
Epoch 24/50
2985/2985 - 8s - loss: 8.5978e-04 - val_loss: 7.6635e-04 - 8s/epoch - 3ms/step
Epoch 25/50
2985/2985 - 8s - loss: 8.3576e-04 - val_loss: 7.0665e-04 - 8s/epoch - 3ms/step
Epoch 26/50
2985/2985 - 8s - loss: 8.1948e-04 - val_loss: 6.8048e-04 - 8s/epoch - 3ms/step
Epoch 27/50
2985/2985 - 8s - loss: 8.3557e-04 - val_loss: 7.0104e-04 - 8s/epoch - 3ms/step
Epoch 28/50
2985/2985 - 8s - loss: 8.1787e-04 - val_loss: 6.3836e-04 - 8s/epoch - 3ms/step
Epoch 29/50
2985/2985 - 8s - loss: 9.3045e-04 - val_loss: 7.1420e-04 - 8s/epoch - 3ms/step
Epoch 30/50
2985/2985 - 8s - loss: 8.5603e-04 - val_loss: 6.9934e-04 - 8s/epoch - 3ms/step
Epoch 31/50
2985/2985 - 8s - loss: 9.3932e-04 - val_loss: 8.8024e-04 - 8s/epoch - 3ms/step
Epoch 32/50
2985/2985 - 8s - loss: 9.5013e-04 - val_loss: 8.2141e-04 - 8s/epoch - 3ms/step
Epoch 33/50
2985/2985 - 8s - loss: 8.9529e-04 - val_loss: 7.4855e-04 - 8s/epoch - 3ms/step
Epoch 34/50
2985/2985 - 8s - loss: 8.8586e-04 - val_loss: 7.1852e-04 - 8s/epoch - 3ms/step
Epoch 35/50
2985/2985 - 8s - loss: 8.6044e-04 - val_loss: 7.7183e-04 - 8s/epoch - 3ms/step
Epoch 36/50
2985/2985 - 8s - loss: 8.5945e-04 - val_loss: 7.7650e-04 - 8s/epoch - 3ms/step
Epoch 37/50
2985/2985 - 8s - loss: 8.4271e-04 - val_loss: 8.3070e-04 - 8s/epoch - 3ms/step
Epoch 38/50
2985/2985 - 8s - loss: 8.4485e-04 - val_loss: 9.7173e-04 - 8s/epoch - 3ms/step
Epoch 39/50
2985/2985 - 8s - loss: 8.3195e-04 - val_loss: 7.1231e-04 - 8s/epoch - 3ms/step
Epoch 40/50
2985/2985 - 8s - loss: 8.2002e-04 - val_loss: 8.8830e-04 - 8s/epoch - 3ms/step
Epoch 41/50
2985/2985 - 8s - loss: 8.2134e-04 - val_loss: 7.1695e-04 - 8s/epoch - 3ms/step
Epoch 42/50
2985/2985 - 8s - loss: 8.0463e-04 - val_loss: 8.2856e-04 - 8s/epoch - 3ms/step
Epoch 43/50
2985/2985 - 8s - loss: 7.9893e-04 - val_loss: 7.8239e-04 - 8s/epoch - 3ms/step
Epoch 44/50
2985/2985 - 8s - loss: 8.3971e-04 - val_loss: 8.0930e-04 - 8s/epoch - 3ms/step
Epoch 45/50
2985/2985 - 8s - loss: 8.1266e-04 - val_loss: 7.3804e-04 - 8s/epoch - 3ms/step
Epoch 46/50
2985/2985 - 8s - loss: 0.0010 - val_loss: 7.9525e-04 - 8s/epoch - 3ms/step
Epoch 47/50
2985/2985 - 8s - loss: 9.2262e-04 - val_loss: 8.3420e-04 - 8s/epoch - 3ms/step
Epoch 48/50
2985/2985 - 8s - loss: 8.9314e-04 - val_loss: 8.9605e-04 - 8s/epoch - 3ms/step
Epoch 49/50
2985/2985 - 8s - loss: 9.2166e-04 - val_loss: 8.3090e-04 - 8s/epoch - 3ms/step
Epoch 50/50
2985/2985 - 8s - loss: 8.7663e-04 - val_loss: 0.0011 - 8s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.001128120464272797
  1/332 [..............................] - ETA: 31s 51/332 [===>..........................] - ETA: 0s 102/332 [========>.....................] - ETA: 0s153/332 [============>.................] - ETA: 0s204/332 [=================>............] - ETA: 0s255/332 [======================>.......] - ETA: 0s306/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 993us/step
correlation 0.012501648937896473
cosine 0.0099739789389863
MAE: 0.018473398
RMSE: 0.03358748
r2: 0.9268163919157606
RMSE zero-vector: 0.23411466903540806
['default', 32, 50, 0.005, 'mse', 0.5, 632, 0.0008766293176449835, 0.001128120464272797, 0.012501648937896473, 0.0099739789389863, 0.018473397940397263, 0.03358748182654381, 0.9268163919157606, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_33"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_34 (InputLayer)       multiple                  0         
                                                                 
 dense_33 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_33 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_33 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_34 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_34 (ReLU)             (None, 632)               0         
                                                                 
 dense_34 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_35 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_35 (ReLU)             (None, 2528)              0         
                                                                 
 dense_35 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_34"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_35 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_34 (InputLayer)       multiple                  0         
                                                                 
 dense_33 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_33 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_33 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_35"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_36 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_34 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_34 (ReLU)             (None, 632)               0         
                                                                 
 dense_34 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_35 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_35 (ReLU)             (None, 2528)              0         
                                                                 
 dense_35 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_36"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_37 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_36 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_36 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_36 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_37 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_37 (ReLU)             (None, 632)               0         
                                                                 
 dense_37 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_38 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_38 (ReLU)             (None, 2528)              0         
                                                                 
 dense_38 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
2985/2985 - 9s - loss: 0.0081 - val_loss: 0.0033 - 9s/epoch - 3ms/step
Epoch 2/100
2985/2985 - 8s - loss: 0.0030 - val_loss: 0.0020 - 8s/epoch - 3ms/step
Epoch 3/100
2985/2985 - 8s - loss: 0.0025 - val_loss: 0.0017 - 8s/epoch - 3ms/step
Epoch 4/100
2985/2985 - 8s - loss: 0.0019 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 5/100
2985/2985 - 8s - loss: 0.0016 - val_loss: 9.6811e-04 - 8s/epoch - 3ms/step
Epoch 6/100
2985/2985 - 8s - loss: 0.0012 - val_loss: 0.0010 - 8s/epoch - 3ms/step
Epoch 7/100
2985/2985 - 8s - loss: 0.0010 - val_loss: 7.2593e-04 - 8s/epoch - 3ms/step
Epoch 8/100
2985/2985 - 8s - loss: 8.7880e-04 - val_loss: 6.5836e-04 - 8s/epoch - 3ms/step
Epoch 9/100
2985/2985 - 8s - loss: 7.6614e-04 - val_loss: 5.5010e-04 - 8s/epoch - 3ms/step
Epoch 10/100
2985/2985 - 8s - loss: 6.9133e-04 - val_loss: 5.9974e-04 - 8s/epoch - 3ms/step
Epoch 11/100
2985/2985 - 8s - loss: 6.2995e-04 - val_loss: 5.0647e-04 - 8s/epoch - 3ms/step
Epoch 12/100
2985/2985 - 8s - loss: 5.7389e-04 - val_loss: 0.0011 - 8s/epoch - 3ms/step
Epoch 13/100
2985/2985 - 8s - loss: 5.8739e-04 - val_loss: 4.6556e-04 - 8s/epoch - 3ms/step
Epoch 14/100
2985/2985 - 8s - loss: 5.0952e-04 - val_loss: 4.1780e-04 - 8s/epoch - 3ms/step
Epoch 15/100
2985/2985 - 8s - loss: 4.8478e-04 - val_loss: 3.9610e-04 - 8s/epoch - 3ms/step
Epoch 16/100
2985/2985 - 8s - loss: 4.4999e-04 - val_loss: 4.0899e-04 - 8s/epoch - 3ms/step
Epoch 17/100
2985/2985 - 8s - loss: 4.2978e-04 - val_loss: 3.5880e-04 - 8s/epoch - 3ms/step
Epoch 18/100
2985/2985 - 8s - loss: 4.1439e-04 - val_loss: 5.7285e-04 - 8s/epoch - 3ms/step
Epoch 19/100
2985/2985 - 8s - loss: 4.0410e-04 - val_loss: 3.4766e-04 - 8s/epoch - 3ms/step
Epoch 20/100
2985/2985 - 8s - loss: 4.1268e-04 - val_loss: 3.2301e-04 - 8s/epoch - 3ms/step
Epoch 21/100
2985/2985 - 8s - loss: 3.7507e-04 - val_loss: 3.0580e-04 - 8s/epoch - 3ms/step
Epoch 22/100
2985/2985 - 8s - loss: 3.6336e-04 - val_loss: 5.6176e-04 - 8s/epoch - 3ms/step
Epoch 23/100
2985/2985 - 8s - loss: 3.9347e-04 - val_loss: 2.9708e-04 - 8s/epoch - 3ms/step
Epoch 24/100
2985/2985 - 8s - loss: 3.4507e-04 - val_loss: 3.1127e-04 - 8s/epoch - 3ms/step
Epoch 25/100
2985/2985 - 8s - loss: 3.3773e-04 - val_loss: 2.9565e-04 - 8s/epoch - 3ms/step
Epoch 26/100
2985/2985 - 8s - loss: 3.3526e-04 - val_loss: 2.6263e-04 - 8s/epoch - 3ms/step
Epoch 27/100
2985/2985 - 8s - loss: 3.1878e-04 - val_loss: 2.8666e-04 - 8s/epoch - 3ms/step
Epoch 28/100
2985/2985 - 8s - loss: 3.1484e-04 - val_loss: 2.5333e-04 - 8s/epoch - 3ms/step
Epoch 29/100
2985/2985 - 8s - loss: 3.1520e-04 - val_loss: 2.3905e-04 - 8s/epoch - 3ms/step
Epoch 30/100
2985/2985 - 8s - loss: 2.9997e-04 - val_loss: 2.6864e-04 - 8s/epoch - 3ms/step
Epoch 31/100
2985/2985 - 8s - loss: 2.9879e-04 - val_loss: 2.5010e-04 - 8s/epoch - 3ms/step
Epoch 32/100
2985/2985 - 8s - loss: 2.8764e-04 - val_loss: 2.4670e-04 - 8s/epoch - 3ms/step
Epoch 33/100
2985/2985 - 8s - loss: 2.8722e-04 - val_loss: 2.5563e-04 - 8s/epoch - 3ms/step
Epoch 34/100
2985/2985 - 8s - loss: 2.8119e-04 - val_loss: 2.3469e-04 - 8s/epoch - 3ms/step
Epoch 35/100
2985/2985 - 8s - loss: 2.6929e-04 - val_loss: 2.3462e-04 - 8s/epoch - 3ms/step
Epoch 36/100
2985/2985 - 8s - loss: 2.7907e-04 - val_loss: 2.3373e-04 - 8s/epoch - 3ms/step
Epoch 37/100
2985/2985 - 8s - loss: 2.6918e-04 - val_loss: 2.3629e-04 - 8s/epoch - 3ms/step
Epoch 38/100
2985/2985 - 8s - loss: 2.6887e-04 - val_loss: 2.1163e-04 - 8s/epoch - 3ms/step
Epoch 39/100
2985/2985 - 8s - loss: 2.6484e-04 - val_loss: 2.1624e-04 - 8s/epoch - 3ms/step
Epoch 40/100
2985/2985 - 8s - loss: 2.5406e-04 - val_loss: 2.7085e-04 - 8s/epoch - 3ms/step
Epoch 41/100
2985/2985 - 8s - loss: 2.5614e-04 - val_loss: 2.1805e-04 - 8s/epoch - 3ms/step
Epoch 42/100
2985/2985 - 8s - loss: 2.4502e-04 - val_loss: 2.0590e-04 - 8s/epoch - 3ms/step
Epoch 43/100
2985/2985 - 8s - loss: 2.4380e-04 - val_loss: 2.0953e-04 - 8s/epoch - 3ms/step
Epoch 44/100
2985/2985 - 8s - loss: 2.4380e-04 - val_loss: 2.4255e-04 - 8s/epoch - 3ms/step
Epoch 45/100
2985/2985 - 8s - loss: 2.5247e-04 - val_loss: 2.1504e-04 - 8s/epoch - 3ms/step
Epoch 46/100
2985/2985 - 8s - loss: 2.3635e-04 - val_loss: 1.9694e-04 - 8s/epoch - 3ms/step
Epoch 47/100
2985/2985 - 8s - loss: 2.4272e-04 - val_loss: 2.0516e-04 - 8s/epoch - 3ms/step
Epoch 48/100
2985/2985 - 8s - loss: 2.3888e-04 - val_loss: 2.1017e-04 - 8s/epoch - 3ms/step
Epoch 49/100
2985/2985 - 8s - loss: 2.2975e-04 - val_loss: 2.3340e-04 - 8s/epoch - 3ms/step
Epoch 50/100
2985/2985 - 8s - loss: 2.3445e-04 - val_loss: 2.1895e-04 - 8s/epoch - 3ms/step
Epoch 51/100
2985/2985 - 8s - loss: 2.3112e-04 - val_loss: 1.8274e-04 - 8s/epoch - 3ms/step
Epoch 52/100
2985/2985 - 8s - loss: 2.2120e-04 - val_loss: 1.8724e-04 - 8s/epoch - 3ms/step
Epoch 53/100
2985/2985 - 8s - loss: 2.2503e-04 - val_loss: 1.9779e-04 - 8s/epoch - 3ms/step
Epoch 54/100
2985/2985 - 8s - loss: 2.1956e-04 - val_loss: 1.8034e-04 - 8s/epoch - 3ms/step
Epoch 55/100
2985/2985 - 8s - loss: 2.1439e-04 - val_loss: 1.9136e-04 - 8s/epoch - 3ms/step
Epoch 56/100
2985/2985 - 8s - loss: 2.1512e-04 - val_loss: 1.9066e-04 - 8s/epoch - 3ms/step
Epoch 57/100
2985/2985 - 8s - loss: 2.1828e-04 - val_loss: 1.7674e-04 - 8s/epoch - 3ms/step
Epoch 58/100
2985/2985 - 8s - loss: 2.1273e-04 - val_loss: 1.8139e-04 - 8s/epoch - 3ms/step
Epoch 59/100
2985/2985 - 8s - loss: 2.0742e-04 - val_loss: 1.7394e-04 - 8s/epoch - 3ms/step
Epoch 60/100
2985/2985 - 8s - loss: 2.1940e-04 - val_loss: 1.7828e-04 - 8s/epoch - 3ms/step
Epoch 61/100
2985/2985 - 8s - loss: 2.0819e-04 - val_loss: 2.1643e-04 - 8s/epoch - 3ms/step
Epoch 62/100
2985/2985 - 8s - loss: 2.0633e-04 - val_loss: 2.1900e-04 - 8s/epoch - 3ms/step
Epoch 63/100
2985/2985 - 8s - loss: 2.0478e-04 - val_loss: 2.0210e-04 - 8s/epoch - 3ms/step
Epoch 64/100
2985/2985 - 8s - loss: 2.0283e-04 - val_loss: 1.8033e-04 - 8s/epoch - 3ms/step
Epoch 65/100
2985/2985 - 8s - loss: 2.0111e-04 - val_loss: 1.9095e-04 - 8s/epoch - 3ms/step
Epoch 66/100
2985/2985 - 8s - loss: 1.9715e-04 - val_loss: 1.8022e-04 - 8s/epoch - 3ms/step
Epoch 67/100
2985/2985 - 8s - loss: 2.0608e-04 - val_loss: 1.5958e-04 - 8s/epoch - 3ms/step
Epoch 68/100
2985/2985 - 8s - loss: 2.0625e-04 - val_loss: 1.7814e-04 - 8s/epoch - 3ms/step
Epoch 69/100
2985/2985 - 8s - loss: 2.0128e-04 - val_loss: 1.7442e-04 - 8s/epoch - 3ms/step
Epoch 70/100
2985/2985 - 8s - loss: 1.9546e-04 - val_loss: 2.3745e-04 - 8s/epoch - 3ms/step
Epoch 71/100
2985/2985 - 8s - loss: 2.0386e-04 - val_loss: 1.6028e-04 - 8s/epoch - 3ms/step
Epoch 72/100
2985/2985 - 8s - loss: 1.9303e-04 - val_loss: 1.8437e-04 - 8s/epoch - 3ms/step
Epoch 73/100
2985/2985 - 8s - loss: 1.9251e-04 - val_loss: 1.6554e-04 - 8s/epoch - 3ms/step
Epoch 74/100
2985/2985 - 8s - loss: 1.8940e-04 - val_loss: 1.5430e-04 - 8s/epoch - 3ms/step
Epoch 75/100
2985/2985 - 8s - loss: 1.9672e-04 - val_loss: 1.6612e-04 - 8s/epoch - 3ms/step
Epoch 76/100
2985/2985 - 8s - loss: 1.9569e-04 - val_loss: 1.7188e-04 - 8s/epoch - 3ms/step
Epoch 77/100
2985/2985 - 8s - loss: 2.0014e-04 - val_loss: 5.1955e-04 - 8s/epoch - 3ms/step
Epoch 78/100
2985/2985 - 8s - loss: 2.4416e-04 - val_loss: 1.5696e-04 - 8s/epoch - 3ms/step
Epoch 79/100
2985/2985 - 8s - loss: 1.8971e-04 - val_loss: 1.9518e-04 - 8s/epoch - 3ms/step
Epoch 80/100
2985/2985 - 8s - loss: 1.9026e-04 - val_loss: 1.7273e-04 - 8s/epoch - 3ms/step
Epoch 81/100
2985/2985 - 8s - loss: 1.8492e-04 - val_loss: 2.1157e-04 - 8s/epoch - 3ms/step
Epoch 82/100
2985/2985 - 8s - loss: 1.9798e-04 - val_loss: 1.6106e-04 - 8s/epoch - 3ms/step
Epoch 83/100
2985/2985 - 8s - loss: 1.8601e-04 - val_loss: 1.6401e-04 - 8s/epoch - 3ms/step
Epoch 84/100
2985/2985 - 8s - loss: 1.8196e-04 - val_loss: 1.7429e-04 - 8s/epoch - 3ms/step
Epoch 85/100
2985/2985 - 8s - loss: 1.8259e-04 - val_loss: 1.8065e-04 - 8s/epoch - 3ms/step
Epoch 86/100
2985/2985 - 8s - loss: 1.8213e-04 - val_loss: 1.6348e-04 - 8s/epoch - 3ms/step
Epoch 87/100
2985/2985 - 8s - loss: 1.8109e-04 - val_loss: 1.6850e-04 - 8s/epoch - 3ms/step
Epoch 88/100
2985/2985 - 8s - loss: 1.8119e-04 - val_loss: 1.9245e-04 - 8s/epoch - 3ms/step
Epoch 89/100
2985/2985 - 8s - loss: 1.8343e-04 - val_loss: 1.6558e-04 - 8s/epoch - 3ms/step
Epoch 90/100
2985/2985 - 8s - loss: 1.8038e-04 - val_loss: 1.5965e-04 - 8s/epoch - 3ms/step
Epoch 91/100
2985/2985 - 8s - loss: 1.7696e-04 - val_loss: 1.5910e-04 - 8s/epoch - 3ms/step
Epoch 92/100
2985/2985 - 8s - loss: 1.7614e-04 - val_loss: 1.8165e-04 - 8s/epoch - 3ms/step
Epoch 93/100
2985/2985 - 8s - loss: 1.8574e-04 - val_loss: 1.5485e-04 - 8s/epoch - 3ms/step
Epoch 94/100
2985/2985 - 8s - loss: 1.7226e-04 - val_loss: 1.5338e-04 - 8s/epoch - 3ms/step
Epoch 95/100
2985/2985 - 8s - loss: 1.7371e-04 - val_loss: 1.5650e-04 - 8s/epoch - 3ms/step
Epoch 96/100
2985/2985 - 8s - loss: 1.7762e-04 - val_loss: 1.6216e-04 - 8s/epoch - 3ms/step
Epoch 97/100
2985/2985 - 8s - loss: 1.7262e-04 - val_loss: 1.5660e-04 - 8s/epoch - 3ms/step
Epoch 98/100
2985/2985 - 8s - loss: 1.7012e-04 - val_loss: 1.6342e-04 - 8s/epoch - 3ms/step
Epoch 99/100
2985/2985 - 8s - loss: 1.7038e-04 - val_loss: 1.4799e-04 - 8s/epoch - 3ms/step
Epoch 100/100
2985/2985 - 8s - loss: 1.7030e-04 - val_loss: 1.6391e-04 - 8s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00016390519158449024
  1/332 [..............................] - ETA: 30s 51/332 [===>..........................] - ETA: 0s 101/332 [========>.....................] - ETA: 0s151/332 [============>.................] - ETA: 0s202/332 [=================>............] - ETA: 0s253/332 [=====================>........] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0018396376889470026
cosine 0.001462926403603552
MAE: 0.0069996472
RMSE: 0.012802539
r2: 0.9893675226408482
RMSE zero-vector: 0.23411466903540806
['default', 32, 100, 0.0005, 'mse', 0.5, 632, 0.00017030171875376254, 0.00016390519158449024, 0.0018396376889470026, 0.001462926403603552, 0.006999647244811058, 0.012802539393305779, 0.9893675226408482, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_36"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_37 (InputLayer)       multiple                  0         
                                                                 
 dense_36 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_36 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_36 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_37 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_37 (ReLU)             (None, 632)               0         
                                                                 
 dense_37 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_38 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_38 (ReLU)             (None, 2528)              0         
                                                                 
 dense_38 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_37"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_38 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_37 (InputLayer)       multiple                  0         
                                                                 
 dense_36 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_36 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_36 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_38"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_39 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_37 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_37 (ReLU)             (None, 632)               0         
                                                                 
 dense_37 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_38 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_38 (ReLU)             (None, 2528)              0         
                                                                 
 dense_38 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_39"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_40 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_39 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_39 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_39 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_40 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_40 (ReLU)             (None, 632)               0         
                                                                 
 dense_40 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_41 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_41 (ReLU)             (None, 2528)              0         
                                                                 
 dense_41 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
2985/2985 - 9s - loss: 0.0076 - val_loss: 0.0028 - 9s/epoch - 3ms/step
Epoch 2/100
2985/2985 - 8s - loss: 0.0028 - val_loss: 0.0017 - 8s/epoch - 3ms/step
Epoch 3/100
2985/2985 - 8s - loss: 0.0020 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 4/100
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0010 - 8s/epoch - 3ms/step
Epoch 5/100
2985/2985 - 8s - loss: 0.0011 - val_loss: 8.1965e-04 - 8s/epoch - 3ms/step
Epoch 6/100
2985/2985 - 8s - loss: 8.8118e-04 - val_loss: 6.8477e-04 - 8s/epoch - 3ms/step
Epoch 7/100
2985/2985 - 8s - loss: 7.6898e-04 - val_loss: 5.9811e-04 - 8s/epoch - 3ms/step
Epoch 8/100
2985/2985 - 8s - loss: 6.7676e-04 - val_loss: 5.4569e-04 - 8s/epoch - 3ms/step
Epoch 9/100
2985/2985 - 8s - loss: 6.2031e-04 - val_loss: 5.2312e-04 - 8s/epoch - 3ms/step
Epoch 10/100
2985/2985 - 8s - loss: 5.9458e-04 - val_loss: 4.8357e-04 - 8s/epoch - 3ms/step
Epoch 11/100
2985/2985 - 8s - loss: 5.6948e-04 - val_loss: 5.7042e-04 - 8s/epoch - 3ms/step
Epoch 12/100
2985/2985 - 8s - loss: 5.3707e-04 - val_loss: 4.8061e-04 - 8s/epoch - 3ms/step
Epoch 13/100
2985/2985 - 8s - loss: 5.1237e-04 - val_loss: 5.9241e-04 - 8s/epoch - 3ms/step
Epoch 14/100
2985/2985 - 8s - loss: 5.1547e-04 - val_loss: 3.8043e-04 - 8s/epoch - 3ms/step
Epoch 15/100
2985/2985 - 8s - loss: 4.5953e-04 - val_loss: 3.7211e-04 - 8s/epoch - 3ms/step
Epoch 16/100
2985/2985 - 8s - loss: 4.3202e-04 - val_loss: 3.8014e-04 - 8s/epoch - 3ms/step
Epoch 17/100
2985/2985 - 8s - loss: 4.2930e-04 - val_loss: 3.9844e-04 - 8s/epoch - 3ms/step
Epoch 18/100
2985/2985 - 8s - loss: 4.1895e-04 - val_loss: 3.5965e-04 - 8s/epoch - 3ms/step
Epoch 19/100
2985/2985 - 8s - loss: 4.2054e-04 - val_loss: 3.5694e-04 - 8s/epoch - 3ms/step
Epoch 20/100
2985/2985 - 8s - loss: 3.8817e-04 - val_loss: 3.2052e-04 - 8s/epoch - 3ms/step
Epoch 21/100
2985/2985 - 8s - loss: 3.7658e-04 - val_loss: 3.3983e-04 - 8s/epoch - 3ms/step
Epoch 22/100
2985/2985 - 8s - loss: 3.7964e-04 - val_loss: 2.9253e-04 - 8s/epoch - 3ms/step
Epoch 23/100
2985/2985 - 8s - loss: 3.6192e-04 - val_loss: 3.1215e-04 - 8s/epoch - 3ms/step
Epoch 24/100
2985/2985 - 8s - loss: 3.6021e-04 - val_loss: 3.1058e-04 - 8s/epoch - 3ms/step
Epoch 25/100
2985/2985 - 8s - loss: 3.4385e-04 - val_loss: 2.8495e-04 - 8s/epoch - 3ms/step
Epoch 26/100
2985/2985 - 8s - loss: 4.0019e-04 - val_loss: 2.7982e-04 - 8s/epoch - 3ms/step
Epoch 27/100
2985/2985 - 8s - loss: 3.4447e-04 - val_loss: 2.6294e-04 - 8s/epoch - 3ms/step
Epoch 28/100
2985/2985 - 8s - loss: 3.3482e-04 - val_loss: 2.6319e-04 - 8s/epoch - 3ms/step
Epoch 29/100
2985/2985 - 8s - loss: 3.2608e-04 - val_loss: 2.6655e-04 - 8s/epoch - 3ms/step
Epoch 30/100
2985/2985 - 8s - loss: 3.1763e-04 - val_loss: 2.7240e-04 - 8s/epoch - 3ms/step
Epoch 31/100
2985/2985 - 8s - loss: 3.1325e-04 - val_loss: 2.4762e-04 - 8s/epoch - 3ms/step
Epoch 32/100
2985/2985 - 8s - loss: 3.2444e-04 - val_loss: 2.4710e-04 - 8s/epoch - 3ms/step
Epoch 33/100
2985/2985 - 8s - loss: 3.0658e-04 - val_loss: 2.4200e-04 - 8s/epoch - 3ms/step
Epoch 34/100
2985/2985 - 8s - loss: 3.1143e-04 - val_loss: 2.4138e-04 - 8s/epoch - 3ms/step
Epoch 35/100
2985/2985 - 8s - loss: 2.9626e-04 - val_loss: 2.3881e-04 - 8s/epoch - 3ms/step
Epoch 36/100
2985/2985 - 8s - loss: 3.0282e-04 - val_loss: 2.3584e-04 - 8s/epoch - 3ms/step
Epoch 37/100
2985/2985 - 8s - loss: 3.3386e-04 - val_loss: 2.3654e-04 - 8s/epoch - 3ms/step
Epoch 38/100
2985/2985 - 8s - loss: 2.9164e-04 - val_loss: 2.3011e-04 - 8s/epoch - 3ms/step
Epoch 39/100
2985/2985 - 8s - loss: 2.8759e-04 - val_loss: 2.2789e-04 - 8s/epoch - 3ms/step
Epoch 40/100
2985/2985 - 8s - loss: 2.8057e-04 - val_loss: 2.6140e-04 - 8s/epoch - 3ms/step
Epoch 41/100
2985/2985 - 8s - loss: 2.8152e-04 - val_loss: 2.1827e-04 - 8s/epoch - 3ms/step
Epoch 42/100
2985/2985 - 8s - loss: 2.7828e-04 - val_loss: 2.3520e-04 - 8s/epoch - 3ms/step
Epoch 43/100
2985/2985 - 8s - loss: 2.8348e-04 - val_loss: 2.1957e-04 - 8s/epoch - 3ms/step
Epoch 44/100
2985/2985 - 8s - loss: 2.7390e-04 - val_loss: 2.6550e-04 - 8s/epoch - 3ms/step
Epoch 45/100
2985/2985 - 8s - loss: 2.7429e-04 - val_loss: 2.0822e-04 - 8s/epoch - 3ms/step
Epoch 46/100
2985/2985 - 8s - loss: 2.6763e-04 - val_loss: 2.1988e-04 - 8s/epoch - 3ms/step
Epoch 47/100
2985/2985 - 8s - loss: 2.6753e-04 - val_loss: 2.1243e-04 - 8s/epoch - 3ms/step
Epoch 48/100
2985/2985 - 8s - loss: 2.6454e-04 - val_loss: 2.0955e-04 - 8s/epoch - 3ms/step
Epoch 49/100
2985/2985 - 8s - loss: 2.6217e-04 - val_loss: 2.3737e-04 - 8s/epoch - 3ms/step
Epoch 50/100
2985/2985 - 8s - loss: 2.6875e-04 - val_loss: 3.1515e-04 - 8s/epoch - 3ms/step
Epoch 51/100
2985/2985 - 8s - loss: 2.8830e-04 - val_loss: 2.0697e-04 - 8s/epoch - 3ms/step
Epoch 52/100
2985/2985 - 8s - loss: 2.5685e-04 - val_loss: 2.0277e-04 - 8s/epoch - 3ms/step
Epoch 53/100
2985/2985 - 8s - loss: 2.7162e-04 - val_loss: 2.0234e-04 - 8s/epoch - 3ms/step
Epoch 54/100
2985/2985 - 8s - loss: 2.5239e-04 - val_loss: 2.0823e-04 - 8s/epoch - 3ms/step
Epoch 55/100
2985/2985 - 8s - loss: 2.4994e-04 - val_loss: 1.9295e-04 - 8s/epoch - 3ms/step
Epoch 56/100
2985/2985 - 8s - loss: 2.5183e-04 - val_loss: 1.9365e-04 - 8s/epoch - 3ms/step
Epoch 57/100
2985/2985 - 8s - loss: 2.4458e-04 - val_loss: 2.0205e-04 - 8s/epoch - 3ms/step
Epoch 58/100
2985/2985 - 8s - loss: 2.5446e-04 - val_loss: 2.1096e-04 - 8s/epoch - 3ms/step
Epoch 59/100
2985/2985 - 8s - loss: 2.4665e-04 - val_loss: 1.9723e-04 - 8s/epoch - 3ms/step
Epoch 60/100
2985/2985 - 8s - loss: 2.6139e-04 - val_loss: 1.9512e-04 - 8s/epoch - 3ms/step
Epoch 61/100
2985/2985 - 8s - loss: 2.4292e-04 - val_loss: 2.0069e-04 - 8s/epoch - 3ms/step
Epoch 62/100
2985/2985 - 8s - loss: 2.3900e-04 - val_loss: 2.0417e-04 - 8s/epoch - 3ms/step
Epoch 63/100
2985/2985 - 8s - loss: 2.3853e-04 - val_loss: 1.9755e-04 - 8s/epoch - 3ms/step
Epoch 64/100
2985/2985 - 8s - loss: 2.3696e-04 - val_loss: 1.8970e-04 - 8s/epoch - 3ms/step
Epoch 65/100
2985/2985 - 8s - loss: 2.3373e-04 - val_loss: 2.0240e-04 - 8s/epoch - 3ms/step
Epoch 66/100
2985/2985 - 8s - loss: 2.6093e-04 - val_loss: 1.9205e-04 - 8s/epoch - 3ms/step
Epoch 67/100
2985/2985 - 8s - loss: 2.3782e-04 - val_loss: 1.8592e-04 - 8s/epoch - 3ms/step
Epoch 68/100
2985/2985 - 8s - loss: 2.3405e-04 - val_loss: 2.0754e-04 - 8s/epoch - 3ms/step
Epoch 69/100
2985/2985 - 8s - loss: 2.7355e-04 - val_loss: 1.8180e-04 - 8s/epoch - 3ms/step
Epoch 70/100
2985/2985 - 8s - loss: 2.3556e-04 - val_loss: 1.7970e-04 - 8s/epoch - 3ms/step
Epoch 71/100
2985/2985 - 8s - loss: 2.2919e-04 - val_loss: 1.8310e-04 - 8s/epoch - 3ms/step
Epoch 72/100
2985/2985 - 8s - loss: 2.2803e-04 - val_loss: 1.7524e-04 - 8s/epoch - 3ms/step
Epoch 73/100
2985/2985 - 8s - loss: 2.2745e-04 - val_loss: 1.7601e-04 - 8s/epoch - 3ms/step
Epoch 74/100
2985/2985 - 8s - loss: 2.3980e-04 - val_loss: 1.8805e-04 - 8s/epoch - 3ms/step
Epoch 75/100
2985/2985 - 8s - loss: 2.3065e-04 - val_loss: 1.7522e-04 - 8s/epoch - 3ms/step
Epoch 76/100
2985/2985 - 8s - loss: 2.2551e-04 - val_loss: 1.7973e-04 - 8s/epoch - 3ms/step
Epoch 77/100
2985/2985 - 8s - loss: 2.3062e-04 - val_loss: 4.5413e-04 - 8s/epoch - 3ms/step
Epoch 78/100
2985/2985 - 8s - loss: 2.9398e-04 - val_loss: 1.8395e-04 - 8s/epoch - 3ms/step
Epoch 79/100
2985/2985 - 8s - loss: 2.2977e-04 - val_loss: 1.8981e-04 - 8s/epoch - 3ms/step
Epoch 80/100
2985/2985 - 8s - loss: 2.2739e-04 - val_loss: 1.9266e-04 - 8s/epoch - 3ms/step
Epoch 81/100
2985/2985 - 8s - loss: 2.2788e-04 - val_loss: 1.8561e-04 - 8s/epoch - 3ms/step
Epoch 82/100
2985/2985 - 8s - loss: 2.4083e-04 - val_loss: 1.8074e-04 - 8s/epoch - 3ms/step
Epoch 83/100
2985/2985 - 8s - loss: 2.2180e-04 - val_loss: 1.7827e-04 - 8s/epoch - 3ms/step
Epoch 84/100
2985/2985 - 8s - loss: 2.1815e-04 - val_loss: 1.6942e-04 - 8s/epoch - 3ms/step
Epoch 85/100
2985/2985 - 8s - loss: 2.1668e-04 - val_loss: 1.8906e-04 - 8s/epoch - 3ms/step
Epoch 86/100
2985/2985 - 8s - loss: 2.2257e-04 - val_loss: 1.6505e-04 - 8s/epoch - 3ms/step
Epoch 87/100
2985/2985 - 8s - loss: 2.1892e-04 - val_loss: 1.8591e-04 - 8s/epoch - 3ms/step
Epoch 88/100
2985/2985 - 8s - loss: 2.1497e-04 - val_loss: 1.7125e-04 - 8s/epoch - 3ms/step
Epoch 89/100
2985/2985 - 8s - loss: 2.1333e-04 - val_loss: 1.7291e-04 - 8s/epoch - 3ms/step
Epoch 90/100
2985/2985 - 8s - loss: 2.1826e-04 - val_loss: 1.6982e-04 - 8s/epoch - 3ms/step
Epoch 91/100
2985/2985 - 8s - loss: 2.1218e-04 - val_loss: 1.7150e-04 - 8s/epoch - 3ms/step
Epoch 92/100
2985/2985 - 8s - loss: 2.1619e-04 - val_loss: 2.3866e-04 - 8s/epoch - 3ms/step
Epoch 93/100
2985/2985 - 8s - loss: 2.1080e-04 - val_loss: 1.8681e-04 - 8s/epoch - 3ms/step
Epoch 94/100
2985/2985 - 8s - loss: 2.1022e-04 - val_loss: 1.6302e-04 - 8s/epoch - 3ms/step
Epoch 95/100
2985/2985 - 8s - loss: 2.0980e-04 - val_loss: 1.6398e-04 - 8s/epoch - 3ms/step
Epoch 96/100
2985/2985 - 8s - loss: 2.1109e-04 - val_loss: 2.2910e-04 - 8s/epoch - 3ms/step
Epoch 97/100
2985/2985 - 8s - loss: 2.1990e-04 - val_loss: 1.6394e-04 - 8s/epoch - 3ms/step
Epoch 98/100
2985/2985 - 8s - loss: 2.0832e-04 - val_loss: 1.6731e-04 - 8s/epoch - 3ms/step
Epoch 99/100
2985/2985 - 8s - loss: 2.0518e-04 - val_loss: 1.6025e-04 - 8s/epoch - 3ms/step
Epoch 100/100
2985/2985 - 8s - loss: 2.0442e-04 - val_loss: 1.8243e-04 - 8s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00018243450904265046
  1/332 [..............................] - ETA: 32s 51/332 [===>..........................] - ETA: 0s 102/332 [========>.....................] - ETA: 0s153/332 [============>.................] - ETA: 0s204/332 [=================>............] - ETA: 0s255/332 [======================>.......] - ETA: 0s306/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 998us/step
correlation 0.0020460231412908424
cosine 0.0016278186093354467
MAE: 0.007356167
RMSE: 0.013506827
r2: 0.9881656371039089
RMSE zero-vector: 0.23411466903540806
['default', 32, 100, 0.001, 'mse', 0.5, 632, 0.00020442291861400008, 0.00018243450904265046, 0.0020460231412908424, 0.0016278186093354467, 0.007356166839599609, 0.01350682694464922, 0.9881656371039089, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_39"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_40 (InputLayer)       multiple                  0         
                                                                 
 dense_39 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_39 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_39 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_40 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_40 (ReLU)             (None, 632)               0         
                                                                 
 dense_40 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_41 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_41 (ReLU)             (None, 2528)              0         
                                                                 
 dense_41 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_40"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_41 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_40 (InputLayer)       multiple                  0         
                                                                 
 dense_39 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_39 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_39 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_41"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_42 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_40 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_40 (ReLU)             (None, 632)               0         
                                                                 
 dense_40 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_41 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_41 (ReLU)             (None, 2528)              0         
                                                                 
 dense_41 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_42"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_43 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_42 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_42 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_42 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_43 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_43 (ReLU)             (None, 632)               0         
                                                                 
 dense_43 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_44 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_44 (ReLU)             (None, 2528)              0         
                                                                 
 dense_44 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
2985/2985 - 9s - loss: 0.0253 - val_loss: 0.0034 - 9s/epoch - 3ms/step
Epoch 2/100
2985/2985 - 8s - loss: 0.0032 - val_loss: 0.0020 - 8s/epoch - 3ms/step
Epoch 3/100
2985/2985 - 8s - loss: 0.0021 - val_loss: 0.0018 - 8s/epoch - 3ms/step
Epoch 4/100
2985/2985 - 8s - loss: 0.0018 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 5/100
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 6/100
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 7/100
2985/2985 - 8s - loss: 0.0012 - val_loss: 9.5787e-04 - 8s/epoch - 3ms/step
Epoch 8/100
2985/2985 - 8s - loss: 0.0011 - val_loss: 9.1277e-04 - 8s/epoch - 3ms/step
Epoch 9/100
2985/2985 - 8s - loss: 0.0011 - val_loss: 8.4811e-04 - 8s/epoch - 3ms/step
Epoch 10/100
2985/2985 - 8s - loss: 0.0011 - val_loss: 8.2277e-04 - 8s/epoch - 3ms/step
Epoch 11/100
2985/2985 - 8s - loss: 9.8288e-04 - val_loss: 8.1876e-04 - 8s/epoch - 3ms/step
Epoch 12/100
2985/2985 - 8s - loss: 9.5236e-04 - val_loss: 7.9162e-04 - 8s/epoch - 3ms/step
Epoch 13/100
2985/2985 - 8s - loss: 9.1182e-04 - val_loss: 7.9565e-04 - 8s/epoch - 3ms/step
Epoch 14/100
2985/2985 - 8s - loss: 8.9030e-04 - val_loss: 7.5108e-04 - 8s/epoch - 3ms/step
Epoch 15/100
2985/2985 - 8s - loss: 8.5928e-04 - val_loss: 6.7875e-04 - 8s/epoch - 3ms/step
Epoch 16/100
2985/2985 - 8s - loss: 8.3562e-04 - val_loss: 7.2481e-04 - 8s/epoch - 3ms/step
Epoch 17/100
2985/2985 - 8s - loss: 8.4762e-04 - val_loss: 6.7973e-04 - 8s/epoch - 3ms/step
Epoch 18/100
2985/2985 - 8s - loss: 8.1827e-04 - val_loss: 6.8578e-04 - 8s/epoch - 3ms/step
Epoch 19/100
2985/2985 - 8s - loss: 8.8497e-04 - val_loss: 6.8469e-04 - 8s/epoch - 3ms/step
Epoch 20/100
2985/2985 - 8s - loss: 8.2821e-04 - val_loss: 6.6323e-04 - 8s/epoch - 3ms/step
Epoch 21/100
2985/2985 - 8s - loss: 8.0866e-04 - val_loss: 6.3408e-04 - 8s/epoch - 3ms/step
Epoch 22/100
2985/2985 - 8s - loss: 7.7781e-04 - val_loss: 6.6267e-04 - 8s/epoch - 3ms/step
Epoch 23/100
2985/2985 - 8s - loss: 7.7075e-04 - val_loss: 6.4815e-04 - 8s/epoch - 3ms/step
Epoch 24/100
2985/2985 - 8s - loss: 7.6845e-04 - val_loss: 6.2562e-04 - 8s/epoch - 3ms/step
Epoch 25/100
2985/2985 - 8s - loss: 7.4804e-04 - val_loss: 6.4096e-04 - 8s/epoch - 3ms/step
Epoch 26/100
2985/2985 - 8s - loss: 7.4258e-04 - val_loss: 5.8346e-04 - 8s/epoch - 3ms/step
Epoch 27/100
2985/2985 - 8s - loss: 8.9321e-04 - val_loss: 6.9553e-04 - 8s/epoch - 3ms/step
Epoch 28/100
2985/2985 - 8s - loss: 8.2365e-04 - val_loss: 6.3953e-04 - 8s/epoch - 3ms/step
Epoch 29/100
2985/2985 - 8s - loss: 8.2061e-04 - val_loss: 6.5586e-04 - 8s/epoch - 3ms/step
Epoch 30/100
2985/2985 - 8s - loss: 7.9302e-04 - val_loss: 0.0011 - 8s/epoch - 3ms/step
Epoch 31/100
2985/2985 - 8s - loss: 8.0028e-04 - val_loss: 6.8902e-04 - 8s/epoch - 3ms/step
Epoch 32/100
2985/2985 - 8s - loss: 8.4796e-04 - val_loss: 6.7772e-04 - 8s/epoch - 3ms/step
Epoch 33/100
2985/2985 - 8s - loss: 7.9154e-04 - val_loss: 6.2914e-04 - 8s/epoch - 3ms/step
Epoch 34/100
2985/2985 - 8s - loss: 7.8522e-04 - val_loss: 6.1624e-04 - 8s/epoch - 3ms/step
Epoch 35/100
2985/2985 - 8s - loss: 7.6516e-04 - val_loss: 6.1900e-04 - 8s/epoch - 3ms/step
Epoch 36/100
2985/2985 - 8s - loss: 7.7356e-04 - val_loss: 7.7123e-04 - 8s/epoch - 3ms/step
Epoch 37/100
2985/2985 - 8s - loss: 7.8127e-04 - val_loss: 6.4211e-04 - 8s/epoch - 3ms/step
Epoch 38/100
2985/2985 - 8s - loss: 7.4843e-04 - val_loss: 6.3300e-04 - 8s/epoch - 3ms/step
Epoch 39/100
2985/2985 - 8s - loss: 8.0705e-04 - val_loss: 6.9353e-04 - 8s/epoch - 3ms/step
Epoch 40/100
2985/2985 - 8s - loss: 7.5758e-04 - val_loss: 8.0140e-04 - 8s/epoch - 3ms/step
Epoch 41/100
2985/2985 - 8s - loss: 7.4776e-04 - val_loss: 6.1968e-04 - 8s/epoch - 3ms/step
Epoch 42/100
2985/2985 - 8s - loss: 7.5054e-04 - val_loss: 6.4382e-04 - 8s/epoch - 3ms/step
Epoch 43/100
2985/2985 - 8s - loss: 7.3047e-04 - val_loss: 6.2508e-04 - 8s/epoch - 3ms/step
Epoch 44/100
2985/2985 - 8s - loss: 7.5813e-04 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 45/100
2985/2985 - 8s - loss: 9.2522e-04 - val_loss: 8.7870e-04 - 8s/epoch - 3ms/step
Epoch 46/100
2985/2985 - 8s - loss: 8.1661e-04 - val_loss: 6.5099e-04 - 8s/epoch - 3ms/step
Epoch 47/100
2985/2985 - 8s - loss: 7.9624e-04 - val_loss: 6.7449e-04 - 8s/epoch - 3ms/step
Epoch 48/100
2985/2985 - 8s - loss: 7.8384e-04 - val_loss: 7.1847e-04 - 8s/epoch - 3ms/step
Epoch 49/100
2985/2985 - 8s - loss: 7.8337e-04 - val_loss: 0.0017 - 8s/epoch - 3ms/step
Epoch 50/100
2985/2985 - 8s - loss: 0.0011 - val_loss: 8.8142e-04 - 8s/epoch - 3ms/step
Epoch 51/100
2985/2985 - 8s - loss: 9.0110e-04 - val_loss: 7.7650e-04 - 8s/epoch - 3ms/step
Epoch 52/100
2985/2985 - 8s - loss: 8.7723e-04 - val_loss: 7.2909e-04 - 8s/epoch - 3ms/step
Epoch 53/100
2985/2985 - 8s - loss: 8.5003e-04 - val_loss: 7.6018e-04 - 8s/epoch - 3ms/step
Epoch 54/100
2985/2985 - 8s - loss: 9.9801e-04 - val_loss: 0.0010 - 8s/epoch - 3ms/step
Epoch 55/100
2985/2985 - 8s - loss: 9.6780e-04 - val_loss: 8.3394e-04 - 8s/epoch - 3ms/step
Epoch 56/100
2985/2985 - 8s - loss: 9.2680e-04 - val_loss: 8.6608e-04 - 8s/epoch - 3ms/step
Epoch 57/100
2985/2985 - 8s - loss: 9.6904e-04 - val_loss: 9.0384e-04 - 8s/epoch - 3ms/step
Epoch 58/100
2985/2985 - 8s - loss: 9.2046e-04 - val_loss: 9.1948e-04 - 8s/epoch - 3ms/step
Epoch 59/100
2985/2985 - 8s - loss: 9.1181e-04 - val_loss: 0.0010 - 8s/epoch - 3ms/step
Epoch 60/100
2985/2985 - 8s - loss: 9.2067e-04 - val_loss: 0.0010 - 8s/epoch - 3ms/step
Epoch 61/100
2985/2985 - 8s - loss: 8.9996e-04 - val_loss: 8.8929e-04 - 8s/epoch - 3ms/step
Epoch 62/100
2985/2985 - 8s - loss: 9.1664e-04 - val_loss: 8.6250e-04 - 8s/epoch - 3ms/step
Epoch 63/100
2985/2985 - 8s - loss: 8.9427e-04 - val_loss: 8.7790e-04 - 8s/epoch - 3ms/step
Epoch 64/100
2985/2985 - 8s - loss: 8.8493e-04 - val_loss: 0.0010 - 8s/epoch - 3ms/step
Epoch 65/100
2985/2985 - 8s - loss: 8.9220e-04 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 66/100
2985/2985 - 8s - loss: 9.2540e-04 - val_loss: 8.7202e-04 - 8s/epoch - 3ms/step
Epoch 67/100
2985/2985 - 8s - loss: 9.0189e-04 - val_loss: 9.0474e-04 - 8s/epoch - 3ms/step
Epoch 68/100
2985/2985 - 8s - loss: 9.3372e-04 - val_loss: 0.0011 - 8s/epoch - 3ms/step
Epoch 69/100
2985/2985 - 8s - loss: 9.0591e-04 - val_loss: 0.0011 - 8s/epoch - 3ms/step
Epoch 70/100
2985/2985 - 8s - loss: 8.8410e-04 - val_loss: 0.0010 - 8s/epoch - 3ms/step
Epoch 71/100
2985/2985 - 8s - loss: 8.8834e-04 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 72/100
2985/2985 - 8s - loss: 9.4702e-04 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 73/100
2985/2985 - 8s - loss: 8.9443e-04 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 74/100
2985/2985 - 8s - loss: 8.8889e-04 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 75/100
2985/2985 - 8s - loss: 8.9158e-04 - val_loss: 0.0020 - 8s/epoch - 3ms/step
Epoch 76/100
2985/2985 - 8s - loss: 9.2161e-04 - val_loss: 0.0011 - 8s/epoch - 3ms/step
Epoch 77/100
2985/2985 - 8s - loss: 8.9528e-04 - val_loss: 0.0022 - 8s/epoch - 3ms/step
Epoch 78/100
2985/2985 - 8s - loss: 9.1138e-04 - val_loss: 0.0015 - 8s/epoch - 3ms/step
Epoch 79/100
2985/2985 - 8s - loss: 8.8199e-04 - val_loss: 0.0016 - 8s/epoch - 3ms/step
Epoch 80/100
2985/2985 - 8s - loss: 9.1003e-04 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 81/100
2985/2985 - 8s - loss: 8.7663e-04 - val_loss: 0.0015 - 8s/epoch - 3ms/step
Epoch 82/100
2985/2985 - 8s - loss: 8.7197e-04 - val_loss: 0.0011 - 8s/epoch - 3ms/step
Epoch 83/100
2985/2985 - 8s - loss: 8.8470e-04 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 84/100
2985/2985 - 8s - loss: 8.7662e-04 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 85/100
2985/2985 - 8s - loss: 9.2015e-04 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 86/100
2985/2985 - 8s - loss: 9.1572e-04 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 87/100
2985/2985 - 8s - loss: 8.8759e-04 - val_loss: 0.0015 - 8s/epoch - 3ms/step
Epoch 88/100
2985/2985 - 8s - loss: 8.7377e-04 - val_loss: 0.0018 - 8s/epoch - 3ms/step
Epoch 89/100
2985/2985 - 8s - loss: 9.2045e-04 - val_loss: 0.0016 - 8s/epoch - 3ms/step
Epoch 90/100
2985/2985 - 8s - loss: 9.3782e-04 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 91/100
2985/2985 - 8s - loss: 9.2314e-04 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 92/100
2985/2985 - 8s - loss: 9.0393e-04 - val_loss: 0.0018 - 8s/epoch - 3ms/step
Epoch 93/100
2985/2985 - 8s - loss: 9.1407e-04 - val_loss: 0.0018 - 8s/epoch - 3ms/step
Epoch 94/100
2985/2985 - 8s - loss: 8.9440e-04 - val_loss: 0.0017 - 8s/epoch - 3ms/step
Epoch 95/100
2985/2985 - 8s - loss: 8.9615e-04 - val_loss: 0.0017 - 8s/epoch - 3ms/step
Epoch 96/100
2985/2985 - 8s - loss: 8.8603e-04 - val_loss: 0.0027 - 8s/epoch - 3ms/step
Epoch 97/100
2985/2985 - 8s - loss: 9.1805e-04 - val_loss: 0.0017 - 8s/epoch - 3ms/step
Epoch 98/100
2985/2985 - 8s - loss: 8.9658e-04 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 99/100
2985/2985 - 8s - loss: 8.8377e-04 - val_loss: 0.0016 - 8s/epoch - 3ms/step
Epoch 100/100
2985/2985 - 8s - loss: 8.8320e-04 - val_loss: 0.0016 - 8s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0015559477033093572
  1/332 [..............................] - ETA: 31s 51/332 [===>..........................] - ETA: 0s 102/332 [========>.....................] - ETA: 0s152/332 [============>.................] - ETA: 0s204/332 [=================>............] - ETA: 0s255/332 [======================>.......] - ETA: 0s305/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 996us/step
correlation 0.013080040306762799
cosine 0.010961416399663141
MAE: 0.017845223
RMSE: 0.039445475
r2: 0.8990624588653418
RMSE zero-vector: 0.23411466903540806
['default', 32, 100, 0.005, 'mse', 0.5, 632, 0.0008832018938846886, 0.0015559477033093572, 0.013080040306762799, 0.010961416399663141, 0.01784522272646427, 0.03944547474384308, 0.8990624588653418, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_42"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_43 (InputLayer)       multiple                  0         
                                                                 
 dense_42 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_42 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_42 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_43 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_43 (ReLU)             (None, 632)               0         
                                                                 
 dense_43 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_44 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_44 (ReLU)             (None, 2528)              0         
                                                                 
 dense_44 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_43 (InputLayer)       multiple                  0         
                                                                 
 dense_42 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_42 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_42 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_44"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_45 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_43 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_43 (ReLU)             (None, 632)               0         
                                                                 
 dense_43 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_44 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_44 (ReLU)             (None, 2528)              0         
                                                                 
 dense_44 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_45"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_46 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_45 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_45 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_45 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_46 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_46 (ReLU)             (None, 632)               0         
                                                                 
 dense_46 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_47 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_47 (ReLU)             (None, 2528)              0         
                                                                 
 dense_47 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
2985/2985 - 9s - loss: 0.0081 - val_loss: 0.0034 - 9s/epoch - 3ms/step
Epoch 2/200
2985/2985 - 8s - loss: 0.0029 - val_loss: 0.0021 - 8s/epoch - 3ms/step
Epoch 3/200
2985/2985 - 8s - loss: 0.0024 - val_loss: 0.0016 - 8s/epoch - 3ms/step
Epoch 4/200
2985/2985 - 8s - loss: 0.0019 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 5/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 9.5503e-04 - 8s/epoch - 3ms/step
Epoch 6/200
2985/2985 - 8s - loss: 0.0012 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 7/200
2985/2985 - 8s - loss: 0.0010 - val_loss: 7.3891e-04 - 8s/epoch - 3ms/step
Epoch 8/200
2985/2985 - 8s - loss: 8.6389e-04 - val_loss: 6.5370e-04 - 8s/epoch - 3ms/step
Epoch 9/200
2985/2985 - 8s - loss: 7.6588e-04 - val_loss: 5.5634e-04 - 8s/epoch - 3ms/step
Epoch 10/200
2985/2985 - 8s - loss: 6.8253e-04 - val_loss: 5.4970e-04 - 8s/epoch - 3ms/step
Epoch 11/200
2985/2985 - 8s - loss: 6.1941e-04 - val_loss: 5.0436e-04 - 8s/epoch - 3ms/step
Epoch 12/200
2985/2985 - 8s - loss: 5.6628e-04 - val_loss: 8.7669e-04 - 8s/epoch - 3ms/step
Epoch 13/200
2985/2985 - 8s - loss: 5.5080e-04 - val_loss: 4.4720e-04 - 8s/epoch - 3ms/step
Epoch 14/200
2985/2985 - 8s - loss: 4.9877e-04 - val_loss: 3.9899e-04 - 8s/epoch - 3ms/step
Epoch 15/200
2985/2985 - 8s - loss: 4.7777e-04 - val_loss: 3.9716e-04 - 8s/epoch - 3ms/step
Epoch 16/200
2985/2985 - 8s - loss: 4.4615e-04 - val_loss: 3.7962e-04 - 8s/epoch - 3ms/step
Epoch 17/200
2985/2985 - 8s - loss: 4.2709e-04 - val_loss: 3.6583e-04 - 8s/epoch - 3ms/step
Epoch 18/200
2985/2985 - 8s - loss: 4.0688e-04 - val_loss: 4.2008e-04 - 8s/epoch - 3ms/step
Epoch 19/200
2985/2985 - 8s - loss: 3.9553e-04 - val_loss: 3.5473e-04 - 8s/epoch - 3ms/step
Epoch 20/200
2985/2985 - 8s - loss: 3.9698e-04 - val_loss: 3.1568e-04 - 8s/epoch - 3ms/step
Epoch 21/200
2985/2985 - 8s - loss: 3.8214e-04 - val_loss: 3.2429e-04 - 8s/epoch - 3ms/step
Epoch 22/200
2985/2985 - 8s - loss: 3.6490e-04 - val_loss: 4.6943e-04 - 8s/epoch - 3ms/step
Epoch 23/200
2985/2985 - 8s - loss: 3.8306e-04 - val_loss: 2.9054e-04 - 8s/epoch - 3ms/step
Epoch 24/200
2985/2985 - 8s - loss: 3.4411e-04 - val_loss: 3.0621e-04 - 8s/epoch - 3ms/step
Epoch 25/200
2985/2985 - 8s - loss: 3.3456e-04 - val_loss: 2.9789e-04 - 8s/epoch - 3ms/step
Epoch 26/200
2985/2985 - 8s - loss: 3.2502e-04 - val_loss: 2.5810e-04 - 8s/epoch - 3ms/step
Epoch 27/200
2985/2985 - 8s - loss: 3.2444e-04 - val_loss: 2.9385e-04 - 8s/epoch - 3ms/step
Epoch 28/200
2985/2985 - 8s - loss: 3.1500e-04 - val_loss: 2.5366e-04 - 8s/epoch - 3ms/step
Epoch 29/200
2985/2985 - 8s - loss: 3.0394e-04 - val_loss: 2.4949e-04 - 8s/epoch - 3ms/step
Epoch 30/200
2985/2985 - 8s - loss: 3.0132e-04 - val_loss: 2.5574e-04 - 8s/epoch - 3ms/step
Epoch 31/200
2985/2985 - 8s - loss: 2.9603e-04 - val_loss: 2.4909e-04 - 8s/epoch - 3ms/step
Epoch 32/200
2985/2985 - 8s - loss: 2.8909e-04 - val_loss: 2.2874e-04 - 8s/epoch - 3ms/step
Epoch 33/200
2985/2985 - 8s - loss: 2.9076e-04 - val_loss: 2.7846e-04 - 8s/epoch - 3ms/step
Epoch 34/200
2985/2985 - 8s - loss: 2.8503e-04 - val_loss: 2.3346e-04 - 8s/epoch - 3ms/step
Epoch 35/200
2985/2985 - 8s - loss: 2.7426e-04 - val_loss: 2.3206e-04 - 8s/epoch - 3ms/step
Epoch 36/200
2985/2985 - 8s - loss: 2.6971e-04 - val_loss: 2.3048e-04 - 8s/epoch - 3ms/step
Epoch 37/200
2985/2985 - 8s - loss: 2.7316e-04 - val_loss: 2.3850e-04 - 8s/epoch - 3ms/step
Epoch 38/200
2985/2985 - 8s - loss: 2.6200e-04 - val_loss: 2.2664e-04 - 8s/epoch - 3ms/step
Epoch 39/200
2985/2985 - 8s - loss: 2.6181e-04 - val_loss: 2.0772e-04 - 8s/epoch - 3ms/step
Epoch 40/200
2985/2985 - 8s - loss: 2.6106e-04 - val_loss: 3.8608e-04 - 8s/epoch - 3ms/step
Epoch 41/200
2985/2985 - 8s - loss: 2.7508e-04 - val_loss: 2.1028e-04 - 8s/epoch - 3ms/step
Epoch 42/200
2985/2985 - 8s - loss: 2.4682e-04 - val_loss: 2.0666e-04 - 8s/epoch - 3ms/step
Epoch 43/200
2985/2985 - 8s - loss: 2.4559e-04 - val_loss: 2.1568e-04 - 8s/epoch - 3ms/step
Epoch 44/200
2985/2985 - 8s - loss: 2.4759e-04 - val_loss: 3.4850e-04 - 8s/epoch - 3ms/step
Epoch 45/200
2985/2985 - 8s - loss: 2.5603e-04 - val_loss: 2.0718e-04 - 8s/epoch - 3ms/step
Epoch 46/200
2985/2985 - 8s - loss: 2.4290e-04 - val_loss: 2.0874e-04 - 8s/epoch - 3ms/step
Epoch 47/200
2985/2985 - 8s - loss: 2.3528e-04 - val_loss: 2.2172e-04 - 8s/epoch - 3ms/step
Epoch 48/200
2985/2985 - 8s - loss: 2.5277e-04 - val_loss: 1.9769e-04 - 8s/epoch - 3ms/step
Epoch 49/200
2985/2985 - 8s - loss: 2.3065e-04 - val_loss: 2.2430e-04 - 8s/epoch - 3ms/step
Epoch 50/200
2985/2985 - 8s - loss: 2.3787e-04 - val_loss: 3.0081e-04 - 8s/epoch - 3ms/step
Epoch 51/200
2985/2985 - 8s - loss: 2.4404e-04 - val_loss: 1.9315e-04 - 8s/epoch - 3ms/step
Epoch 52/200
2985/2985 - 8s - loss: 2.2577e-04 - val_loss: 1.8858e-04 - 8s/epoch - 3ms/step
Epoch 53/200
2985/2985 - 8s - loss: 2.2285e-04 - val_loss: 1.9898e-04 - 8s/epoch - 3ms/step
Epoch 54/200
2985/2985 - 8s - loss: 2.1966e-04 - val_loss: 1.8630e-04 - 8s/epoch - 3ms/step
Epoch 55/200
2985/2985 - 8s - loss: 2.1531e-04 - val_loss: 1.8513e-04 - 8s/epoch - 3ms/step
Epoch 56/200
2985/2985 - 8s - loss: 2.1658e-04 - val_loss: 1.8720e-04 - 8s/epoch - 3ms/step
Epoch 57/200
2985/2985 - 8s - loss: 2.1514e-04 - val_loss: 1.9586e-04 - 8s/epoch - 3ms/step
Epoch 58/200
2985/2985 - 8s - loss: 2.1507e-04 - val_loss: 1.7933e-04 - 8s/epoch - 3ms/step
Epoch 59/200
2985/2985 - 8s - loss: 2.1104e-04 - val_loss: 1.8570e-04 - 8s/epoch - 3ms/step
Epoch 60/200
2985/2985 - 8s - loss: 2.0986e-04 - val_loss: 1.7504e-04 - 8s/epoch - 3ms/step
Epoch 61/200
2985/2985 - 8s - loss: 2.0633e-04 - val_loss: 1.6613e-04 - 8s/epoch - 3ms/step
Epoch 62/200
2985/2985 - 8s - loss: 2.0607e-04 - val_loss: 1.9457e-04 - 8s/epoch - 3ms/step
Epoch 63/200
2985/2985 - 8s - loss: 2.0683e-04 - val_loss: 1.8168e-04 - 8s/epoch - 3ms/step
Epoch 64/200
2985/2985 - 8s - loss: 2.0549e-04 - val_loss: 1.8463e-04 - 8s/epoch - 3ms/step
Epoch 65/200
2985/2985 - 8s - loss: 2.0794e-04 - val_loss: 1.8748e-04 - 8s/epoch - 3ms/step
Epoch 66/200
2985/2985 - 8s - loss: 2.0167e-04 - val_loss: 1.7815e-04 - 8s/epoch - 3ms/step
Epoch 67/200
2985/2985 - 8s - loss: 1.9866e-04 - val_loss: 1.6878e-04 - 8s/epoch - 3ms/step
Epoch 68/200
2985/2985 - 8s - loss: 2.0323e-04 - val_loss: 2.0456e-04 - 8s/epoch - 3ms/step
Epoch 69/200
2985/2985 - 8s - loss: 1.9812e-04 - val_loss: 1.8165e-04 - 8s/epoch - 3ms/step
Epoch 70/200
2985/2985 - 8s - loss: 2.0190e-04 - val_loss: 5.0944e-04 - 8s/epoch - 3ms/step
Epoch 71/200
2985/2985 - 8s - loss: 2.1787e-04 - val_loss: 1.8359e-04 - 8s/epoch - 3ms/step
Epoch 72/200
2985/2985 - 8s - loss: 2.0499e-04 - val_loss: 1.5605e-04 - 8s/epoch - 3ms/step
Epoch 73/200
2985/2985 - 8s - loss: 1.9676e-04 - val_loss: 2.0008e-04 - 8s/epoch - 3ms/step
Epoch 74/200
2985/2985 - 8s - loss: 1.9677e-04 - val_loss: 1.7117e-04 - 8s/epoch - 3ms/step
Epoch 75/200
2985/2985 - 8s - loss: 2.0185e-04 - val_loss: 1.6725e-04 - 8s/epoch - 3ms/step
Epoch 76/200
2985/2985 - 8s - loss: 1.9223e-04 - val_loss: 1.7164e-04 - 8s/epoch - 3ms/step
Epoch 77/200
2985/2985 - 8s - loss: 1.9485e-04 - val_loss: 3.1325e-04 - 8s/epoch - 3ms/step
Epoch 78/200
2985/2985 - 8s - loss: 1.9936e-04 - val_loss: 1.6637e-04 - 8s/epoch - 3ms/step
Epoch 79/200
2985/2985 - 8s - loss: 1.8805e-04 - val_loss: 1.8114e-04 - 8s/epoch - 3ms/step
Epoch 80/200
2985/2985 - 8s - loss: 1.8924e-04 - val_loss: 1.7327e-04 - 8s/epoch - 3ms/step
Epoch 81/200
2985/2985 - 8s - loss: 1.8741e-04 - val_loss: 1.8716e-04 - 8s/epoch - 3ms/step
Epoch 82/200
2985/2985 - 8s - loss: 1.8785e-04 - val_loss: 1.6884e-04 - 8s/epoch - 3ms/step
Epoch 83/200
2985/2985 - 8s - loss: 1.8512e-04 - val_loss: 1.6547e-04 - 8s/epoch - 3ms/step
Epoch 84/200
2985/2985 - 8s - loss: 1.8302e-04 - val_loss: 1.6997e-04 - 8s/epoch - 3ms/step
Epoch 85/200
2985/2985 - 8s - loss: 1.8324e-04 - val_loss: 1.4915e-04 - 8s/epoch - 3ms/step
Epoch 86/200
2985/2985 - 8s - loss: 1.8072e-04 - val_loss: 1.5806e-04 - 8s/epoch - 3ms/step
Epoch 87/200
2985/2985 - 8s - loss: 1.8138e-04 - val_loss: 1.7291e-04 - 8s/epoch - 3ms/step
Epoch 88/200
2985/2985 - 8s - loss: 1.8070e-04 - val_loss: 1.8016e-04 - 8s/epoch - 3ms/step
Epoch 89/200
2985/2985 - 8s - loss: 1.8287e-04 - val_loss: 1.5923e-04 - 8s/epoch - 3ms/step
Epoch 90/200
2985/2985 - 8s - loss: 1.7596e-04 - val_loss: 1.6581e-04 - 8s/epoch - 3ms/step
Epoch 91/200
2985/2985 - 8s - loss: 1.7529e-04 - val_loss: 1.6080e-04 - 8s/epoch - 3ms/step
Epoch 92/200
2985/2985 - 8s - loss: 1.7537e-04 - val_loss: 1.6429e-04 - 8s/epoch - 3ms/step
Epoch 93/200
2985/2985 - 8s - loss: 1.7688e-04 - val_loss: 1.7173e-04 - 8s/epoch - 3ms/step
Epoch 94/200
2985/2985 - 8s - loss: 1.7758e-04 - val_loss: 1.5768e-04 - 8s/epoch - 3ms/step
Epoch 95/200
2985/2985 - 8s - loss: 1.7669e-04 - val_loss: 1.7645e-04 - 8s/epoch - 3ms/step
Epoch 96/200
2985/2985 - 8s - loss: 1.7667e-04 - val_loss: 1.8333e-04 - 8s/epoch - 3ms/step
Epoch 97/200
2985/2985 - 8s - loss: 1.7218e-04 - val_loss: 1.6008e-04 - 8s/epoch - 3ms/step
Epoch 98/200
2985/2985 - 8s - loss: 1.7628e-04 - val_loss: 1.6241e-04 - 8s/epoch - 3ms/step
Epoch 99/200
2985/2985 - 8s - loss: 1.7069e-04 - val_loss: 1.4365e-04 - 8s/epoch - 3ms/step
Epoch 100/200
2985/2985 - 8s - loss: 1.7140e-04 - val_loss: 1.5368e-04 - 8s/epoch - 3ms/step
Epoch 101/200
2985/2985 - 8s - loss: 1.7012e-04 - val_loss: 1.5562e-04 - 8s/epoch - 3ms/step
Epoch 102/200
2985/2985 - 8s - loss: 1.7342e-04 - val_loss: 1.6832e-04 - 8s/epoch - 3ms/step
Epoch 103/200
2985/2985 - 8s - loss: 1.7142e-04 - val_loss: 1.5848e-04 - 8s/epoch - 3ms/step
Epoch 104/200
2985/2985 - 8s - loss: 1.7136e-04 - val_loss: 1.7255e-04 - 8s/epoch - 3ms/step
Epoch 105/200
2985/2985 - 8s - loss: 1.6628e-04 - val_loss: 1.7596e-04 - 8s/epoch - 3ms/step
Epoch 106/200
2985/2985 - 8s - loss: 1.6812e-04 - val_loss: 1.5488e-04 - 8s/epoch - 3ms/step
Epoch 107/200
2985/2985 - 8s - loss: 1.6510e-04 - val_loss: 1.7186e-04 - 8s/epoch - 3ms/step
Epoch 108/200
2985/2985 - 8s - loss: 1.6910e-04 - val_loss: 1.6366e-04 - 8s/epoch - 3ms/step
Epoch 109/200
2985/2985 - 8s - loss: 1.6724e-04 - val_loss: 1.5728e-04 - 8s/epoch - 3ms/step
Epoch 110/200
2985/2985 - 8s - loss: 1.6559e-04 - val_loss: 1.5065e-04 - 8s/epoch - 3ms/step
Epoch 111/200
2985/2985 - 8s - loss: 1.6464e-04 - val_loss: 1.5789e-04 - 8s/epoch - 3ms/step
Epoch 112/200
2985/2985 - 8s - loss: 1.6859e-04 - val_loss: 1.5137e-04 - 8s/epoch - 3ms/step
Epoch 113/200
2985/2985 - 8s - loss: 1.6516e-04 - val_loss: 1.6951e-04 - 8s/epoch - 3ms/step
Epoch 114/200
2985/2985 - 8s - loss: 1.6817e-04 - val_loss: 1.4752e-04 - 8s/epoch - 3ms/step
Epoch 115/200
2985/2985 - 8s - loss: 1.6343e-04 - val_loss: 1.5412e-04 - 8s/epoch - 3ms/step
Epoch 116/200
2985/2985 - 8s - loss: 1.6320e-04 - val_loss: 1.5982e-04 - 8s/epoch - 3ms/step
Epoch 117/200
2985/2985 - 8s - loss: 1.6030e-04 - val_loss: 1.3944e-04 - 8s/epoch - 3ms/step
Epoch 118/200
2985/2985 - 8s - loss: 1.6127e-04 - val_loss: 1.3531e-04 - 8s/epoch - 3ms/step
Epoch 119/200
2985/2985 - 8s - loss: 1.6987e-04 - val_loss: 1.4683e-04 - 8s/epoch - 3ms/step
Epoch 120/200
2985/2985 - 8s - loss: 1.6038e-04 - val_loss: 1.4496e-04 - 8s/epoch - 3ms/step
Epoch 121/200
2985/2985 - 8s - loss: 1.6029e-04 - val_loss: 1.6193e-04 - 8s/epoch - 3ms/step
Epoch 122/200
2985/2985 - 8s - loss: 1.6390e-04 - val_loss: 1.4583e-04 - 8s/epoch - 3ms/step
Epoch 123/200
2985/2985 - 8s - loss: 1.6200e-04 - val_loss: 1.5027e-04 - 8s/epoch - 3ms/step
Epoch 124/200
2985/2985 - 8s - loss: 1.6073e-04 - val_loss: 1.5121e-04 - 8s/epoch - 3ms/step
Epoch 125/200
2985/2985 - 8s - loss: 1.5864e-04 - val_loss: 1.4034e-04 - 8s/epoch - 3ms/step
Epoch 126/200
2985/2985 - 8s - loss: 1.5755e-04 - val_loss: 1.4843e-04 - 8s/epoch - 3ms/step
Epoch 127/200
2985/2985 - 8s - loss: 1.5944e-04 - val_loss: 1.4005e-04 - 8s/epoch - 3ms/step
Epoch 128/200
2985/2985 - 8s - loss: 1.5720e-04 - val_loss: 1.4503e-04 - 8s/epoch - 3ms/step
Epoch 129/200
2985/2985 - 8s - loss: 1.5847e-04 - val_loss: 1.5836e-04 - 8s/epoch - 3ms/step
Epoch 130/200
2985/2985 - 8s - loss: 1.5496e-04 - val_loss: 1.3637e-04 - 8s/epoch - 3ms/step
Epoch 131/200
2985/2985 - 8s - loss: 1.5901e-04 - val_loss: 1.4999e-04 - 8s/epoch - 3ms/step
Epoch 132/200
2985/2985 - 8s - loss: 1.5511e-04 - val_loss: 1.3360e-04 - 8s/epoch - 3ms/step
Epoch 133/200
2985/2985 - 8s - loss: 1.5505e-04 - val_loss: 1.3577e-04 - 8s/epoch - 3ms/step
Epoch 134/200
2985/2985 - 8s - loss: 1.5640e-04 - val_loss: 1.4334e-04 - 8s/epoch - 3ms/step
Epoch 135/200
2985/2985 - 8s - loss: 1.5808e-04 - val_loss: 1.4618e-04 - 8s/epoch - 3ms/step
Epoch 136/200
2985/2985 - 8s - loss: 1.5809e-04 - val_loss: 1.3185e-04 - 8s/epoch - 3ms/step
Epoch 137/200
2985/2985 - 8s - loss: 1.5993e-04 - val_loss: 1.5403e-04 - 8s/epoch - 3ms/step
Epoch 138/200
2985/2985 - 8s - loss: 1.5303e-04 - val_loss: 1.5451e-04 - 8s/epoch - 3ms/step
Epoch 139/200
2985/2985 - 8s - loss: 1.5598e-04 - val_loss: 1.9403e-04 - 8s/epoch - 3ms/step
Epoch 140/200
2985/2985 - 8s - loss: 1.9889e-04 - val_loss: 1.3798e-04 - 8s/epoch - 3ms/step
Epoch 141/200
2985/2985 - 8s - loss: 1.5801e-04 - val_loss: 1.5540e-04 - 8s/epoch - 3ms/step
Epoch 142/200
2985/2985 - 8s - loss: 1.5690e-04 - val_loss: 1.5172e-04 - 8s/epoch - 3ms/step
Epoch 143/200
2985/2985 - 8s - loss: 1.5521e-04 - val_loss: 1.4477e-04 - 8s/epoch - 3ms/step
Epoch 144/200
2985/2985 - 8s - loss: 1.5663e-04 - val_loss: 1.4586e-04 - 8s/epoch - 3ms/step
Epoch 145/200
2985/2985 - 8s - loss: 1.5882e-04 - val_loss: 1.3046e-04 - 8s/epoch - 3ms/step
Epoch 146/200
2985/2985 - 8s - loss: 1.5985e-04 - val_loss: 1.3705e-04 - 8s/epoch - 3ms/step
Epoch 147/200
2985/2985 - 8s - loss: 1.5438e-04 - val_loss: 1.5131e-04 - 8s/epoch - 3ms/step
Epoch 148/200
2985/2985 - 8s - loss: 1.5195e-04 - val_loss: 1.5072e-04 - 8s/epoch - 3ms/step
Epoch 149/200
2985/2985 - 8s - loss: 1.4896e-04 - val_loss: 1.2743e-04 - 8s/epoch - 3ms/step
Epoch 150/200
2985/2985 - 8s - loss: 1.5089e-04 - val_loss: 1.3758e-04 - 8s/epoch - 3ms/step
Epoch 151/200
2985/2985 - 8s - loss: 1.4840e-04 - val_loss: 1.4488e-04 - 8s/epoch - 3ms/step
Epoch 152/200
2985/2985 - 8s - loss: 1.5158e-04 - val_loss: 1.4609e-04 - 8s/epoch - 3ms/step
Epoch 153/200
2985/2985 - 8s - loss: 1.4980e-04 - val_loss: 1.3513e-04 - 8s/epoch - 3ms/step
Epoch 154/200
2985/2985 - 8s - loss: 1.5034e-04 - val_loss: 1.3150e-04 - 8s/epoch - 3ms/step
Epoch 155/200
2985/2985 - 8s - loss: 1.4711e-04 - val_loss: 1.3214e-04 - 8s/epoch - 3ms/step
Epoch 156/200
2985/2985 - 8s - loss: 1.4669e-04 - val_loss: 1.4178e-04 - 8s/epoch - 3ms/step
Epoch 157/200
2985/2985 - 8s - loss: 1.4463e-04 - val_loss: 1.4309e-04 - 8s/epoch - 3ms/step
Epoch 158/200
2985/2985 - 8s - loss: 1.4982e-04 - val_loss: 1.4088e-04 - 8s/epoch - 3ms/step
Epoch 159/200
2985/2985 - 8s - loss: 1.4774e-04 - val_loss: 1.2679e-04 - 8s/epoch - 3ms/step
Epoch 160/200
2985/2985 - 8s - loss: 1.5230e-04 - val_loss: 1.3099e-04 - 8s/epoch - 3ms/step
Epoch 161/200
2985/2985 - 8s - loss: 1.5125e-04 - val_loss: 1.3511e-04 - 8s/epoch - 3ms/step
Epoch 162/200
2985/2985 - 8s - loss: 1.5018e-04 - val_loss: 1.3069e-04 - 8s/epoch - 3ms/step
Epoch 163/200
2985/2985 - 8s - loss: 1.4505e-04 - val_loss: 1.3512e-04 - 8s/epoch - 3ms/step
Epoch 164/200
2985/2985 - 8s - loss: 1.5109e-04 - val_loss: 1.8927e-04 - 8s/epoch - 3ms/step
Epoch 165/200
2985/2985 - 8s - loss: 1.4734e-04 - val_loss: 1.3871e-04 - 8s/epoch - 3ms/step
Epoch 166/200
2985/2985 - 8s - loss: 1.4467e-04 - val_loss: 1.5470e-04 - 8s/epoch - 3ms/step
Epoch 167/200
2985/2985 - 8s - loss: 1.4944e-04 - val_loss: 1.2510e-04 - 8s/epoch - 3ms/step
Epoch 168/200
2985/2985 - 8s - loss: 1.4599e-04 - val_loss: 1.4191e-04 - 8s/epoch - 3ms/step
Epoch 169/200
2985/2985 - 8s - loss: 1.4422e-04 - val_loss: 1.5104e-04 - 8s/epoch - 3ms/step
Epoch 170/200
2985/2985 - 8s - loss: 1.4798e-04 - val_loss: 1.9918e-04 - 8s/epoch - 3ms/step
Epoch 171/200
2985/2985 - 8s - loss: 1.6226e-04 - val_loss: 1.5057e-04 - 8s/epoch - 3ms/step
Epoch 172/200
2985/2985 - 8s - loss: 1.4450e-04 - val_loss: 1.2699e-04 - 8s/epoch - 3ms/step
Epoch 173/200
2985/2985 - 8s - loss: 1.4412e-04 - val_loss: 1.3501e-04 - 8s/epoch - 3ms/step
Epoch 174/200
2985/2985 - 8s - loss: 1.4388e-04 - val_loss: 1.1742e-04 - 8s/epoch - 3ms/step
Epoch 175/200
2985/2985 - 8s - loss: 1.4331e-04 - val_loss: 1.2891e-04 - 8s/epoch - 3ms/step
Epoch 176/200
2985/2985 - 8s - loss: 1.4532e-04 - val_loss: 1.2697e-04 - 8s/epoch - 3ms/step
Epoch 177/200
2985/2985 - 8s - loss: 1.4514e-04 - val_loss: 1.2051e-04 - 8s/epoch - 3ms/step
Epoch 178/200
2985/2985 - 8s - loss: 1.4249e-04 - val_loss: 1.2177e-04 - 8s/epoch - 3ms/step
Epoch 179/200
2985/2985 - 8s - loss: 1.4238e-04 - val_loss: 1.3431e-04 - 8s/epoch - 3ms/step
Epoch 180/200
2985/2985 - 8s - loss: 1.5091e-04 - val_loss: 1.6042e-04 - 8s/epoch - 3ms/step
Epoch 181/200
2985/2985 - 8s - loss: 1.4603e-04 - val_loss: 1.2143e-04 - 8s/epoch - 3ms/step
Epoch 182/200
2985/2985 - 8s - loss: 1.4366e-04 - val_loss: 1.7826e-04 - 8s/epoch - 3ms/step
Epoch 183/200
2985/2985 - 8s - loss: 1.4134e-04 - val_loss: 1.3591e-04 - 8s/epoch - 3ms/step
Epoch 184/200
2985/2985 - 8s - loss: 1.4255e-04 - val_loss: 1.2663e-04 - 8s/epoch - 3ms/step
Epoch 185/200
2985/2985 - 8s - loss: 1.4139e-04 - val_loss: 1.5594e-04 - 8s/epoch - 3ms/step
Epoch 186/200
2985/2985 - 8s - loss: 1.4528e-04 - val_loss: 1.2966e-04 - 8s/epoch - 3ms/step
Epoch 187/200
2985/2985 - 8s - loss: 1.4236e-04 - val_loss: 1.4068e-04 - 8s/epoch - 3ms/step
Epoch 188/200
2985/2985 - 8s - loss: 1.4164e-04 - val_loss: 1.2367e-04 - 8s/epoch - 3ms/step
Epoch 189/200
2985/2985 - 8s - loss: 1.4261e-04 - val_loss: 1.3008e-04 - 8s/epoch - 3ms/step
Epoch 190/200
2985/2985 - 8s - loss: 1.4039e-04 - val_loss: 1.2524e-04 - 8s/epoch - 3ms/step
Epoch 191/200
2985/2985 - 8s - loss: 1.3923e-04 - val_loss: 1.5924e-04 - 8s/epoch - 3ms/step
Epoch 192/200
2985/2985 - 8s - loss: 1.4449e-04 - val_loss: 1.5084e-04 - 8s/epoch - 3ms/step
Epoch 193/200
2985/2985 - 8s - loss: 1.4766e-04 - val_loss: 1.2480e-04 - 8s/epoch - 3ms/step
Epoch 194/200
2985/2985 - 8s - loss: 1.4181e-04 - val_loss: 2.0931e-04 - 8s/epoch - 3ms/step
Epoch 195/200
2985/2985 - 8s - loss: 1.4539e-04 - val_loss: 1.2686e-04 - 8s/epoch - 3ms/step
Epoch 196/200
2985/2985 - 8s - loss: 1.3788e-04 - val_loss: 1.2688e-04 - 8s/epoch - 3ms/step
Epoch 197/200
2985/2985 - 8s - loss: 1.3995e-04 - val_loss: 1.2277e-04 - 8s/epoch - 3ms/step
Epoch 198/200
2985/2985 - 8s - loss: 1.3871e-04 - val_loss: 1.6586e-04 - 8s/epoch - 3ms/step
Epoch 199/200
2985/2985 - 8s - loss: 1.3698e-04 - val_loss: 1.2241e-04 - 8s/epoch - 3ms/step
Epoch 200/200
2985/2985 - 8s - loss: 1.4140e-04 - val_loss: 1.2987e-04 - 8s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00012987280206289142
  1/332 [..............................] - ETA: 29s 51/332 [===>..........................] - ETA: 0s 102/332 [========>.....................] - ETA: 0s153/332 [============>.................] - ETA: 0s204/332 [=================>............] - ETA: 0s256/332 [======================>.......] - ETA: 0s307/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 989us/step
correlation 0.0014599500918361407
cosine 0.00115018359589388
MAE: 0.0060347207
RMSE: 0.011396169
r2: 0.991575851497826
RMSE zero-vector: 0.23411466903540806
['default', 32, 200, 0.0005, 'mse', 0.5, 632, 0.00014140411803964525, 0.00012987280206289142, 0.0014599500918361407, 0.00115018359589388, 0.006034720689058304, 0.011396168731153011, 0.991575851497826, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_45"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_46 (InputLayer)       multiple                  0         
                                                                 
 dense_45 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_45 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_45 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_46 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_46 (ReLU)             (None, 632)               0         
                                                                 
 dense_46 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_47 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_47 (ReLU)             (None, 2528)              0         
                                                                 
 dense_47 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_46"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_47 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_46 (InputLayer)       multiple                  0         
                                                                 
 dense_45 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_45 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_45 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_47"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_48 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_46 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_46 (ReLU)             (None, 632)               0         
                                                                 
 dense_46 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_47 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_47 (ReLU)             (None, 2528)              0         
                                                                 
 dense_47 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_48"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_49 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_48 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_48 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_48 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_49 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_49 (ReLU)             (None, 632)               0         
                                                                 
 dense_49 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_50 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_50 (ReLU)             (None, 2528)              0         
                                                                 
 dense_50 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
2985/2985 - 9s - loss: 0.0076 - val_loss: 0.0032 - 9s/epoch - 3ms/step
Epoch 2/200
2985/2985 - 8s - loss: 0.0029 - val_loss: 0.0018 - 8s/epoch - 3ms/step
Epoch 3/200
2985/2985 - 8s - loss: 0.0020 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 4/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 9.8633e-04 - 8s/epoch - 3ms/step
Epoch 5/200
2985/2985 - 8s - loss: 0.0011 - val_loss: 8.1509e-04 - 8s/epoch - 3ms/step
Epoch 6/200
2985/2985 - 8s - loss: 8.7170e-04 - val_loss: 6.9170e-04 - 8s/epoch - 3ms/step
Epoch 7/200
2985/2985 - 8s - loss: 7.5747e-04 - val_loss: 5.8518e-04 - 8s/epoch - 3ms/step
Epoch 8/200
2985/2985 - 8s - loss: 6.8745e-04 - val_loss: 5.7301e-04 - 8s/epoch - 3ms/step
Epoch 9/200
2985/2985 - 8s - loss: 6.2067e-04 - val_loss: 5.0577e-04 - 8s/epoch - 3ms/step
Epoch 10/200
2985/2985 - 8s - loss: 5.8097e-04 - val_loss: 4.5370e-04 - 8s/epoch - 3ms/step
Epoch 11/200
2985/2985 - 8s - loss: 5.5333e-04 - val_loss: 4.4553e-04 - 8s/epoch - 3ms/step
Epoch 12/200
2985/2985 - 8s - loss: 5.3584e-04 - val_loss: 4.5333e-04 - 8s/epoch - 3ms/step
Epoch 13/200
2985/2985 - 8s - loss: 5.0648e-04 - val_loss: 4.3288e-04 - 8s/epoch - 3ms/step
Epoch 14/200
2985/2985 - 8s - loss: 4.7447e-04 - val_loss: 3.7271e-04 - 8s/epoch - 3ms/step
Epoch 15/200
2985/2985 - 8s - loss: 4.5633e-04 - val_loss: 3.7387e-04 - 8s/epoch - 3ms/step
Epoch 16/200
2985/2985 - 8s - loss: 4.3353e-04 - val_loss: 3.8351e-04 - 8s/epoch - 3ms/step
Epoch 17/200
2985/2985 - 8s - loss: 4.2852e-04 - val_loss: 3.4520e-04 - 8s/epoch - 3ms/step
Epoch 18/200
2985/2985 - 8s - loss: 4.0730e-04 - val_loss: 3.4361e-04 - 8s/epoch - 3ms/step
Epoch 19/200
2985/2985 - 8s - loss: 4.2437e-04 - val_loss: 3.5696e-04 - 8s/epoch - 3ms/step
Epoch 20/200
2985/2985 - 8s - loss: 3.9680e-04 - val_loss: 3.1526e-04 - 8s/epoch - 3ms/step
Epoch 21/200
2985/2985 - 8s - loss: 3.7929e-04 - val_loss: 3.1308e-04 - 8s/epoch - 3ms/step
Epoch 22/200
2985/2985 - 8s - loss: 3.7443e-04 - val_loss: 4.1212e-04 - 8s/epoch - 3ms/step
Epoch 23/200
2985/2985 - 8s - loss: 3.9165e-04 - val_loss: 2.9173e-04 - 8s/epoch - 3ms/step
Epoch 24/200
2985/2985 - 8s - loss: 3.5491e-04 - val_loss: 3.0397e-04 - 8s/epoch - 3ms/step
Epoch 25/200
2985/2985 - 8s - loss: 3.5961e-04 - val_loss: 2.8318e-04 - 8s/epoch - 3ms/step
Epoch 26/200
2985/2985 - 8s - loss: 3.4786e-04 - val_loss: 2.6122e-04 - 8s/epoch - 3ms/step
Epoch 27/200
2985/2985 - 8s - loss: 3.3695e-04 - val_loss: 2.6762e-04 - 8s/epoch - 3ms/step
Epoch 28/200
2985/2985 - 8s - loss: 3.2884e-04 - val_loss: 2.6417e-04 - 8s/epoch - 3ms/step
Epoch 29/200
2985/2985 - 8s - loss: 3.2789e-04 - val_loss: 2.6016e-04 - 8s/epoch - 3ms/step
Epoch 30/200
2985/2985 - 8s - loss: 3.1619e-04 - val_loss: 2.6501e-04 - 8s/epoch - 3ms/step
Epoch 31/200
2985/2985 - 8s - loss: 3.1583e-04 - val_loss: 2.4528e-04 - 8s/epoch - 3ms/step
Epoch 32/200
2985/2985 - 8s - loss: 3.0721e-04 - val_loss: 2.3881e-04 - 8s/epoch - 3ms/step
Epoch 33/200
2985/2985 - 8s - loss: 3.0334e-04 - val_loss: 2.3662e-04 - 8s/epoch - 3ms/step
Epoch 34/200
2985/2985 - 8s - loss: 3.0330e-04 - val_loss: 2.3737e-04 - 8s/epoch - 3ms/step
Epoch 35/200
2985/2985 - 8s - loss: 3.1269e-04 - val_loss: 2.5468e-04 - 8s/epoch - 3ms/step
Epoch 36/200
2985/2985 - 8s - loss: 3.0757e-04 - val_loss: 2.4404e-04 - 8s/epoch - 3ms/step
Epoch 37/200
2985/2985 - 8s - loss: 2.9566e-04 - val_loss: 2.3675e-04 - 8s/epoch - 3ms/step
Epoch 38/200
2985/2985 - 8s - loss: 2.8731e-04 - val_loss: 2.2194e-04 - 8s/epoch - 3ms/step
Epoch 39/200
2985/2985 - 8s - loss: 2.8399e-04 - val_loss: 2.2634e-04 - 8s/epoch - 3ms/step
Epoch 40/200
2985/2985 - 8s - loss: 2.8604e-04 - val_loss: 4.3853e-04 - 8s/epoch - 3ms/step
Epoch 41/200
2985/2985 - 8s - loss: 3.4501e-04 - val_loss: 2.3902e-04 - 8s/epoch - 3ms/step
Epoch 42/200
2985/2985 - 8s - loss: 2.9221e-04 - val_loss: 2.2783e-04 - 8s/epoch - 3ms/step
Epoch 43/200
2985/2985 - 8s - loss: 2.7898e-04 - val_loss: 2.2984e-04 - 8s/epoch - 3ms/step
Epoch 44/200
2985/2985 - 8s - loss: 2.7597e-04 - val_loss: 2.4050e-04 - 8s/epoch - 3ms/step
Epoch 45/200
2985/2985 - 8s - loss: 2.7458e-04 - val_loss: 2.1061e-04 - 8s/epoch - 3ms/step
Epoch 46/200
2985/2985 - 8s - loss: 2.6868e-04 - val_loss: 2.1345e-04 - 8s/epoch - 3ms/step
Epoch 47/200
2985/2985 - 8s - loss: 2.8713e-04 - val_loss: 2.1330e-04 - 8s/epoch - 3ms/step
Epoch 48/200
2985/2985 - 8s - loss: 2.6615e-04 - val_loss: 2.0916e-04 - 8s/epoch - 3ms/step
Epoch 49/200
2985/2985 - 8s - loss: 2.6578e-04 - val_loss: 2.1445e-04 - 8s/epoch - 3ms/step
Epoch 50/200
2985/2985 - 8s - loss: 2.6045e-04 - val_loss: 3.2129e-04 - 8s/epoch - 3ms/step
Epoch 51/200
2985/2985 - 8s - loss: 2.9333e-04 - val_loss: 2.0287e-04 - 8s/epoch - 3ms/step
Epoch 52/200
2985/2985 - 8s - loss: 2.6191e-04 - val_loss: 2.0290e-04 - 8s/epoch - 3ms/step
Epoch 53/200
2985/2985 - 8s - loss: 2.6332e-04 - val_loss: 2.1900e-04 - 8s/epoch - 3ms/step
Epoch 54/200
2985/2985 - 8s - loss: 2.5352e-04 - val_loss: 2.1374e-04 - 8s/epoch - 3ms/step
Epoch 55/200
2985/2985 - 8s - loss: 2.5484e-04 - val_loss: 1.9963e-04 - 8s/epoch - 3ms/step
Epoch 56/200
2985/2985 - 8s - loss: 2.5208e-04 - val_loss: 1.9644e-04 - 8s/epoch - 3ms/step
Epoch 57/200
2985/2985 - 8s - loss: 2.5331e-04 - val_loss: 1.9875e-04 - 8s/epoch - 3ms/step
Epoch 58/200
2985/2985 - 8s - loss: 2.4888e-04 - val_loss: 1.9483e-04 - 8s/epoch - 3ms/step
Epoch 59/200
2985/2985 - 8s - loss: 2.4574e-04 - val_loss: 1.9040e-04 - 8s/epoch - 3ms/step
Epoch 60/200
2985/2985 - 8s - loss: 2.4295e-04 - val_loss: 1.8762e-04 - 8s/epoch - 3ms/step
Epoch 61/200
2985/2985 - 8s - loss: 2.4093e-04 - val_loss: 1.9336e-04 - 8s/epoch - 3ms/step
Epoch 62/200
2985/2985 - 8s - loss: 2.4269e-04 - val_loss: 1.9907e-04 - 8s/epoch - 3ms/step
Epoch 63/200
2985/2985 - 8s - loss: 2.4913e-04 - val_loss: 2.2557e-04 - 8s/epoch - 3ms/step
Epoch 64/200
2985/2985 - 8s - loss: 2.4291e-04 - val_loss: 1.8149e-04 - 8s/epoch - 3ms/step
Epoch 65/200
2985/2985 - 8s - loss: 2.3877e-04 - val_loss: 1.9717e-04 - 8s/epoch - 3ms/step
Epoch 66/200
2985/2985 - 8s - loss: 2.3525e-04 - val_loss: 1.9006e-04 - 8s/epoch - 3ms/step
Epoch 67/200
2985/2985 - 8s - loss: 2.3710e-04 - val_loss: 1.7915e-04 - 8s/epoch - 3ms/step
Epoch 68/200
2985/2985 - 8s - loss: 2.3281e-04 - val_loss: 2.0014e-04 - 8s/epoch - 3ms/step
Epoch 69/200
2985/2985 - 8s - loss: 2.6976e-04 - val_loss: 1.8221e-04 - 8s/epoch - 3ms/step
Epoch 70/200
2985/2985 - 8s - loss: 2.3463e-04 - val_loss: 1.9060e-04 - 8s/epoch - 3ms/step
Epoch 71/200
2985/2985 - 8s - loss: 2.3029e-04 - val_loss: 1.8354e-04 - 8s/epoch - 3ms/step
Epoch 72/200
2985/2985 - 8s - loss: 2.3025e-04 - val_loss: 1.7627e-04 - 8s/epoch - 3ms/step
Epoch 73/200
2985/2985 - 8s - loss: 2.4821e-04 - val_loss: 1.8289e-04 - 8s/epoch - 3ms/step
Epoch 74/200
2985/2985 - 8s - loss: 2.2892e-04 - val_loss: 1.7994e-04 - 8s/epoch - 3ms/step
Epoch 75/200
2985/2985 - 8s - loss: 2.2538e-04 - val_loss: 1.8039e-04 - 8s/epoch - 3ms/step
Epoch 76/200
2985/2985 - 8s - loss: 2.3240e-04 - val_loss: 1.7599e-04 - 8s/epoch - 3ms/step
Epoch 77/200
2985/2985 - 8s - loss: 2.2724e-04 - val_loss: 4.2157e-04 - 8s/epoch - 3ms/step
Epoch 78/200
2985/2985 - 8s - loss: 2.6471e-04 - val_loss: 1.8453e-04 - 8s/epoch - 3ms/step
Epoch 79/200
2985/2985 - 8s - loss: 2.2570e-04 - val_loss: 1.8255e-04 - 8s/epoch - 3ms/step
Epoch 80/200
2985/2985 - 8s - loss: 2.3046e-04 - val_loss: 1.8096e-04 - 8s/epoch - 3ms/step
Epoch 81/200
2985/2985 - 8s - loss: 2.2291e-04 - val_loss: 2.0587e-04 - 8s/epoch - 3ms/step
Epoch 82/200
2985/2985 - 8s - loss: 2.2018e-04 - val_loss: 1.7101e-04 - 8s/epoch - 3ms/step
Epoch 83/200
2985/2985 - 8s - loss: 2.1963e-04 - val_loss: 1.7721e-04 - 8s/epoch - 3ms/step
Epoch 84/200
2985/2985 - 8s - loss: 2.2002e-04 - val_loss: 1.7109e-04 - 8s/epoch - 3ms/step
Epoch 85/200
2985/2985 - 8s - loss: 2.2304e-04 - val_loss: 2.2154e-04 - 8s/epoch - 3ms/step
Epoch 86/200
2985/2985 - 8s - loss: 2.1895e-04 - val_loss: 1.6659e-04 - 8s/epoch - 3ms/step
Epoch 87/200
2985/2985 - 8s - loss: 2.1670e-04 - val_loss: 1.7784e-04 - 8s/epoch - 3ms/step
Epoch 88/200
2985/2985 - 8s - loss: 2.1941e-04 - val_loss: 1.6332e-04 - 8s/epoch - 3ms/step
Epoch 89/200
2985/2985 - 8s - loss: 2.1831e-04 - val_loss: 1.7471e-04 - 8s/epoch - 3ms/step
Epoch 90/200
2985/2985 - 8s - loss: 2.2179e-04 - val_loss: 1.8285e-04 - 8s/epoch - 3ms/step
Epoch 91/200
2985/2985 - 8s - loss: 2.1620e-04 - val_loss: 1.7550e-04 - 8s/epoch - 3ms/step
Epoch 92/200
2985/2985 - 8s - loss: 2.1476e-04 - val_loss: 2.1661e-04 - 8s/epoch - 3ms/step
Epoch 93/200
2985/2985 - 8s - loss: 2.1220e-04 - val_loss: 1.7764e-04 - 8s/epoch - 3ms/step
Epoch 94/200
2985/2985 - 8s - loss: 2.1123e-04 - val_loss: 1.6013e-04 - 8s/epoch - 3ms/step
Epoch 95/200
2985/2985 - 8s - loss: 2.1397e-04 - val_loss: 1.6355e-04 - 8s/epoch - 3ms/step
Epoch 96/200
2985/2985 - 8s - loss: 2.0922e-04 - val_loss: 1.9382e-04 - 8s/epoch - 3ms/step
Epoch 97/200
2985/2985 - 8s - loss: 2.0917e-04 - val_loss: 1.6256e-04 - 8s/epoch - 3ms/step
Epoch 98/200
2985/2985 - 8s - loss: 2.0948e-04 - val_loss: 1.6218e-04 - 8s/epoch - 3ms/step
Epoch 99/200
2985/2985 - 8s - loss: 2.0567e-04 - val_loss: 1.6531e-04 - 8s/epoch - 3ms/step
Epoch 100/200
2985/2985 - 8s - loss: 2.0513e-04 - val_loss: 1.6338e-04 - 8s/epoch - 3ms/step
Epoch 101/200
2985/2985 - 8s - loss: 2.0418e-04 - val_loss: 1.5819e-04 - 8s/epoch - 3ms/step
Epoch 102/200
2985/2985 - 8s - loss: 2.1629e-04 - val_loss: 1.7653e-04 - 8s/epoch - 3ms/step
Epoch 103/200
2985/2985 - 8s - loss: 2.1974e-04 - val_loss: 1.6223e-04 - 8s/epoch - 3ms/step
Epoch 104/200
2985/2985 - 8s - loss: 2.0996e-04 - val_loss: 2.4292e-04 - 8s/epoch - 3ms/step
Epoch 105/200
2985/2985 - 8s - loss: 2.3048e-04 - val_loss: 1.7472e-04 - 8s/epoch - 3ms/step
Epoch 106/200
2985/2985 - 8s - loss: 2.0786e-04 - val_loss: 1.5779e-04 - 8s/epoch - 3ms/step
Epoch 107/200
2985/2985 - 8s - loss: 2.1173e-04 - val_loss: 1.5544e-04 - 8s/epoch - 3ms/step
Epoch 108/200
2985/2985 - 8s - loss: 2.0566e-04 - val_loss: 1.6369e-04 - 8s/epoch - 3ms/step
Epoch 109/200
2985/2985 - 8s - loss: 2.0000e-04 - val_loss: 1.6395e-04 - 8s/epoch - 3ms/step
Epoch 110/200
2985/2985 - 8s - loss: 2.0875e-04 - val_loss: 1.7105e-04 - 8s/epoch - 3ms/step
Epoch 111/200
2985/2985 - 8s - loss: 1.9971e-04 - val_loss: 1.5228e-04 - 8s/epoch - 3ms/step
Epoch 112/200
2985/2985 - 8s - loss: 2.0080e-04 - val_loss: 1.6521e-04 - 8s/epoch - 3ms/step
Epoch 113/200
2985/2985 - 8s - loss: 2.0023e-04 - val_loss: 2.6234e-04 - 8s/epoch - 3ms/step
Epoch 114/200
2985/2985 - 8s - loss: 2.2554e-04 - val_loss: 1.6084e-04 - 8s/epoch - 3ms/step
Epoch 115/200
2985/2985 - 8s - loss: 2.0135e-04 - val_loss: 1.5542e-04 - 8s/epoch - 3ms/step
Epoch 116/200
2985/2985 - 8s - loss: 1.9940e-04 - val_loss: 1.5604e-04 - 8s/epoch - 3ms/step
Epoch 117/200
2985/2985 - 8s - loss: 1.9665e-04 - val_loss: 1.4766e-04 - 8s/epoch - 3ms/step
Epoch 118/200
2985/2985 - 8s - loss: 1.9809e-04 - val_loss: 1.5663e-04 - 8s/epoch - 3ms/step
Epoch 119/200
2985/2985 - 8s - loss: 1.9550e-04 - val_loss: 1.5646e-04 - 8s/epoch - 3ms/step
Epoch 120/200
2985/2985 - 8s - loss: 1.9550e-04 - val_loss: 1.5787e-04 - 8s/epoch - 3ms/step
Epoch 121/200
2985/2985 - 8s - loss: 1.9480e-04 - val_loss: 1.7778e-04 - 8s/epoch - 3ms/step
Epoch 122/200
2985/2985 - 8s - loss: 1.9775e-04 - val_loss: 1.5289e-04 - 8s/epoch - 3ms/step
Epoch 123/200
2985/2985 - 8s - loss: 2.0104e-04 - val_loss: 1.5693e-04 - 8s/epoch - 3ms/step
Epoch 124/200
2985/2985 - 8s - loss: 1.9320e-04 - val_loss: 1.6583e-04 - 8s/epoch - 3ms/step
Epoch 125/200
2985/2985 - 8s - loss: 1.9407e-04 - val_loss: 1.5815e-04 - 8s/epoch - 3ms/step
Epoch 126/200
2985/2985 - 8s - loss: 1.9576e-04 - val_loss: 1.5236e-04 - 8s/epoch - 3ms/step
Epoch 127/200
2985/2985 - 8s - loss: 1.9181e-04 - val_loss: 1.5096e-04 - 8s/epoch - 3ms/step
Epoch 128/200
2985/2985 - 8s - loss: 1.9837e-04 - val_loss: 1.5449e-04 - 8s/epoch - 3ms/step
Epoch 129/200
2985/2985 - 8s - loss: 1.9740e-04 - val_loss: 1.4602e-04 - 8s/epoch - 3ms/step
Epoch 130/200
2985/2985 - 8s - loss: 1.9430e-04 - val_loss: 1.4883e-04 - 8s/epoch - 3ms/step
Epoch 131/200
2985/2985 - 8s - loss: 1.9055e-04 - val_loss: 1.4953e-04 - 8s/epoch - 3ms/step
Epoch 132/200
2985/2985 - 8s - loss: 1.8852e-04 - val_loss: 1.5251e-04 - 8s/epoch - 3ms/step
Epoch 133/200
2985/2985 - 8s - loss: 1.8970e-04 - val_loss: 1.4822e-04 - 8s/epoch - 3ms/step
Epoch 134/200
2985/2985 - 8s - loss: 1.8961e-04 - val_loss: 1.6654e-04 - 8s/epoch - 3ms/step
Epoch 135/200
2985/2985 - 8s - loss: 1.9150e-04 - val_loss: 1.7538e-04 - 8s/epoch - 3ms/step
Epoch 136/200
2985/2985 - 8s - loss: 1.9422e-04 - val_loss: 1.4404e-04 - 8s/epoch - 3ms/step
Epoch 137/200
2985/2985 - 8s - loss: 1.8765e-04 - val_loss: 1.6837e-04 - 8s/epoch - 3ms/step
Epoch 138/200
2985/2985 - 8s - loss: 1.9232e-04 - val_loss: 1.6025e-04 - 8s/epoch - 3ms/step
Epoch 139/200
2985/2985 - 8s - loss: 1.9478e-04 - val_loss: 1.4454e-04 - 8s/epoch - 3ms/step
Epoch 140/200
2985/2985 - 8s - loss: 1.8802e-04 - val_loss: 1.5848e-04 - 8s/epoch - 3ms/step
Epoch 141/200
2985/2985 - 8s - loss: 1.9643e-04 - val_loss: 1.9378e-04 - 8s/epoch - 3ms/step
Epoch 142/200
2985/2985 - 8s - loss: 2.0686e-04 - val_loss: 1.5483e-04 - 8s/epoch - 3ms/step
Epoch 143/200
2985/2985 - 8s - loss: 1.9457e-04 - val_loss: 1.5237e-04 - 8s/epoch - 3ms/step
Epoch 144/200
2985/2985 - 8s - loss: 1.8963e-04 - val_loss: 1.4372e-04 - 8s/epoch - 3ms/step
Epoch 145/200
2985/2985 - 8s - loss: 1.8780e-04 - val_loss: 1.4681e-04 - 8s/epoch - 3ms/step
Epoch 146/200
2985/2985 - 8s - loss: 1.8482e-04 - val_loss: 1.4738e-04 - 8s/epoch - 3ms/step
Epoch 147/200
2985/2985 - 8s - loss: 1.8459e-04 - val_loss: 1.4529e-04 - 8s/epoch - 3ms/step
Epoch 148/200
2985/2985 - 8s - loss: 1.8634e-04 - val_loss: 1.8261e-04 - 8s/epoch - 3ms/step
Epoch 149/200
2985/2985 - 8s - loss: 1.8535e-04 - val_loss: 1.4312e-04 - 8s/epoch - 3ms/step
Epoch 150/200
2985/2985 - 8s - loss: 2.1191e-04 - val_loss: 1.4797e-04 - 8s/epoch - 3ms/step
Epoch 151/200
2985/2985 - 8s - loss: 1.8779e-04 - val_loss: 1.4488e-04 - 8s/epoch - 3ms/step
Epoch 152/200
2985/2985 - 8s - loss: 1.8486e-04 - val_loss: 1.5702e-04 - 8s/epoch - 3ms/step
Epoch 153/200
2985/2985 - 8s - loss: 1.8495e-04 - val_loss: 1.5289e-04 - 8s/epoch - 3ms/step
Epoch 154/200
2985/2985 - 8s - loss: 1.8605e-04 - val_loss: 1.3991e-04 - 8s/epoch - 3ms/step
Epoch 155/200
2985/2985 - 8s - loss: 1.8255e-04 - val_loss: 1.4905e-04 - 8s/epoch - 3ms/step
Epoch 156/200
2985/2985 - 8s - loss: 1.8015e-04 - val_loss: 1.6208e-04 - 8s/epoch - 3ms/step
Epoch 157/200
2985/2985 - 8s - loss: 1.8030e-04 - val_loss: 1.6681e-04 - 8s/epoch - 3ms/step
Epoch 158/200
2985/2985 - 8s - loss: 1.8953e-04 - val_loss: 1.4263e-04 - 8s/epoch - 3ms/step
Epoch 159/200
2985/2985 - 8s - loss: 1.8016e-04 - val_loss: 1.6854e-04 - 8s/epoch - 3ms/step
Epoch 160/200
2985/2985 - 8s - loss: 1.8257e-04 - val_loss: 1.4051e-04 - 8s/epoch - 3ms/step
Epoch 161/200
2985/2985 - 8s - loss: 1.8752e-04 - val_loss: 1.5298e-04 - 8s/epoch - 3ms/step
Epoch 162/200
2985/2985 - 8s - loss: 1.8088e-04 - val_loss: 1.4649e-04 - 8s/epoch - 3ms/step
Epoch 163/200
2985/2985 - 8s - loss: 1.8347e-04 - val_loss: 1.3684e-04 - 8s/epoch - 3ms/step
Epoch 164/200
2985/2985 - 8s - loss: 1.8342e-04 - val_loss: 1.4130e-04 - 8s/epoch - 3ms/step
Epoch 165/200
2985/2985 - 8s - loss: 1.7913e-04 - val_loss: 1.4654e-04 - 8s/epoch - 3ms/step
Epoch 166/200
2985/2985 - 8s - loss: 2.0119e-04 - val_loss: 1.5037e-04 - 8s/epoch - 3ms/step
Epoch 167/200
2985/2985 - 8s - loss: 1.8923e-04 - val_loss: 1.4431e-04 - 8s/epoch - 3ms/step
Epoch 168/200
2985/2985 - 8s - loss: 1.8085e-04 - val_loss: 1.4941e-04 - 8s/epoch - 3ms/step
Epoch 169/200
2985/2985 - 8s - loss: 1.7830e-04 - val_loss: 1.5738e-04 - 8s/epoch - 3ms/step
Epoch 170/200
2985/2985 - 8s - loss: 1.8519e-04 - val_loss: 1.4078e-04 - 8s/epoch - 3ms/step
Epoch 171/200
2985/2985 - 8s - loss: 1.7756e-04 - val_loss: 1.4904e-04 - 8s/epoch - 3ms/step
Epoch 172/200
2985/2985 - 8s - loss: 1.7595e-04 - val_loss: 1.5313e-04 - 8s/epoch - 3ms/step
Epoch 173/200
2985/2985 - 8s - loss: 1.8268e-04 - val_loss: 1.4868e-04 - 8s/epoch - 3ms/step
Epoch 174/200
2985/2985 - 8s - loss: 1.7927e-04 - val_loss: 1.3849e-04 - 8s/epoch - 3ms/step
Epoch 175/200
2985/2985 - 8s - loss: 1.8541e-04 - val_loss: 1.4357e-04 - 8s/epoch - 3ms/step
Epoch 176/200
2985/2985 - 8s - loss: 1.7793e-04 - val_loss: 1.3490e-04 - 8s/epoch - 3ms/step
Epoch 177/200
2985/2985 - 8s - loss: 1.7577e-04 - val_loss: 1.3093e-04 - 8s/epoch - 3ms/step
Epoch 178/200
2985/2985 - 8s - loss: 1.7544e-04 - val_loss: 1.3788e-04 - 8s/epoch - 3ms/step
Epoch 179/200
2985/2985 - 8s - loss: 1.7643e-04 - val_loss: 1.3850e-04 - 8s/epoch - 3ms/step
Epoch 180/200
2985/2985 - 8s - loss: 1.7681e-04 - val_loss: 1.4473e-04 - 8s/epoch - 3ms/step
Epoch 181/200
2985/2985 - 8s - loss: 1.7587e-04 - val_loss: 1.4314e-04 - 8s/epoch - 3ms/step
Epoch 182/200
2985/2985 - 8s - loss: 1.7878e-04 - val_loss: 3.2313e-04 - 8s/epoch - 3ms/step
Epoch 183/200
2985/2985 - 8s - loss: 2.0326e-04 - val_loss: 1.4455e-04 - 8s/epoch - 3ms/step
Epoch 184/200
2985/2985 - 8s - loss: 1.8172e-04 - val_loss: 1.4575e-04 - 8s/epoch - 3ms/step
Epoch 185/200
2985/2985 - 8s - loss: 1.7957e-04 - val_loss: 2.0004e-04 - 8s/epoch - 3ms/step
Epoch 186/200
2985/2985 - 8s - loss: 1.7808e-04 - val_loss: 1.3331e-04 - 8s/epoch - 3ms/step
Epoch 187/200
2985/2985 - 8s - loss: 1.7572e-04 - val_loss: 1.4281e-04 - 8s/epoch - 3ms/step
Epoch 188/200
2985/2985 - 8s - loss: 1.7284e-04 - val_loss: 1.4146e-04 - 8s/epoch - 3ms/step
Epoch 189/200
2985/2985 - 8s - loss: 1.7945e-04 - val_loss: 1.5534e-04 - 8s/epoch - 3ms/step
Epoch 190/200
2985/2985 - 8s - loss: 1.7571e-04 - val_loss: 1.4035e-04 - 8s/epoch - 3ms/step
Epoch 191/200
2985/2985 - 8s - loss: 1.7626e-04 - val_loss: 1.4632e-04 - 8s/epoch - 3ms/step
Epoch 192/200
2985/2985 - 8s - loss: 1.7373e-04 - val_loss: 1.5295e-04 - 8s/epoch - 3ms/step
Epoch 193/200
2985/2985 - 8s - loss: 1.8406e-04 - val_loss: 1.7074e-04 - 8s/epoch - 3ms/step
Epoch 194/200
2985/2985 - 8s - loss: 1.8240e-04 - val_loss: 2.2120e-04 - 8s/epoch - 3ms/step
Epoch 195/200
2985/2985 - 8s - loss: 1.7995e-04 - val_loss: 1.3659e-04 - 8s/epoch - 3ms/step
Epoch 196/200
2985/2985 - 8s - loss: 1.7156e-04 - val_loss: 1.4701e-04 - 8s/epoch - 3ms/step
Epoch 197/200
2985/2985 - 8s - loss: 1.7477e-04 - val_loss: 1.4627e-04 - 8s/epoch - 3ms/step
Epoch 198/200
2985/2985 - 8s - loss: 1.7219e-04 - val_loss: 1.4148e-04 - 8s/epoch - 3ms/step
Epoch 199/200
2985/2985 - 8s - loss: 1.7049e-04 - val_loss: 1.3744e-04 - 8s/epoch - 3ms/step
Epoch 200/200
2985/2985 - 8s - loss: 1.7130e-04 - val_loss: 1.4431e-04 - 8s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00014430559531319886
  1/332 [..............................] - ETA: 29s 51/332 [===>..........................] - ETA: 0s 101/332 [========>.....................] - ETA: 0s152/332 [============>.................] - ETA: 0s203/332 [=================>............] - ETA: 0s254/332 [=====================>........] - ETA: 0s305/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 992us/step
correlation 0.0016316207476125947
cosine 0.0012890056331697787
MAE: 0.006572011
RMSE: 0.012012716
r2: 0.9906393554413524
RMSE zero-vector: 0.23411466903540806
['default', 32, 200, 0.001, 'mse', 0.5, 632, 0.0001712962839519605, 0.00014430559531319886, 0.0016316207476125947, 0.0012890056331697787, 0.006572010926902294, 0.012012716382741928, 0.9906393554413524, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_48"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_49 (InputLayer)       multiple                  0         
                                                                 
 dense_48 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_48 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_48 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_49 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_49 (ReLU)             (None, 632)               0         
                                                                 
 dense_49 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_50 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_50 (ReLU)             (None, 2528)              0         
                                                                 
 dense_50 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_49"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_50 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_49 (InputLayer)       multiple                  0         
                                                                 
 dense_48 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_48 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_48 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_50"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_51 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_49 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_49 (ReLU)             (None, 632)               0         
                                                                 
 dense_49 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_50 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_50 (ReLU)             (None, 2528)              0         
                                                                 
 dense_50 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_51"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_52 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_51 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_51 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_51 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_52 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_52 (ReLU)             (None, 632)               0         
                                                                 
 dense_52 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_53 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_53 (ReLU)             (None, 2528)              0         
                                                                 
 dense_53 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
2985/2985 - 9s - loss: 0.0248 - val_loss: 0.0036 - 9s/epoch - 3ms/step
Epoch 2/200
2985/2985 - 8s - loss: 0.0058 - val_loss: 0.0058 - 8s/epoch - 3ms/step
Epoch 3/200
2985/2985 - 8s - loss: 0.0049 - val_loss: 0.0035 - 8s/epoch - 3ms/step
Epoch 4/200
2985/2985 - 8s - loss: 0.0034 - val_loss: 0.0025 - 8s/epoch - 3ms/step
Epoch 5/200
2985/2985 - 8s - loss: 0.0026 - val_loss: 0.0021 - 8s/epoch - 3ms/step
Epoch 6/200
2985/2985 - 8s - loss: 0.0023 - val_loss: 0.0020 - 8s/epoch - 3ms/step
Epoch 7/200
2985/2985 - 8s - loss: 0.0022 - val_loss: 0.0017 - 8s/epoch - 3ms/step
Epoch 8/200
2985/2985 - 8s - loss: 0.0020 - val_loss: 0.0017 - 8s/epoch - 3ms/step
Epoch 9/200
2985/2985 - 8s - loss: 0.0019 - val_loss: 0.0018 - 8s/epoch - 3ms/step
Epoch 10/200
2985/2985 - 8s - loss: 0.0018 - val_loss: 0.0016 - 8s/epoch - 3ms/step
Epoch 11/200
2985/2985 - 8s - loss: 0.0017 - val_loss: 0.0015 - 8s/epoch - 3ms/step
Epoch 12/200
2985/2985 - 8s - loss: 0.0017 - val_loss: 0.0015 - 8s/epoch - 3ms/step
Epoch 13/200
2985/2985 - 8s - loss: 0.0017 - val_loss: 0.0017 - 8s/epoch - 3ms/step
Epoch 14/200
2985/2985 - 8s - loss: 0.0017 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 15/200
2985/2985 - 8s - loss: 0.0016 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 16/200
2985/2985 - 8s - loss: 0.0016 - val_loss: 0.0015 - 8s/epoch - 3ms/step
Epoch 17/200
2985/2985 - 8s - loss: 0.0016 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 18/200
2985/2985 - 8s - loss: 0.0016 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 19/200
2985/2985 - 8s - loss: 0.0016 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 20/200
2985/2985 - 8s - loss: 0.0016 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 21/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 22/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 23/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 24/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0015 - 8s/epoch - 3ms/step
Epoch 25/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 26/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 27/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 28/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 29/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 30/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 31/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 32/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 33/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 34/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 35/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 36/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 37/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 38/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 39/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 40/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0033 - 8s/epoch - 3ms/step
Epoch 41/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 42/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 43/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 44/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 45/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 46/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 47/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 48/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 49/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 50/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 51/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 52/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 53/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 54/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 55/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 56/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 57/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 58/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 59/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 60/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 61/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 62/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 63/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 64/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 65/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 66/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 67/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 68/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 69/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 70/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 71/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 72/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0015 - 8s/epoch - 3ms/step
Epoch 73/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 74/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 75/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 76/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 77/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0017 - 8s/epoch - 3ms/step
Epoch 78/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 79/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 80/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 81/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 82/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 83/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 84/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 85/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 86/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 87/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 88/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 89/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 90/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 91/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 92/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 93/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 94/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 95/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 96/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 97/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 98/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 99/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 100/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 101/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 102/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 103/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 104/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 105/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 106/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 107/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 108/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 109/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 110/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 111/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 112/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 113/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0018 - 8s/epoch - 3ms/step
Epoch 114/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 115/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 116/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 117/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 118/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 119/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 120/200
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 121/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 122/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 123/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 124/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 125/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 126/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 127/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 128/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 129/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 130/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 131/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 132/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 133/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 134/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 135/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 136/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 137/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 138/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 139/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 140/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 141/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 142/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 143/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 144/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0018 - 8s/epoch - 3ms/step
Epoch 145/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 146/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 147/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 148/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 149/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 150/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 151/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 152/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 153/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 154/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 155/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 156/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 157/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 158/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 159/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 160/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 161/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 162/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 163/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 164/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0015 - 8s/epoch - 3ms/step
Epoch 165/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 166/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 167/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 168/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 169/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 170/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 171/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 172/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 173/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 174/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 175/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 176/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 177/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 178/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 179/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 180/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 181/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 182/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 183/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 184/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 185/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 186/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 187/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 188/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 189/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 190/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 191/200
2985/2985 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 192/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 193/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 194/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 195/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 196/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 197/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0013 - 8s/epoch - 3ms/step
Epoch 198/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0016 - 8s/epoch - 3ms/step
Epoch 199/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 200/200
2985/2985 - 8s - loss: 0.0013 - val_loss: 0.0013 - 8s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00125995057169348
  1/332 [..............................] - ETA: 31s 51/332 [===>..........................] - ETA: 0s 102/332 [========>.....................] - ETA: 0s153/332 [============>.................] - ETA: 0s202/332 [=================>............] - ETA: 0s253/332 [=====================>........] - ETA: 0s304/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 999us/step
correlation 0.014854710792039325
cosine 0.011870437137055209
MAE: 0.01972702
RMSE: 0.03549577
r2: 0.918264122931648
RMSE zero-vector: 0.23411466903540806
['default', 32, 200, 0.005, 'mse', 0.5, 632, 0.001334169995971024, 0.00125995057169348, 0.014854710792039325, 0.011870437137055209, 0.01972701959311962, 0.03549576923251152, 0.918264122931648, 0.23411466903540806] [<class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'str'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Full AutoEncoder
Model: "model_51"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_52 (InputLayer)       multiple                  0         
                                                                 
 dense_51 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_51 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_51 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_52 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_52 (ReLU)             (None, 632)               0         
                                                                 
 dense_52 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_53 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_53 (ReLU)             (None, 2528)              0         
                                                                 
 dense_53 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_52"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_53 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_52 (InputLayer)       multiple                  0         
                                                                 
 dense_51 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_51 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_51 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_53"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_54 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_52 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_52 (ReLU)             (None, 632)               0         
                                                                 
 dense_52 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_53 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_53 (ReLU)             (None, 2528)              0         
                                                                 
 dense_53 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_54 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_54 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_54 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_55 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_55 (ReLU)             (None, 632)               0         
                                                                 
 dense_55 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_56 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_56 (ReLU)             (None, 2528)              0         
                                                                 
 dense_56 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-26 06:51:33.321741: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 460.48MiB (rounded to 482853120)requested by op _EagerConst
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2022-12-26 06:51:33.321909: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] BFCAllocator dump for GPU_0_bfc
2022-12-26 06:51:33.321936: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (256): 	Total Chunks: 399, Chunks in use: 399. 99.8KiB allocated for chunks. 99.8KiB in use in bin. 2.2KiB client-requested in use in bin.
2022-12-26 06:51:33.321955: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-12-26 06:51:33.321974: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2022-12-26 06:51:33.321994: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2048): 	Total Chunks: 199, Chunks in use: 199. 498.8KiB allocated for chunks. 498.8KiB in use in bin. 491.3KiB client-requested in use in bin.
2022-12-26 06:51:33.322013: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4096): 	Total Chunks: 58, Chunks in use: 58. 287.5KiB allocated for chunks. 287.5KiB in use in bin. 276.5KiB client-requested in use in bin.
2022-12-26 06:51:33.322031: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8192): 	Total Chunks: 399, Chunks in use: 399. 3.92MiB allocated for chunks. 3.92MiB in use in bin. 3.84MiB client-requested in use in bin.
2022-12-26 06:51:33.322050: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16384): 	Total Chunks: 9, Chunks in use: 8. 176.8KiB allocated for chunks. 146.5KiB in use in bin. 79.0KiB client-requested in use in bin.
2022-12-26 06:51:33.322066: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-12-26 06:51:33.322081: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-12-26 06:51:33.322097: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-12-26 06:51:33.322112: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-12-26 06:51:33.322127: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-12-26 06:51:33.322144: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1048576): 	Total Chunks: 1, Chunks in use: 0. 1.20MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-12-26 06:51:33.322159: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-12-26 06:51:33.322179: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4194304): 	Total Chunks: 107, Chunks in use: 105. 652.14MiB allocated for chunks. 639.95MiB in use in bin. 639.95MiB client-requested in use in bin.
2022-12-26 06:51:33.322215: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8388608): 	Total Chunks: 112, Chunks in use: 112. 1.34GiB allocated for chunks. 1.34GiB in use in bin. 1.30GiB client-requested in use in bin.
2022-12-26 06:51:33.322236: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16777216): 	Total Chunks: 5, Chunks in use: 3. 109.57MiB allocated for chunks. 55.99MiB in use in bin. 36.57MiB client-requested in use in bin.
2022-12-26 06:51:33.322254: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (33554432): 	Total Chunks: 21, Chunks in use: 21. 1.05GiB allocated for chunks. 1.05GiB in use in bin. 1.05GiB client-requested in use in bin.
2022-12-26 06:51:33.322270: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-12-26 06:51:33.322286: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-12-26 06:51:33.322304: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (268435456): 	Total Chunks: 38, Chunks in use: 37. 17.02GiB allocated for chunks. 16.64GiB in use in bin. 16.64GiB client-requested in use in bin.
2022-12-26 06:51:33.322325: I tensorflow/core/common_runtime/bfc_allocator.cc:1056] Bin for 460.48MiB was 256.00MiB, Chunk State: 
2022-12-26 06:51:33.322332: I tensorflow/core/common_runtime/bfc_allocator.cc:1062]   Size: 388.21MiB | Requested Size: 158.0KiB | in_use: 0 | bin_num: 20, prev:   Size: 460.48MiB | Requested Size: 460.48MiB | in_use: 1 | bin_num: -1
2022-12-26 06:51:33.322335: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 21639593984
2022-12-26 06:51:33.322340: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e000000 of size 1280 next 1
2022-12-26 06:51:33.322344: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e000500 of size 256 next 2
2022-12-26 06:51:33.322347: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e000600 of size 256 next 3
2022-12-26 06:51:33.322350: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e000700 of size 256 next 5
2022-12-26 06:51:33.322353: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e000800 of size 10240 next 6
2022-12-26 06:51:33.322356: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e003000 of size 256 next 4
2022-12-26 06:51:33.322359: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e003100 of size 10240 next 7
2022-12-26 06:51:33.322361: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e005900 of size 10240 next 10
2022-12-26 06:51:33.322364: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e008100 of size 10240 next 11
2022-12-26 06:51:33.322367: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e00a900 of size 10240 next 12
2022-12-26 06:51:33.322370: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e00d100 of size 256 next 13
2022-12-26 06:51:33.322373: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e00d200 of size 256 next 14
2022-12-26 06:51:33.322376: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e00d300 of size 2560 next 16
2022-12-26 06:51:33.322379: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e00dd00 of size 2560 next 17
2022-12-26 06:51:33.322382: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e00e700 of size 2560 next 15
2022-12-26 06:51:33.322385: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e00f100 of size 2560 next 18
2022-12-26 06:51:33.322388: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e00fb00 of size 2560 next 20
2022-12-26 06:51:33.322392: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5e010500 of size 12769536 next 19
2022-12-26 06:51:33.322395: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5ec3de00 of size 12729344 next 8
2022-12-26 06:51:33.322400: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd5f861a00 of size 12781568 next 9
2022-12-26 06:51:33.322403: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60492200 of size 10240 next 22
2022-12-26 06:51:33.322406: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60494a00 of size 10240 next 21
2022-12-26 06:51:33.322409: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60497200 of size 10240 next 23
2022-12-26 06:51:33.322412: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60499a00 of size 10240 next 26
2022-12-26 06:51:33.322415: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6049c200 of size 10240 next 27
2022-12-26 06:51:33.322418: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6049ea00 of size 5120 next 30
2022-12-26 06:51:33.322421: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6049fe00 of size 256 next 28
2022-12-26 06:51:33.322423: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6049ff00 of size 256 next 29
2022-12-26 06:51:33.322426: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a0000 of size 256 next 33
2022-12-26 06:51:33.322429: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a0100 of size 256 next 34
2022-12-26 06:51:33.322432: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a0200 of size 256 next 37
2022-12-26 06:51:33.322435: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a0300 of size 256 next 38
2022-12-26 06:51:33.322438: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a0400 of size 256 next 39
2022-12-26 06:51:33.322441: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a0500 of size 256 next 40
2022-12-26 06:51:33.322443: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a0600 of size 256 next 41
2022-12-26 06:51:33.322446: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a0700 of size 256 next 42
2022-12-26 06:51:33.322449: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a0800 of size 256 next 43
2022-12-26 06:51:33.322452: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a0900 of size 256 next 44
2022-12-26 06:51:33.322455: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a0a00 of size 10240 next 46
2022-12-26 06:51:33.322458: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a3200 of size 10240 next 47
2022-12-26 06:51:33.322461: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a5a00 of size 10240 next 48
2022-12-26 06:51:33.322464: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a8200 of size 2560 next 49
2022-12-26 06:51:33.322466: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a8c00 of size 2560 next 50
2022-12-26 06:51:33.322469: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604a9600 of size 2560 next 51
2022-12-26 06:51:33.322472: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604aa000 of size 10240 next 53
2022-12-26 06:51:33.322475: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604ac800 of size 10240 next 54
2022-12-26 06:51:33.322478: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604af000 of size 10240 next 55
2022-12-26 06:51:33.322481: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604b1800 of size 5120 next 57
2022-12-26 06:51:33.322484: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604b2c00 of size 10240 next 59
2022-12-26 06:51:33.322489: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604b5400 of size 10240 next 60
2022-12-26 06:51:33.322491: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604b7c00 of size 10240 next 61
2022-12-26 06:51:33.322494: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604ba400 of size 2560 next 62
2022-12-26 06:51:33.322497: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604bae00 of size 2560 next 63
2022-12-26 06:51:33.322500: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604bb800 of size 2560 next 64
2022-12-26 06:51:33.322503: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604bc200 of size 10240 next 66
2022-12-26 06:51:33.322506: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604bea00 of size 10240 next 67
2022-12-26 06:51:33.322509: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c1200 of size 10240 next 68
2022-12-26 06:51:33.322512: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c3a00 of size 5120 next 70
2022-12-26 06:51:33.322514: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c4e00 of size 256 next 71
2022-12-26 06:51:33.322517: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c4f00 of size 256 next 72
2022-12-26 06:51:33.322520: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c5000 of size 256 next 73
2022-12-26 06:51:33.322523: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c5100 of size 256 next 74
2022-12-26 06:51:33.322526: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c5200 of size 256 next 75
2022-12-26 06:51:33.322529: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c5300 of size 256 next 76
2022-12-26 06:51:33.322532: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c5400 of size 256 next 77
2022-12-26 06:51:33.322534: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c5500 of size 256 next 78
2022-12-26 06:51:33.322537: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c5600 of size 256 next 79
2022-12-26 06:51:33.322540: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c5700 of size 256 next 80
2022-12-26 06:51:33.322543: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c5800 of size 256 next 81
2022-12-26 06:51:33.322546: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c5900 of size 256 next 101
2022-12-26 06:51:33.322549: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c5a00 of size 256 next 98
2022-12-26 06:51:33.322552: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c5b00 of size 256 next 96
2022-12-26 06:51:33.322555: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c5c00 of size 10240 next 428
2022-12-26 06:51:33.322557: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604c8400 of size 10240 next 425
2022-12-26 06:51:33.322560: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604cac00 of size 10240 next 390
2022-12-26 06:51:33.322563: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604cd400 of size 10240 next 113
2022-12-26 06:51:33.322566: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604cfc00 of size 10240 next 108
2022-12-26 06:51:33.322569: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604d2400 of size 2560 next 85
2022-12-26 06:51:33.322572: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604d2e00 of size 2560 next 118
2022-12-26 06:51:33.322575: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604d3800 of size 2560 next 91
2022-12-26 06:51:33.322578: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604d4200 of size 2560 next 123
2022-12-26 06:51:33.322582: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604d4c00 of size 2560 next 120
2022-12-26 06:51:33.322585: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604d5600 of size 10240 next 89
2022-12-26 06:51:33.322588: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604d7e00 of size 10240 next 126
2022-12-26 06:51:33.322591: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604da600 of size 10240 next 84
2022-12-26 06:51:33.322594: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604dce00 of size 10240 next 87
2022-12-26 06:51:33.322596: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604df600 of size 10240 next 95
2022-12-26 06:51:33.322599: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604e1e00 of size 5120 next 94
2022-12-26 06:51:33.322602: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604e3200 of size 256 next 115
2022-12-26 06:51:33.322605: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604e3300 of size 256 next 86
2022-12-26 06:51:33.322608: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604e3400 of size 256 next 125
2022-12-26 06:51:33.322611: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604e3500 of size 256 next 124
2022-12-26 06:51:33.322614: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604e3600 of size 10240 next 93
2022-12-26 06:51:33.322617: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604e5e00 of size 10240 next 121
2022-12-26 06:51:33.322620: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604e8600 of size 19968 next 122
2022-12-26 06:51:33.322623: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604ed400 of size 256 next 119
2022-12-26 06:51:33.322626: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604ed500 of size 2560 next 116
2022-12-26 06:51:33.322629: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604edf00 of size 2560 next 97
2022-12-26 06:51:33.322631: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604ee900 of size 2560 next 128
2022-12-26 06:51:33.322634: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604ef300 of size 10240 next 130
2022-12-26 06:51:33.322637: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604f1b00 of size 10240 next 129
2022-12-26 06:51:33.322640: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604f4300 of size 10240 next 127
2022-12-26 06:51:33.322643: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604f6b00 of size 5120 next 132
2022-12-26 06:51:33.322646: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604f7f00 of size 10240 next 134
2022-12-26 06:51:33.322649: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604fa700 of size 10240 next 135
2022-12-26 06:51:33.322652: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604fcf00 of size 10240 next 136
2022-12-26 06:51:33.322655: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd604ff700 of size 2560 next 138
2022-12-26 06:51:33.322657: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60500100 of size 2560 next 139
2022-12-26 06:51:33.322660: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60500b00 of size 2560 next 140
2022-12-26 06:51:33.322663: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60501500 of size 10240 next 141
2022-12-26 06:51:33.322666: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60503d00 of size 10240 next 142
2022-12-26 06:51:33.322669: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60506500 of size 10240 next 143
2022-12-26 06:51:33.322672: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60508d00 of size 5120 next 145
2022-12-26 06:51:33.322678: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050a100 of size 256 next 146
2022-12-26 06:51:33.322681: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050a200 of size 256 next 147
2022-12-26 06:51:33.322684: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050a300 of size 256 next 148
2022-12-26 06:51:33.322687: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050a400 of size 256 next 149
2022-12-26 06:51:33.322690: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050a500 of size 256 next 150
2022-12-26 06:51:33.322693: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050a600 of size 256 next 151
2022-12-26 06:51:33.322696: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050a700 of size 256 next 152
2022-12-26 06:51:33.322698: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050a800 of size 256 next 153
2022-12-26 06:51:33.322701: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050a900 of size 256 next 154
2022-12-26 06:51:33.322704: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050aa00 of size 256 next 155
2022-12-26 06:51:33.322707: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050ab00 of size 256 next 156
2022-12-26 06:51:33.322710: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050ac00 of size 256 next 173
2022-12-26 06:51:33.322713: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050ad00 of size 256 next 191
2022-12-26 06:51:33.322716: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050ae00 of size 256 next 106
2022-12-26 06:51:33.322718: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050af00 of size 10240 next 189
2022-12-26 06:51:33.322721: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050d700 of size 10240 next 496
2022-12-26 06:51:33.322724: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6050ff00 of size 10240 next 501
2022-12-26 06:51:33.322727: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60512700 of size 10240 next 161
2022-12-26 06:51:33.322730: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60514f00 of size 10240 next 168
2022-12-26 06:51:33.322733: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60517700 of size 2560 next 195
2022-12-26 06:51:33.322736: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60518100 of size 2560 next 175
2022-12-26 06:51:33.322739: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60518b00 of size 2560 next 166
2022-12-26 06:51:33.322742: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60519500 of size 2560 next 171
2022-12-26 06:51:33.322744: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60519f00 of size 2560 next 174
2022-12-26 06:51:33.322747: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6051a900 of size 10240 next 169
2022-12-26 06:51:33.322750: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6051d100 of size 10240 next 177
2022-12-26 06:51:33.322753: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6051f900 of size 10240 next 196
2022-12-26 06:51:33.322756: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60522100 of size 12544 next 99
2022-12-26 06:51:33.322759: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60525200 of size 256 next 102
2022-12-26 06:51:33.322762: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60525300 of size 256 next 103
2022-12-26 06:51:33.322765: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60525400 of size 256 next 112
2022-12-26 06:51:33.322768: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60525500 of size 256 next 185
2022-12-26 06:51:33.322772: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60525600 of size 256 next 167
2022-12-26 06:51:33.322775: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60525700 of size 256 next 170
2022-12-26 06:51:33.322778: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60525800 of size 10240 next 158
2022-12-26 06:51:33.322781: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60528000 of size 5120 next 201
2022-12-26 06:51:33.322783: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60529400 of size 256 next 198
2022-12-26 06:51:33.322786: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60529500 of size 256 next 165
2022-12-26 06:51:33.322789: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60529600 of size 256 next 200
2022-12-26 06:51:33.322792: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60529700 of size 256 next 190
2022-12-26 06:51:33.322795: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60529800 of size 10240 next 194
2022-12-26 06:51:33.322798: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6052c000 of size 10240 next 82
2022-12-26 06:51:33.322801: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6052e800 of size 10240 next 164
2022-12-26 06:51:33.322804: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60531000 of size 2560 next 159
2022-12-26 06:51:33.322806: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60531a00 of size 2560 next 197
2022-12-26 06:51:33.322809: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60532400 of size 2560 next 205
2022-12-26 06:51:33.322812: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60532e00 of size 10240 next 160
2022-12-26 06:51:33.322815: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60535600 of size 10240 next 202
2022-12-26 06:51:33.322818: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60537e00 of size 10240 next 203
2022-12-26 06:51:33.322821: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6053a600 of size 5120 next 172
2022-12-26 06:51:33.322824: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6053ba00 of size 10240 next 204
2022-12-26 06:51:33.322827: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6053e200 of size 10240 next 206
2022-12-26 06:51:33.322830: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60540a00 of size 10240 next 207
2022-12-26 06:51:33.322832: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60543200 of size 2560 next 209
2022-12-26 06:51:33.322835: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60543c00 of size 2560 next 210
2022-12-26 06:51:33.322838: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60544600 of size 2560 next 211
2022-12-26 06:51:33.322841: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60545000 of size 10240 next 213
2022-12-26 06:51:33.322844: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60547800 of size 10240 next 214
2022-12-26 06:51:33.322847: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054a000 of size 10240 next 215
2022-12-26 06:51:33.322850: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054c800 of size 5120 next 217
2022-12-26 06:51:33.322853: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054dc00 of size 256 next 218
2022-12-26 06:51:33.322856: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054dd00 of size 256 next 219
2022-12-26 06:51:33.322858: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054de00 of size 256 next 220
2022-12-26 06:51:33.322861: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054df00 of size 256 next 221
2022-12-26 06:51:33.322865: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054e000 of size 256 next 222
2022-12-26 06:51:33.322868: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054e100 of size 256 next 223
2022-12-26 06:51:33.322871: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054e200 of size 256 next 224
2022-12-26 06:51:33.322874: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054e300 of size 256 next 225
2022-12-26 06:51:33.322877: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054e400 of size 256 next 226
2022-12-26 06:51:33.322880: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054e500 of size 256 next 227
2022-12-26 06:51:33.322883: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054e600 of size 256 next 228
2022-12-26 06:51:33.322886: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054e700 of size 256 next 238
2022-12-26 06:51:33.322888: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054e800 of size 256 next 257
2022-12-26 06:51:33.322891: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054e900 of size 256 next 261
2022-12-26 06:51:33.322894: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054ea00 of size 256 next 234
2022-12-26 06:51:33.322897: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054eb00 of size 256 next 236
2022-12-26 06:51:33.322900: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054ec00 of size 256 next 255
2022-12-26 06:51:33.322903: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054ed00 of size 256 next 243
2022-12-26 06:51:33.322906: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6054ee00 of size 10240 next 576
2022-12-26 06:51:33.322908: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60551600 of size 10240 next 233
2022-12-26 06:51:33.322911: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60553e00 of size 10240 next 570
2022-12-26 06:51:33.322914: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60556600 of size 10240 next 272
2022-12-26 06:51:33.322917: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60558e00 of size 10240 next 230
2022-12-26 06:51:33.322920: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6055b600 of size 2560 next 231
2022-12-26 06:51:33.322923: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6055c000 of size 2560 next 240
2022-12-26 06:51:33.322926: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6055ca00 of size 2560 next 269
2022-12-26 06:51:33.322929: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6055d400 of size 2560 next 270
2022-12-26 06:51:33.322932: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6055de00 of size 2560 next 265
2022-12-26 06:51:33.322935: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6055e800 of size 9472 next 162
2022-12-26 06:51:33.322937: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60560d00 of size 256 next 163
2022-12-26 06:51:33.322940: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60560e00 of size 10240 next 263
2022-12-26 06:51:33.322943: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60563600 of size 10240 next 235
2022-12-26 06:51:33.322946: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60565e00 of size 10240 next 242
2022-12-26 06:51:33.322949: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60568600 of size 10240 next 237
2022-12-26 06:51:33.322952: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6056ae00 of size 10240 next 232
2022-12-26 06:51:33.322955: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6056d600 of size 256 next 249
2022-12-26 06:51:33.322959: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6056d700 of size 256 next 247
2022-12-26 06:51:33.322962: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6056d800 of size 256 next 266
2022-12-26 06:51:33.322965: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6056d900 of size 256 next 245
2022-12-26 06:51:33.322967: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6056da00 of size 10240 next 252
2022-12-26 06:51:33.322970: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60570200 of size 10240 next 254
2022-12-26 06:51:33.322973: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60572a00 of size 10240 next 262
2022-12-26 06:51:33.322976: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60575200 of size 2560 next 244
2022-12-26 06:51:33.322979: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60575c00 of size 2560 next 264
2022-12-26 06:51:33.322982: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60576600 of size 2560 next 259
2022-12-26 06:51:33.322985: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60577000 of size 10240 next 271
2022-12-26 06:51:33.322988: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60579800 of size 10240 next 268
2022-12-26 06:51:33.322991: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6057c000 of size 10240 next 276
2022-12-26 06:51:33.322993: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6057e800 of size 5120 next 273
2022-12-26 06:51:33.322996: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6057fc00 of size 10240 next 253
2022-12-26 06:51:33.322999: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60582400 of size 10240 next 274
2022-12-26 06:51:33.323002: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60584c00 of size 10240 next 277
2022-12-26 06:51:33.323005: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60587400 of size 2560 next 279
2022-12-26 06:51:33.323008: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60587e00 of size 2560 next 280
2022-12-26 06:51:33.323011: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60588800 of size 2560 next 281
2022-12-26 06:51:33.323014: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60589200 of size 10240 next 283
2022-12-26 06:51:33.323017: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6058ba00 of size 10240 next 284
2022-12-26 06:51:33.323019: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6058e200 of size 10240 next 285
2022-12-26 06:51:33.323022: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60590a00 of size 5120 next 287
2022-12-26 06:51:33.323025: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60591e00 of size 256 next 288
2022-12-26 06:51:33.323028: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60591f00 of size 256 next 289
2022-12-26 06:51:33.323031: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60592000 of size 256 next 290
2022-12-26 06:51:33.323034: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60592100 of size 256 next 291
2022-12-26 06:51:33.323037: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60592200 of size 256 next 292
2022-12-26 06:51:33.323040: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60592300 of size 256 next 293
2022-12-26 06:51:33.323042: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60592400 of size 256 next 294
2022-12-26 06:51:33.323045: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60592500 of size 256 next 295
2022-12-26 06:51:33.323048: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60592600 of size 256 next 296
2022-12-26 06:51:33.323052: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60592700 of size 256 next 297
2022-12-26 06:51:33.323055: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60592800 of size 256 next 298
2022-12-26 06:51:33.323058: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60592900 of size 10240 next 650
2022-12-26 06:51:33.323061: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60595100 of size 10240 next 314
2022-12-26 06:51:33.323064: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60597900 of size 10240 next 647
2022-12-26 06:51:33.323066: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6059a100 of size 10240 next 342
2022-12-26 06:51:33.323069: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6059c900 of size 10240 next 319
2022-12-26 06:51:33.323072: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6059f100 of size 2560 next 332
2022-12-26 06:51:33.323075: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6059fb00 of size 2560 next 320
2022-12-26 06:51:33.323078: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605a0500 of size 2560 next 300
2022-12-26 06:51:33.323081: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605a0f00 of size 10240 next 325
2022-12-26 06:51:33.323084: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605a3700 of size 10240 next 301
2022-12-26 06:51:33.323087: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605a5f00 of size 10240 next 321
2022-12-26 06:51:33.323090: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605a8700 of size 10240 next 326
2022-12-26 06:51:33.323092: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605aaf00 of size 10240 next 303
2022-12-26 06:51:33.323095: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605ad700 of size 5120 next 341
2022-12-26 06:51:33.323098: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605aeb00 of size 256 next 333
2022-12-26 06:51:33.323101: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605aec00 of size 256 next 318
2022-12-26 06:51:33.323104: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605aed00 of size 256 next 330
2022-12-26 06:51:33.323107: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605aee00 of size 10240 next 328
2022-12-26 06:51:33.323110: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605b1600 of size 10240 next 329
2022-12-26 06:51:33.323113: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605b3e00 of size 17920 next 311
2022-12-26 06:51:33.323116: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605b8400 of size 256 next 308
2022-12-26 06:51:33.323119: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605b8500 of size 256 next 309
2022-12-26 06:51:33.323121: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605b8600 of size 256 next 315
2022-12-26 06:51:33.323124: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605b8700 of size 256 next 340
2022-12-26 06:51:33.323127: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605b8800 of size 256 next 338
2022-12-26 06:51:33.323130: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605b8900 of size 256 next 322
2022-12-26 06:51:33.323133: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605b8a00 of size 2560 next 339
2022-12-26 06:51:33.323136: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605b9400 of size 4352 next 304
2022-12-26 06:51:33.323139: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605ba500 of size 256 next 305
2022-12-26 06:51:33.323142: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605ba600 of size 2560 next 313
2022-12-26 06:51:33.323146: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605bb000 of size 2560 next 299
2022-12-26 06:51:33.323149: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605bba00 of size 2560 next 324
2022-12-26 06:51:33.323152: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605bc400 of size 10240 next 310
2022-12-26 06:51:33.323155: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605bec00 of size 10240 next 345
2022-12-26 06:51:33.323157: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605c1400 of size 10240 next 307
2022-12-26 06:51:33.323160: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605c3c00 of size 5120 next 344
2022-12-26 06:51:33.323163: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605c5000 of size 10240 next 348
2022-12-26 06:51:33.323166: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605c7800 of size 10240 next 349
2022-12-26 06:51:33.323169: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605ca000 of size 10240 next 350
2022-12-26 06:51:33.323172: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605cc800 of size 2560 next 352
2022-12-26 06:51:33.323175: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605cd200 of size 2560 next 353
2022-12-26 06:51:33.323178: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605cdc00 of size 2560 next 358
2022-12-26 06:51:33.323181: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605ce600 of size 10240 next 355
2022-12-26 06:51:33.323183: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d0e00 of size 10240 next 356
2022-12-26 06:51:33.323186: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d3600 of size 10240 next 360
2022-12-26 06:51:33.323189: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d5e00 of size 5120 next 361
2022-12-26 06:51:33.323192: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d7200 of size 256 next 354
2022-12-26 06:51:33.323195: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d7300 of size 256 next 362
2022-12-26 06:51:33.323198: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d7400 of size 256 next 363
2022-12-26 06:51:33.323201: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d7500 of size 256 next 364
2022-12-26 06:51:33.323204: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d7600 of size 256 next 365
2022-12-26 06:51:33.323206: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d7700 of size 256 next 366
2022-12-26 06:51:33.323209: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d7800 of size 256 next 367
2022-12-26 06:51:33.323212: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d7900 of size 256 next 368
2022-12-26 06:51:33.323215: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d7a00 of size 256 next 369
2022-12-26 06:51:33.323218: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d7b00 of size 256 next 110
2022-12-26 06:51:33.323221: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d7c00 of size 256 next 370
2022-12-26 06:51:33.323224: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d7d00 of size 256 next 401
2022-12-26 06:51:33.323226: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d7e00 of size 256 next 384
2022-12-26 06:51:33.323229: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d7f00 of size 256 next 418
2022-12-26 06:51:33.323232: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d8000 of size 256 next 398
2022-12-26 06:51:33.323235: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d8100 of size 256 next 386
2022-12-26 06:51:33.323239: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605d8200 of size 10240 next 720
2022-12-26 06:51:33.323242: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605daa00 of size 10240 next 412
2022-12-26 06:51:33.323245: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605dd200 of size 10240 next 716
2022-12-26 06:51:33.323248: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605dfa00 of size 10240 next 395
2022-12-26 06:51:33.323250: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605e2200 of size 10240 next 402
2022-12-26 06:51:33.323253: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605e4a00 of size 2560 next 394
2022-12-26 06:51:33.323256: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605e5400 of size 2560 next 397
2022-12-26 06:51:33.323259: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605e5e00 of size 2560 next 409
2022-12-26 06:51:33.323262: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605e6800 of size 2560 next 183
2022-12-26 06:51:33.323265: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605e7200 of size 2560 next 403
2022-12-26 06:51:33.323268: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605e7c00 of size 10240 next 131
2022-12-26 06:51:33.323271: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605ea400 of size 10240 next 406
2022-12-26 06:51:33.323274: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605ecc00 of size 10240 next 417
2022-12-26 06:51:33.323276: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605ef400 of size 10240 next 416
2022-12-26 06:51:33.323279: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605f1c00 of size 10240 next 372
2022-12-26 06:51:33.323282: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605f4400 of size 5120 next 388
2022-12-26 06:51:33.323285: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605f5800 of size 256 next 374
2022-12-26 06:51:33.323288: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605f5900 of size 256 next 410
2022-12-26 06:51:33.323291: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605f5a00 of size 256 next 375
2022-12-26 06:51:33.323294: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605f5b00 of size 10240 next 377
2022-12-26 06:51:33.323297: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605f8300 of size 10240 next 381
2022-12-26 06:51:33.323300: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605fab00 of size 10240 next 376
2022-12-26 06:51:33.323302: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605fd300 of size 2560 next 404
2022-12-26 06:51:33.323305: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605fdd00 of size 2560 next 371
2022-12-26 06:51:33.323308: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605fe700 of size 2560 next 382
2022-12-26 06:51:33.323311: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd605ff100 of size 10240 next 405
2022-12-26 06:51:33.323314: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60601900 of size 10240 next 396
2022-12-26 06:51:33.323317: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60604100 of size 10240 next 373
2022-12-26 06:51:33.323320: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60606900 of size 5120 next 413
2022-12-26 06:51:33.323323: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60607d00 of size 10240 next 378
2022-12-26 06:51:33.323326: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6060a500 of size 10240 next 424
2022-12-26 06:51:33.323329: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6060cd00 of size 10240 next 420
2022-12-26 06:51:33.323332: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6060f500 of size 2560 next 423
2022-12-26 06:51:33.323335: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6060ff00 of size 2560 next 393
2022-12-26 06:51:33.323338: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60610900 of size 2560 next 426
2022-12-26 06:51:33.323341: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60611300 of size 10240 next 427
2022-12-26 06:51:33.323344: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60613b00 of size 10240 next 380
2022-12-26 06:51:33.323347: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60616300 of size 10240 next 392
2022-12-26 06:51:33.323350: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60618b00 of size 5120 next 429
2022-12-26 06:51:33.323353: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60619f00 of size 256 next 430
2022-12-26 06:51:33.323356: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061a000 of size 256 next 431
2022-12-26 06:51:33.323358: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061a100 of size 256 next 432
2022-12-26 06:51:33.323361: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061a200 of size 256 next 433
2022-12-26 06:51:33.323364: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061a300 of size 256 next 434
2022-12-26 06:51:33.323367: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061a400 of size 256 next 435
2022-12-26 06:51:33.323370: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061a500 of size 256 next 436
2022-12-26 06:51:33.323373: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061a600 of size 256 next 437
2022-12-26 06:51:33.323376: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061a700 of size 256 next 438
2022-12-26 06:51:33.323378: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061a800 of size 256 next 439
2022-12-26 06:51:33.323381: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061a900 of size 256 next 440
2022-12-26 06:51:33.323384: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061aa00 of size 256 next 443
2022-12-26 06:51:33.323387: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061ab00 of size 256 next 444
2022-12-26 06:51:33.323390: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061ac00 of size 256 next 460
2022-12-26 06:51:33.323393: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061ad00 of size 10240 next 462
2022-12-26 06:51:33.323396: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6061d500 of size 19968 next 475
2022-12-26 06:51:33.323398: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60622300 of size 256 next 451
2022-12-26 06:51:33.323401: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60622400 of size 256 next 450
2022-12-26 06:51:33.323404: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60622500 of size 256 next 468
2022-12-26 06:51:33.323407: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60622600 of size 256 next 480
2022-12-26 06:51:33.323410: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60622700 of size 18944 next 379
2022-12-26 06:51:33.323415: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60627100 of size 256 next 385
2022-12-26 06:51:33.323418: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60627200 of size 256 next 407
2022-12-26 06:51:33.323421: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60627300 of size 10240 next 455
2022-12-26 06:51:33.323424: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60629b00 of size 10240 next 446
2022-12-26 06:51:33.323428: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6062c300 of size 2560 next 478
2022-12-26 06:51:33.323431: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6062cd00 of size 2560 next 648
2022-12-26 06:51:33.323434: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6062d700 of size 2560 next 796
2022-12-26 06:51:33.323436: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6062e100 of size 2560 next 482
2022-12-26 06:51:33.323439: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6062eb00 of size 2560 next 459
2022-12-26 06:51:33.323442: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6062f500 of size 10240 next 486
2022-12-26 06:51:33.323445: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60631d00 of size 10240 next 469
2022-12-26 06:51:33.323448: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60634500 of size 10240 next 474
2022-12-26 06:51:33.323451: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60636d00 of size 10240 next 481
2022-12-26 06:51:33.323454: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60639500 of size 10240 next 466
2022-12-26 06:51:33.323457: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6063bd00 of size 5120 next 445
2022-12-26 06:51:33.323459: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6063d100 of size 256 next 461
2022-12-26 06:51:33.323462: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6063d200 of size 256 next 458
2022-12-26 06:51:33.323465: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6063d300 of size 256 next 484
2022-12-26 06:51:33.323468: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6063d400 of size 256 next 477
2022-12-26 06:51:33.323471: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6063d500 of size 10240 next 452
2022-12-26 06:51:33.323474: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6063fd00 of size 10240 next 483
2022-12-26 06:51:33.323477: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60642500 of size 10240 next 463
2022-12-26 06:51:33.323480: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60644d00 of size 2560 next 453
2022-12-26 06:51:33.323482: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60645700 of size 2560 next 471
2022-12-26 06:51:33.323485: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60646100 of size 2560 next 467
2022-12-26 06:51:33.323488: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60646b00 of size 10240 next 457
2022-12-26 06:51:33.323491: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60649300 of size 10240 next 473
2022-12-26 06:51:33.323494: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6064bb00 of size 10240 next 485
2022-12-26 06:51:33.323497: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6064e300 of size 5120 next 470
2022-12-26 06:51:33.323500: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6064f700 of size 10240 next 441
2022-12-26 06:51:33.323502: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60651f00 of size 10240 next 487
2022-12-26 06:51:33.323505: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60654700 of size 10240 next 488
2022-12-26 06:51:33.323508: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60656f00 of size 2560 next 490
2022-12-26 06:51:33.323511: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60657900 of size 2560 next 491
2022-12-26 06:51:33.323514: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60658300 of size 2560 next 492
2022-12-26 06:51:33.323517: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60658d00 of size 10240 next 494
2022-12-26 06:51:33.323521: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6065b500 of size 10240 next 495
2022-12-26 06:51:33.323524: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6065dd00 of size 10240 next 498
2022-12-26 06:51:33.323527: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60660500 of size 5120 next 500
2022-12-26 06:51:33.323529: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60661900 of size 256 next 499
2022-12-26 06:51:33.323532: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60661a00 of size 256 next 502
2022-12-26 06:51:33.323535: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60661b00 of size 256 next 503
2022-12-26 06:51:33.323538: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60661c00 of size 256 next 504
2022-12-26 06:51:33.323541: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60661d00 of size 256 next 505
2022-12-26 06:51:33.323544: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60661e00 of size 256 next 506
2022-12-26 06:51:33.323547: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60661f00 of size 256 next 507
2022-12-26 06:51:33.323549: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60662000 of size 256 next 508
2022-12-26 06:51:33.323552: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60662100 of size 256 next 509
2022-12-26 06:51:33.323555: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60662200 of size 256 next 510
2022-12-26 06:51:33.323558: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60662300 of size 256 next 511
2022-12-26 06:51:33.323561: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60662400 of size 256 next 530
2022-12-26 06:51:33.323564: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60662500 of size 256 next 550
2022-12-26 06:51:33.323567: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60662600 of size 256 next 523
2022-12-26 06:51:33.323569: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60662700 of size 256 next 520
2022-12-26 06:51:33.323572: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60662800 of size 256 next 540
2022-12-26 06:51:33.323575: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60662900 of size 10240 next 536
2022-12-26 06:51:33.323578: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60665100 of size 10240 next 516
2022-12-26 06:51:33.323581: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60667900 of size 10240 next 512
2022-12-26 06:51:33.323584: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6066a100 of size 10240 next 734
2022-12-26 06:51:33.323587: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6066c900 of size 10240 next 736
2022-12-26 06:51:33.323589: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6066f100 of size 2560 next 522
2022-12-26 06:51:33.323592: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6066fb00 of size 2560 next 539
2022-12-26 06:51:33.323595: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60670500 of size 2560 next 546
2022-12-26 06:51:33.323598: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60670f00 of size 2560 next 515
2022-12-26 06:51:33.323601: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60671900 of size 2560 next 534
2022-12-26 06:51:33.323604: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60672300 of size 10240 next 543
2022-12-26 06:51:33.323607: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60674b00 of size 10240 next 559
2022-12-26 06:51:33.323610: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60677300 of size 10240 next 519
2022-12-26 06:51:33.323615: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60679b00 of size 10240 next 526
2022-12-26 06:51:33.323618: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6067c300 of size 10240 next 525
2022-12-26 06:51:33.323621: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6067eb00 of size 5120 next 557
2022-12-26 06:51:33.323624: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6067ff00 of size 256 next 558
2022-12-26 06:51:33.323627: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60680000 of size 256 next 527
2022-12-26 06:51:33.323630: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60680100 of size 256 next 553
2022-12-26 06:51:33.323633: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60680200 of size 10240 next 538
2022-12-26 06:51:33.323636: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60682a00 of size 10240 next 521
2022-12-26 06:51:33.323638: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60685200 of size 20224 next 542
2022-12-26 06:51:33.323641: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6068a100 of size 256 next 548
2022-12-26 06:51:33.323644: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6068a200 of size 2560 next 533
2022-12-26 06:51:33.323647: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6068ac00 of size 2560 next 513
2022-12-26 06:51:33.323650: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6068b600 of size 2560 next 547
2022-12-26 06:51:33.323653: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6068c000 of size 10240 next 537
2022-12-26 06:51:33.323656: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6068e800 of size 10240 next 561
2022-12-26 06:51:33.323659: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60691000 of size 10240 next 531
2022-12-26 06:51:33.323662: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60693800 of size 5120 next 552
2022-12-26 06:51:33.323664: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60694c00 of size 10240 next 556
2022-12-26 06:51:33.323667: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60697400 of size 10240 next 563
2022-12-26 06:51:33.323670: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60699c00 of size 10240 next 560
2022-12-26 06:51:33.323673: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6069c400 of size 2560 next 566
2022-12-26 06:51:33.323677: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6069ce00 of size 2560 next 567
2022-12-26 06:51:33.323680: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6069d800 of size 2560 next 565
2022-12-26 06:51:33.323683: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6069e200 of size 10240 next 569
2022-12-26 06:51:33.323686: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a0a00 of size 10240 next 571
2022-12-26 06:51:33.323689: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a3200 of size 10240 next 554
2022-12-26 06:51:33.323692: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a5a00 of size 5120 next 549
2022-12-26 06:51:33.323695: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a6e00 of size 256 next 579
2022-12-26 06:51:33.323698: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a6f00 of size 256 next 551
2022-12-26 06:51:33.323701: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7000 of size 256 next 578
2022-12-26 06:51:33.323704: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7100 of size 256 next 574
2022-12-26 06:51:33.323706: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7200 of size 256 next 575
2022-12-26 06:51:33.323711: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7300 of size 256 next 584
2022-12-26 06:51:33.323713: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7400 of size 256 next 581
2022-12-26 06:51:33.323716: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7500 of size 256 next 585
2022-12-26 06:51:33.323719: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7600 of size 256 next 583
2022-12-26 06:51:33.323722: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7700 of size 256 next 586
2022-12-26 06:51:33.323725: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7800 of size 256 next 587
2022-12-26 06:51:33.323728: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7900 of size 256 next 616
2022-12-26 06:51:33.323731: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7a00 of size 256 next 606
2022-12-26 06:51:33.323733: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7b00 of size 256 next 614
2022-12-26 06:51:33.323736: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7c00 of size 256 next 627
2022-12-26 06:51:33.323739: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7d00 of size 256 next 609
2022-12-26 06:51:33.323742: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606a7e00 of size 10240 next 622
2022-12-26 06:51:33.323745: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606aa600 of size 10240 next 936
2022-12-26 06:51:33.323748: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606ace00 of size 10240 next 625
2022-12-26 06:51:33.323751: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606af600 of size 10240 next 600
2022-12-26 06:51:33.323753: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606b1e00 of size 10240 next 590
2022-12-26 06:51:33.323756: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606b4600 of size 2560 next 598
2022-12-26 06:51:33.323759: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606b5000 of size 2560 next 595
2022-12-26 06:51:33.323762: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606b5a00 of size 2560 next 568
2022-12-26 06:51:33.323765: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606b6400 of size 2560 next 621
2022-12-26 06:51:33.323768: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606b6e00 of size 2560 next 602
2022-12-26 06:51:33.323771: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606b7800 of size 10240 next 588
2022-12-26 06:51:33.323774: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606ba000 of size 10240 next 605
2022-12-26 06:51:33.323776: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606bc800 of size 10240 next 623
2022-12-26 06:51:33.323779: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606bf000 of size 10240 next 604
2022-12-26 06:51:33.323782: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606c1800 of size 10240 next 613
2022-12-26 06:51:33.323785: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606c4000 of size 5120 next 611
2022-12-26 06:51:33.323788: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606c5400 of size 256 next 619
2022-12-26 06:51:33.323791: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606c5500 of size 256 next 591
2022-12-26 06:51:33.323794: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606c5600 of size 256 next 634
2022-12-26 06:51:33.323797: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606c5700 of size 2560 next 599
2022-12-26 06:51:33.323799: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606c6100 of size 4352 next 544
2022-12-26 06:51:33.323804: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606c7200 of size 256 next 545
2022-12-26 06:51:33.323807: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606c7300 of size 256 next 596
2022-12-26 06:51:33.323810: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606c7400 of size 10240 next 608
2022-12-26 06:51:33.323813: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606c9c00 of size 10240 next 610
2022-12-26 06:51:33.323816: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606cc400 of size 10240 next 624
2022-12-26 06:51:33.323819: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606cec00 of size 2560 next 635
2022-12-26 06:51:33.323822: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606cf600 of size 10240 next 636
2022-12-26 06:51:33.323825: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606d1e00 of size 10240 next 626
2022-12-26 06:51:33.323827: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606d4600 of size 10240 next 639
2022-12-26 06:51:33.323830: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606d6e00 of size 5120 next 640
2022-12-26 06:51:33.323833: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606d8200 of size 10240 next 641
2022-12-26 06:51:33.323836: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606daa00 of size 10240 next 642
2022-12-26 06:51:33.323839: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606dd200 of size 10240 next 631
2022-12-26 06:51:33.323842: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606dfa00 of size 2560 next 644
2022-12-26 06:51:33.323845: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606e0400 of size 2560 next 645
2022-12-26 06:51:33.323848: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606e0e00 of size 2560 next 646
2022-12-26 06:51:33.323850: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606e1800 of size 10240 next 638
2022-12-26 06:51:33.323853: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606e4000 of size 10240 next 479
2022-12-26 06:51:33.323856: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606e6800 of size 10240 next 637
2022-12-26 06:51:33.323859: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606e9000 of size 5120 next 653
2022-12-26 06:51:33.323862: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606ea400 of size 256 next 652
2022-12-26 06:51:33.323865: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606ea500 of size 256 next 654
2022-12-26 06:51:33.323868: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606ea600 of size 256 next 655
2022-12-26 06:51:33.323871: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606ea700 of size 256 next 656
2022-12-26 06:51:33.323874: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606ea800 of size 256 next 657
2022-12-26 06:51:33.323876: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606ea900 of size 256 next 658
2022-12-26 06:51:33.323879: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606eaa00 of size 256 next 659
2022-12-26 06:51:33.323882: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606eab00 of size 256 next 660
2022-12-26 06:51:33.323885: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606eac00 of size 256 next 617
2022-12-26 06:51:33.323888: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606ead00 of size 256 next 662
2022-12-26 06:51:33.323891: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606eae00 of size 256 next 663
2022-12-26 06:51:33.323893: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606eaf00 of size 10240 next 687
2022-12-26 06:51:33.323898: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606ed700 of size 10240 next 676
2022-12-26 06:51:33.323901: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606eff00 of size 10240 next 664
2022-12-26 06:51:33.323903: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606f2700 of size 10240 next 684
2022-12-26 06:51:33.323906: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606f4f00 of size 5120 next 679
2022-12-26 06:51:33.323909: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606f6300 of size 256 next 667
2022-12-26 06:51:33.323912: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606f6400 of size 256 next 671
2022-12-26 06:51:33.323915: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606f6500 of size 256 next 709
2022-12-26 06:51:33.323918: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606f6600 of size 10240 next 708
2022-12-26 06:51:33.323921: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606f8e00 of size 10240 next 697
2022-12-26 06:51:33.323923: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606fb600 of size 10240 next 688
2022-12-26 06:51:33.323926: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606fde00 of size 2560 next 701
2022-12-26 06:51:33.323929: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606fe800 of size 2560 next 694
2022-12-26 06:51:33.323932: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606ff200 of size 2560 next 710
2022-12-26 06:51:33.323935: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd606ffc00 of size 12032 next 593
2022-12-26 06:51:33.323938: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60702b00 of size 256 next 594
2022-12-26 06:51:33.323941: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60702c00 of size 256 next 690
2022-12-26 06:51:33.323944: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60702d00 of size 256 next 680
2022-12-26 06:51:33.323947: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60702e00 of size 256 next 669
2022-12-26 06:51:33.323950: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60702f00 of size 256 next 703
2022-12-26 06:51:33.323952: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60703000 of size 256 next 699
2022-12-26 06:51:33.323955: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60703100 of size 256 next 700
2022-12-26 06:51:33.323958: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60703200 of size 10240 next 1011
2022-12-26 06:51:33.323961: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60705a00 of size 10240 next 1010
2022-12-26 06:51:33.323964: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60708200 of size 10240 next 695
2022-12-26 06:51:33.323967: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6070aa00 of size 10240 next 685
2022-12-26 06:51:33.323970: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6070d200 of size 10240 next 706
2022-12-26 06:51:33.323973: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6070fa00 of size 2560 next 689
2022-12-26 06:51:33.323975: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60710400 of size 2560 next 672
2022-12-26 06:51:33.323978: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60710e00 of size 2560 next 678
2022-12-26 06:51:33.323981: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60711800 of size 2560 next 691
2022-12-26 06:51:33.323984: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60712200 of size 2560 next 702
2022-12-26 06:51:33.323987: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60712c00 of size 15616 next 665
2022-12-26 06:51:33.323991: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60716900 of size 256 next 666
2022-12-26 06:51:33.323994: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60716a00 of size 10240 next 711
2022-12-26 06:51:33.323997: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60719200 of size 10240 next 712
2022-12-26 06:51:33.324000: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6071ba00 of size 5120 next 713
2022-12-26 06:51:33.324003: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6071ce00 of size 10240 next 715
2022-12-26 06:51:33.324006: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6071f600 of size 10240 next 717
2022-12-26 06:51:33.324008: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60721e00 of size 10240 next 721
2022-12-26 06:51:33.324011: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60724600 of size 2560 next 718
2022-12-26 06:51:33.324014: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60725000 of size 2560 next 722
2022-12-26 06:51:33.324017: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60725a00 of size 2560 next 724
2022-12-26 06:51:33.324020: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60726400 of size 10240 next 726
2022-12-26 06:51:33.324023: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60728c00 of size 10240 next 727
2022-12-26 06:51:33.324026: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072b400 of size 10240 next 728
2022-12-26 06:51:33.324029: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072dc00 of size 5120 next 730
2022-12-26 06:51:33.324032: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072f000 of size 256 next 731
2022-12-26 06:51:33.324034: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072f100 of size 256 next 732
2022-12-26 06:51:33.324037: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072f200 of size 256 next 733
2022-12-26 06:51:33.324040: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072f300 of size 256 next 573
2022-12-26 06:51:33.324043: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072f400 of size 256 next 737
2022-12-26 06:51:33.324046: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072f500 of size 256 next 735
2022-12-26 06:51:33.324049: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072f600 of size 256 next 739
2022-12-26 06:51:33.324052: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072f700 of size 256 next 740
2022-12-26 06:51:33.324054: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072f800 of size 256 next 529
2022-12-26 06:51:33.324057: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072f900 of size 256 next 741
2022-12-26 06:51:33.324060: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072fa00 of size 256 next 742
2022-12-26 06:51:33.324063: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072fb00 of size 256 next 754
2022-12-26 06:51:33.324066: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072fc00 of size 256 next 750
2022-12-26 06:51:33.324069: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072fd00 of size 256 next 762
2022-12-26 06:51:33.324072: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6072fe00 of size 10240 next 1149
2022-12-26 06:51:33.324074: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60732600 of size 10240 next 1152
2022-12-26 06:51:33.324077: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60734e00 of size 10240 next 831
2022-12-26 06:51:33.324080: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60737600 of size 2560 next 843
2022-12-26 06:51:33.324084: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60738000 of size 2560 next 840
2022-12-26 06:51:33.324087: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60738a00 of size 2560 next 855
2022-12-26 06:51:33.324090: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60739400 of size 2560 next 849
2022-12-26 06:51:33.324093: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60739e00 of size 2560 next 783
2022-12-26 06:51:33.324096: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6073a800 of size 10240 next 829
2022-12-26 06:51:33.324098: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6073d000 of size 10240 next 852
2022-12-26 06:51:33.324101: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6073f800 of size 10240 next 856
2022-12-26 06:51:33.324104: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60742000 of size 10240 next 860
2022-12-26 06:51:33.324107: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60744800 of size 10240 next 854
2022-12-26 06:51:33.324110: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60747000 of size 5120 next 822
2022-12-26 06:51:33.324113: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60748400 of size 256 next 828
2022-12-26 06:51:33.324116: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60748500 of size 256 next 836
2022-12-26 06:51:33.324119: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60748600 of size 256 next 823
2022-12-26 06:51:33.324122: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60748700 of size 10240 next 835
2022-12-26 06:51:33.324124: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6074af00 of size 10240 next 847
2022-12-26 06:51:33.324127: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6074d700 of size 10240 next 841
2022-12-26 06:51:33.324130: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6074ff00 of size 2560 next 844
2022-12-26 06:51:33.324133: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60750900 of size 2560 next 832
2022-12-26 06:51:33.324136: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60751300 of size 2560 next 825
2022-12-26 06:51:33.324139: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60751d00 of size 10240 next 846
2022-12-26 06:51:33.324142: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60754500 of size 10240 next 818
2022-12-26 06:51:33.324145: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60756d00 of size 10240 next 848
2022-12-26 06:51:33.324148: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60759500 of size 5120 next 853
2022-12-26 06:51:33.324150: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6075a900 of size 10240 next 749
2022-12-26 06:51:33.324153: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6075d100 of size 10240 next 862
2022-12-26 06:51:33.324156: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6075f900 of size 10240 next 864
2022-12-26 06:51:33.324159: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60762100 of size 2560 next 867
2022-12-26 06:51:33.324162: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60762b00 of size 2560 next 582
2022-12-26 06:51:33.324165: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60763500 of size 2560 next 698
2022-12-26 06:51:33.324168: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60763f00 of size 10240 next 738
2022-12-26 06:51:33.324171: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60766700 of size 10240 next 870
2022-12-26 06:51:33.324174: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60768f00 of size 10240 next 871
2022-12-26 06:51:33.324178: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076b700 of size 5120 next 873
2022-12-26 06:51:33.324180: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076cb00 of size 256 next 874
2022-12-26 06:51:33.324183: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076cc00 of size 256 next 875
2022-12-26 06:51:33.324186: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076cd00 of size 256 next 876
2022-12-26 06:51:33.324189: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076ce00 of size 256 next 877
2022-12-26 06:51:33.324192: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076cf00 of size 256 next 878
2022-12-26 06:51:33.324195: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076d000 of size 256 next 879
2022-12-26 06:51:33.324198: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076d100 of size 256 next 883
2022-12-26 06:51:33.324200: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076d200 of size 256 next 784
2022-12-26 06:51:33.324203: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076d300 of size 256 next 763
2022-12-26 06:51:33.324206: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076d400 of size 256 next 759
2022-12-26 06:51:33.324209: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076d500 of size 256 next 881
2022-12-26 06:51:33.324212: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076d600 of size 256 next 887
2022-12-26 06:51:33.324215: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076d700 of size 256 next 889
2022-12-26 06:51:33.324217: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076d800 of size 256 next 886
2022-12-26 06:51:33.324220: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076d900 of size 256 next 906
2022-12-26 06:51:33.324223: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076da00 of size 256 next 909
2022-12-26 06:51:33.324226: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6076db00 of size 10240 next 1223
2022-12-26 06:51:33.324229: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60770300 of size 10240 next 1219
2022-12-26 06:51:33.324232: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60772b00 of size 10240 next 1221
2022-12-26 06:51:33.324235: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60775300 of size 10240 next 902
2022-12-26 06:51:33.324238: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60777b00 of size 10240 next 896
2022-12-26 06:51:33.324240: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6077a300 of size 2560 next 927
2022-12-26 06:51:33.324243: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6077ad00 of size 2560 next 890
2022-12-26 06:51:33.324246: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6077b700 of size 2560 next 904
2022-12-26 06:51:33.324249: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6077c100 of size 2560 next 912
2022-12-26 06:51:33.324252: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6077cb00 of size 2560 next 898
2022-12-26 06:51:33.324255: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6077d500 of size 10240 next 917
2022-12-26 06:51:33.324258: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6077fd00 of size 17920 next 788
2022-12-26 06:51:33.324261: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60784300 of size 256 next 747
2022-12-26 06:51:33.324264: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60784400 of size 256 next 780
2022-12-26 06:51:33.324266: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60784500 of size 256 next 673
2022-12-26 06:51:33.324271: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60784600 of size 10240 next 1079
2022-12-26 06:51:33.324274: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60786e00 of size 10240 next 1009
2022-12-26 06:51:33.324277: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60789600 of size 10240 next 756
2022-12-26 06:51:33.324280: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6078be00 of size 10240 next 757
2022-12-26 06:51:33.324283: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6078e600 of size 10240 next 773
2022-12-26 06:51:33.324285: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60790e00 of size 2560 next 766
2022-12-26 06:51:33.324288: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60791800 of size 2560 next 743
2022-12-26 06:51:33.324291: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60792200 of size 2560 next 767
2022-12-26 06:51:33.324294: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60792c00 of size 2560 next 753
2022-12-26 06:51:33.324297: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60793600 of size 2560 next 760
2022-12-26 06:51:33.324300: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60794000 of size 10240 next 772
2022-12-26 06:51:33.324303: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60796800 of size 10240 next 769
2022-12-26 06:51:33.324306: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60799000 of size 10240 next 775
2022-12-26 06:51:33.324308: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6079b800 of size 10240 next 755
2022-12-26 06:51:33.324311: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6079e000 of size 10240 next 752
2022-12-26 06:51:33.324314: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607a0800 of size 5120 next 745
2022-12-26 06:51:33.324317: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607a1c00 of size 256 next 787
2022-12-26 06:51:33.324320: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607a1d00 of size 256 next 765
2022-12-26 06:51:33.324323: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607a1e00 of size 256 next 748
2022-12-26 06:51:33.324326: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607a1f00 of size 10240 next 758
2022-12-26 06:51:33.324329: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607a4700 of size 10240 next 781
2022-12-26 06:51:33.324331: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607a6f00 of size 10240 next 764
2022-12-26 06:51:33.324334: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607a9700 of size 2560 next 774
2022-12-26 06:51:33.324337: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607aa100 of size 2560 next 761
2022-12-26 06:51:33.324340: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607aab00 of size 2560 next 779
2022-12-26 06:51:33.324343: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607ab500 of size 10240 next 791
2022-12-26 06:51:33.324346: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607add00 of size 10240 next 661
2022-12-26 06:51:33.324349: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607b0500 of size 10240 next 562
2022-12-26 06:51:33.324352: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607b2d00 of size 5120 next 603
2022-12-26 06:51:33.324355: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607b4100 of size 10240 next 302
2022-12-26 06:51:33.324357: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607b6900 of size 10240 next 649
2022-12-26 06:51:33.324360: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607b9100 of size 10240 next 797
2022-12-26 06:51:33.324364: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607bb900 of size 2560 next 799
2022-12-26 06:51:33.324367: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607bc300 of size 2560 next 800
2022-12-26 06:51:33.324370: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607bcd00 of size 2560 next 801
2022-12-26 06:51:33.324373: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607bd700 of size 10240 next 803
2022-12-26 06:51:33.324376: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607bff00 of size 10240 next 804
2022-12-26 06:51:33.324379: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c2700 of size 10240 next 805
2022-12-26 06:51:33.324382: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c4f00 of size 5120 next 806
2022-12-26 06:51:33.324385: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c6300 of size 256 next 807
2022-12-26 06:51:33.324388: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c6400 of size 256 next 808
2022-12-26 06:51:33.324390: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c6500 of size 256 next 809
2022-12-26 06:51:33.324393: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c6600 of size 256 next 810
2022-12-26 06:51:33.324396: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c6700 of size 256 next 811
2022-12-26 06:51:33.324399: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c6800 of size 256 next 812
2022-12-26 06:51:33.324402: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c6900 of size 256 next 813
2022-12-26 06:51:33.324405: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c6a00 of size 256 next 814
2022-12-26 06:51:33.324408: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c6b00 of size 256 next 815
2022-12-26 06:51:33.324410: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c6c00 of size 256 next 816
2022-12-26 06:51:33.324413: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c6d00 of size 256 next 817
2022-12-26 06:51:33.324416: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c6e00 of size 256 next 842
2022-12-26 06:51:33.324419: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c6f00 of size 256 next 820
2022-12-26 06:51:33.324422: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c7000 of size 256 next 821
2022-12-26 06:51:33.324425: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c7100 of size 256 next 845
2022-12-26 06:51:33.324427: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c7200 of size 256 next 819
2022-12-26 06:51:33.324430: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c7300 of size 256 next 833
2022-12-26 06:51:33.324433: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c7400 of size 256 next 839
2022-12-26 06:51:33.324436: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c7500 of size 10240 next 830
2022-12-26 06:51:33.324439: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607c9d00 of size 16128 next 746
2022-12-26 06:51:33.324442: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607cdc00 of size 256 next 696
2022-12-26 06:51:33.324445: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607cdd00 of size 256 next 913
2022-12-26 06:51:33.324448: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607cde00 of size 256 next 905
2022-12-26 06:51:33.324450: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607cdf00 of size 10240 next 918
2022-12-26 06:51:33.324453: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607d0700 of size 10240 next 925
2022-12-26 06:51:33.324458: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607d2f00 of size 10240 next 895
2022-12-26 06:51:33.324461: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607d5700 of size 5120 next 919
2022-12-26 06:51:33.324463: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607d6b00 of size 256 next 908
2022-12-26 06:51:33.324466: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607d6c00 of size 256 next 910
2022-12-26 06:51:33.324469: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607d6d00 of size 256 next 897
2022-12-26 06:51:33.324472: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607d6e00 of size 10240 next 892
2022-12-26 06:51:33.324475: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607d9600 of size 10240 next 893
2022-12-26 06:51:33.324478: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607dbe00 of size 10240 next 911
2022-12-26 06:51:33.324481: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607de600 of size 2560 next 916
2022-12-26 06:51:33.324484: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607df000 of size 2560 next 915
2022-12-26 06:51:33.324486: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607dfa00 of size 2560 next 900
2022-12-26 06:51:33.324489: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607e0400 of size 10240 next 888
2022-12-26 06:51:33.324492: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607e2c00 of size 10240 next 924
2022-12-26 06:51:33.324495: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607e5400 of size 10240 next 921
2022-12-26 06:51:33.324498: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607e7c00 of size 5120 next 907
2022-12-26 06:51:33.324501: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607e9000 of size 10240 next 885
2022-12-26 06:51:33.324504: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607eb800 of size 10240 next 929
2022-12-26 06:51:33.324506: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607ee000 of size 10240 next 930
2022-12-26 06:51:33.324509: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607f0800 of size 2560 next 932
2022-12-26 06:51:33.324512: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607f1200 of size 2560 next 933
2022-12-26 06:51:33.324515: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607f1c00 of size 2560 next 934
2022-12-26 06:51:33.324518: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607f2600 of size 10240 next 937
2022-12-26 06:51:33.324521: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607f4e00 of size 10240 next 939
2022-12-26 06:51:33.324524: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607f7600 of size 10240 next 589
2022-12-26 06:51:33.324527: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607f9e00 of size 5120 next 790
2022-12-26 06:51:33.324530: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fb200 of size 256 next 938
2022-12-26 06:51:33.324532: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fb300 of size 256 next 859
2022-12-26 06:51:33.324535: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fb400 of size 256 next 940
2022-12-26 06:51:33.324538: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fb500 of size 256 next 941
2022-12-26 06:51:33.324541: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fb600 of size 256 next 942
2022-12-26 06:51:33.324544: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fb700 of size 256 next 943
2022-12-26 06:51:33.324547: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fb800 of size 256 next 944
2022-12-26 06:51:33.324551: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fb900 of size 256 next 945
2022-12-26 06:51:33.324554: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fba00 of size 256 next 946
2022-12-26 06:51:33.324556: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fbb00 of size 256 next 947
2022-12-26 06:51:33.324559: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fbc00 of size 256 next 948
2022-12-26 06:51:33.324562: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fbd00 of size 256 next 984
2022-12-26 06:51:33.324565: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fbe00 of size 256 next 954
2022-12-26 06:51:33.324568: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fbf00 of size 256 next 988
2022-12-26 06:51:33.324571: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fc000 of size 256 next 958
2022-12-26 06:51:33.324573: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fc100 of size 256 next 976
2022-12-26 06:51:33.324576: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fc200 of size 10240 next 1238
2022-12-26 06:51:33.324579: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd607fea00 of size 10240 next 1290
2022-12-26 06:51:33.324582: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60801200 of size 10240 next 1296
2022-12-26 06:51:33.324585: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60803a00 of size 10240 next 955
2022-12-26 06:51:33.324588: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60806200 of size 10240 next 961
2022-12-26 06:51:33.324591: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60808a00 of size 2560 next 971
2022-12-26 06:51:33.324594: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60809400 of size 2560 next 972
2022-12-26 06:51:33.324597: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60809e00 of size 2560 next 962
2022-12-26 06:51:33.324599: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6080a800 of size 2560 next 966
2022-12-26 06:51:33.324602: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6080b200 of size 2560 next 975
2022-12-26 06:51:33.324605: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6080bc00 of size 10240 next 985
2022-12-26 06:51:33.324608: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6080e400 of size 10240 next 969
2022-12-26 06:51:33.324611: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60810c00 of size 10240 next 974
2022-12-26 06:51:33.324614: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60813400 of size 10240 next 949
2022-12-26 06:51:33.324617: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60815c00 of size 10240 next 967
2022-12-26 06:51:33.324620: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60818400 of size 5120 next 990
2022-12-26 06:51:33.324622: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60819800 of size 256 next 987
2022-12-26 06:51:33.324625: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60819900 of size 256 next 989
2022-12-26 06:51:33.324628: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60819a00 of size 256 next 964
2022-12-26 06:51:33.324631: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60819b00 of size 10240 next 884
2022-12-26 06:51:33.324634: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6081c300 of size 10240 next 977
2022-12-26 06:51:33.324637: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6081eb00 of size 10240 next 950
2022-12-26 06:51:33.324640: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60821300 of size 2560 next 951
2022-12-26 06:51:33.324644: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60821d00 of size 2560 next 979
2022-12-26 06:51:33.324647: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60822700 of size 2560 next 982
2022-12-26 06:51:33.324650: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60823100 of size 10240 next 953
2022-12-26 06:51:33.324652: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60825900 of size 10240 next 965
2022-12-26 06:51:33.324655: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60828100 of size 10240 next 992
2022-12-26 06:51:33.324658: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6082a900 of size 5120 next 997
2022-12-26 06:51:33.324661: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6082bd00 of size 2560 next 994
2022-12-26 06:51:33.324664: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6082c700 of size 2560 next 1002
2022-12-26 06:51:33.324667: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6082d100 of size 4864 next 981
2022-12-26 06:51:33.324670: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6082e400 of size 256 next 973
2022-12-26 06:51:33.324673: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6082e500 of size 10240 next 993
2022-12-26 06:51:33.324677: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60830d00 of size 10240 next 999
2022-12-26 06:51:33.324680: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60833500 of size 10240 next 1001
2022-12-26 06:51:33.324683: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60835d00 of size 10240 next 1006
2022-12-26 06:51:33.324686: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60838500 of size 10240 next 1003
2022-12-26 06:51:33.324689: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6083ad00 of size 10240 next 1007
2022-12-26 06:51:33.324691: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6083d500 of size 5120 next 863
2022-12-26 06:51:33.324694: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6083e900 of size 256 next 959
2022-12-26 06:51:33.324697: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6083ea00 of size 256 next 857
2022-12-26 06:51:33.324700: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6083eb00 of size 256 next 777
2022-12-26 06:51:33.324703: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6083ec00 of size 256 next 869
2022-12-26 06:51:33.324706: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6083ed00 of size 256 next 960
2022-12-26 06:51:33.324709: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6083ee00 of size 256 next 1013
2022-12-26 06:51:33.324712: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6083ef00 of size 256 next 1014
2022-12-26 06:51:33.324715: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6083f000 of size 256 next 1015
2022-12-26 06:51:33.324717: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6083f100 of size 256 next 1016
2022-12-26 06:51:33.324720: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6083f200 of size 256 next 1017
2022-12-26 06:51:33.324723: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6083f300 of size 256 next 1018
2022-12-26 06:51:33.324726: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6083f400 of size 10240 next 1059
2022-12-26 06:51:33.324729: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60841c00 of size 10240 next 1064
2022-12-26 06:51:33.324732: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60844400 of size 5120 next 1038
2022-12-26 06:51:33.324735: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60845800 of size 10240 next 1028
2022-12-26 06:51:33.324739: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60848000 of size 10240 next 1065
2022-12-26 06:51:33.324742: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6084a800 of size 10240 next 1066
2022-12-26 06:51:33.324745: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6084d000 of size 2560 next 1068
2022-12-26 06:51:33.324747: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6084da00 of size 2560 next 1069
2022-12-26 06:51:33.324750: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6084e400 of size 2560 next 1070
2022-12-26 06:51:33.324753: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6084ee00 of size 10240 next 1072
2022-12-26 06:51:33.324756: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60851600 of size 10240 next 1073
2022-12-26 06:51:33.324759: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60853e00 of size 10240 next 1074
2022-12-26 06:51:33.324762: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60856600 of size 5120 next 1076
2022-12-26 06:51:33.324765: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60857a00 of size 256 next 1077
2022-12-26 06:51:33.324768: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60857b00 of size 256 next 1078
2022-12-26 06:51:33.324771: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60857c00 of size 256 next 1080
2022-12-26 06:51:33.324773: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60857d00 of size 256 next 894
2022-12-26 06:51:33.324776: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60857e00 of size 256 next 1081
2022-12-26 06:51:33.324779: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60857f00 of size 256 next 1012
2022-12-26 06:51:33.324782: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60858000 of size 256 next 682
2022-12-26 06:51:33.324785: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60858100 of size 256 next 880
2022-12-26 06:51:33.324788: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60858200 of size 256 next 1083
2022-12-26 06:51:33.324791: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60858300 of size 256 next 1084
2022-12-26 06:51:33.324793: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60858400 of size 256 next 1085
2022-12-26 06:51:33.324796: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60858500 of size 5120 next 1110
2022-12-26 06:51:33.324799: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60859900 of size 256 next 1119
2022-12-26 06:51:33.324802: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60859a00 of size 256 next 1118
2022-12-26 06:51:33.324805: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60859b00 of size 256 next 1127
2022-12-26 06:51:33.324808: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60859c00 of size 10240 next 1113
2022-12-26 06:51:33.324811: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6085c400 of size 10240 next 1112
2022-12-26 06:51:33.324814: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6085ec00 of size 10240 next 1133
2022-12-26 06:51:33.324816: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60861400 of size 2560 next 1088
2022-12-26 06:51:33.324819: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60861e00 of size 2560 next 1093
2022-12-26 06:51:33.324822: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60862800 of size 2560 next 1090
2022-12-26 06:51:33.324825: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60863200 of size 10240 next 952
2022-12-26 06:51:33.324828: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60865a00 of size 10240 next 1130
2022-12-26 06:51:33.324832: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60868200 of size 10240 next 1134
2022-12-26 06:51:33.324835: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6086aa00 of size 5120 next 1129
2022-12-26 06:51:33.324838: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6086be00 of size 18176 next 1087
2022-12-26 06:51:33.324841: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60870500 of size 256 next 1125
2022-12-26 06:51:33.324844: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60870600 of size 256 next 1092
2022-12-26 06:51:33.324847: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60870700 of size 256 next 1103
2022-12-26 06:51:33.324850: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60870800 of size 256 next 1086
2022-12-26 06:51:33.324853: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60870900 of size 256 next 1094
2022-12-26 06:51:33.324856: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60870a00 of size 10240 next 1411
2022-12-26 06:51:33.324858: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60873200 of size 10240 next 1098
2022-12-26 06:51:33.324861: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60875a00 of size 12800 next 1104
2022-12-26 06:51:33.324864: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60878c00 of size 256 next 1121
2022-12-26 06:51:33.324867: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60878d00 of size 10240 next 1443
2022-12-26 06:51:33.324870: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6087b500 of size 10240 next 1117
2022-12-26 06:51:33.324873: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6087dd00 of size 2560 next 1106
2022-12-26 06:51:33.324876: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6087e700 of size 2560 next 1089
2022-12-26 06:51:33.324879: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6087f100 of size 2560 next 1096
2022-12-26 06:51:33.324882: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6087fb00 of size 2560 next 1122
2022-12-26 06:51:33.324885: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60880500 of size 2560 next 1107
2022-12-26 06:51:33.324888: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60880f00 of size 10240 next 1128
2022-12-26 06:51:33.324891: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60883700 of size 10240 next 1100
2022-12-26 06:51:33.324894: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60885f00 of size 10240 next 1095
2022-12-26 06:51:33.324896: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60888700 of size 10240 next 1124
2022-12-26 06:51:33.324899: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6088af00 of size 12032 next 1051
2022-12-26 06:51:33.324902: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6088de00 of size 256 next 1022
2022-12-26 06:51:33.324905: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6088df00 of size 256 next 1063
2022-12-26 06:51:33.324908: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6088e000 of size 256 next 1025
2022-12-26 06:51:33.324911: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6088e100 of size 256 next 1041
2022-12-26 06:51:33.324914: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6088e200 of size 256 next 1034
2022-12-26 06:51:33.324917: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6088e300 of size 256 next 1024
2022-12-26 06:51:33.324920: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6088e400 of size 256 next 1021
2022-12-26 06:51:33.324924: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6088e500 of size 256 next 1023
2022-12-26 06:51:33.324927: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6088e600 of size 10240 next 1364
2022-12-26 06:51:33.324930: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60890e00 of size 10240 next 1045
2022-12-26 06:51:33.324933: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60893600 of size 10240 next 1371
2022-12-26 06:51:33.324936: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60895e00 of size 10240 next 1032
2022-12-26 06:51:33.324939: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60898600 of size 10240 next 1033
2022-12-26 06:51:33.324942: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6089ae00 of size 2560 next 1040
2022-12-26 06:51:33.324944: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6089b800 of size 2560 next 1026
2022-12-26 06:51:33.324947: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6089c200 of size 2560 next 1057
2022-12-26 06:51:33.324950: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6089cc00 of size 2560 next 1020
2022-12-26 06:51:33.324953: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6089d600 of size 2560 next 1029
2022-12-26 06:51:33.324956: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6089e000 of size 10240 next 1042
2022-12-26 06:51:33.324959: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608a0800 of size 10240 next 1048
2022-12-26 06:51:33.324962: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608a3000 of size 10240 next 1056
2022-12-26 06:51:33.324965: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608a5800 of size 10240 next 1050
2022-12-26 06:51:33.324968: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608a8000 of size 10240 next 1062
2022-12-26 06:51:33.324971: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608aa800 of size 5120 next 1052
2022-12-26 06:51:33.324974: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608abc00 of size 256 next 1058
2022-12-26 06:51:33.324977: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608abd00 of size 256 next 1030
2022-12-26 06:51:33.324979: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608abe00 of size 10240 next 1049
2022-12-26 06:51:33.324982: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608ae600 of size 10240 next 1046
2022-12-26 06:51:33.324985: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608b0e00 of size 10240 next 1027
2022-12-26 06:51:33.324988: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608b3600 of size 2560 next 1031
2022-12-26 06:51:33.324991: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608b4000 of size 2560 next 1044
2022-12-26 06:51:33.324994: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608b4a00 of size 2560 next 1047
2022-12-26 06:51:33.324997: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608b5400 of size 16896 next 978
2022-12-26 06:51:33.325002: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608b9600 of size 256 next 980
2022-12-26 06:51:33.325005: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608b9700 of size 10240 next 1135
2022-12-26 06:51:33.325008: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608bbf00 of size 10240 next 1131
2022-12-26 06:51:33.325011: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608be700 of size 2560 next 1137
2022-12-26 06:51:33.325014: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608bf100 of size 2560 next 1139
2022-12-26 06:51:33.325017: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608bfb00 of size 2560 next 1140
2022-12-26 06:51:33.325022: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c0500 of size 10240 next 1142
2022-12-26 06:51:33.325025: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c2d00 of size 10240 next 1143
2022-12-26 06:51:33.325027: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c5500 of size 10240 next 1144
2022-12-26 06:51:33.325030: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c7d00 of size 5120 next 1146
2022-12-26 06:51:33.325033: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c9100 of size 256 next 1147
2022-12-26 06:51:33.325036: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c9200 of size 256 next 1148
2022-12-26 06:51:33.325039: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c9300 of size 256 next 1150
2022-12-26 06:51:33.325042: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c9400 of size 256 next 1151
2022-12-26 06:51:33.325045: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c9500 of size 256 next 1005
2022-12-26 06:51:33.325048: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c9600 of size 256 next 1004
2022-12-26 06:51:33.325051: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c9700 of size 256 next 1154
2022-12-26 06:51:33.325054: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c9800 of size 256 next 1155
2022-12-26 06:51:33.325056: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c9900 of size 256 next 1156
2022-12-26 06:51:33.325059: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c9a00 of size 256 next 1157
2022-12-26 06:51:33.325062: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c9b00 of size 256 next 1158
2022-12-26 06:51:33.325065: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608c9c00 of size 10240 next 1207
2022-12-26 06:51:33.325068: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608cc400 of size 10240 next 1215
2022-12-26 06:51:33.325071: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608cec00 of size 10240 next 1206
2022-12-26 06:51:33.325074: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608d1400 of size 10240 next 1214
2022-12-26 06:51:33.325077: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608d3c00 of size 10240 next 1224
2022-12-26 06:51:33.325080: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608d6400 of size 10240 next 1205
2022-12-26 06:51:33.325083: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608d8c00 of size 5120 next 1216
2022-12-26 06:51:33.325086: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608da000 of size 256 next 1212
2022-12-26 06:51:33.325089: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608da100 of size 256 next 1217
2022-12-26 06:51:33.325091: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608da200 of size 256 next 1226
2022-12-26 06:51:33.325094: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608da300 of size 256 next 1222
2022-12-26 06:51:33.325097: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608da400 of size 256 next 1227
2022-12-26 06:51:33.325100: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608da500 of size 256 next 1225
2022-12-26 06:51:33.325103: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608da600 of size 256 next 1228
2022-12-26 06:51:33.325106: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608da700 of size 256 next 1229
2022-12-26 06:51:33.325109: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608da800 of size 256 next 1230
2022-12-26 06:51:33.325112: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608da900 of size 256 next 1231
2022-12-26 06:51:33.325116: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608daa00 of size 256 next 1234
2022-12-26 06:51:33.325119: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608dab00 of size 10240 next 1245
2022-12-26 06:51:33.325122: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608dd300 of size 10240 next 1243
2022-12-26 06:51:33.325125: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608dfb00 of size 10240 next 1244
2022-12-26 06:51:33.325128: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608e2300 of size 5120 next 1237
2022-12-26 06:51:33.325131: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608e3700 of size 10240 next 1272
2022-12-26 06:51:33.325134: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608e5f00 of size 10240 next 1280
2022-12-26 06:51:33.325137: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608e8700 of size 10240 next 1281
2022-12-26 06:51:33.325139: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608eaf00 of size 2560 next 1283
2022-12-26 06:51:33.325142: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608eb900 of size 2560 next 1284
2022-12-26 06:51:33.325145: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608ec300 of size 10240 next 1286
2022-12-26 06:51:33.325148: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608eeb00 of size 10240 next 1287
2022-12-26 06:51:33.325151: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f1300 of size 10240 next 1288
2022-12-26 06:51:33.325154: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f3b00 of size 5120 next 1291
2022-12-26 06:51:33.325157: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f4f00 of size 256 next 1293
2022-12-26 06:51:33.325160: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5000 of size 256 next 1292
2022-12-26 06:51:33.325163: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5100 of size 256 next 1294
2022-12-26 06:51:33.325166: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5200 of size 256 next 1295
2022-12-26 06:51:33.325169: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5300 of size 256 next 1298
2022-12-26 06:51:33.325171: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5400 of size 256 next 1299
2022-12-26 06:51:33.325174: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5500 of size 256 next 1300
2022-12-26 06:51:33.325177: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5600 of size 256 next 1301
2022-12-26 06:51:33.325180: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5700 of size 256 next 1302
2022-12-26 06:51:33.325183: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5800 of size 256 next 1303
2022-12-26 06:51:33.325186: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5900 of size 256 next 1304
2022-12-26 06:51:33.325189: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5a00 of size 256 next 1307
2022-12-26 06:51:33.325192: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5b00 of size 256 next 1313
2022-12-26 06:51:33.325195: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5c00 of size 256 next 1340
2022-12-26 06:51:33.325197: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5d00 of size 256 next 1321
2022-12-26 06:51:33.325200: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5e00 of size 256 next 1315
2022-12-26 06:51:33.325203: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f5f00 of size 256 next 1325
2022-12-26 06:51:33.325206: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f6000 of size 256 next 1336
2022-12-26 06:51:33.325211: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f6100 of size 10240 next 1306
2022-12-26 06:51:33.325214: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608f8900 of size 10240 next 1658
2022-12-26 06:51:33.325217: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608fb100 of size 10240 next 1343
2022-12-26 06:51:33.325220: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd608fd900 of size 10240 next 1348
2022-12-26 06:51:33.325222: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60900100 of size 10240 next 1305
2022-12-26 06:51:33.325225: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60902900 of size 2560 next 1341
2022-12-26 06:51:33.325228: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60903300 of size 2560 next 1320
2022-12-26 06:51:33.325231: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60903d00 of size 2560 next 1322
2022-12-26 06:51:33.325234: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60904700 of size 2560 next 1310
2022-12-26 06:51:33.325237: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60905100 of size 2560 next 1329
2022-12-26 06:51:33.325240: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60905b00 of size 10240 next 1311
2022-12-26 06:51:33.325243: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60908300 of size 10240 next 1314
2022-12-26 06:51:33.325246: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6090ab00 of size 10240 next 1328
2022-12-26 06:51:33.325249: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6090d300 of size 10240 next 1338
2022-12-26 06:51:33.325252: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6090fb00 of size 10240 next 1323
2022-12-26 06:51:33.325255: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60912300 of size 5120 next 1327
2022-12-26 06:51:33.325258: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60913700 of size 256 next 1319
2022-12-26 06:51:33.325260: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60913800 of size 256 next 1333
2022-12-26 06:51:33.325263: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fdd60913900 of size 30976 next 1279
2022-12-26 06:51:33.325266: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6091b200 of size 256 next 1269
2022-12-26 06:51:33.325269: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6091b300 of size 256 next 1247
2022-12-26 06:51:33.325272: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6091b400 of size 256 next 1262
2022-12-26 06:51:33.325275: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6091b500 of size 256 next 1260
2022-12-26 06:51:33.325278: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6091b600 of size 256 next 1267
2022-12-26 06:51:33.325281: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6091b700 of size 256 next 1166
2022-12-26 06:51:33.325284: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6091b800 of size 256 next 1168
2022-12-26 06:51:33.325287: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6091b900 of size 10240 next 1512
2022-12-26 06:51:33.325290: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6091e100 of size 2560 next 1181
2022-12-26 06:51:33.325293: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6091eb00 of size 2560 next 1178
2022-12-26 06:51:33.325295: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6091f500 of size 2560 next 1163
2022-12-26 06:51:33.325298: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6091ff00 of size 2560 next 1183
2022-12-26 06:51:33.325301: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60920900 of size 2560 next 1189
2022-12-26 06:51:33.325306: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60921300 of size 10240 next 1194
2022-12-26 06:51:33.325309: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60923b00 of size 10240 next 1162
2022-12-26 06:51:33.325312: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60926300 of size 10240 next 1177
2022-12-26 06:51:33.325315: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60928b00 of size 10240 next 1170
2022-12-26 06:51:33.325317: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6092b300 of size 10240 next 1160
2022-12-26 06:51:33.325320: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6092db00 of size 5120 next 1174
2022-12-26 06:51:33.325323: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6092ef00 of size 256 next 1165
2022-12-26 06:51:33.325326: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6092f000 of size 256 next 1195
2022-12-26 06:51:33.325329: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6092f100 of size 256 next 1188
2022-12-26 06:51:33.325332: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6092f200 of size 10240 next 1184
2022-12-26 06:51:33.325335: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60931a00 of size 10240 next 1175
2022-12-26 06:51:33.325338: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60934200 of size 10240 next 1159
2022-12-26 06:51:33.325341: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60936a00 of size 2560 next 1196
2022-12-26 06:51:33.325344: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60937400 of size 2560 next 1190
2022-12-26 06:51:33.325347: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60937e00 of size 2560 next 1193
2022-12-26 06:51:33.325350: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60938800 of size 10240 next 1185
2022-12-26 06:51:33.325352: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6093b000 of size 10240 next 1198
2022-12-26 06:51:33.325355: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6093d800 of size 10240 next 1204
2022-12-26 06:51:33.325358: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60940000 of size 5120 next 1197
2022-12-26 06:51:33.325361: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60941400 of size 2560 next 1211
2022-12-26 06:51:33.325364: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60941e00 of size 2560 next 1213
2022-12-26 06:51:33.325367: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60942800 of size 4352 next 1171
2022-12-26 06:51:33.325370: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60943900 of size 256 next 1186
2022-12-26 06:51:33.325373: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60943a00 of size 256 next 1201
2022-12-26 06:51:33.325376: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60943b00 of size 256 next 1172
2022-12-26 06:51:33.325379: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60943c00 of size 256 next 1191
2022-12-26 06:51:33.325382: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60943d00 of size 256 next 1169
2022-12-26 06:51:33.325385: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60943e00 of size 10240 next 1515
2022-12-26 06:51:33.325388: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60946600 of size 11520 next 1173
2022-12-26 06:51:33.325391: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60949300 of size 256 next 1167
2022-12-26 06:51:33.325394: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60949400 of size 10240 next 1199
2022-12-26 06:51:33.325398: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6094bc00 of size 15616 next 1115
2022-12-26 06:51:33.325401: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6094f900 of size 256 next 1116
2022-12-26 06:51:33.325404: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6094fa00 of size 10240 next 1275
2022-12-26 06:51:33.325406: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60952200 of size 10240 next 1253
2022-12-26 06:51:33.325409: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60954a00 of size 10240 next 1589
2022-12-26 06:51:33.325412: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60957200 of size 10240 next 1579
2022-12-26 06:51:33.325415: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60959a00 of size 10240 next 1590
2022-12-26 06:51:33.325418: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6095c200 of size 2560 next 1236
2022-12-26 06:51:33.325421: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6095cc00 of size 2560 next 1271
2022-12-26 06:51:33.325424: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6095d600 of size 2560 next 1257
2022-12-26 06:51:33.325427: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6095e000 of size 2560 next 1263
2022-12-26 06:51:33.325430: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6095ea00 of size 2560 next 1232
2022-12-26 06:51:33.325433: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6095f400 of size 10240 next 1252
2022-12-26 06:51:33.325436: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60961c00 of size 10240 next 1254
2022-12-26 06:51:33.325439: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60964400 of size 10240 next 1248
2022-12-26 06:51:33.325442: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60966c00 of size 10240 next 1249
2022-12-26 06:51:33.325445: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60969400 of size 10240 next 1264
2022-12-26 06:51:33.325447: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6096bc00 of size 5120 next 1255
2022-12-26 06:51:33.325450: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6096d000 of size 256 next 1270
2022-12-26 06:51:33.325453: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6096d100 of size 10240 next 1241
2022-12-26 06:51:33.325456: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6096f900 of size 10240 next 1276
2022-12-26 06:51:33.325459: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60972100 of size 10240 next 1268
2022-12-26 06:51:33.325462: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60974900 of size 2560 next 1114
2022-12-26 06:51:33.325465: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60975300 of size 2560 next 1251
2022-12-26 06:51:33.325468: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60975d00 of size 2560 next 1261
2022-12-26 06:51:33.325471: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60976700 of size 3840 next 1235
2022-12-26 06:51:33.325474: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60977600 of size 256 next 1240
2022-12-26 06:51:33.325477: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60977700 of size 256 next 1273
2022-12-26 06:51:33.325480: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60977800 of size 256 next 1250
2022-12-26 06:51:33.325483: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fdd60977900 of size 1256960 next 24
2022-12-26 06:51:33.325486: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd60aaa700 of size 6390784 next 25
2022-12-26 06:51:33.325489: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd610c2b00 of size 12781568 next 45
2022-12-26 06:51:33.325493: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd61cf3300 of size 6390784 next 52
2022-12-26 06:51:33.325496: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd6230b700 of size 6390784 next 32
2022-12-26 06:51:33.325499: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd62923b00 of size 12781568 next 31
2022-12-26 06:51:33.325502: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd63554300 of size 482853120 next 35
2022-12-26 06:51:33.325505: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd801d0400 of size 482853120 next 36
2022-12-26 06:51:33.325508: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd9ce4c500 of size 12781568 next 56
2022-12-26 06:51:33.325511: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd9da7cd00 of size 12781568 next 58
2022-12-26 06:51:33.325514: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd9e6ad500 of size 6390784 next 65
2022-12-26 06:51:33.325517: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd9ecc5900 of size 12781568 next 69
2022-12-26 06:51:33.325520: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd9f8f6100 of size 6390784 next 92
2022-12-26 06:51:33.325523: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdd9ff0e500 of size 6390784 next 88
2022-12-26 06:51:33.325526: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdda0526900 of size 6390784 next 107
2022-12-26 06:51:33.325529: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdda0b3ed00 of size 6390784 next 109
2022-12-26 06:51:33.325532: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdda1157100 of size 12781568 next 100
2022-12-26 06:51:33.325535: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdda1d87900 of size 12781568 next 105
2022-12-26 06:51:33.325537: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdda29b8100 of size 12781568 next 114
2022-12-26 06:51:33.325540: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdda35e8900 of size 12781568 next 104
2022-12-26 06:51:33.325543: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdda4219100 of size 12781568 next 133
2022-12-26 06:51:33.325546: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdda4e49900 of size 6390784 next 137
2022-12-26 06:51:33.325549: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdda5461d00 of size 11446784 next 83
2022-12-26 06:51:33.325552: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdda5f4c700 of size 53654272 next 111
2022-12-26 06:51:33.325555: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdda9277a00 of size 482853120 next 117
2022-12-26 06:51:33.325558: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fddc5ef3b00 of size 482853120 next 90
2022-12-26 06:51:33.325561: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdde2b6fc00 of size 12781568 next 144
2022-12-26 06:51:33.325564: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdde37a0400 of size 53654272 next 179
2022-12-26 06:51:33.325567: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdde6acb700 of size 53654272 next 187
2022-12-26 06:51:33.325570: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdde9df6a00 of size 12781568 next 193
2022-12-26 06:51:33.325573: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fddeaa27200 of size 6390784 next 192
2022-12-26 06:51:33.325576: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fddeb03f600 of size 6390784 next 182
2022-12-26 06:51:33.325579: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fddeb657a00 of size 6390784 next 181
2022-12-26 06:51:33.325582: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fddebc6fe00 of size 6390784 next 184
2022-12-26 06:51:33.325586: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fddec288200 of size 12781568 next 176
2022-12-26 06:51:33.325589: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fddeceb8a00 of size 12781568 next 180
2022-12-26 06:51:33.325592: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fddedae9200 of size 15309568 next 157
2022-12-26 06:51:33.325595: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fddee982d00 of size 12781568 next 199
2022-12-26 06:51:33.325598: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fddef5b3500 of size 482853120 next 188
2022-12-26 06:51:33.325601: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde0c22f600 of size 482853120 next 178
2022-12-26 06:51:33.325604: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde28eab700 of size 6390784 next 208
2022-12-26 06:51:33.325607: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde294c3b00 of size 6390784 next 212
2022-12-26 06:51:33.325610: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde29adbf00 of size 12781568 next 216
2022-12-26 06:51:33.325613: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde2a70c700 of size 53654272 next 250
2022-12-26 06:51:33.325616: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde2da37a00 of size 53654272 next 258
2022-12-26 06:51:33.325619: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde30d62d00 of size 6390784 next 256
2022-12-26 06:51:33.325621: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde3137b100 of size 6390784 next 241
2022-12-26 06:51:33.325624: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde31993500 of size 6390784 next 229
2022-12-26 06:51:33.325627: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde31fab900 of size 6390784 next 260
2022-12-26 06:51:33.325630: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde325c3d00 of size 12781568 next 577
2022-12-26 06:51:33.325633: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde331f4500 of size 12781568 next 248
2022-12-26 06:51:33.325636: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde33e24d00 of size 12781568 next 251
2022-12-26 06:51:33.325639: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde34a55500 of size 12781568 next 239
2022-12-26 06:51:33.325642: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde35685d00 of size 482853120 next 267
2022-12-26 06:51:33.325645: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde52301e00 of size 482853120 next 246
2022-12-26 06:51:33.325648: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde6ef7df00 of size 12781568 next 275
2022-12-26 06:51:33.325651: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde6fbae700 of size 6390784 next 278
2022-12-26 06:51:33.325654: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde701c6b00 of size 6390784 next 282
2022-12-26 06:51:33.325656: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde707def00 of size 12781568 next 286
2022-12-26 06:51:33.325659: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde7140f700 of size 53654272 next 323
2022-12-26 06:51:33.325662: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde7473aa00 of size 53654272 next 327
2022-12-26 06:51:33.325665: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde77a65d00 of size 6390784 next 343
2022-12-26 06:51:33.325668: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde7807e100 of size 6390784 next 316
2022-12-26 06:51:33.325671: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde78696500 of size 6390784 next 306
2022-12-26 06:51:33.325674: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde78cae900 of size 6390784 next 334
2022-12-26 06:51:33.325679: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde792c6d00 of size 12781568 next 337
2022-12-26 06:51:33.325682: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde79ef7500 of size 12781568 next 336
2022-12-26 06:51:33.325685: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde7ab27d00 of size 12781568 next 312
2022-12-26 06:51:33.325688: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde7b758500 of size 12781568 next 331
2022-12-26 06:51:33.325691: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde7c388d00 of size 482853120 next 317
2022-12-26 06:51:33.325694: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fde99004e00 of size 482853120 next 335
2022-12-26 06:51:33.325697: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdeb5c80f00 of size 12781568 next 347
2022-12-26 06:51:33.325700: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdeb68b1700 of size 6390784 next 351
2022-12-26 06:51:33.325703: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdeb6ec9b00 of size 6390784 next 357
2022-12-26 06:51:33.325706: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdeb74e1f00 of size 12781568 next 359
2022-12-26 06:51:33.325708: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdeb8112700 of size 53654272 next 411
2022-12-26 06:51:33.325711: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdebb43da00 of size 53654272 next 387
2022-12-26 06:51:33.325714: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdebe768d00 of size 6390784 next 414
2022-12-26 06:51:33.325717: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdebed81100 of size 6390784 next 408
2022-12-26 06:51:33.325720: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdebf399500 of size 6390784 next 383
2022-12-26 06:51:33.325723: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdebf9b1900 of size 6390784 next 723
2022-12-26 06:51:33.325726: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdebffc9d00 of size 12781568 next 572
2022-12-26 06:51:33.325729: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdec0bfa500 of size 12781568 next 389
2022-12-26 06:51:33.325732: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdec182ad00 of size 12781568 next 415
2022-12-26 06:51:33.325735: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdec245b500 of size 12781568 next 419
2022-12-26 06:51:33.325738: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdec308bd00 of size 482853120 next 399
2022-12-26 06:51:33.325741: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdedfd07e00 of size 482853120 next 391
2022-12-26 06:51:33.325744: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdefc983f00 of size 12781568 next 400
2022-12-26 06:51:33.325746: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdefd5b4700 of size 6390784 next 422
2022-12-26 06:51:33.325749: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdefdbccb00 of size 6390784 next 421
2022-12-26 06:51:33.325752: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdefe1e4f00 of size 12781568 next 186
2022-12-26 06:51:33.325755: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdefee15700 of size 6390784 next 346
2022-12-26 06:51:33.325758: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdeff42db00 of size 6390784 next 476
2022-12-26 06:51:33.325761: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdeffa45f00 of size 6390784 next 472
2022-12-26 06:51:33.325764: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf0005e300 of size 6390784 next 795
2022-12-26 06:51:33.325767: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf00676700 of size 12781568 next 793
2022-12-26 06:51:33.325771: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf012a6f00 of size 12781568 next 442
2022-12-26 06:51:33.325774: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf01ed7700 of size 12781568 next 454
2022-12-26 06:51:33.325777: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf02b07f00 of size 12781568 next 447
2022-12-26 06:51:33.325780: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf03738700 of size 482853120 next 456
2022-12-26 06:51:33.325783: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf203b4800 of size 482853120 next 464
2022-12-26 06:51:33.325786: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf3d030900 of size 12781568 next 448
2022-12-26 06:51:33.325789: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf3dc61100 of size 6390784 next 489
2022-12-26 06:51:33.325792: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf3e279500 of size 6390784 next 493
2022-12-26 06:51:33.325795: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf3e891900 of size 12781568 next 497
2022-12-26 06:51:33.325797: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf3f4c2100 of size 6390784 next 517
2022-12-26 06:51:33.325800: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf3fada500 of size 6390784 next 532
2022-12-26 06:51:33.325803: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf400f2900 of size 6390784 next 528
2022-12-26 06:51:33.325806: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf4070ad00 of size 6390784 next 866
2022-12-26 06:51:33.325809: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf40d23100 of size 12781568 next 865
2022-12-26 06:51:33.325812: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf41953900 of size 12781568 next 524
2022-12-26 06:51:33.325815: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf42584100 of size 12781568 next 518
2022-12-26 06:51:33.325818: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf431b4900 of size 12781568 next 541
2022-12-26 06:51:33.325821: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf43de5100 of size 482853120 next 535
2022-12-26 06:51:33.325824: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf60a61200 of size 482853120 next 514
2022-12-26 06:51:33.325827: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf7d6dd300 of size 12781568 next 555
2022-12-26 06:51:33.325830: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf7e30db00 of size 6390784 next 564
2022-12-26 06:51:33.325833: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf7e925f00 of size 6390784 next 449
2022-12-26 06:51:33.325835: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf7ef3e300 of size 12781568 next 465
2022-12-26 06:51:33.325838: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf7fb6eb00 of size 6390784 next 592
2022-12-26 06:51:33.325841: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf80186f00 of size 6390784 next 612
2022-12-26 06:51:33.325844: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf8079f300 of size 6390784 next 620
2022-12-26 06:51:33.325847: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf80db7700 of size 6390784 next 615
2022-12-26 06:51:33.325850: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf813cfb00 of size 12781568 next 618
2022-12-26 06:51:33.325853: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf82000300 of size 12781568 next 628
2022-12-26 06:51:33.325856: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf82c30b00 of size 12781568 next 633
2022-12-26 06:51:33.325859: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf83861300 of size 6390784 next 675
2022-12-26 06:51:33.325863: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf83e79700 of size 6390784 next 630
2022-12-26 06:51:33.325866: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf84491b00 of size 12781568 next 632
2022-12-26 06:51:33.325869: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf850c2300 of size 6390784 next 643
2022-12-26 06:51:33.325872: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf856da700 of size 11446784 next 601
2022-12-26 06:51:33.325875: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf861c5100 of size 53654272 next 607
2022-12-26 06:51:33.325878: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdf894f0400 of size 482853120 next 597
2022-12-26 06:51:33.325881: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfa616c500 of size 482853120 next 629
2022-12-26 06:51:33.325883: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfc2de8600 of size 12781568 next 651
2022-12-26 06:51:33.325886: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfc3a18e00 of size 6390784 next 677
2022-12-26 06:51:33.325889: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfc4031200 of size 6390784 next 705
2022-12-26 06:51:33.325892: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfc4649600 of size 12781568 next 683
2022-12-26 06:51:33.325895: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfc5279e00 of size 12781568 next 681
2022-12-26 06:51:33.325898: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfc5eaa600 of size 12781568 next 707
2022-12-26 06:51:33.325901: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfc6adae00 of size 12781568 next 668
2022-12-26 06:51:33.325904: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfc770b600 of size 12781568 next 714
2022-12-26 06:51:33.325907: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfc833be00 of size 6390784 next 719
2022-12-26 06:51:33.325910: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfc8954200 of size 6390784 next 725
2022-12-26 06:51:33.325913: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfc8f6c600 of size 12781568 next 729
2022-12-26 06:51:33.325916: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfc9b9ce00 of size 6390784 next 771
2022-12-26 06:51:33.325918: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfca1b5200 of size 11446784 next 704
2022-12-26 06:51:33.325921: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfcac9fc00 of size 12781568 next 674
2022-12-26 06:51:33.325924: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfcb8d0400 of size 53654272 next 692
2022-12-26 06:51:33.325927: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfcebfb700 of size 482853120 next 693
2022-12-26 06:51:33.325930: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fdfeb877800 of size 482853120 next 670
2022-12-26 06:51:33.325933: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0084f3900 of size 6390784 next 786
2022-12-26 06:51:33.325936: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe008b0bd00 of size 6390784 next 686
2022-12-26 06:51:33.325939: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe009124100 of size 12781568 next 1082
2022-12-26 06:51:33.325942: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe009d54900 of size 12781568 next 785
2022-12-26 06:51:33.325945: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe00a985100 of size 12781568 next 744
2022-12-26 06:51:33.325948: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe00b5b5900 of size 12781568 next 782
2022-12-26 06:51:33.325951: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe00c1e6100 of size 12781568 next 794
2022-12-26 06:51:33.325955: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe00ce16900 of size 6390784 next 798
2022-12-26 06:51:33.325958: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe00d42ed00 of size 6390784 next 802
2022-12-26 06:51:33.325961: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe00da47100 of size 17837568 next 751
2022-12-26 06:51:33.325966: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe00eb49f00 of size 53654272 next 778
2022-12-26 06:51:33.325969: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe011e75200 of size 482853120 next 768
2022-12-26 06:51:33.325972: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe02eaf1300 of size 482853120 next 770
2022-12-26 06:51:33.325975: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe04b76d400 of size 6390784 next 792
2022-12-26 06:51:33.325978: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe04bd85800 of size 19172352 next 838
2022-12-26 06:51:33.325981: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe04cfce400 of size 12781568 next 827
2022-12-26 06:51:33.325984: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe04dbfec00 of size 21700352 next 837
2022-12-26 06:51:33.325987: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe04f0b0b00 of size 6390784 next 834
2022-12-26 06:51:33.325990: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe04f6c8f00 of size 6390784 next 858
2022-12-26 06:51:33.325993: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe04fce1300 of size 6390784 next 861
2022-12-26 06:51:33.325996: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0502f9700 of size 12781568 next 1153
2022-12-26 06:51:33.325999: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe050f29f00 of size 482853120 next 826
2022-12-26 06:51:33.326002: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe06dba6000 of size 482853120 next 824
2022-12-26 06:51:33.326005: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe08a822100 of size 12781568 next 850
2022-12-26 06:51:33.326008: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe08b452900 of size 6390784 next 580
2022-12-26 06:51:33.326011: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe08ba6ad00 of size 6390784 next 868
2022-12-26 06:51:33.326013: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe08c083100 of size 12781568 next 872
2022-12-26 06:51:33.326016: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe08ccb3900 of size 6390784 next 926
2022-12-26 06:51:33.326019: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe08d2cbd00 of size 6390784 next 901
2022-12-26 06:51:33.326022: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe08d8e4100 of size 12781568 next 899
2022-12-26 06:51:33.326025: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe08e514900 of size 12781568 next 923
2022-12-26 06:51:33.326028: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe08f145100 of size 12781568 next 928
2022-12-26 06:51:33.326031: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe08fd75900 of size 15309568 next 914
2022-12-26 06:51:33.326034: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe090c0f400 of size 6390784 next 903
2022-12-26 06:51:33.326037: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe091227800 of size 6390784 next 920
2022-12-26 06:51:33.326040: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe09183fc00 of size 12781568 next 1220
2022-12-26 06:51:33.326043: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe092470400 of size 482853120 next 891
2022-12-26 06:51:33.326046: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0af0ec500 of size 482853120 next 922
2022-12-26 06:51:33.326049: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0cbd68600 of size 6390784 next 931
2022-12-26 06:51:33.326053: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0cc380a00 of size 6390784 next 935
2022-12-26 06:51:33.326056: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0cc998e00 of size 12781568 next 789
2022-12-26 06:51:33.326058: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0cd5c9600 of size 53654272 next 776
2022-12-26 06:51:33.326061: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0d08f4900 of size 53654272 next 956
2022-12-26 06:51:33.326064: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0d3c1fc00 of size 12781568 next 986
2022-12-26 06:51:33.326067: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0d4850400 of size 6390784 next 983
2022-12-26 06:51:33.326070: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0d4e68800 of size 6390784 next 851
2022-12-26 06:51:33.326073: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0d5480c00 of size 6390784 next 970
2022-12-26 06:51:33.326076: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0d5a99000 of size 6390784 next 996
2022-12-26 06:51:33.326079: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0d60b1400 of size 12781568 next 968
2022-12-26 06:51:33.326082: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0d6ce1c00 of size 12781568 next 995
2022-12-26 06:51:33.326085: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0d7912400 of size 15309568 next 991
2022-12-26 06:51:33.326088: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0d87abf00 of size 12781568 next 1297
2022-12-26 06:51:33.326091: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0d93dc700 of size 482853120 next 963
2022-12-26 06:51:33.326094: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe0f6058800 of size 482853120 next 957
2022-12-26 06:51:33.326097: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe112cd4900 of size 6390784 next 998
2022-12-26 06:51:33.326099: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1132ecd00 of size 6390784 next 1000
2022-12-26 06:51:33.326102: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe113905100 of size 12781568 next 1008
2022-12-26 06:51:33.326105: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe114535900 of size 53654272 next 1054
2022-12-26 06:51:33.326108: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe117860c00 of size 53654272 next 1035
2022-12-26 06:51:33.326111: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe11ab8bf00 of size 6390784 next 1037
2022-12-26 06:51:33.326114: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe11b1a4300 of size 6390784 next 1060
2022-12-26 06:51:33.326117: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe11b7bc700 of size 6390784 next 1055
2022-12-26 06:51:33.326120: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe11bdd4b00 of size 6390784 next 1266
2022-12-26 06:51:33.326123: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe11c3ecf00 of size 12781568 next 1365
2022-12-26 06:51:33.326126: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe11d01d700 of size 12781568 next 1061
2022-12-26 06:51:33.326129: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe11dc4df00 of size 12781568 next 1043
2022-12-26 06:51:33.326132: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe11e87e700 of size 12781568 next 1019
2022-12-26 06:51:33.326135: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe11f4aef00 of size 482853120 next 1036
2022-12-26 06:51:33.326138: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe13c12b000 of size 482853120 next 1053
2022-12-26 06:51:33.326141: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe158da7100 of size 12781568 next 1039
2022-12-26 06:51:33.326145: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1599d7900 of size 6390784 next 1067
2022-12-26 06:51:33.326147: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe159fefd00 of size 6390784 next 1071
2022-12-26 06:51:33.326150: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe15a608100 of size 12781568 next 1075
2022-12-26 06:51:33.326153: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe15b238900 of size 53654272 next 1101
2022-12-26 06:51:33.326156: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe15e563c00 of size 53654272 next 1108
2022-12-26 06:51:33.326159: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe16188ef00 of size 6390784 next 1120
2022-12-26 06:51:33.326162: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe161ea7300 of size 6390784 next 1099
2022-12-26 06:51:33.326165: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1624bf700 of size 6390784 next 1111
2022-12-26 06:51:33.326168: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe162ad7b00 of size 6390784 next 1444
2022-12-26 06:51:33.326171: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1630eff00 of size 12781568 next 1412
2022-12-26 06:51:33.326174: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe163d20700 of size 12781568 next 1123
2022-12-26 06:51:33.326177: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe164950f00 of size 12781568 next 1102
2022-12-26 06:51:33.326180: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe165581700 of size 12781568 next 1097
2022-12-26 06:51:33.326182: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1661b1f00 of size 482853120 next 1109
2022-12-26 06:51:33.326185: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe182e2e000 of size 482853120 next 1126
2022-12-26 06:51:33.326188: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe19faaa100 of size 12781568 next 1136
2022-12-26 06:51:33.326191: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1a06da900 of size 6390784 next 1138
2022-12-26 06:51:33.326194: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1a0cf2d00 of size 6390784 next 1141
2022-12-26 06:51:33.326197: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1a130b100 of size 12781568 next 1145
2022-12-26 06:51:33.326200: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1a1f3b900 of size 53654272 next 1187
2022-12-26 06:51:33.326203: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1a5266c00 of size 53654272 next 1179
2022-12-26 06:51:33.326206: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1a8591f00 of size 6390784 next 1203
2022-12-26 06:51:33.326209: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1a8baa300 of size 6390784 next 1161
2022-12-26 06:51:33.326212: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1a91c2700 of size 6390784 next 1180
2022-12-26 06:51:33.326215: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1a97dab00 of size 6390784 next 1200
2022-12-26 06:51:33.326218: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1a9df2f00 of size 12781568 next 1516
2022-12-26 06:51:33.326220: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1aaa23700 of size 12781568 next 1192
2022-12-26 06:51:33.326223: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1ab653f00 of size 12781568 next 882
2022-12-26 06:51:33.326226: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1ac284700 of size 12781568 next 1164
2022-12-26 06:51:33.326229: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1aceb4f00 of size 482853120 next 1182
2022-12-26 06:51:33.326235: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1c9b31000 of size 482853120 next 1202
2022-12-26 06:51:33.326238: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1e67ad100 of size 12781568 next 1208
2022-12-26 06:51:33.326241: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1e73dd900 of size 6390784 next 1209
2022-12-26 06:51:33.326244: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1e79f5d00 of size 6390784 next 1210
2022-12-26 06:51:33.326247: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1e800e100 of size 12781568 next 1218
2022-12-26 06:51:33.326250: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1e8c3e900 of size 6390784 next 1242
2022-12-26 06:51:33.326253: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1e9256d00 of size 6390784 next 1239
2022-12-26 06:51:33.326255: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1e986f100 of size 6390784 next 1274
2022-12-26 06:51:33.326258: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1e9e87500 of size 6390784 next 1582
2022-12-26 06:51:33.326261: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1ea49f900 of size 12781568 next 1246
2022-12-26 06:51:33.326264: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1eb0d0100 of size 12781568 next 1256
2022-12-26 06:51:33.326267: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1ebd00900 of size 12781568 next 1105
2022-12-26 06:51:33.326270: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1ec931100 of size 12781568 next 1233
2022-12-26 06:51:33.326273: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe1ed561900 of size 482853120 next 1259
2022-12-26 06:51:33.326276: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe20a1dda00 of size 482853120 next 1265
2022-12-26 06:51:33.326279: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe226e59b00 of size 12781568 next 1258
2022-12-26 06:51:33.326282: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe227a8a300 of size 6390784 next 1282
2022-12-26 06:51:33.326285: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe2280a2700 of size 6390784 next 1285
2022-12-26 06:51:33.326288: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe2286bab00 of size 12781568 next 1289
2022-12-26 06:51:33.326291: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fe2292eb300 of size 6390784 next 1335
2022-12-26 06:51:33.326294: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe229903700 of size 6390784 next 1347
2022-12-26 06:51:33.326296: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe229f1bb00 of size 6390784 next 1326
2022-12-26 06:51:33.326299: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fe22a533f00 of size 6390784 next 1342
2022-12-26 06:51:33.326302: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe22ab4c300 of size 12781568 next 1312
2022-12-26 06:51:33.326305: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fe22b77cb00 of size 25563136 next 1334
2022-12-26 06:51:33.326308: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe22cfddb00 of size 12781568 next 1331
2022-12-26 06:51:33.326311: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fe22dc0e300 of size 30619136 next 1308
2022-12-26 06:51:33.326314: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe22f941900 of size 53654272 next 1345
2022-12-26 06:51:33.326317: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fe232c6cc00 of size 482853120 next 1317
2022-12-26 06:51:33.326320: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fe24f8e8d00 of size 407073536 next 18446744073709551615
2022-12-26 06:51:33.326323: I tensorflow/core/common_runtime/bfc_allocator.cc:1094]      Summary of in-use Chunks by size: 
2022-12-26 06:51:33.326329: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 399 Chunks of size 256 totalling 99.8KiB
2022-12-26 06:51:33.326333: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 1280 totalling 1.2KiB
2022-12-26 06:51:33.326336: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 198 Chunks of size 2560 totalling 495.0KiB
2022-12-26 06:51:33.326339: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 3840 totalling 3.8KiB
2022-12-26 06:51:33.326343: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 3 Chunks of size 4352 totalling 12.8KiB
2022-12-26 06:51:33.326346: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 4864 totalling 4.8KiB
2022-12-26 06:51:33.326349: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 54 Chunks of size 5120 totalling 270.0KiB
2022-12-26 06:51:33.326353: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 9472 totalling 9.2KiB
2022-12-26 06:51:33.326356: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 390 Chunks of size 10240 totalling 3.81MiB
2022-12-26 06:51:33.326359: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 11520 totalling 11.2KiB
2022-12-26 06:51:33.326363: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 12032 totalling 23.5KiB
2022-12-26 06:51:33.326366: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 12544 totalling 12.2KiB
2022-12-26 06:51:33.326369: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 12800 totalling 12.5KiB
2022-12-26 06:51:33.326373: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 15616 totalling 30.5KiB
2022-12-26 06:51:33.326376: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 16128 totalling 15.8KiB
2022-12-26 06:51:33.326379: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 16896 totalling 16.5KiB
2022-12-26 06:51:33.326383: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 17920 totalling 35.0KiB
2022-12-26 06:51:33.326386: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 18176 totalling 17.8KiB
2022-12-26 06:51:33.326389: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 18944 totalling 18.5KiB
2022-12-26 06:51:33.326392: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 19968 totalling 39.0KiB
2022-12-26 06:51:33.326396: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 20224 totalling 19.8KiB
2022-12-26 06:51:33.326399: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 105 Chunks of size 6390784 totalling 639.95MiB
2022-12-26 06:51:33.326402: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 3 Chunks of size 11446784 totalling 32.75MiB
2022-12-26 06:51:33.326406: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 12729344 totalling 12.14MiB
2022-12-26 06:51:33.326409: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 12769536 totalling 12.18MiB
2022-12-26 06:51:33.326412: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 104 Chunks of size 12781568 totalling 1.24GiB
2022-12-26 06:51:33.326416: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 3 Chunks of size 15309568 totalling 43.80MiB
2022-12-26 06:51:33.326419: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 17837568 totalling 17.01MiB
2022-12-26 06:51:33.326422: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 19172352 totalling 18.28MiB
2022-12-26 06:51:33.326426: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 21700352 totalling 20.69MiB
2022-12-26 06:51:33.326429: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 21 Chunks of size 53654272 totalling 1.05GiB
2022-12-26 06:51:33.326432: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 37 Chunks of size 482853120 totalling 16.64GiB
2022-12-26 06:51:33.326436: I tensorflow/core/common_runtime/bfc_allocator.cc:1101] Sum Total of in-use chunks: 19.71GiB
2022-12-26 06:51:33.326442: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] total_region_allocated_bytes_: 21639593984 memory_limit_: 21639593984 available bytes: 0 curr_region_allocation_bytes_: 43279187968
2022-12-26 06:51:33.326448: I tensorflow/core/common_runtime/bfc_allocator.cc:1109] Stats: 
Limit:                     21639593984
InUse:                     21162268672
MaxInUse:                  21162268672
NumAllocs:                   895351764
MaxAllocSize:                482853120
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-26 06:51:33.326469: W tensorflow/core/common_runtime/bfc_allocator.cc:491] ***************************************************************************************************_
Traceback (most recent call last):
  File "autoencoder_MP_Gap_Feats.py", line 43, in <module>
    main()
  File "autoencoder_MP_Gap_Feats.py", line 14, in main
    HyperParameterTestEncoding( dataset = Xtoencode,
  File "/auto/globalscratch/users/r/g/rgouvea/MODNet_Perovskite_Benchmark/Featurization/CompressionFunctions.py", line 95, in HyperParameterTestEncoding
    TestEncoding( prefix_name = prefix_name,
  File "/auto/globalscratch/users/r/g/rgouvea/MODNet_Perovskite_Benchmark/Featurization/CompressionFunctions.py", line 217, in TestEncoding
    history = model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size,
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py", line 102, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Mon Dec 26 06:51:41 CET 2022
done
