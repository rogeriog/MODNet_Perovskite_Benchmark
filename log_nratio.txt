start
Wed Dec 28 01:21:44 CET 2022
2022-12-28 01:21:45.246099: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-28 01:21:45.326073: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-12-28 01:22:17,924 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fb241fad7f0> object, created with modnet version 0.1.12
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
./DATAFILES/MP_GapFeats_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
2022-12-28 01:22:19.925800: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense (Dense)               (None, 1896)              2398440   
                                                                 
 batch_normalization (BatchN  (None, 1896)             7584      
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 1896)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               478044    
                                                                 
 batch_normalization_1 (Batc  (None, 252)              1008      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 252)               0         
                                                                 
 dense_1 (Dense)             (None, 1896)              479688    
                                                                 
 batch_normalization_2 (Batc  (None, 1896)             7584      
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 1896)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              2397808   
                                                                 
=================================================================
Total params: 5,770,156
Trainable params: 5,762,068
Non-trainable params: 8,088
_________________________________________________________________
Epoch 1/200
1493/1493 - 51s - loss: 0.0100 - val_loss: 0.0053 - 51s/epoch - 34ms/step
Epoch 2/200
1493/1493 - 50s - loss: 0.0036 - val_loss: 0.0029 - 50s/epoch - 33ms/step
Epoch 3/200
1493/1493 - 50s - loss: 0.0024 - val_loss: 0.0020 - 50s/epoch - 33ms/step
Epoch 4/200
1493/1493 - 50s - loss: 0.0020 - val_loss: 0.0026 - 50s/epoch - 33ms/step
Epoch 5/200
1493/1493 - 50s - loss: 0.0018 - val_loss: 0.0015 - 50s/epoch - 33ms/step
Epoch 6/200
1493/1493 - 50s - loss: 0.0016 - val_loss: 0.0017 - 50s/epoch - 33ms/step
Epoch 7/200
1493/1493 - 50s - loss: 0.0016 - val_loss: 0.0013 - 50s/epoch - 33ms/step
Epoch 8/200
1493/1493 - 50s - loss: 0.0014 - val_loss: 0.0016 - 50s/epoch - 33ms/step
Epoch 9/200
1493/1493 - 50s - loss: 0.0013 - val_loss: 0.0021 - 50s/epoch - 33ms/step
Epoch 10/200
1493/1493 - 50s - loss: 0.0014 - val_loss: 0.0011 - 50s/epoch - 33ms/step
Epoch 11/200
1493/1493 - 50s - loss: 0.0012 - val_loss: 0.0013 - 50s/epoch - 33ms/step
Epoch 12/200
1493/1493 - 50s - loss: 0.0011 - val_loss: 0.0022 - 50s/epoch - 33ms/step
Epoch 13/200
1493/1493 - 50s - loss: 0.0011 - val_loss: 0.0014 - 50s/epoch - 33ms/step
Epoch 14/200
1493/1493 - 50s - loss: 0.0010 - val_loss: 9.2582e-04 - 50s/epoch - 33ms/step
Epoch 15/200
1493/1493 - 50s - loss: 9.3940e-04 - val_loss: 8.8343e-04 - 50s/epoch - 33ms/step
Epoch 16/200
1493/1493 - 50s - loss: 8.5354e-04 - val_loss: 7.6262e-04 - 50s/epoch - 33ms/step
Epoch 17/200
1493/1493 - 50s - loss: 8.0710e-04 - val_loss: 9.0554e-04 - 50s/epoch - 33ms/step
Epoch 18/200
1493/1493 - 50s - loss: 7.8566e-04 - val_loss: 8.9083e-04 - 50s/epoch - 33ms/step
Epoch 19/200
1493/1493 - 50s - loss: 7.4269e-04 - val_loss: 9.0606e-04 - 50s/epoch - 33ms/step
Epoch 20/200
1493/1493 - 50s - loss: 7.1779e-04 - val_loss: 6.5894e-04 - 50s/epoch - 33ms/step
Epoch 21/200
1493/1493 - 50s - loss: 6.7534e-04 - val_loss: 7.7195e-04 - 50s/epoch - 33ms/step
Epoch 22/200
1493/1493 - 50s - loss: 6.5383e-04 - val_loss: 8.1651e-04 - 50s/epoch - 33ms/step
Epoch 23/200
1493/1493 - 50s - loss: 6.4559e-04 - val_loss: 9.6530e-04 - 50s/epoch - 33ms/step
Epoch 24/200
1493/1493 - 50s - loss: 6.3758e-04 - val_loss: 5.7262e-04 - 50s/epoch - 33ms/step
Epoch 25/200
1493/1493 - 50s - loss: 5.9059e-04 - val_loss: 5.5598e-04 - 50s/epoch - 33ms/step
Epoch 26/200
1493/1493 - 50s - loss: 5.6316e-04 - val_loss: 6.9107e-04 - 50s/epoch - 33ms/step
Epoch 27/200
1493/1493 - 50s - loss: 5.4256e-04 - val_loss: 5.2096e-04 - 50s/epoch - 33ms/step
Epoch 28/200
1493/1493 - 50s - loss: 5.2194e-04 - val_loss: 6.7194e-04 - 50s/epoch - 33ms/step
Epoch 29/200
1493/1493 - 50s - loss: 5.1266e-04 - val_loss: 6.0278e-04 - 50s/epoch - 33ms/step
Epoch 30/200
1493/1493 - 50s - loss: 5.1980e-04 - val_loss: 5.0164e-04 - 50s/epoch - 33ms/step
Epoch 31/200
1493/1493 - 50s - loss: 4.8645e-04 - val_loss: 5.9261e-04 - 50s/epoch - 33ms/step
Epoch 32/200
1493/1493 - 50s - loss: 5.2328e-04 - val_loss: 4.3235e-04 - 50s/epoch - 33ms/step
Epoch 33/200
1493/1493 - 50s - loss: 4.7179e-04 - val_loss: 4.6754e-04 - 50s/epoch - 33ms/step
Epoch 34/200
1493/1493 - 50s - loss: 4.5724e-04 - val_loss: 4.3130e-04 - 50s/epoch - 33ms/step
Epoch 35/200
1493/1493 - 50s - loss: 4.4421e-04 - val_loss: 7.2801e-04 - 50s/epoch - 33ms/step
Epoch 36/200
1493/1493 - 50s - loss: 4.7005e-04 - val_loss: 5.0165e-04 - 50s/epoch - 33ms/step
Epoch 37/200
1493/1493 - 50s - loss: 4.3503e-04 - val_loss: 4.3935e-04 - 50s/epoch - 33ms/step
Epoch 38/200
1493/1493 - 50s - loss: 4.3289e-04 - val_loss: 4.1239e-04 - 50s/epoch - 34ms/step
Epoch 39/200
1493/1493 - 50s - loss: 4.1801e-04 - val_loss: 3.8818e-04 - 50s/epoch - 33ms/step
Epoch 40/200
1493/1493 - 50s - loss: 4.0755e-04 - val_loss: 5.0533e-04 - 50s/epoch - 33ms/step
Epoch 41/200
1493/1493 - 50s - loss: 4.0603e-04 - val_loss: 3.8782e-04 - 50s/epoch - 33ms/step
Epoch 42/200
1493/1493 - 50s - loss: 3.9713e-04 - val_loss: 3.7385e-04 - 50s/epoch - 33ms/step
Epoch 43/200
1493/1493 - 50s - loss: 3.9480e-04 - val_loss: 4.3783e-04 - 50s/epoch - 33ms/step
Epoch 44/200
1493/1493 - 50s - loss: 3.9344e-04 - val_loss: 0.0011 - 50s/epoch - 33ms/step
Epoch 45/200
1493/1493 - 50s - loss: 4.4198e-04 - val_loss: 3.5979e-04 - 50s/epoch - 33ms/step
Epoch 46/200
1493/1493 - 50s - loss: 3.7513e-04 - val_loss: 3.4469e-04 - 50s/epoch - 33ms/step
Epoch 47/200
1493/1493 - 50s - loss: 3.7190e-04 - val_loss: 3.4117e-04 - 50s/epoch - 34ms/step
Epoch 48/200
1493/1493 - 50s - loss: 3.6927e-04 - val_loss: 3.9871e-04 - 50s/epoch - 33ms/step
Epoch 49/200
1493/1493 - 50s - loss: 3.6321e-04 - val_loss: 4.3387e-04 - 50s/epoch - 33ms/step
Epoch 50/200
1493/1493 - 50s - loss: 3.5888e-04 - val_loss: 4.7464e-04 - 50s/epoch - 33ms/step
Epoch 51/200
1493/1493 - 50s - loss: 3.6543e-04 - val_loss: 3.3919e-04 - 50s/epoch - 33ms/step
Epoch 52/200
1493/1493 - 50s - loss: 3.4637e-04 - val_loss: 3.4708e-04 - 50s/epoch - 33ms/step
Epoch 53/200
1493/1493 - 50s - loss: 3.5200e-04 - val_loss: 3.4057e-04 - 50s/epoch - 33ms/step
Epoch 54/200
1493/1493 - 50s - loss: 3.4189e-04 - val_loss: 3.6695e-04 - 50s/epoch - 34ms/step
Epoch 55/200
1493/1493 - 50s - loss: 3.3491e-04 - val_loss: 3.3400e-04 - 50s/epoch - 33ms/step
Epoch 56/200
1493/1493 - 50s - loss: 3.3388e-04 - val_loss: 3.4732e-04 - 50s/epoch - 33ms/step
Epoch 57/200
1493/1493 - 50s - loss: 3.2764e-04 - val_loss: 3.5168e-04 - 50s/epoch - 34ms/step
Epoch 58/200
1493/1493 - 50s - loss: 3.2793e-04 - val_loss: 3.0672e-04 - 50s/epoch - 33ms/step
Epoch 59/200
1493/1493 - 50s - loss: 3.2320e-04 - val_loss: 3.7703e-04 - 50s/epoch - 33ms/step
Epoch 60/200
1493/1493 - 50s - loss: 3.2393e-04 - val_loss: 4.7908e-04 - 50s/epoch - 33ms/step
Epoch 61/200
1493/1493 - 50s - loss: 3.5518e-04 - val_loss: 3.1308e-04 - 50s/epoch - 33ms/step
Epoch 62/200
1493/1493 - 50s - loss: 3.2062e-04 - val_loss: 3.2766e-04 - 50s/epoch - 33ms/step
Epoch 63/200
1493/1493 - 50s - loss: 3.1250e-04 - val_loss: 3.2752e-04 - 50s/epoch - 33ms/step
Epoch 64/200
1493/1493 - 50s - loss: 3.1402e-04 - val_loss: 0.0011 - 50s/epoch - 33ms/step
Epoch 65/200
1493/1493 - 50s - loss: 3.7297e-04 - val_loss: 5.1968e-04 - 50s/epoch - 33ms/step
Epoch 66/200
1493/1493 - 50s - loss: 3.4220e-04 - val_loss: 3.0390e-04 - 50s/epoch - 34ms/step
Epoch 67/200
1493/1493 - 50s - loss: 3.0801e-04 - val_loss: 3.0377e-04 - 50s/epoch - 33ms/step
Epoch 68/200
1493/1493 - 50s - loss: 3.0660e-04 - val_loss: 5.8861e-04 - 50s/epoch - 33ms/step
Epoch 69/200
1493/1493 - 50s - loss: 3.6935e-04 - val_loss: 3.5125e-04 - 50s/epoch - 33ms/step
Epoch 70/200
1493/1493 - 50s - loss: 3.0901e-04 - val_loss: 4.8920e-04 - 50s/epoch - 33ms/step
Epoch 71/200
1493/1493 - 50s - loss: 3.4168e-04 - val_loss: 2.9060e-04 - 50s/epoch - 33ms/step
Epoch 72/200
1493/1493 - 50s - loss: 3.0365e-04 - val_loss: 3.1573e-04 - 50s/epoch - 33ms/step
Epoch 73/200
1493/1493 - 50s - loss: 3.0381e-04 - val_loss: 3.5804e-04 - 50s/epoch - 33ms/step
Epoch 74/200
1493/1493 - 50s - loss: 3.0579e-04 - val_loss: 2.9967e-04 - 50s/epoch - 33ms/step
Epoch 75/200
1493/1493 - 50s - loss: 2.9155e-04 - val_loss: 2.8460e-04 - 50s/epoch - 33ms/step
Epoch 76/200
1493/1493 - 50s - loss: 2.8762e-04 - val_loss: 2.8132e-04 - 50s/epoch - 33ms/step
Epoch 77/200
1493/1493 - 50s - loss: 2.9248e-04 - val_loss: 3.9422e-04 - 50s/epoch - 33ms/step
Epoch 78/200
1493/1493 - 50s - loss: 2.9194e-04 - val_loss: 3.1236e-04 - 50s/epoch - 33ms/step
Epoch 79/200
1493/1493 - 50s - loss: 2.9289e-04 - val_loss: 4.3223e-04 - 50s/epoch - 34ms/step
Epoch 80/200
1493/1493 - 50s - loss: 3.1327e-04 - val_loss: 2.8663e-04 - 50s/epoch - 33ms/step
Epoch 81/200
1493/1493 - 50s - loss: 2.8642e-04 - val_loss: 3.2258e-04 - 50s/epoch - 34ms/step
Epoch 82/200
1493/1493 - 50s - loss: 2.8021e-04 - val_loss: 2.8388e-04 - 50s/epoch - 33ms/step
Epoch 83/200
1493/1493 - 50s - loss: 2.7580e-04 - val_loss: 2.9105e-04 - 50s/epoch - 33ms/step
Epoch 84/200
1493/1493 - 50s - loss: 2.7568e-04 - val_loss: 3.0841e-04 - 50s/epoch - 33ms/step
Epoch 85/200
1493/1493 - 50s - loss: 2.7527e-04 - val_loss: 3.0565e-04 - 50s/epoch - 33ms/step
Epoch 86/200
1493/1493 - 50s - loss: 2.7409e-04 - val_loss: 2.6783e-04 - 50s/epoch - 33ms/step
Epoch 87/200
1493/1493 - 50s - loss: 2.7009e-04 - val_loss: 2.7855e-04 - 50s/epoch - 33ms/step
Epoch 88/200
1493/1493 - 50s - loss: 2.6917e-04 - val_loss: 3.3360e-04 - 50s/epoch - 34ms/step
Epoch 89/200
1493/1493 - 50s - loss: 2.8127e-04 - val_loss: 2.7703e-04 - 50s/epoch - 33ms/step
Epoch 90/200
1493/1493 - 50s - loss: 2.6649e-04 - val_loss: 2.7140e-04 - 50s/epoch - 33ms/step
Epoch 91/200
1493/1493 - 50s - loss: 2.6473e-04 - val_loss: 3.2513e-04 - 50s/epoch - 33ms/step
Epoch 92/200
1493/1493 - 50s - loss: 2.6689e-04 - val_loss: 4.2992e-04 - 50s/epoch - 33ms/step
Epoch 93/200
1493/1493 - 50s - loss: 2.9294e-04 - val_loss: 3.0786e-04 - 50s/epoch - 33ms/step
Epoch 94/200
1493/1493 - 50s - loss: 2.7076e-04 - val_loss: 2.4510e-04 - 50s/epoch - 33ms/step
Epoch 95/200
1493/1493 - 50s - loss: 2.6720e-04 - val_loss: 2.8115e-04 - 50s/epoch - 34ms/step
Epoch 96/200
1493/1493 - 50s - loss: 2.6469e-04 - val_loss: 2.7919e-04 - 50s/epoch - 33ms/step
Epoch 97/200
1493/1493 - 50s - loss: 2.6058e-04 - val_loss: 2.5105e-04 - 50s/epoch - 33ms/step
Epoch 98/200
1493/1493 - 50s - loss: 2.6904e-04 - val_loss: 9.0108e-04 - 50s/epoch - 33ms/step
Epoch 99/200
1493/1493 - 50s - loss: 3.6229e-04 - val_loss: 2.3782e-04 - 50s/epoch - 34ms/step
Epoch 100/200
1493/1493 - 50s - loss: 2.6622e-04 - val_loss: 2.5081e-04 - 50s/epoch - 33ms/step
Epoch 101/200
1493/1493 - 50s - loss: 2.6034e-04 - val_loss: 5.8939e-04 - 50s/epoch - 33ms/step
Epoch 102/200
1493/1493 - 50s - loss: 3.0041e-04 - val_loss: 3.3456e-04 - 50s/epoch - 34ms/step
Epoch 103/200
1493/1493 - 50s - loss: 2.7059e-04 - val_loss: 2.5411e-04 - 50s/epoch - 33ms/step
Epoch 104/200
1493/1493 - 50s - loss: 2.5689e-04 - val_loss: 3.5210e-04 - 50s/epoch - 34ms/step
Epoch 105/200
1493/1493 - 50s - loss: 2.7062e-04 - val_loss: 2.3842e-04 - 50s/epoch - 33ms/step
Epoch 106/200
1493/1493 - 50s - loss: 2.5234e-04 - val_loss: 2.6489e-04 - 50s/epoch - 34ms/step
Epoch 107/200
1493/1493 - 50s - loss: 2.5161e-04 - val_loss: 2.6857e-04 - 50s/epoch - 33ms/step
Epoch 108/200
1493/1493 - 50s - loss: 2.5807e-04 - val_loss: 2.6366e-04 - 50s/epoch - 33ms/step
Epoch 109/200
1493/1493 - 50s - loss: 2.5101e-04 - val_loss: 2.6049e-04 - 50s/epoch - 33ms/step
Epoch 110/200
1493/1493 - 50s - loss: 2.4741e-04 - val_loss: 2.6830e-04 - 50s/epoch - 33ms/step
Epoch 111/200
1493/1493 - 50s - loss: 2.4896e-04 - val_loss: 4.6166e-04 - 50s/epoch - 33ms/step
Epoch 112/200
1493/1493 - 50s - loss: 2.8914e-04 - val_loss: 2.4171e-04 - 50s/epoch - 33ms/step
Epoch 113/200
1493/1493 - 50s - loss: 2.4739e-04 - val_loss: 4.1905e-04 - 50s/epoch - 33ms/step
Epoch 114/200
1493/1493 - 50s - loss: 2.8347e-04 - val_loss: 2.4977e-04 - 50s/epoch - 33ms/step
Epoch 115/200
1493/1493 - 50s - loss: 2.5097e-04 - val_loss: 3.6939e-04 - 50s/epoch - 33ms/step
Epoch 116/200
1493/1493 - 50s - loss: 2.5224e-04 - val_loss: 2.8559e-04 - 50s/epoch - 33ms/step
Epoch 117/200
1493/1493 - 50s - loss: 2.4560e-04 - val_loss: 2.7380e-04 - 50s/epoch - 33ms/step
Epoch 118/200
1493/1493 - 50s - loss: 2.5177e-04 - val_loss: 2.4517e-04 - 50s/epoch - 33ms/step
Epoch 119/200
1493/1493 - 50s - loss: 2.4222e-04 - val_loss: 2.4754e-04 - 50s/epoch - 33ms/step
Epoch 120/200
1493/1493 - 50s - loss: 2.4105e-04 - val_loss: 2.3152e-04 - 50s/epoch - 33ms/step
Epoch 121/200
1493/1493 - 50s - loss: 2.4249e-04 - val_loss: 3.3854e-04 - 50s/epoch - 33ms/step
Epoch 122/200
1493/1493 - 50s - loss: 2.4510e-04 - val_loss: 2.3021e-04 - 50s/epoch - 33ms/step
Epoch 123/200
1493/1493 - 50s - loss: 2.3739e-04 - val_loss: 2.5289e-04 - 50s/epoch - 33ms/step
Epoch 124/200
1493/1493 - 50s - loss: 2.3749e-04 - val_loss: 3.9780e-04 - 50s/epoch - 33ms/step
Epoch 125/200
1493/1493 - 50s - loss: 2.6033e-04 - val_loss: 2.2440e-04 - 50s/epoch - 33ms/step
Epoch 126/200
1493/1493 - 50s - loss: 2.3916e-04 - val_loss: 5.1062e-04 - 50s/epoch - 34ms/step
Epoch 127/200
1493/1493 - 50s - loss: 2.7262e-04 - val_loss: 2.4901e-04 - 50s/epoch - 34ms/step
Epoch 128/200
1493/1493 - 50s - loss: 2.3616e-04 - val_loss: 2.2135e-04 - 50s/epoch - 33ms/step
Epoch 129/200
1493/1493 - 50s - loss: 2.3390e-04 - val_loss: 2.5662e-04 - 50s/epoch - 33ms/step
Epoch 130/200
1493/1493 - 50s - loss: 2.3355e-04 - val_loss: 2.4365e-04 - 50s/epoch - 33ms/step
Epoch 131/200
1493/1493 - 50s - loss: 2.3788e-04 - val_loss: 4.7245e-04 - 50s/epoch - 33ms/step
Epoch 132/200
1493/1493 - 50s - loss: 2.7159e-04 - val_loss: 2.2274e-04 - 50s/epoch - 33ms/step
Epoch 133/200
1493/1493 - 50s - loss: 2.3411e-04 - val_loss: 2.3479e-04 - 50s/epoch - 33ms/step
Epoch 134/200
1493/1493 - 50s - loss: 2.3105e-04 - val_loss: 2.3372e-04 - 50s/epoch - 33ms/step
Epoch 135/200
1493/1493 - 50s - loss: 2.2882e-04 - val_loss: 2.1931e-04 - 50s/epoch - 34ms/step
Epoch 136/200
1493/1493 - 50s - loss: 2.3185e-04 - val_loss: 4.0062e-04 - 50s/epoch - 33ms/step
Epoch 137/200
1493/1493 - 50s - loss: 2.7210e-04 - val_loss: 4.0856e-04 - 50s/epoch - 33ms/step
Epoch 138/200
1493/1493 - 50s - loss: 2.8008e-04 - val_loss: 2.6444e-04 - 50s/epoch - 33ms/step
Epoch 139/200
1493/1493 - 50s - loss: 2.4056e-04 - val_loss: 2.4768e-04 - 50s/epoch - 33ms/step
Epoch 140/200
1493/1493 - 50s - loss: 2.3454e-04 - val_loss: 2.1992e-04 - 50s/epoch - 33ms/step
Epoch 141/200
1493/1493 - 50s - loss: 2.3059e-04 - val_loss: 3.7348e-04 - 50s/epoch - 33ms/step
Epoch 142/200
1493/1493 - 50s - loss: 2.5655e-04 - val_loss: 2.2684e-04 - 50s/epoch - 33ms/step
Epoch 143/200
1493/1493 - 50s - loss: 2.2851e-04 - val_loss: 2.7222e-04 - 50s/epoch - 33ms/step
Epoch 144/200
1493/1493 - 50s - loss: 2.3545e-04 - val_loss: 2.2415e-04 - 50s/epoch - 33ms/step
Epoch 145/200
1493/1493 - 50s - loss: 2.2549e-04 - val_loss: 2.2052e-04 - 50s/epoch - 34ms/step
Epoch 146/200
1493/1493 - 50s - loss: 2.2678e-04 - val_loss: 2.2601e-04 - 50s/epoch - 33ms/step
Epoch 147/200
1493/1493 - 50s - loss: 2.2333e-04 - val_loss: 2.3503e-04 - 50s/epoch - 33ms/step
Epoch 148/200
1493/1493 - 50s - loss: 2.2422e-04 - val_loss: 3.6393e-04 - 50s/epoch - 33ms/step
Epoch 149/200
1493/1493 - 50s - loss: 2.2351e-04 - val_loss: 2.1559e-04 - 50s/epoch - 33ms/step
Epoch 150/200
1493/1493 - 50s - loss: 2.2098e-04 - val_loss: 2.1545e-04 - 50s/epoch - 33ms/step
Epoch 151/200
1493/1493 - 50s - loss: 2.2551e-04 - val_loss: 2.2084e-04 - 50s/epoch - 33ms/step
Epoch 152/200
1493/1493 - 50s - loss: 2.2239e-04 - val_loss: 2.9976e-04 - 50s/epoch - 33ms/step
Epoch 153/200
1493/1493 - 50s - loss: 2.2810e-04 - val_loss: 2.1866e-04 - 50s/epoch - 33ms/step
Epoch 154/200
1493/1493 - 50s - loss: 2.2216e-04 - val_loss: 3.8340e-04 - 50s/epoch - 33ms/step
Epoch 155/200
1493/1493 - 50s - loss: 2.5283e-04 - val_loss: 2.2282e-04 - 50s/epoch - 33ms/step
Epoch 156/200
1493/1493 - 50s - loss: 2.2321e-04 - val_loss: 2.2245e-04 - 50s/epoch - 33ms/step
Epoch 157/200
1493/1493 - 50s - loss: 2.1866e-04 - val_loss: 2.4066e-04 - 50s/epoch - 33ms/step
Epoch 158/200
1493/1493 - 50s - loss: 2.2290e-04 - val_loss: 2.1980e-04 - 50s/epoch - 33ms/step
Epoch 159/200
1493/1493 - 50s - loss: 2.1800e-04 - val_loss: 2.1441e-04 - 50s/epoch - 33ms/step
Epoch 160/200
1493/1493 - 50s - loss: 2.2143e-04 - val_loss: 3.0742e-04 - 50s/epoch - 33ms/step
Epoch 161/200
1493/1493 - 50s - loss: 2.4863e-04 - val_loss: 2.0542e-04 - 50s/epoch - 33ms/step
Epoch 162/200
1493/1493 - 50s - loss: 2.1873e-04 - val_loss: 2.2596e-04 - 50s/epoch - 33ms/step
Epoch 163/200
1493/1493 - 50s - loss: 2.1996e-04 - val_loss: 2.8009e-04 - 50s/epoch - 34ms/step
Epoch 164/200
1493/1493 - 50s - loss: 2.3035e-04 - val_loss: 2.2088e-04 - 50s/epoch - 33ms/step
Epoch 165/200
1493/1493 - 50s - loss: 2.2719e-04 - val_loss: 2.1840e-04 - 50s/epoch - 33ms/step
Epoch 166/200
1493/1493 - 50s - loss: 2.1749e-04 - val_loss: 2.1523e-04 - 50s/epoch - 33ms/step
Epoch 167/200
1493/1493 - 50s - loss: 2.1629e-04 - val_loss: 2.1286e-04 - 50s/epoch - 33ms/step
Epoch 168/200
1493/1493 - 50s - loss: 2.3647e-04 - val_loss: 1.9855e-04 - 50s/epoch - 33ms/step
Epoch 169/200
1493/1493 - 50s - loss: 2.1550e-04 - val_loss: 2.7263e-04 - 50s/epoch - 33ms/step
Epoch 170/200
1493/1493 - 50s - loss: 2.1771e-04 - val_loss: 3.8482e-04 - 50s/epoch - 33ms/step
Epoch 171/200
1493/1493 - 50s - loss: 2.4998e-04 - val_loss: 3.4878e-04 - 50s/epoch - 33ms/step
Epoch 172/200
1493/1493 - 50s - loss: 2.4294e-04 - val_loss: 2.2270e-04 - 50s/epoch - 33ms/step
Epoch 173/200
1493/1493 - 50s - loss: 2.1733e-04 - val_loss: 2.0008e-04 - 50s/epoch - 33ms/step
Epoch 174/200
1493/1493 - 50s - loss: 2.1273e-04 - val_loss: 2.6340e-04 - 50s/epoch - 34ms/step
Epoch 175/200
1493/1493 - 50s - loss: 2.1398e-04 - val_loss: 2.1532e-04 - 50s/epoch - 33ms/step
Epoch 176/200
1493/1493 - 50s - loss: 2.1752e-04 - val_loss: 3.2353e-04 - 50s/epoch - 34ms/step
Epoch 177/200
1493/1493 - 50s - loss: 2.3697e-04 - val_loss: 1.9858e-04 - 50s/epoch - 33ms/step
Epoch 178/200
1493/1493 - 50s - loss: 2.1174e-04 - val_loss: 2.1763e-04 - 50s/epoch - 34ms/step
Epoch 179/200
1493/1493 - 50s - loss: 2.1089e-04 - val_loss: 2.0563e-04 - 50s/epoch - 33ms/step
Epoch 180/200
1493/1493 - 50s - loss: 2.1105e-04 - val_loss: 2.4093e-04 - 50s/epoch - 33ms/step
Epoch 181/200
1493/1493 - 50s - loss: 2.1806e-04 - val_loss: 2.6958e-04 - 50s/epoch - 33ms/step
Epoch 182/200
1493/1493 - 50s - loss: 2.1713e-04 - val_loss: 2.2964e-04 - 50s/epoch - 33ms/step
Epoch 183/200
1493/1493 - 50s - loss: 2.1026e-04 - val_loss: 2.2856e-04 - 50s/epoch - 33ms/step
Epoch 184/200
1493/1493 - 50s - loss: 2.1188e-04 - val_loss: 2.0906e-04 - 50s/epoch - 33ms/step
Epoch 185/200
1493/1493 - 50s - loss: 2.0681e-04 - val_loss: 2.5178e-04 - 50s/epoch - 33ms/step
Epoch 186/200
1493/1493 - 50s - loss: 2.0897e-04 - val_loss: 2.1004e-04 - 50s/epoch - 33ms/step
Epoch 187/200
1493/1493 - 50s - loss: 2.0689e-04 - val_loss: 2.1944e-04 - 50s/epoch - 33ms/step
Epoch 188/200
1493/1493 - 50s - loss: 2.2123e-04 - val_loss: 2.6617e-04 - 50s/epoch - 33ms/step
Epoch 189/200
1493/1493 - 50s - loss: 2.1694e-04 - val_loss: 2.2141e-04 - 50s/epoch - 33ms/step
Epoch 190/200
1493/1493 - 50s - loss: 2.1134e-04 - val_loss: 2.4112e-04 - 50s/epoch - 33ms/step
Epoch 191/200
1493/1493 - 50s - loss: 2.1202e-04 - val_loss: 2.1829e-04 - 50s/epoch - 33ms/step
Epoch 192/200
1493/1493 - 50s - loss: 2.0822e-04 - val_loss: 2.6900e-04 - 50s/epoch - 33ms/step
Epoch 193/200
1493/1493 - 50s - loss: 2.2396e-04 - val_loss: 3.4723e-04 - 50s/epoch - 33ms/step
Epoch 194/200
1493/1493 - 50s - loss: 2.3591e-04 - val_loss: 3.7347e-04 - 50s/epoch - 33ms/step
Epoch 195/200
1493/1493 - 50s - loss: 2.3510e-04 - val_loss: 2.3812e-04 - 50s/epoch - 33ms/step
Epoch 196/200
1493/1493 - 50s - loss: 2.1338e-04 - val_loss: 2.0570e-04 - 50s/epoch - 34ms/step
Epoch 197/200
1493/1493 - 50s - loss: 2.0723e-04 - val_loss: 2.1508e-04 - 50s/epoch - 33ms/step
Epoch 198/200
1493/1493 - 50s - loss: 2.0874e-04 - val_loss: 2.6168e-04 - 50s/epoch - 33ms/step
Epoch 199/200
1493/1493 - 50s - loss: 2.1188e-04 - val_loss: 2.0707e-04 - 50s/epoch - 34ms/step
Epoch 200/200
1493/1493 - 50s - loss: 2.0555e-04 - val_loss: 2.1332e-04 - 50s/epoch - 33ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00021331799507606775
  1/332 [..............................] - ETA: 37s 11/332 [..............................] - ETA: 1s  22/332 [>.............................] - ETA: 1s 33/332 [=>............................] - ETA: 1s 44/332 [==>...........................] - ETA: 1s 55/332 [===>..........................] - ETA: 1s 66/332 [====>.........................] - ETA: 1s 77/332 [=====>........................] - ETA: 1s 88/332 [======>.......................] - ETA: 1s 99/332 [=======>......................] - ETA: 1s110/332 [========>.....................] - ETA: 1s121/332 [=========>....................] - ETA: 1s132/332 [==========>...................] - ETA: 0s143/332 [===========>..................] - ETA: 0s154/332 [============>.................] - ETA: 0s165/332 [=============>................] - ETA: 0s176/332 [==============>...............] - ETA: 0s187/332 [===============>..............] - ETA: 0s198/332 [================>.............] - ETA: 0s209/332 [=================>............] - ETA: 0s220/332 [==================>...........] - ETA: 0s231/332 [===================>..........] - ETA: 0s242/332 [====================>.........] - ETA: 0s253/332 [=====================>........] - ETA: 0s264/332 [======================>.......] - ETA: 0s275/332 [=======================>......] - ETA: 0s286/332 [========================>.....] - ETA: 0s297/332 [=========================>....] - ETA: 0s308/332 [==========================>...] - ETA: 0s319/332 [===========================>..] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 5ms/step
correlation 0.0024594713551477805
cosine 0.0019404829121336953
MAE: 0.007963079
RMSE: 0.014605403
r2: 0.9861619153568315
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 1896)              2398440   
                                                                 
 batch_normalization (BatchN  (None, 1896)             7584      
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 1896)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               478044    
                                                                 
 batch_normalization_1 (Batc  (None, 252)              1008      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 252)               0         
                                                                 
 dense_1 (Dense)             (None, 1896)              479688    
                                                                 
 batch_normalization_2 (Batc  (None, 1896)             7584      
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 1896)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              2397808   
                                                                 
=================================================================
Total params: 5,770,156
Trainable params: 5,762,068
Non-trainable params: 8,088
_________________________________________________________________
Encoder
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 1896)              2398440   
                                                                 
 batch_normalization (BatchN  (None, 1896)             7584      
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 1896)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               478044    
                                                                 
=================================================================
Total params: 2,884,068
Trainable params: 2,880,276
Non-trainable params: 3,792
_________________________________________________________________
Decoder
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 252)]             0         
                                                                 
 batch_normalization_1 (Batc  (None, 252)              1008      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 252)               0         
                                                                 
 dense_1 (Dense)             (None, 1896)              479688    
                                                                 
 batch_normalization_2 (Batc  (None, 1896)             7584      
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 1896)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              2397808   
                                                                 
=================================================================
Total params: 2,886,088
Trainable params: 2,881,792
Non-trainable params: 4,296
_________________________________________________________________
['1.5custom_n_b', 'mse', 64, 200, 0.0005, 0.2, 252, 0.00020555446099024266, 0.00021331799507606775, 0.0024594713551477805, 0.0019404829121336953, 0.007963079027831554, 0.014605402946472168, 0.9861619153568315, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_3 (Dense)             (None, 2022)              2557830   
                                                                 
 batch_normalization_3 (Batc  (None, 2022)             8088      
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2022)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               509796    
                                                                 
 batch_normalization_4 (Batc  (None, 252)              1008      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 252)               0         
                                                                 
 dense_4 (Dense)             (None, 2022)              511566    
                                                                 
 batch_normalization_5 (Batc  (None, 2022)             8088      
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2022)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              2557072   
                                                                 
=================================================================
Total params: 6,153,448
Trainable params: 6,144,856
Non-trainable params: 8,592
_________________________________________________________________
Epoch 1/200
1493/1493 - 54s - loss: 0.0101 - val_loss: 0.0045 - 54s/epoch - 36ms/step
Epoch 2/200
1493/1493 - 54s - loss: 0.0035 - val_loss: 0.0031 - 54s/epoch - 36ms/step
Epoch 3/200
1493/1493 - 54s - loss: 0.0025 - val_loss: 0.0020 - 54s/epoch - 36ms/step
Epoch 4/200
1493/1493 - 53s - loss: 0.0021 - val_loss: 0.0030 - 53s/epoch - 36ms/step
Epoch 5/200
1493/1493 - 54s - loss: 0.0019 - val_loss: 0.0015 - 54s/epoch - 36ms/step
Epoch 6/200
1493/1493 - 54s - loss: 0.0017 - val_loss: 0.0017 - 54s/epoch - 36ms/step
Epoch 7/200
1493/1493 - 54s - loss: 0.0016 - val_loss: 0.0015 - 54s/epoch - 36ms/step
Epoch 8/200
1493/1493 - 54s - loss: 0.0015 - val_loss: 0.0018 - 54s/epoch - 36ms/step
Epoch 9/200
1493/1493 - 54s - loss: 0.0013 - val_loss: 0.0014 - 54s/epoch - 36ms/step
Epoch 10/200
1493/1493 - 53s - loss: 0.0012 - val_loss: 0.0010 - 53s/epoch - 36ms/step
Epoch 11/200
1493/1493 - 54s - loss: 0.0012 - val_loss: 0.0010 - 54s/epoch - 36ms/step
Epoch 12/200
1493/1493 - 54s - loss: 0.0010 - val_loss: 0.0012 - 54s/epoch - 36ms/step
Epoch 13/200
1493/1493 - 54s - loss: 0.0010 - val_loss: 0.0013 - 54s/epoch - 36ms/step
Epoch 14/200
1493/1493 - 54s - loss: 9.5192e-04 - val_loss: 8.5689e-04 - 54s/epoch - 36ms/step
Epoch 15/200
1493/1493 - 54s - loss: 9.0196e-04 - val_loss: 8.3033e-04 - 54s/epoch - 36ms/step
Epoch 16/200
1493/1493 - 53s - loss: 8.1609e-04 - val_loss: 7.2276e-04 - 53s/epoch - 36ms/step
Epoch 17/200
1493/1493 - 54s - loss: 7.7549e-04 - val_loss: 8.7561e-04 - 54s/epoch - 36ms/step
Epoch 18/200
1493/1493 - 54s - loss: 7.4863e-04 - val_loss: 0.0011 - 54s/epoch - 36ms/step
Epoch 19/200
1493/1493 - 54s - loss: 7.3036e-04 - val_loss: 7.8825e-04 - 54s/epoch - 36ms/step
Epoch 20/200
1493/1493 - 54s - loss: 6.8002e-04 - val_loss: 6.0325e-04 - 54s/epoch - 36ms/step
Epoch 21/200
1493/1493 - 54s - loss: 6.5411e-04 - val_loss: 7.8593e-04 - 54s/epoch - 36ms/step
Epoch 22/200
1493/1493 - 53s - loss: 6.2763e-04 - val_loss: 8.3586e-04 - 53s/epoch - 36ms/step
Epoch 23/200
1493/1493 - 54s - loss: 6.3936e-04 - val_loss: 0.0013 - 54s/epoch - 36ms/step
Epoch 24/200
1493/1493 - 54s - loss: 6.5260e-04 - val_loss: 5.2833e-04 - 54s/epoch - 36ms/step
Epoch 25/200
1493/1493 - 53s - loss: 5.7218e-04 - val_loss: 5.2620e-04 - 53s/epoch - 36ms/step
Epoch 26/200
1493/1493 - 54s - loss: 5.4825e-04 - val_loss: 6.0954e-04 - 54s/epoch - 36ms/step
Epoch 27/200
1493/1493 - 54s - loss: 5.2387e-04 - val_loss: 5.0648e-04 - 54s/epoch - 36ms/step
Epoch 28/200
1493/1493 - 54s - loss: 5.0943e-04 - val_loss: 5.7509e-04 - 54s/epoch - 36ms/step
Epoch 29/200
1493/1493 - 53s - loss: 4.9652e-04 - val_loss: 5.8408e-04 - 53s/epoch - 36ms/step
Epoch 30/200
1493/1493 - 54s - loss: 4.9632e-04 - val_loss: 4.6091e-04 - 54s/epoch - 36ms/step
Epoch 31/200
1493/1493 - 53s - loss: 4.7465e-04 - val_loss: 7.8072e-04 - 53s/epoch - 36ms/step
Epoch 32/200
1493/1493 - 54s - loss: 5.1315e-04 - val_loss: 4.4616e-04 - 54s/epoch - 36ms/step
Epoch 33/200
1493/1493 - 54s - loss: 4.5666e-04 - val_loss: 4.2742e-04 - 54s/epoch - 36ms/step
Epoch 34/200
1493/1493 - 54s - loss: 4.4413e-04 - val_loss: 4.0440e-04 - 54s/epoch - 36ms/step
Epoch 35/200
1493/1493 - 53s - loss: 4.2990e-04 - val_loss: 7.0663e-04 - 53s/epoch - 36ms/step
Epoch 36/200
1493/1493 - 54s - loss: 4.4979e-04 - val_loss: 4.5610e-04 - 54s/epoch - 36ms/step
Epoch 37/200
1493/1493 - 54s - loss: 4.2243e-04 - val_loss: 4.7865e-04 - 54s/epoch - 36ms/step
Epoch 38/200
1493/1493 - 54s - loss: 4.3081e-04 - val_loss: 4.1519e-04 - 54s/epoch - 36ms/step
Epoch 39/200
1493/1493 - 54s - loss: 4.0360e-04 - val_loss: 3.7887e-04 - 54s/epoch - 36ms/step
Epoch 40/200
1493/1493 - 54s - loss: 3.9440e-04 - val_loss: 4.3270e-04 - 54s/epoch - 36ms/step
Epoch 41/200
1493/1493 - 53s - loss: 3.9304e-04 - val_loss: 3.5161e-04 - 53s/epoch - 36ms/step
Epoch 42/200
1493/1493 - 54s - loss: 3.8585e-04 - val_loss: 3.6319e-04 - 54s/epoch - 36ms/step
Epoch 43/200
1493/1493 - 54s - loss: 3.8177e-04 - val_loss: 4.3643e-04 - 54s/epoch - 36ms/step
Epoch 44/200
1493/1493 - 54s - loss: 3.8235e-04 - val_loss: 7.6415e-04 - 54s/epoch - 36ms/step
Epoch 45/200
1493/1493 - 54s - loss: 4.0807e-04 - val_loss: 3.2687e-04 - 54s/epoch - 36ms/step
Epoch 46/200
1493/1493 - 54s - loss: 3.6358e-04 - val_loss: 3.4308e-04 - 54s/epoch - 36ms/step
Epoch 47/200
1493/1493 - 53s - loss: 3.6113e-04 - val_loss: 3.6689e-04 - 53s/epoch - 36ms/step
Epoch 48/200
1493/1493 - 54s - loss: 3.5604e-04 - val_loss: 3.6334e-04 - 54s/epoch - 36ms/step
Epoch 49/200
1493/1493 - 54s - loss: 3.5373e-04 - val_loss: 8.0964e-04 - 54s/epoch - 36ms/step
Epoch 50/200
1493/1493 - 54s - loss: 3.8467e-04 - val_loss: 4.0774e-04 - 54s/epoch - 36ms/step
Epoch 51/200
1493/1493 - 54s - loss: 3.5358e-04 - val_loss: 3.4481e-04 - 54s/epoch - 36ms/step
Epoch 52/200
1493/1493 - 54s - loss: 3.3872e-04 - val_loss: 3.4662e-04 - 54s/epoch - 36ms/step
Epoch 53/200
1493/1493 - 54s - loss: 3.4222e-04 - val_loss: 6.0330e-04 - 54s/epoch - 36ms/step
Epoch 54/200
1493/1493 - 53s - loss: 3.6230e-04 - val_loss: 3.4251e-04 - 53s/epoch - 36ms/step
Epoch 55/200
1493/1493 - 54s - loss: 3.2902e-04 - val_loss: 3.0535e-04 - 54s/epoch - 36ms/step
Epoch 56/200
1493/1493 - 54s - loss: 3.2640e-04 - val_loss: 3.2850e-04 - 54s/epoch - 36ms/step
Epoch 57/200
1493/1493 - 54s - loss: 3.2022e-04 - val_loss: 3.5331e-04 - 54s/epoch - 36ms/step
Epoch 58/200
1493/1493 - 54s - loss: 3.2377e-04 - val_loss: 2.9420e-04 - 54s/epoch - 36ms/step
Epoch 59/200
1493/1493 - 54s - loss: 3.1579e-04 - val_loss: 3.3237e-04 - 54s/epoch - 36ms/step
Epoch 60/200
1493/1493 - 53s - loss: 3.1691e-04 - val_loss: 5.6917e-04 - 53s/epoch - 36ms/step
Epoch 61/200
1493/1493 - 54s - loss: 3.6958e-04 - val_loss: 2.9360e-04 - 54s/epoch - 36ms/step
Epoch 62/200
1493/1493 - 54s - loss: 3.1420e-04 - val_loss: 2.9923e-04 - 54s/epoch - 36ms/step
Epoch 63/200
1493/1493 - 54s - loss: 3.0674e-04 - val_loss: 3.2278e-04 - 54s/epoch - 36ms/step
Epoch 64/200
1493/1493 - 54s - loss: 3.0600e-04 - val_loss: 7.2881e-04 - 54s/epoch - 36ms/step
Epoch 65/200
1493/1493 - 54s - loss: 3.3809e-04 - val_loss: 3.2356e-04 - 54s/epoch - 36ms/step
Epoch 66/200
1493/1493 - 53s - loss: 3.1032e-04 - val_loss: 2.9561e-04 - 53s/epoch - 36ms/step
Epoch 67/200
1493/1493 - 54s - loss: 2.9968e-04 - val_loss: 2.8444e-04 - 54s/epoch - 36ms/step
Epoch 68/200
1493/1493 - 54s - loss: 2.9633e-04 - val_loss: 4.6875e-04 - 54s/epoch - 36ms/step
Epoch 69/200
1493/1493 - 54s - loss: 3.2741e-04 - val_loss: 3.0860e-04 - 54s/epoch - 36ms/step
Epoch 70/200
1493/1493 - 54s - loss: 2.9778e-04 - val_loss: 5.8091e-04 - 54s/epoch - 36ms/step
Epoch 71/200
1493/1493 - 54s - loss: 3.4345e-04 - val_loss: 2.7007e-04 - 54s/epoch - 36ms/step
Epoch 72/200
1493/1493 - 53s - loss: 2.9685e-04 - val_loss: 2.9600e-04 - 53s/epoch - 36ms/step
Epoch 73/200
1493/1493 - 54s - loss: 2.9632e-04 - val_loss: 2.9116e-04 - 54s/epoch - 36ms/step
Epoch 74/200
1493/1493 - 54s - loss: 2.9157e-04 - val_loss: 2.9289e-04 - 54s/epoch - 36ms/step
Epoch 75/200
1493/1493 - 54s - loss: 2.8563e-04 - val_loss: 3.0693e-04 - 54s/epoch - 36ms/step
Epoch 76/200
1493/1493 - 54s - loss: 2.8776e-04 - val_loss: 2.8990e-04 - 54s/epoch - 36ms/step
Epoch 77/200
1493/1493 - 54s - loss: 2.8313e-04 - val_loss: 4.4155e-04 - 54s/epoch - 36ms/step
Epoch 78/200
1493/1493 - 54s - loss: 2.8870e-04 - val_loss: 5.1481e-04 - 54s/epoch - 36ms/step
Epoch 79/200
1493/1493 - 54s - loss: 3.0346e-04 - val_loss: 3.1185e-04 - 54s/epoch - 36ms/step
Epoch 80/200
1493/1493 - 54s - loss: 2.8831e-04 - val_loss: 2.5874e-04 - 54s/epoch - 36ms/step
Epoch 81/200
1493/1493 - 54s - loss: 2.8019e-04 - val_loss: 3.5036e-04 - 54s/epoch - 36ms/step
Epoch 82/200
1493/1493 - 54s - loss: 2.7458e-04 - val_loss: 2.7571e-04 - 54s/epoch - 36ms/step
Epoch 83/200
1493/1493 - 54s - loss: 2.7064e-04 - val_loss: 2.8758e-04 - 54s/epoch - 36ms/step
Epoch 84/200
1493/1493 - 54s - loss: 2.7091e-04 - val_loss: 2.7599e-04 - 54s/epoch - 36ms/step
Epoch 85/200
1493/1493 - 53s - loss: 2.7250e-04 - val_loss: 3.4464e-04 - 53s/epoch - 36ms/step
Epoch 86/200
1493/1493 - 54s - loss: 2.7517e-04 - val_loss: 2.5820e-04 - 54s/epoch - 36ms/step
Epoch 87/200
1493/1493 - 54s - loss: 2.6493e-04 - val_loss: 2.5939e-04 - 54s/epoch - 36ms/step
Epoch 88/200
1493/1493 - 54s - loss: 2.6378e-04 - val_loss: 2.8181e-04 - 54s/epoch - 36ms/step
Epoch 89/200
1493/1493 - 54s - loss: 2.7729e-04 - val_loss: 2.6889e-04 - 54s/epoch - 36ms/step
Epoch 90/200
1493/1493 - 54s - loss: 2.6258e-04 - val_loss: 2.5887e-04 - 54s/epoch - 36ms/step
Epoch 91/200
1493/1493 - 53s - loss: 2.6185e-04 - val_loss: 3.0180e-04 - 53s/epoch - 36ms/step
Epoch 92/200
1493/1493 - 54s - loss: 2.6010e-04 - val_loss: 4.9261e-04 - 54s/epoch - 36ms/step
Epoch 93/200
1493/1493 - 54s - loss: 2.9451e-04 - val_loss: 2.7480e-04 - 54s/epoch - 36ms/step
Epoch 94/200
1493/1493 - 54s - loss: 2.6176e-04 - val_loss: 2.3463e-04 - 54s/epoch - 36ms/step
Epoch 95/200
1493/1493 - 54s - loss: 2.5764e-04 - val_loss: 2.7539e-04 - 54s/epoch - 36ms/step
Epoch 96/200
1493/1493 - 54s - loss: 2.6065e-04 - val_loss: 2.6655e-04 - 54s/epoch - 36ms/step
Epoch 97/200
1493/1493 - 53s - loss: 2.5453e-04 - val_loss: 2.7823e-04 - 53s/epoch - 36ms/step
Epoch 98/200
1493/1493 - 54s - loss: 2.6354e-04 - val_loss: 5.4129e-04 - 54s/epoch - 36ms/step
Epoch 99/200
1493/1493 - 54s - loss: 3.1408e-04 - val_loss: 2.3748e-04 - 54s/epoch - 36ms/step
Epoch 100/200
1493/1493 - 54s - loss: 2.6293e-04 - val_loss: 2.3627e-04 - 54s/epoch - 36ms/step
Epoch 101/200
1493/1493 - 54s - loss: 2.5427e-04 - val_loss: 3.6203e-04 - 54s/epoch - 36ms/step
Epoch 102/200
1493/1493 - 54s - loss: 2.7252e-04 - val_loss: 2.9795e-04 - 54s/epoch - 36ms/step
Epoch 103/200
1493/1493 - 54s - loss: 2.5529e-04 - val_loss: 2.4633e-04 - 54s/epoch - 36ms/step
Epoch 104/200
1493/1493 - 53s - loss: 2.5095e-04 - val_loss: 4.2686e-04 - 53s/epoch - 36ms/step
Epoch 105/200
1493/1493 - 54s - loss: 2.6950e-04 - val_loss: 2.4031e-04 - 54s/epoch - 36ms/step
Epoch 106/200
1493/1493 - 54s - loss: 2.4833e-04 - val_loss: 2.5582e-04 - 54s/epoch - 36ms/step
Epoch 107/200
1493/1493 - 54s - loss: 2.4678e-04 - val_loss: 2.5349e-04 - 54s/epoch - 36ms/step
Epoch 108/200
1493/1493 - 54s - loss: 2.5157e-04 - val_loss: 2.5792e-04 - 54s/epoch - 36ms/step
Epoch 109/200
1493/1493 - 54s - loss: 2.4684e-04 - val_loss: 2.6155e-04 - 54s/epoch - 36ms/step
Epoch 110/200
1493/1493 - 53s - loss: 2.4386e-04 - val_loss: 2.5555e-04 - 53s/epoch - 36ms/step
Epoch 111/200
1493/1493 - 54s - loss: 2.4451e-04 - val_loss: 3.3954e-04 - 54s/epoch - 36ms/step
Epoch 112/200
1493/1493 - 54s - loss: 2.5801e-04 - val_loss: 2.3278e-04 - 54s/epoch - 36ms/step
Epoch 113/200
1493/1493 - 54s - loss: 2.4483e-04 - val_loss: 6.4670e-04 - 54s/epoch - 36ms/step
Epoch 114/200
1493/1493 - 54s - loss: 2.9748e-04 - val_loss: 2.4508e-04 - 54s/epoch - 36ms/step
Epoch 115/200
1493/1493 - 54s - loss: 2.5212e-04 - val_loss: 3.2763e-04 - 54s/epoch - 36ms/step
Epoch 116/200
1493/1493 - 53s - loss: 2.4553e-04 - val_loss: 2.5414e-04 - 53s/epoch - 36ms/step
Epoch 117/200
1493/1493 - 54s - loss: 2.4055e-04 - val_loss: 2.7837e-04 - 54s/epoch - 36ms/step
Epoch 118/200
1493/1493 - 54s - loss: 2.4748e-04 - val_loss: 2.3487e-04 - 54s/epoch - 36ms/step
Epoch 119/200
1493/1493 - 54s - loss: 2.3871e-04 - val_loss: 2.3591e-04 - 54s/epoch - 36ms/step
Epoch 120/200
1493/1493 - 54s - loss: 2.3783e-04 - val_loss: 2.3296e-04 - 54s/epoch - 36ms/step
Epoch 121/200
1493/1493 - 54s - loss: 2.3891e-04 - val_loss: 3.8525e-04 - 54s/epoch - 36ms/step
Epoch 122/200
1493/1493 - 53s - loss: 2.5382e-04 - val_loss: 2.2570e-04 - 53s/epoch - 36ms/step
Epoch 123/200
1493/1493 - 54s - loss: 2.3470e-04 - val_loss: 2.3943e-04 - 54s/epoch - 36ms/step
Epoch 124/200
1493/1493 - 54s - loss: 2.3595e-04 - val_loss: 3.9297e-04 - 54s/epoch - 36ms/step
Epoch 125/200
1493/1493 - 54s - loss: 2.6536e-04 - val_loss: 2.2679e-04 - 54s/epoch - 36ms/step
Epoch 126/200
1493/1493 - 54s - loss: 2.3646e-04 - val_loss: 3.5574e-04 - 54s/epoch - 36ms/step
Epoch 127/200
1493/1493 - 54s - loss: 2.5610e-04 - val_loss: 2.8299e-04 - 54s/epoch - 36ms/step
Epoch 128/200
1493/1493 - 54s - loss: 2.4071e-04 - val_loss: 2.1867e-04 - 54s/epoch - 36ms/step
Epoch 129/200
1493/1493 - 53s - loss: 2.3289e-04 - val_loss: 2.3627e-04 - 53s/epoch - 36ms/step
Epoch 130/200
1493/1493 - 54s - loss: 2.3253e-04 - val_loss: 2.5393e-04 - 54s/epoch - 36ms/step
Epoch 131/200
1493/1493 - 54s - loss: 2.4113e-04 - val_loss: 4.4320e-04 - 54s/epoch - 36ms/step
Epoch 132/200
1493/1493 - 54s - loss: 2.7898e-04 - val_loss: 2.1149e-04 - 54s/epoch - 36ms/step
Epoch 133/200
1493/1493 - 54s - loss: 2.3275e-04 - val_loss: 2.3037e-04 - 54s/epoch - 36ms/step
Epoch 134/200
1493/1493 - 54s - loss: 2.2973e-04 - val_loss: 2.1959e-04 - 54s/epoch - 36ms/step
Epoch 135/200
1493/1493 - 53s - loss: 2.2711e-04 - val_loss: 2.2164e-04 - 53s/epoch - 36ms/step
Epoch 136/200
1493/1493 - 54s - loss: 2.2864e-04 - val_loss: 2.6396e-04 - 54s/epoch - 36ms/step
Epoch 137/200
1493/1493 - 54s - loss: 2.4271e-04 - val_loss: 3.0176e-04 - 54s/epoch - 36ms/step
Epoch 138/200
1493/1493 - 54s - loss: 2.4248e-04 - val_loss: 2.1931e-04 - 54s/epoch - 36ms/step
Epoch 139/200
1493/1493 - 54s - loss: 2.2871e-04 - val_loss: 2.6677e-04 - 54s/epoch - 36ms/step
Epoch 140/200
1493/1493 - 54s - loss: 2.3823e-04 - val_loss: 2.1722e-04 - 54s/epoch - 36ms/step
Epoch 141/200
1493/1493 - 54s - loss: 2.2656e-04 - val_loss: 3.0500e-04 - 54s/epoch - 36ms/step
Epoch 142/200
1493/1493 - 54s - loss: 2.4162e-04 - val_loss: 2.2111e-04 - 54s/epoch - 36ms/step
Epoch 143/200
1493/1493 - 54s - loss: 2.2459e-04 - val_loss: 2.5023e-04 - 54s/epoch - 36ms/step
Epoch 144/200
1493/1493 - 54s - loss: 2.3231e-04 - val_loss: 2.2948e-04 - 54s/epoch - 36ms/step
Epoch 145/200
1493/1493 - 54s - loss: 2.2358e-04 - val_loss: 2.2056e-04 - 54s/epoch - 36ms/step
Epoch 146/200
1493/1493 - 54s - loss: 2.2334e-04 - val_loss: 2.1226e-04 - 54s/epoch - 36ms/step
Epoch 147/200
1493/1493 - 53s - loss: 2.2082e-04 - val_loss: 2.1961e-04 - 53s/epoch - 36ms/step
Epoch 148/200
1493/1493 - 54s - loss: 2.2119e-04 - val_loss: 3.2778e-04 - 54s/epoch - 36ms/step
Epoch 149/200
1493/1493 - 54s - loss: 2.2000e-04 - val_loss: 2.2389e-04 - 54s/epoch - 36ms/step
Epoch 150/200
1493/1493 - 54s - loss: 2.1844e-04 - val_loss: 2.1580e-04 - 54s/epoch - 36ms/step
Epoch 151/200
1493/1493 - 54s - loss: 2.2091e-04 - val_loss: 2.1139e-04 - 54s/epoch - 36ms/step
Epoch 152/200
1493/1493 - 54s - loss: 2.1862e-04 - val_loss: 2.6231e-04 - 54s/epoch - 36ms/step
Epoch 153/200
1493/1493 - 54s - loss: 2.2149e-04 - val_loss: 2.2730e-04 - 54s/epoch - 36ms/step
Epoch 154/200
1493/1493 - 53s - loss: 2.1980e-04 - val_loss: 3.2552e-04 - 53s/epoch - 36ms/step
Epoch 155/200
1493/1493 - 54s - loss: 2.4667e-04 - val_loss: 2.1593e-04 - 54s/epoch - 36ms/step
Epoch 156/200
1493/1493 - 54s - loss: 2.2085e-04 - val_loss: 2.1998e-04 - 54s/epoch - 36ms/step
Epoch 157/200
1493/1493 - 54s - loss: 2.1618e-04 - val_loss: 2.4861e-04 - 54s/epoch - 36ms/step
Epoch 158/200
1493/1493 - 54s - loss: 2.2180e-04 - val_loss: 2.2752e-04 - 54s/epoch - 36ms/step
Epoch 159/200
1493/1493 - 54s - loss: 2.1606e-04 - val_loss: 2.1762e-04 - 54s/epoch - 36ms/step
Epoch 160/200
1493/1493 - 53s - loss: 2.2145e-04 - val_loss: 2.7866e-04 - 53s/epoch - 36ms/step
Epoch 161/200
1493/1493 - 54s - loss: 2.4287e-04 - val_loss: 2.0375e-04 - 54s/epoch - 36ms/step
Epoch 162/200
1493/1493 - 54s - loss: 2.1639e-04 - val_loss: 2.1360e-04 - 54s/epoch - 36ms/step
Epoch 163/200
1493/1493 - 54s - loss: 2.1686e-04 - val_loss: 2.5816e-04 - 54s/epoch - 36ms/step
Epoch 164/200
1493/1493 - 54s - loss: 2.1957e-04 - val_loss: 2.1176e-04 - 54s/epoch - 36ms/step
Epoch 165/200
1493/1493 - 54s - loss: 2.2385e-04 - val_loss: 2.1267e-04 - 54s/epoch - 36ms/step
Epoch 166/200
1493/1493 - 53s - loss: 2.1451e-04 - val_loss: 2.0881e-04 - 53s/epoch - 36ms/step
Epoch 167/200
1493/1493 - 54s - loss: 2.1396e-04 - val_loss: 2.1935e-04 - 54s/epoch - 36ms/step
Epoch 168/200
1493/1493 - 54s - loss: 2.2047e-04 - val_loss: 2.0359e-04 - 54s/epoch - 36ms/step
Epoch 169/200
1493/1493 - 54s - loss: 2.1275e-04 - val_loss: 2.5430e-04 - 54s/epoch - 36ms/step
Epoch 170/200
1493/1493 - 53s - loss: 2.1824e-04 - val_loss: 4.9772e-04 - 53s/epoch - 36ms/step
Epoch 171/200
1493/1493 - 54s - loss: 2.6229e-04 - val_loss: 3.4573e-04 - 54s/epoch - 36ms/step
Epoch 172/200
1493/1493 - 53s - loss: 2.3715e-04 - val_loss: 2.0133e-04 - 53s/epoch - 36ms/step
Epoch 173/200
1493/1493 - 53s - loss: 2.1404e-04 - val_loss: 2.0137e-04 - 53s/epoch - 36ms/step
Epoch 174/200
1493/1493 - 53s - loss: 2.1089e-04 - val_loss: 2.1530e-04 - 53s/epoch - 36ms/step
Epoch 175/200
1493/1493 - 53s - loss: 2.1272e-04 - val_loss: 2.0739e-04 - 53s/epoch - 36ms/step
Epoch 176/200
1493/1493 - 53s - loss: 2.1523e-04 - val_loss: 2.9246e-04 - 53s/epoch - 36ms/step
Epoch 177/200
1493/1493 - 53s - loss: 2.3517e-04 - val_loss: 1.9809e-04 - 53s/epoch - 36ms/step
Epoch 178/200
1493/1493 - 53s - loss: 2.1083e-04 - val_loss: 2.0553e-04 - 53s/epoch - 36ms/step
Epoch 179/200
1493/1493 - 53s - loss: 2.1066e-04 - val_loss: 2.0498e-04 - 53s/epoch - 36ms/step
Epoch 180/200
1493/1493 - 53s - loss: 2.1084e-04 - val_loss: 3.3648e-04 - 53s/epoch - 36ms/step
Epoch 181/200
1493/1493 - 53s - loss: 2.2468e-04 - val_loss: 2.2427e-04 - 53s/epoch - 36ms/step
Epoch 182/200
1493/1493 - 53s - loss: 2.1096e-04 - val_loss: 2.2262e-04 - 53s/epoch - 36ms/step
Epoch 183/200
1493/1493 - 53s - loss: 2.0928e-04 - val_loss: 2.2401e-04 - 53s/epoch - 36ms/step
Epoch 184/200
1493/1493 - 53s - loss: 2.1324e-04 - val_loss: 2.0020e-04 - 53s/epoch - 36ms/step
Epoch 185/200
1493/1493 - 53s - loss: 2.0590e-04 - val_loss: 2.3713e-04 - 53s/epoch - 36ms/step
Epoch 186/200
1493/1493 - 53s - loss: 2.0746e-04 - val_loss: 2.1047e-04 - 53s/epoch - 36ms/step
Epoch 187/200
1493/1493 - 53s - loss: 2.0584e-04 - val_loss: 2.1734e-04 - 53s/epoch - 36ms/step
Epoch 188/200
1493/1493 - 53s - loss: 2.1023e-04 - val_loss: 2.4961e-04 - 53s/epoch - 36ms/step
Epoch 189/200
1493/1493 - 53s - loss: 2.1140e-04 - val_loss: 2.1058e-04 - 53s/epoch - 36ms/step
Epoch 190/200
1493/1493 - 53s - loss: 2.1018e-04 - val_loss: 2.4843e-04 - 53s/epoch - 36ms/step
Epoch 191/200
1493/1493 - 53s - loss: 2.1237e-04 - val_loss: 2.4368e-04 - 53s/epoch - 36ms/step
Epoch 192/200
1493/1493 - 53s - loss: 2.0803e-04 - val_loss: 2.1929e-04 - 53s/epoch - 36ms/step
Epoch 193/200
1493/1493 - 53s - loss: 2.1248e-04 - val_loss: 3.3695e-04 - 53s/epoch - 36ms/step
Epoch 194/200
1493/1493 - 53s - loss: 2.4109e-04 - val_loss: 4.2917e-04 - 53s/epoch - 36ms/step
Epoch 195/200
1493/1493 - 53s - loss: 2.3167e-04 - val_loss: 2.0833e-04 - 53s/epoch - 36ms/step
Epoch 196/200
1493/1493 - 53s - loss: 2.1023e-04 - val_loss: 1.9611e-04 - 53s/epoch - 36ms/step
Epoch 197/200
1493/1493 - 53s - loss: 2.0609e-04 - val_loss: 2.0075e-04 - 53s/epoch - 36ms/step
Epoch 198/200
1493/1493 - 53s - loss: 2.0739e-04 - val_loss: 2.4464e-04 - 53s/epoch - 36ms/step
Epoch 199/200
1493/1493 - 53s - loss: 2.1375e-04 - val_loss: 2.0704e-04 - 53s/epoch - 36ms/step
Epoch 200/200
1493/1493 - 53s - loss: 2.0466e-04 - val_loss: 2.0545e-04 - 53s/epoch - 36ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00020545088045764714
  1/332 [..............................] - ETA: 29s 11/332 [..............................] - ETA: 1s  21/332 [>.............................] - ETA: 1s 31/332 [=>............................] - ETA: 1s 41/332 [==>...........................] - ETA: 1s 51/332 [===>..........................] - ETA: 1s 61/332 [====>.........................] - ETA: 1s 71/332 [=====>........................] - ETA: 1s 81/332 [======>.......................] - ETA: 1s 91/332 [=======>......................] - ETA: 1s101/332 [========>.....................] - ETA: 1s111/332 [=========>....................] - ETA: 1s121/332 [=========>....................] - ETA: 1s131/332 [==========>...................] - ETA: 1s141/332 [===========>..................] - ETA: 0s151/332 [============>.................] - ETA: 0s161/332 [=============>................] - ETA: 0s171/332 [==============>...............] - ETA: 0s181/332 [===============>..............] - ETA: 0s191/332 [================>.............] - ETA: 0s201/332 [=================>............] - ETA: 0s211/332 [==================>...........] - ETA: 0s221/332 [==================>...........] - ETA: 0s231/332 [===================>..........] - ETA: 0s241/332 [====================>.........] - ETA: 0s251/332 [=====================>........] - ETA: 0s261/332 [======================>.......] - ETA: 0s271/332 [=======================>......] - ETA: 0s281/332 [========================>.....] - ETA: 0s291/332 [=========================>....] - ETA: 0s301/332 [==========================>...] - ETA: 0s311/332 [===========================>..] - ETA: 0s321/332 [============================>.] - ETA: 0s331/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 5ms/step
correlation 0.0023637364888433867
cosine 0.0018629418918424132
MAE: 0.007816281
RMSE: 0.01433355
r2: 0.9866723531899937
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 2022)              2557830   
                                                                 
 batch_normalization_3 (Batc  (None, 2022)             8088      
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2022)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               509796    
                                                                 
 batch_normalization_4 (Batc  (None, 252)              1008      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 252)               0         
                                                                 
 dense_4 (Dense)             (None, 2022)              511566    
                                                                 
 batch_normalization_5 (Batc  (None, 2022)             8088      
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2022)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              2557072   
                                                                 
=================================================================
Total params: 6,153,448
Trainable params: 6,144,856
Non-trainable params: 8,592
_________________________________________________________________
Encoder
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 2022)              2557830   
                                                                 
 batch_normalization_3 (Batc  (None, 2022)             8088      
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2022)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               509796    
                                                                 
=================================================================
Total params: 3,075,714
Trainable params: 3,071,670
Non-trainable params: 4,044
_________________________________________________________________
Decoder
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 252)]             0         
                                                                 
 batch_normalization_4 (Batc  (None, 252)              1008      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 252)               0         
                                                                 
 dense_4 (Dense)             (None, 2022)              511566    
                                                                 
 batch_normalization_5 (Batc  (None, 2022)             8088      
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2022)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              2557072   
                                                                 
=================================================================
Total params: 3,077,734
Trainable params: 3,073,186
Non-trainable params: 4,548
_________________________________________________________________
['1.6custom_n_b', 'mse', 64, 200, 0.0005, 0.2, 252, 0.0002046617737505585, 0.00020545088045764714, 0.0023637364888433867, 0.0018629418918424132, 0.007816281169652939, 0.01433354988694191, 0.9866723531899937, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_6 (Dense)             (None, 2148)              2717220   
                                                                 
 batch_normalization_6 (Batc  (None, 2148)             8592      
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2148)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               541548    
                                                                 
 batch_normalization_7 (Batc  (None, 252)              1008      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 252)               0         
                                                                 
 dense_7 (Dense)             (None, 2148)              543444    
                                                                 
 batch_normalization_8 (Batc  (None, 2148)             8592      
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2148)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              2716336   
                                                                 
=================================================================
Total params: 6,536,740
Trainable params: 6,527,644
Non-trainable params: 9,096
_________________________________________________________________
Epoch 1/200
1493/1493 - 57s - loss: 0.0102 - val_loss: 0.0051 - 57s/epoch - 38ms/step
Epoch 2/200
1493/1493 - 56s - loss: 0.0035 - val_loss: 0.0031 - 56s/epoch - 38ms/step
Epoch 3/200
1493/1493 - 56s - loss: 0.0025 - val_loss: 0.0021 - 56s/epoch - 37ms/step
Epoch 4/200
1493/1493 - 56s - loss: 0.0021 - val_loss: 0.0025 - 56s/epoch - 37ms/step
Epoch 5/200
1493/1493 - 56s - loss: 0.0019 - val_loss: 0.0016 - 56s/epoch - 38ms/step
Epoch 6/200
1493/1493 - 56s - loss: 0.0017 - val_loss: 0.0020 - 56s/epoch - 38ms/step
Epoch 7/200
1493/1493 - 56s - loss: 0.0016 - val_loss: 0.0013 - 56s/epoch - 38ms/step
Epoch 8/200
1493/1493 - 56s - loss: 0.0015 - val_loss: 0.0020 - 56s/epoch - 38ms/step
Epoch 9/200
1493/1493 - 56s - loss: 0.0013 - val_loss: 0.0012 - 56s/epoch - 37ms/step
Epoch 10/200
1493/1493 - 56s - loss: 0.0012 - val_loss: 0.0010 - 56s/epoch - 37ms/step
Epoch 11/200
1493/1493 - 56s - loss: 0.0012 - val_loss: 9.8644e-04 - 56s/epoch - 37ms/step
Epoch 12/200
1493/1493 - 56s - loss: 0.0010 - val_loss: 0.0015 - 56s/epoch - 38ms/step
Epoch 13/200
1493/1493 - 56s - loss: 0.0010 - val_loss: 0.0015 - 56s/epoch - 37ms/step
Epoch 14/200
1493/1493 - 56s - loss: 0.0010 - val_loss: 7.8875e-04 - 56s/epoch - 38ms/step
Epoch 15/200
1493/1493 - 56s - loss: 8.8815e-04 - val_loss: 8.3194e-04 - 56s/epoch - 37ms/step
Epoch 16/200
1493/1493 - 56s - loss: 8.0763e-04 - val_loss: 7.0949e-04 - 56s/epoch - 38ms/step
Epoch 17/200
1493/1493 - 56s - loss: 7.5893e-04 - val_loss: 8.3109e-04 - 56s/epoch - 38ms/step
Epoch 18/200
1493/1493 - 56s - loss: 7.3910e-04 - val_loss: 0.0013 - 56s/epoch - 38ms/step
Epoch 19/200
1493/1493 - 56s - loss: 7.6948e-04 - val_loss: 7.9338e-04 - 56s/epoch - 38ms/step
Epoch 20/200
1493/1493 - 56s - loss: 6.7249e-04 - val_loss: 6.3012e-04 - 56s/epoch - 38ms/step
Epoch 21/200
1493/1493 - 56s - loss: 6.4064e-04 - val_loss: 7.8438e-04 - 56s/epoch - 37ms/step
Epoch 22/200
1493/1493 - 56s - loss: 6.2063e-04 - val_loss: 8.1323e-04 - 56s/epoch - 38ms/step
Epoch 23/200
1493/1493 - 56s - loss: 6.3350e-04 - val_loss: 6.5419e-04 - 56s/epoch - 38ms/step
Epoch 24/200
1493/1493 - 56s - loss: 5.7939e-04 - val_loss: 5.5075e-04 - 56s/epoch - 38ms/step
Epoch 25/200
1493/1493 - 56s - loss: 5.5735e-04 - val_loss: 5.4891e-04 - 56s/epoch - 38ms/step
Epoch 26/200
1493/1493 - 56s - loss: 5.3391e-04 - val_loss: 6.7981e-04 - 56s/epoch - 38ms/step
Epoch 27/200
1493/1493 - 56s - loss: 5.1385e-04 - val_loss: 4.8745e-04 - 56s/epoch - 37ms/step
Epoch 28/200
1493/1493 - 56s - loss: 4.9322e-04 - val_loss: 8.9318e-04 - 56s/epoch - 38ms/step
Epoch 29/200
1493/1493 - 56s - loss: 4.9085e-04 - val_loss: 5.4661e-04 - 56s/epoch - 37ms/step
Epoch 30/200
1493/1493 - 56s - loss: 4.7800e-04 - val_loss: 4.8106e-04 - 56s/epoch - 38ms/step
Epoch 31/200
1493/1493 - 56s - loss: 4.6134e-04 - val_loss: 5.6270e-04 - 56s/epoch - 37ms/step
Epoch 32/200
1493/1493 - 56s - loss: 4.8103e-04 - val_loss: 4.8647e-04 - 56s/epoch - 38ms/step
Epoch 33/200
1493/1493 - 56s - loss: 4.5380e-04 - val_loss: 4.1597e-04 - 56s/epoch - 37ms/step
Epoch 34/200
1493/1493 - 56s - loss: 4.3402e-04 - val_loss: 3.9242e-04 - 56s/epoch - 38ms/step
Epoch 35/200
1493/1493 - 56s - loss: 4.1813e-04 - val_loss: 5.5044e-04 - 56s/epoch - 37ms/step
Epoch 36/200
1493/1493 - 56s - loss: 4.3044e-04 - val_loss: 4.6829e-04 - 56s/epoch - 38ms/step
Epoch 37/200
1493/1493 - 56s - loss: 4.1002e-04 - val_loss: 4.2454e-04 - 56s/epoch - 37ms/step
Epoch 38/200
1493/1493 - 56s - loss: 4.0680e-04 - val_loss: 3.9102e-04 - 56s/epoch - 38ms/step
Epoch 39/200
1493/1493 - 56s - loss: 3.9339e-04 - val_loss: 3.8001e-04 - 56s/epoch - 37ms/step
Epoch 40/200
1493/1493 - 56s - loss: 3.8084e-04 - val_loss: 4.8724e-04 - 56s/epoch - 38ms/step
Epoch 41/200
1493/1493 - 56s - loss: 3.8231e-04 - val_loss: 3.3216e-04 - 56s/epoch - 38ms/step
Epoch 42/200
1493/1493 - 56s - loss: 3.7472e-04 - val_loss: 3.7606e-04 - 56s/epoch - 38ms/step
Epoch 43/200
1493/1493 - 56s - loss: 3.7256e-04 - val_loss: 4.8013e-04 - 56s/epoch - 38ms/step
Epoch 44/200
1493/1493 - 56s - loss: 3.7483e-04 - val_loss: 8.5687e-04 - 56s/epoch - 37ms/step
Epoch 45/200
1493/1493 - 56s - loss: 4.0355e-04 - val_loss: 3.2477e-04 - 56s/epoch - 37ms/step
Epoch 46/200
1493/1493 - 56s - loss: 3.5553e-04 - val_loss: 3.1382e-04 - 56s/epoch - 38ms/step
Epoch 47/200
1493/1493 - 56s - loss: 3.5067e-04 - val_loss: 3.6996e-04 - 56s/epoch - 38ms/step
Epoch 48/200
1493/1493 - 56s - loss: 3.5084e-04 - val_loss: 3.9347e-04 - 56s/epoch - 38ms/step
Epoch 49/200
1493/1493 - 56s - loss: 3.4560e-04 - val_loss: 8.5439e-04 - 56s/epoch - 37ms/step
Epoch 50/200
1493/1493 - 56s - loss: 4.0230e-04 - val_loss: 6.8080e-04 - 56s/epoch - 37ms/step
Epoch 51/200
1493/1493 - 56s - loss: 3.6532e-04 - val_loss: 3.2728e-04 - 56s/epoch - 37ms/step
Epoch 52/200
1493/1493 - 56s - loss: 3.2951e-04 - val_loss: 3.2700e-04 - 56s/epoch - 37ms/step
Epoch 53/200
1493/1493 - 56s - loss: 3.3153e-04 - val_loss: 5.3188e-04 - 56s/epoch - 38ms/step
Epoch 54/200
1493/1493 - 56s - loss: 3.5205e-04 - val_loss: 3.1307e-04 - 56s/epoch - 37ms/step
Epoch 55/200
1493/1493 - 56s - loss: 3.1947e-04 - val_loss: 3.0941e-04 - 56s/epoch - 38ms/step
Epoch 56/200
1493/1493 - 56s - loss: 3.1807e-04 - val_loss: 3.4140e-04 - 56s/epoch - 38ms/step
Epoch 57/200
1493/1493 - 56s - loss: 3.1418e-04 - val_loss: 3.2241e-04 - 56s/epoch - 37ms/step
Epoch 58/200
1493/1493 - 56s - loss: 3.1161e-04 - val_loss: 2.8668e-04 - 56s/epoch - 38ms/step
Epoch 59/200
1493/1493 - 56s - loss: 3.0677e-04 - val_loss: 3.3724e-04 - 56s/epoch - 38ms/step
Epoch 60/200
1493/1493 - 56s - loss: 3.0747e-04 - val_loss: 5.4858e-04 - 56s/epoch - 37ms/step
Epoch 61/200
1493/1493 - 56s - loss: 3.6366e-04 - val_loss: 2.9730e-04 - 56s/epoch - 38ms/step
Epoch 62/200
1493/1493 - 56s - loss: 3.0987e-04 - val_loss: 3.2640e-04 - 56s/epoch - 38ms/step
Epoch 63/200
1493/1493 - 56s - loss: 2.9795e-04 - val_loss: 3.0176e-04 - 56s/epoch - 37ms/step
Epoch 64/200
1493/1493 - 56s - loss: 2.9990e-04 - val_loss: 0.0010 - 56s/epoch - 37ms/step
Epoch 65/200
1493/1493 - 56s - loss: 3.7155e-04 - val_loss: 7.4476e-04 - 56s/epoch - 37ms/step
Epoch 66/200
1493/1493 - 56s - loss: 3.3198e-04 - val_loss: 2.7766e-04 - 56s/epoch - 37ms/step
Epoch 67/200
1493/1493 - 56s - loss: 2.9270e-04 - val_loss: 2.8105e-04 - 56s/epoch - 37ms/step
Epoch 68/200
1493/1493 - 56s - loss: 2.9080e-04 - val_loss: 5.4209e-04 - 56s/epoch - 37ms/step
Epoch 69/200
1493/1493 - 56s - loss: 3.4124e-04 - val_loss: 2.7676e-04 - 56s/epoch - 37ms/step
Epoch 70/200
1493/1493 - 56s - loss: 2.8963e-04 - val_loss: 4.9824e-04 - 56s/epoch - 37ms/step
Epoch 71/200
1493/1493 - 56s - loss: 3.1609e-04 - val_loss: 2.6658e-04 - 56s/epoch - 38ms/step
Epoch 72/200
1493/1493 - 56s - loss: 2.8761e-04 - val_loss: 3.0809e-04 - 56s/epoch - 37ms/step
Epoch 73/200
1493/1493 - 56s - loss: 2.8687e-04 - val_loss: 2.7429e-04 - 56s/epoch - 38ms/step
Epoch 74/200
1493/1493 - 56s - loss: 2.8390e-04 - val_loss: 2.9678e-04 - 56s/epoch - 37ms/step
Epoch 75/200
1493/1493 - 56s - loss: 2.7591e-04 - val_loss: 2.8088e-04 - 56s/epoch - 37ms/step
Epoch 76/200
1493/1493 - 56s - loss: 2.7550e-04 - val_loss: 2.8294e-04 - 56s/epoch - 38ms/step
Epoch 77/200
1493/1493 - 56s - loss: 2.7003e-04 - val_loss: 4.0805e-04 - 56s/epoch - 38ms/step
Epoch 78/200
1493/1493 - 56s - loss: 2.8667e-04 - val_loss: 2.6673e-04 - 56s/epoch - 38ms/step
Epoch 79/200
1493/1493 - 56s - loss: 2.7020e-04 - val_loss: 4.0713e-04 - 56s/epoch - 38ms/step
Epoch 80/200
1493/1493 - 56s - loss: 2.9455e-04 - val_loss: 2.7406e-04 - 56s/epoch - 38ms/step
Epoch 81/200
1493/1493 - 56s - loss: 2.7123e-04 - val_loss: 3.6498e-04 - 56s/epoch - 38ms/step
Epoch 82/200
1493/1493 - 56s - loss: 2.6543e-04 - val_loss: 2.6085e-04 - 56s/epoch - 38ms/step
Epoch 83/200
1493/1493 - 56s - loss: 2.6097e-04 - val_loss: 2.8782e-04 - 56s/epoch - 38ms/step
Epoch 84/200
1493/1493 - 56s - loss: 2.6201e-04 - val_loss: 2.8404e-04 - 56s/epoch - 38ms/step
Epoch 85/200
1493/1493 - 56s - loss: 2.7765e-04 - val_loss: 2.9528e-04 - 56s/epoch - 38ms/step
Epoch 86/200
1493/1493 - 56s - loss: 2.6832e-04 - val_loss: 2.6468e-04 - 56s/epoch - 38ms/step
Epoch 87/200
1493/1493 - 56s - loss: 2.5686e-04 - val_loss: 2.5734e-04 - 56s/epoch - 38ms/step
Epoch 88/200
1493/1493 - 56s - loss: 2.5433e-04 - val_loss: 3.0791e-04 - 56s/epoch - 38ms/step
Epoch 89/200
1493/1493 - 56s - loss: 2.7257e-04 - val_loss: 2.5840e-04 - 56s/epoch - 38ms/step
Epoch 90/200
1493/1493 - 56s - loss: 2.5209e-04 - val_loss: 2.6925e-04 - 56s/epoch - 37ms/step
Epoch 91/200
1493/1493 - 56s - loss: 2.5103e-04 - val_loss: 2.9033e-04 - 56s/epoch - 38ms/step
Epoch 92/200
1493/1493 - 56s - loss: 2.4993e-04 - val_loss: 4.1604e-04 - 56s/epoch - 38ms/step
Epoch 93/200
1493/1493 - 56s - loss: 2.7856e-04 - val_loss: 2.4447e-04 - 56s/epoch - 37ms/step
Epoch 94/200
1493/1493 - 56s - loss: 2.4976e-04 - val_loss: 2.2711e-04 - 56s/epoch - 37ms/step
Epoch 95/200
1493/1493 - 56s - loss: 2.4929e-04 - val_loss: 2.7871e-04 - 56s/epoch - 38ms/step
Epoch 96/200
1493/1493 - 56s - loss: 2.5063e-04 - val_loss: 2.5679e-04 - 56s/epoch - 38ms/step
Epoch 97/200
1493/1493 - 56s - loss: 2.4705e-04 - val_loss: 2.2838e-04 - 56s/epoch - 38ms/step
Epoch 98/200
1493/1493 - 56s - loss: 2.5159e-04 - val_loss: 4.1993e-04 - 56s/epoch - 37ms/step
Epoch 99/200
1493/1493 - 56s - loss: 2.7575e-04 - val_loss: 2.3483e-04 - 56s/epoch - 37ms/step
Epoch 100/200
1493/1493 - 56s - loss: 2.4908e-04 - val_loss: 2.3649e-04 - 56s/epoch - 38ms/step
Epoch 101/200
1493/1493 - 56s - loss: 2.4256e-04 - val_loss: 3.0171e-04 - 56s/epoch - 38ms/step
Epoch 102/200
1493/1493 - 56s - loss: 2.5101e-04 - val_loss: 2.8776e-04 - 56s/epoch - 37ms/step
Epoch 103/200
1493/1493 - 56s - loss: 2.4904e-04 - val_loss: 2.3441e-04 - 56s/epoch - 38ms/step
Epoch 104/200
1493/1493 - 56s - loss: 2.3919e-04 - val_loss: 3.6287e-04 - 56s/epoch - 37ms/step
Epoch 105/200
1493/1493 - 56s - loss: 2.5775e-04 - val_loss: 2.4076e-04 - 56s/epoch - 37ms/step
Epoch 106/200
1493/1493 - 56s - loss: 2.3751e-04 - val_loss: 2.4595e-04 - 56s/epoch - 37ms/step
Epoch 107/200
1493/1493 - 56s - loss: 2.3698e-04 - val_loss: 2.7127e-04 - 56s/epoch - 38ms/step
Epoch 108/200
1493/1493 - 56s - loss: 2.4574e-04 - val_loss: 2.3696e-04 - 56s/epoch - 37ms/step
Epoch 109/200
1493/1493 - 56s - loss: 2.3592e-04 - val_loss: 2.4123e-04 - 56s/epoch - 38ms/step
Epoch 110/200
1493/1493 - 56s - loss: 2.3329e-04 - val_loss: 2.5106e-04 - 56s/epoch - 38ms/step
Epoch 111/200
1493/1493 - 56s - loss: 2.3420e-04 - val_loss: 2.5806e-04 - 56s/epoch - 37ms/step
Epoch 112/200
1493/1493 - 56s - loss: 2.3662e-04 - val_loss: 2.3128e-04 - 56s/epoch - 38ms/step
Epoch 113/200
1493/1493 - 56s - loss: 2.3459e-04 - val_loss: 5.0074e-04 - 56s/epoch - 38ms/step
Epoch 114/200
1493/1493 - 56s - loss: 2.8225e-04 - val_loss: 2.3097e-04 - 56s/epoch - 37ms/step
Epoch 115/200
1493/1493 - 56s - loss: 2.3626e-04 - val_loss: 2.7221e-04 - 56s/epoch - 38ms/step
Epoch 116/200
1493/1493 - 56s - loss: 2.2994e-04 - val_loss: 2.4811e-04 - 56s/epoch - 37ms/step
Epoch 117/200
1493/1493 - 56s - loss: 2.3144e-04 - val_loss: 4.6489e-04 - 56s/epoch - 37ms/step
Epoch 118/200
1493/1493 - 56s - loss: 2.7539e-04 - val_loss: 2.3015e-04 - 56s/epoch - 37ms/step
Epoch 119/200
1493/1493 - 56s - loss: 2.3270e-04 - val_loss: 2.4190e-04 - 56s/epoch - 37ms/step
Epoch 120/200
1493/1493 - 56s - loss: 2.2927e-04 - val_loss: 2.1478e-04 - 56s/epoch - 37ms/step
Epoch 121/200
1493/1493 - 56s - loss: 2.2923e-04 - val_loss: 3.0019e-04 - 56s/epoch - 38ms/step
Epoch 122/200
1493/1493 - 56s - loss: 2.2888e-04 - val_loss: 2.2491e-04 - 56s/epoch - 37ms/step
Epoch 123/200
1493/1493 - 56s - loss: 2.2409e-04 - val_loss: 2.4164e-04 - 56s/epoch - 37ms/step
Epoch 124/200
1493/1493 - 56s - loss: 2.2627e-04 - val_loss: 3.9865e-04 - 56s/epoch - 37ms/step
Epoch 125/200
1493/1493 - 56s - loss: 2.5802e-04 - val_loss: 2.2006e-04 - 56s/epoch - 37ms/step
Epoch 126/200
1493/1493 - 56s - loss: 2.2919e-04 - val_loss: 4.0299e-04 - 56s/epoch - 37ms/step
Epoch 127/200
1493/1493 - 56s - loss: 2.5020e-04 - val_loss: 5.3329e-04 - 56s/epoch - 37ms/step
Epoch 128/200
1493/1493 - 56s - loss: 2.6043e-04 - val_loss: 2.1424e-04 - 56s/epoch - 37ms/step
Epoch 129/200
1493/1493 - 56s - loss: 2.2479e-04 - val_loss: 2.2952e-04 - 56s/epoch - 37ms/step
Epoch 130/200
1493/1493 - 56s - loss: 2.2324e-04 - val_loss: 2.2113e-04 - 56s/epoch - 37ms/step
Epoch 131/200
1493/1493 - 56s - loss: 2.2707e-04 - val_loss: 4.4241e-04 - 56s/epoch - 38ms/step
Epoch 132/200
1493/1493 - 56s - loss: 2.7249e-04 - val_loss: 2.1248e-04 - 56s/epoch - 37ms/step
Epoch 133/200
1493/1493 - 56s - loss: 2.2319e-04 - val_loss: 2.3199e-04 - 56s/epoch - 37ms/step
Epoch 134/200
1493/1493 - 56s - loss: 2.2194e-04 - val_loss: 2.1768e-04 - 56s/epoch - 37ms/step
Epoch 135/200
1493/1493 - 56s - loss: 2.1884e-04 - val_loss: 2.1243e-04 - 56s/epoch - 37ms/step
Epoch 136/200
1493/1493 - 56s - loss: 2.2240e-04 - val_loss: 3.5904e-04 - 56s/epoch - 37ms/step
Epoch 137/200
1493/1493 - 56s - loss: 2.7348e-04 - val_loss: 3.4359e-04 - 56s/epoch - 38ms/step
Epoch 138/200
1493/1493 - 56s - loss: 2.5200e-04 - val_loss: 2.3258e-04 - 56s/epoch - 37ms/step
Epoch 139/200
1493/1493 - 56s - loss: 2.3056e-04 - val_loss: 3.0755e-04 - 56s/epoch - 38ms/step
Epoch 140/200
1493/1493 - 56s - loss: 2.4771e-04 - val_loss: 2.0431e-04 - 56s/epoch - 37ms/step
Epoch 141/200
1493/1493 - 56s - loss: 2.2102e-04 - val_loss: 3.3342e-04 - 56s/epoch - 37ms/step
Epoch 142/200
1493/1493 - 56s - loss: 2.3422e-04 - val_loss: 2.1346e-04 - 56s/epoch - 37ms/step
Epoch 143/200
1493/1493 - 56s - loss: 2.1748e-04 - val_loss: 2.5597e-04 - 56s/epoch - 38ms/step
Epoch 144/200
1493/1493 - 56s - loss: 2.2650e-04 - val_loss: 2.1498e-04 - 56s/epoch - 37ms/step
Epoch 145/200
1493/1493 - 56s - loss: 2.1441e-04 - val_loss: 2.1425e-04 - 56s/epoch - 37ms/step
Epoch 146/200
1493/1493 - 56s - loss: 2.1637e-04 - val_loss: 2.0683e-04 - 56s/epoch - 37ms/step
Epoch 147/200
1493/1493 - 56s - loss: 2.1212e-04 - val_loss: 2.2332e-04 - 56s/epoch - 37ms/step
Epoch 148/200
1493/1493 - 56s - loss: 2.1344e-04 - val_loss: 2.6131e-04 - 56s/epoch - 37ms/step
Epoch 149/200
1493/1493 - 56s - loss: 2.1055e-04 - val_loss: 2.1774e-04 - 56s/epoch - 37ms/step
Epoch 150/200
1493/1493 - 56s - loss: 2.0927e-04 - val_loss: 2.1275e-04 - 56s/epoch - 37ms/step
Epoch 151/200
1493/1493 - 56s - loss: 2.1131e-04 - val_loss: 2.1255e-04 - 56s/epoch - 37ms/step
Epoch 152/200
1493/1493 - 56s - loss: 2.1077e-04 - val_loss: 2.9525e-04 - 56s/epoch - 37ms/step
Epoch 153/200
1493/1493 - 56s - loss: 2.2245e-04 - val_loss: 2.1104e-04 - 56s/epoch - 37ms/step
Epoch 154/200
1493/1493 - 56s - loss: 2.1030e-04 - val_loss: 2.7960e-04 - 56s/epoch - 37ms/step
Epoch 155/200
1493/1493 - 56s - loss: 2.2056e-04 - val_loss: 2.0827e-04 - 56s/epoch - 37ms/step
Epoch 156/200
1493/1493 - 56s - loss: 2.0919e-04 - val_loss: 2.1044e-04 - 56s/epoch - 37ms/step
Epoch 157/200
1493/1493 - 56s - loss: 2.0637e-04 - val_loss: 2.4187e-04 - 56s/epoch - 37ms/step
Epoch 158/200
1493/1493 - 56s - loss: 2.1073e-04 - val_loss: 2.3957e-04 - 56s/epoch - 37ms/step
Epoch 159/200
1493/1493 - 56s - loss: 2.0537e-04 - val_loss: 2.0371e-04 - 56s/epoch - 37ms/step
Epoch 160/200
1493/1493 - 56s - loss: 2.0485e-04 - val_loss: 2.1461e-04 - 56s/epoch - 37ms/step
Epoch 161/200
1493/1493 - 56s - loss: 2.0569e-04 - val_loss: 2.1192e-04 - 56s/epoch - 37ms/step
Epoch 162/200
1493/1493 - 56s - loss: 2.0434e-04 - val_loss: 2.0992e-04 - 56s/epoch - 37ms/step
Epoch 163/200
1493/1493 - 56s - loss: 2.0481e-04 - val_loss: 2.6440e-04 - 56s/epoch - 37ms/step
Epoch 164/200
1493/1493 - 56s - loss: 2.1720e-04 - val_loss: 2.1299e-04 - 56s/epoch - 37ms/step
Epoch 165/200
1493/1493 - 56s - loss: 2.1101e-04 - val_loss: 2.0384e-04 - 56s/epoch - 37ms/step
Epoch 166/200
1493/1493 - 56s - loss: 2.0404e-04 - val_loss: 2.0813e-04 - 56s/epoch - 37ms/step
Epoch 167/200
1493/1493 - 56s - loss: 2.0422e-04 - val_loss: 2.1285e-04 - 56s/epoch - 37ms/step
Epoch 168/200
1493/1493 - 56s - loss: 2.0287e-04 - val_loss: 2.0030e-04 - 56s/epoch - 37ms/step
Epoch 169/200
1493/1493 - 56s - loss: 2.0201e-04 - val_loss: 2.5545e-04 - 56s/epoch - 37ms/step
Epoch 170/200
1493/1493 - 56s - loss: 2.0589e-04 - val_loss: 4.7370e-04 - 56s/epoch - 37ms/step
Epoch 171/200
1493/1493 - 56s - loss: 2.4952e-04 - val_loss: 4.7907e-04 - 56s/epoch - 37ms/step
Epoch 172/200
1493/1493 - 56s - loss: 2.5595e-04 - val_loss: 2.8120e-04 - 56s/epoch - 37ms/step
Epoch 173/200
1493/1493 - 56s - loss: 2.1677e-04 - val_loss: 1.8921e-04 - 56s/epoch - 37ms/step
Epoch 174/200
1493/1493 - 56s - loss: 2.0296e-04 - val_loss: 2.0130e-04 - 56s/epoch - 37ms/step
Epoch 175/200
1493/1493 - 56s - loss: 2.0409e-04 - val_loss: 2.0004e-04 - 56s/epoch - 37ms/step
Epoch 176/200
1493/1493 - 56s - loss: 2.0372e-04 - val_loss: 2.1058e-04 - 56s/epoch - 37ms/step
Epoch 177/200
1493/1493 - 56s - loss: 2.0593e-04 - val_loss: 1.9452e-04 - 56s/epoch - 37ms/step
Epoch 178/200
1493/1493 - 56s - loss: 2.0114e-04 - val_loss: 1.9507e-04 - 56s/epoch - 37ms/step
Epoch 179/200
1493/1493 - 56s - loss: 1.9895e-04 - val_loss: 2.3306e-04 - 56s/epoch - 37ms/step
Epoch 180/200
1493/1493 - 56s - loss: 2.0920e-04 - val_loss: 2.2642e-04 - 56s/epoch - 37ms/step
Epoch 181/200
1493/1493 - 56s - loss: 2.0330e-04 - val_loss: 2.3018e-04 - 56s/epoch - 37ms/step
Epoch 182/200
1493/1493 - 56s - loss: 2.0095e-04 - val_loss: 2.1046e-04 - 56s/epoch - 37ms/step
Epoch 183/200
1493/1493 - 56s - loss: 1.9952e-04 - val_loss: 2.2911e-04 - 56s/epoch - 37ms/step
Epoch 184/200
1493/1493 - 56s - loss: 2.0176e-04 - val_loss: 1.9137e-04 - 56s/epoch - 37ms/step
Epoch 185/200
1493/1493 - 56s - loss: 1.9532e-04 - val_loss: 2.1615e-04 - 56s/epoch - 37ms/step
Epoch 186/200
1493/1493 - 56s - loss: 1.9711e-04 - val_loss: 2.0586e-04 - 56s/epoch - 37ms/step
Epoch 187/200
1493/1493 - 56s - loss: 1.9520e-04 - val_loss: 2.0629e-04 - 56s/epoch - 37ms/step
Epoch 188/200
1493/1493 - 56s - loss: 2.0202e-04 - val_loss: 2.7662e-04 - 56s/epoch - 37ms/step
Epoch 189/200
1493/1493 - 56s - loss: 2.0073e-04 - val_loss: 2.0525e-04 - 56s/epoch - 37ms/step
Epoch 190/200
1493/1493 - 56s - loss: 1.9860e-04 - val_loss: 2.6801e-04 - 56s/epoch - 37ms/step
Epoch 191/200
1493/1493 - 56s - loss: 2.0402e-04 - val_loss: 2.5293e-04 - 56s/epoch - 37ms/step
Epoch 192/200
1493/1493 - 56s - loss: 1.9964e-04 - val_loss: 2.0784e-04 - 56s/epoch - 37ms/step
Epoch 193/200
1493/1493 - 56s - loss: 2.0001e-04 - val_loss: 3.7747e-04 - 56s/epoch - 37ms/step
Epoch 194/200
1493/1493 - 56s - loss: 2.2868e-04 - val_loss: 4.6779e-04 - 56s/epoch - 37ms/step
Epoch 195/200
1493/1493 - 56s - loss: 2.5249e-04 - val_loss: 3.5345e-04 - 56s/epoch - 37ms/step
Epoch 196/200
1493/1493 - 56s - loss: 2.1451e-04 - val_loss: 1.8833e-04 - 56s/epoch - 37ms/step
Epoch 197/200
1493/1493 - 56s - loss: 1.9906e-04 - val_loss: 2.5999e-04 - 56s/epoch - 37ms/step
Epoch 198/200
1493/1493 - 56s - loss: 2.0721e-04 - val_loss: 2.5603e-04 - 56s/epoch - 37ms/step
Epoch 199/200
1493/1493 - 56s - loss: 2.0585e-04 - val_loss: 1.9659e-04 - 56s/epoch - 37ms/step
Epoch 200/200
1493/1493 - 56s - loss: 1.9599e-04 - val_loss: 2.0181e-04 - 56s/epoch - 37ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00020180991850793362
  1/332 [..............................] - ETA: 26s 10/332 [..............................] - ETA: 1s  20/332 [>.............................] - ETA: 1s 30/332 [=>............................] - ETA: 1s 40/332 [==>...........................] - ETA: 1s 50/332 [===>..........................] - ETA: 1s 60/332 [====>.........................] - ETA: 1s 70/332 [=====>........................] - ETA: 1s 80/332 [======>.......................] - ETA: 1s 90/332 [=======>......................] - ETA: 1s100/332 [========>.....................] - ETA: 1s110/332 [========>.....................] - ETA: 1s120/332 [=========>....................] - ETA: 1s130/332 [==========>...................] - ETA: 1s140/332 [===========>..................] - ETA: 1s150/332 [============>.................] - ETA: 0s160/332 [=============>................] - ETA: 0s170/332 [==============>...............] - ETA: 0s180/332 [===============>..............] - ETA: 0s190/332 [================>.............] - ETA: 0s200/332 [=================>............] - ETA: 0s210/332 [=================>............] - ETA: 0s220/332 [==================>...........] - ETA: 0s230/332 [===================>..........] - ETA: 0s240/332 [====================>.........] - ETA: 0s250/332 [=====================>........] - ETA: 0s260/332 [======================>.......] - ETA: 0s270/332 [=======================>......] - ETA: 0s280/332 [========================>.....] - ETA: 0s290/332 [=========================>....] - ETA: 0s300/332 [==========================>...] - ETA: 0s310/332 [===========================>..] - ETA: 0s320/332 [===========================>..] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 5ms/step
correlation 0.002314657680591996
cosine 0.0018240666045721999
MAE: 0.0078071402
RMSE: 0.014205975
r2: 0.9869085338635954
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 2148)              2717220   
                                                                 
 batch_normalization_6 (Batc  (None, 2148)             8592      
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2148)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               541548    
                                                                 
 batch_normalization_7 (Batc  (None, 252)              1008      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 252)               0         
                                                                 
 dense_7 (Dense)             (None, 2148)              543444    
                                                                 
 batch_normalization_8 (Batc  (None, 2148)             8592      
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2148)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              2716336   
                                                                 
=================================================================
Total params: 6,536,740
Trainable params: 6,527,644
Non-trainable params: 9,096
_________________________________________________________________
Encoder
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 2148)              2717220   
                                                                 
 batch_normalization_6 (Batc  (None, 2148)             8592      
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2148)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               541548    
                                                                 
=================================================================
Total params: 3,267,360
Trainable params: 3,263,064
Non-trainable params: 4,296
_________________________________________________________________
Decoder
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 252)]             0         
                                                                 
 batch_normalization_7 (Batc  (None, 252)              1008      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 252)               0         
                                                                 
 dense_7 (Dense)             (None, 2148)              543444    
                                                                 
 batch_normalization_8 (Batc  (None, 2148)             8592      
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2148)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              2716336   
                                                                 
=================================================================
Total params: 3,269,380
Trainable params: 3,264,580
Non-trainable params: 4,800
_________________________________________________________________
['1.7custom_n_b', 'mse', 64, 200, 0.0005, 0.2, 252, 0.00019598928338382393, 0.00020180991850793362, 0.002314657680591996, 0.0018240666045721999, 0.007807140238583088, 0.014205975458025932, 0.9869085338635954, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_9 (Dense)             (None, 2275)              2877875   
                                                                 
 batch_normalization_9 (Batc  (None, 2275)             9100      
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2275)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               573552    
                                                                 
 batch_normalization_10 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 252)               0         
                                                                 
 dense_10 (Dense)            (None, 2275)              575575    
                                                                 
 batch_normalization_11 (Bat  (None, 2275)             9100      
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2275)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              2876864   
                                                                 
=================================================================
Total params: 6,923,074
Trainable params: 6,913,470
Non-trainable params: 9,604
_________________________________________________________________
Epoch 1/200
1493/1493 - 61s - loss: 0.0099 - val_loss: 0.0050 - 61s/epoch - 41ms/step
Epoch 2/200
1493/1493 - 60s - loss: 0.0035 - val_loss: 0.0032 - 60s/epoch - 40ms/step
Epoch 3/200
1493/1493 - 60s - loss: 0.0025 - val_loss: 0.0022 - 60s/epoch - 40ms/step
Epoch 4/200
1493/1493 - 60s - loss: 0.0021 - val_loss: 0.0041 - 60s/epoch - 40ms/step
Epoch 5/200
1493/1493 - 60s - loss: 0.0020 - val_loss: 0.0016 - 60s/epoch - 40ms/step
Epoch 6/200
1493/1493 - 60s - loss: 0.0017 - val_loss: 0.0018 - 60s/epoch - 40ms/step
Epoch 7/200
1493/1493 - 60s - loss: 0.0017 - val_loss: 0.0016 - 60s/epoch - 40ms/step
Epoch 8/200
1493/1493 - 60s - loss: 0.0015 - val_loss: 0.0017 - 60s/epoch - 40ms/step
Epoch 9/200
1493/1493 - 60s - loss: 0.0014 - val_loss: 0.0040 - 60s/epoch - 40ms/step
Epoch 10/200
1493/1493 - 60s - loss: 0.0015 - val_loss: 0.0011 - 60s/epoch - 40ms/step
Epoch 11/200
1493/1493 - 60s - loss: 0.0012 - val_loss: 0.0010 - 60s/epoch - 40ms/step
Epoch 12/200
1493/1493 - 60s - loss: 0.0011 - val_loss: 0.0013 - 60s/epoch - 40ms/step
Epoch 13/200
1493/1493 - 60s - loss: 0.0010 - val_loss: 0.0020 - 60s/epoch - 40ms/step
Epoch 14/200
1493/1493 - 60s - loss: 0.0011 - val_loss: 8.9139e-04 - 60s/epoch - 40ms/step
Epoch 15/200
1493/1493 - 60s - loss: 9.0004e-04 - val_loss: 8.0975e-04 - 60s/epoch - 40ms/step
Epoch 16/200
1493/1493 - 60s - loss: 8.1168e-04 - val_loss: 7.2645e-04 - 60s/epoch - 40ms/step
Epoch 17/200
1493/1493 - 60s - loss: 7.6189e-04 - val_loss: 0.0011 - 60s/epoch - 40ms/step
Epoch 18/200
1493/1493 - 60s - loss: 7.7792e-04 - val_loss: 8.7109e-04 - 60s/epoch - 40ms/step
Epoch 19/200
1493/1493 - 60s - loss: 7.2702e-04 - val_loss: 7.5778e-04 - 60s/epoch - 40ms/step
Epoch 20/200
1493/1493 - 60s - loss: 6.7258e-04 - val_loss: 5.9021e-04 - 60s/epoch - 40ms/step
Epoch 21/200
1493/1493 - 60s - loss: 6.4733e-04 - val_loss: 7.2932e-04 - 60s/epoch - 40ms/step
Epoch 22/200
1493/1493 - 60s - loss: 6.1940e-04 - val_loss: 6.3235e-04 - 60s/epoch - 40ms/step
Epoch 23/200
1493/1493 - 60s - loss: 5.8563e-04 - val_loss: 6.4933e-04 - 60s/epoch - 40ms/step
Epoch 24/200
1493/1493 - 60s - loss: 5.8269e-04 - val_loss: 5.4360e-04 - 60s/epoch - 40ms/step
Epoch 25/200
1493/1493 - 60s - loss: 5.4995e-04 - val_loss: 5.0368e-04 - 60s/epoch - 40ms/step
Epoch 26/200
1493/1493 - 60s - loss: 5.2782e-04 - val_loss: 6.2265e-04 - 60s/epoch - 40ms/step
Epoch 27/200
1493/1493 - 60s - loss: 5.1238e-04 - val_loss: 5.0714e-04 - 60s/epoch - 40ms/step
Epoch 28/200
1493/1493 - 60s - loss: 4.9089e-04 - val_loss: 7.3392e-04 - 60s/epoch - 40ms/step
Epoch 29/200
1493/1493 - 60s - loss: 4.9268e-04 - val_loss: 5.0214e-04 - 60s/epoch - 40ms/step
Epoch 30/200
1493/1493 - 60s - loss: 4.7250e-04 - val_loss: 5.0523e-04 - 60s/epoch - 40ms/step
Epoch 31/200
1493/1493 - 60s - loss: 4.6035e-04 - val_loss: 6.3214e-04 - 60s/epoch - 40ms/step
Epoch 32/200
1493/1493 - 60s - loss: 4.9561e-04 - val_loss: 4.6650e-04 - 60s/epoch - 40ms/step
Epoch 33/200
1493/1493 - 60s - loss: 4.4927e-04 - val_loss: 4.2071e-04 - 60s/epoch - 40ms/step
Epoch 34/200
1493/1493 - 60s - loss: 4.3239e-04 - val_loss: 4.0489e-04 - 60s/epoch - 40ms/step
Epoch 35/200
1493/1493 - 60s - loss: 4.1923e-04 - val_loss: 8.7257e-04 - 60s/epoch - 40ms/step
Epoch 36/200
1493/1493 - 60s - loss: 4.6529e-04 - val_loss: 4.6161e-04 - 60s/epoch - 40ms/step
Epoch 37/200
1493/1493 - 60s - loss: 4.1078e-04 - val_loss: 5.0354e-04 - 60s/epoch - 40ms/step
Epoch 38/200
1493/1493 - 60s - loss: 4.4620e-04 - val_loss: 3.7870e-04 - 60s/epoch - 40ms/step
Epoch 39/200
1493/1493 - 60s - loss: 4.0120e-04 - val_loss: 3.6364e-04 - 60s/epoch - 40ms/step
Epoch 40/200
1493/1493 - 60s - loss: 3.8738e-04 - val_loss: 4.1377e-04 - 60s/epoch - 40ms/step
Epoch 41/200
1493/1493 - 60s - loss: 3.8586e-04 - val_loss: 3.3800e-04 - 60s/epoch - 40ms/step
Epoch 42/200
1493/1493 - 60s - loss: 3.7869e-04 - val_loss: 3.5674e-04 - 60s/epoch - 40ms/step
Epoch 43/200
1493/1493 - 60s - loss: 3.7309e-04 - val_loss: 3.7150e-04 - 60s/epoch - 40ms/step
Epoch 44/200
1493/1493 - 60s - loss: 3.7437e-04 - val_loss: 9.4812e-04 - 60s/epoch - 40ms/step
Epoch 45/200
1493/1493 - 60s - loss: 4.1668e-04 - val_loss: 3.2269e-04 - 60s/epoch - 40ms/step
Epoch 46/200
1493/1493 - 60s - loss: 3.5721e-04 - val_loss: 3.3086e-04 - 60s/epoch - 40ms/step
Epoch 47/200
1493/1493 - 60s - loss: 3.5864e-04 - val_loss: 3.4049e-04 - 60s/epoch - 40ms/step
Epoch 48/200
1493/1493 - 60s - loss: 3.5067e-04 - val_loss: 3.8458e-04 - 60s/epoch - 40ms/step
Epoch 49/200
1493/1493 - 60s - loss: 3.5258e-04 - val_loss: 8.4807e-04 - 60s/epoch - 40ms/step
Epoch 50/200
1493/1493 - 60s - loss: 4.4308e-04 - val_loss: 8.3784e-04 - 60s/epoch - 40ms/step
Epoch 51/200
1493/1493 - 60s - loss: 3.7833e-04 - val_loss: 3.1689e-04 - 60s/epoch - 40ms/step
Epoch 52/200
1493/1493 - 60s - loss: 3.3561e-04 - val_loss: 3.1998e-04 - 60s/epoch - 40ms/step
Epoch 53/200
1493/1493 - 60s - loss: 3.3901e-04 - val_loss: 3.9943e-04 - 60s/epoch - 40ms/step
Epoch 54/200
1493/1493 - 60s - loss: 3.4690e-04 - val_loss: 3.3445e-04 - 60s/epoch - 40ms/step
Epoch 55/200
1493/1493 - 60s - loss: 3.2474e-04 - val_loss: 3.1600e-04 - 60s/epoch - 40ms/step
Epoch 56/200
1493/1493 - 60s - loss: 3.2281e-04 - val_loss: 3.2987e-04 - 60s/epoch - 40ms/step
Epoch 57/200
1493/1493 - 60s - loss: 3.1715e-04 - val_loss: 3.3193e-04 - 60s/epoch - 40ms/step
Epoch 58/200
1493/1493 - 60s - loss: 3.1683e-04 - val_loss: 2.9151e-04 - 60s/epoch - 40ms/step
Epoch 59/200
1493/1493 - 60s - loss: 3.1140e-04 - val_loss: 3.1700e-04 - 60s/epoch - 40ms/step
Epoch 60/200
1493/1493 - 60s - loss: 3.1320e-04 - val_loss: 6.0014e-04 - 60s/epoch - 40ms/step
Epoch 61/200
1493/1493 - 60s - loss: 4.1766e-04 - val_loss: 2.8750e-04 - 60s/epoch - 40ms/step
Epoch 62/200
1493/1493 - 60s - loss: 3.1372e-04 - val_loss: 3.2010e-04 - 60s/epoch - 40ms/step
Epoch 63/200
1493/1493 - 60s - loss: 3.0389e-04 - val_loss: 3.2841e-04 - 60s/epoch - 40ms/step
Epoch 64/200
1493/1493 - 60s - loss: 3.0396e-04 - val_loss: 7.7922e-04 - 60s/epoch - 40ms/step
Epoch 65/200
1493/1493 - 60s - loss: 3.5154e-04 - val_loss: 3.4048e-04 - 60s/epoch - 40ms/step
Epoch 66/200
1493/1493 - 60s - loss: 3.0847e-04 - val_loss: 2.9681e-04 - 60s/epoch - 40ms/step
Epoch 67/200
1493/1493 - 60s - loss: 2.9681e-04 - val_loss: 2.8767e-04 - 60s/epoch - 40ms/step
Epoch 68/200
1493/1493 - 60s - loss: 2.9421e-04 - val_loss: 5.0487e-04 - 60s/epoch - 40ms/step
Epoch 69/200
1493/1493 - 60s - loss: 3.3724e-04 - val_loss: 3.0161e-04 - 60s/epoch - 40ms/step
Epoch 70/200
1493/1493 - 60s - loss: 2.9232e-04 - val_loss: 4.5162e-04 - 60s/epoch - 40ms/step
Epoch 71/200
1493/1493 - 60s - loss: 3.2796e-04 - val_loss: 2.7918e-04 - 60s/epoch - 40ms/step
Epoch 72/200
1493/1493 - 60s - loss: 2.9201e-04 - val_loss: 3.0026e-04 - 60s/epoch - 40ms/step
Epoch 73/200
1493/1493 - 60s - loss: 2.9191e-04 - val_loss: 2.8886e-04 - 60s/epoch - 40ms/step
Epoch 74/200
1493/1493 - 60s - loss: 2.8791e-04 - val_loss: 2.9977e-04 - 60s/epoch - 40ms/step
Epoch 75/200
1493/1493 - 60s - loss: 2.8016e-04 - val_loss: 2.8492e-04 - 60s/epoch - 40ms/step
Epoch 76/200
1493/1493 - 60s - loss: 2.7665e-04 - val_loss: 2.7968e-04 - 60s/epoch - 40ms/step
Epoch 77/200
1493/1493 - 60s - loss: 2.8088e-04 - val_loss: 3.7443e-04 - 60s/epoch - 40ms/step
Epoch 78/200
1493/1493 - 60s - loss: 2.8091e-04 - val_loss: 2.6808e-04 - 60s/epoch - 40ms/step
Epoch 79/200
1493/1493 - 60s - loss: 2.7693e-04 - val_loss: 4.7088e-04 - 60s/epoch - 40ms/step
Epoch 80/200
1493/1493 - 60s - loss: 3.2122e-04 - val_loss: 2.7674e-04 - 60s/epoch - 40ms/step
Epoch 81/200
1493/1493 - 60s - loss: 2.7923e-04 - val_loss: 3.5822e-04 - 60s/epoch - 40ms/step
Epoch 82/200
1493/1493 - 60s - loss: 2.7019e-04 - val_loss: 2.6217e-04 - 60s/epoch - 40ms/step
Epoch 83/200
1493/1493 - 60s - loss: 2.6596e-04 - val_loss: 2.8327e-04 - 60s/epoch - 40ms/step
Epoch 84/200
1493/1493 - 60s - loss: 2.6578e-04 - val_loss: 2.8097e-04 - 60s/epoch - 40ms/step
Epoch 85/200
1493/1493 - 60s - loss: 2.7205e-04 - val_loss: 3.3030e-04 - 60s/epoch - 40ms/step
Epoch 86/200
1493/1493 - 60s - loss: 2.7712e-04 - val_loss: 2.5899e-04 - 60s/epoch - 40ms/step
Epoch 87/200
1493/1493 - 60s - loss: 2.6003e-04 - val_loss: 2.5262e-04 - 60s/epoch - 40ms/step
Epoch 88/200
1493/1493 - 60s - loss: 2.5904e-04 - val_loss: 4.7274e-04 - 60s/epoch - 40ms/step
Epoch 89/200
1493/1493 - 60s - loss: 2.8525e-04 - val_loss: 2.5757e-04 - 60s/epoch - 40ms/step
Epoch 90/200
1493/1493 - 60s - loss: 2.5882e-04 - val_loss: 2.5738e-04 - 60s/epoch - 40ms/step
Epoch 91/200
1493/1493 - 60s - loss: 2.5715e-04 - val_loss: 2.9813e-04 - 60s/epoch - 40ms/step
Epoch 92/200
1493/1493 - 60s - loss: 2.5565e-04 - val_loss: 3.9164e-04 - 60s/epoch - 40ms/step
Epoch 93/200
1493/1493 - 60s - loss: 2.8499e-04 - val_loss: 2.5864e-04 - 60s/epoch - 40ms/step
Epoch 94/200
1493/1493 - 60s - loss: 2.5716e-04 - val_loss: 2.4295e-04 - 60s/epoch - 40ms/step
Epoch 95/200
1493/1493 - 60s - loss: 2.5386e-04 - val_loss: 2.6865e-04 - 60s/epoch - 40ms/step
Epoch 96/200
1493/1493 - 60s - loss: 2.5449e-04 - val_loss: 2.6700e-04 - 60s/epoch - 40ms/step
Epoch 97/200
1493/1493 - 60s - loss: 2.5103e-04 - val_loss: 2.3883e-04 - 60s/epoch - 40ms/step
Epoch 98/200
1493/1493 - 60s - loss: 2.6415e-04 - val_loss: 5.8398e-04 - 60s/epoch - 40ms/step
Epoch 99/200
1493/1493 - 60s - loss: 3.0116e-04 - val_loss: 2.4005e-04 - 60s/epoch - 40ms/step
Epoch 100/200
1493/1493 - 60s - loss: 2.5583e-04 - val_loss: 2.5283e-04 - 60s/epoch - 40ms/step
Epoch 101/200
1493/1493 - 60s - loss: 2.4745e-04 - val_loss: 2.6030e-04 - 60s/epoch - 40ms/step
Epoch 102/200
1493/1493 - 60s - loss: 2.4907e-04 - val_loss: 2.9123e-04 - 60s/epoch - 40ms/step
Epoch 103/200
1493/1493 - 60s - loss: 2.4989e-04 - val_loss: 2.3827e-04 - 60s/epoch - 40ms/step
Epoch 104/200
1493/1493 - 60s - loss: 2.4449e-04 - val_loss: 4.0094e-04 - 60s/epoch - 40ms/step
Epoch 105/200
1493/1493 - 60s - loss: 2.6340e-04 - val_loss: 2.3363e-04 - 60s/epoch - 40ms/step
Epoch 106/200
1493/1493 - 60s - loss: 2.4229e-04 - val_loss: 2.5296e-04 - 60s/epoch - 40ms/step
Epoch 107/200
1493/1493 - 60s - loss: 2.4093e-04 - val_loss: 2.6268e-04 - 60s/epoch - 40ms/step
Epoch 108/200
1493/1493 - 60s - loss: 2.4466e-04 - val_loss: 2.4164e-04 - 60s/epoch - 40ms/step
Epoch 109/200
1493/1493 - 60s - loss: 2.4048e-04 - val_loss: 2.5157e-04 - 60s/epoch - 40ms/step
Epoch 110/200
1493/1493 - 60s - loss: 2.3801e-04 - val_loss: 2.9233e-04 - 60s/epoch - 40ms/step
Epoch 111/200
1493/1493 - 60s - loss: 2.4046e-04 - val_loss: 3.8770e-04 - 60s/epoch - 40ms/step
Epoch 112/200
1493/1493 - 60s - loss: 2.8545e-04 - val_loss: 2.2389e-04 - 60s/epoch - 40ms/step
Epoch 113/200
1493/1493 - 60s - loss: 2.4028e-04 - val_loss: 5.1150e-04 - 60s/epoch - 40ms/step
Epoch 114/200
1493/1493 - 60s - loss: 2.8540e-04 - val_loss: 2.5147e-04 - 60s/epoch - 40ms/step
Epoch 115/200
1493/1493 - 60s - loss: 2.4500e-04 - val_loss: 2.8322e-04 - 60s/epoch - 40ms/step
Epoch 116/200
1493/1493 - 60s - loss: 2.3830e-04 - val_loss: 2.4383e-04 - 60s/epoch - 40ms/step
Epoch 117/200
1493/1493 - 60s - loss: 2.3547e-04 - val_loss: 2.7168e-04 - 60s/epoch - 40ms/step
Epoch 118/200
1493/1493 - 60s - loss: 2.4578e-04 - val_loss: 2.4273e-04 - 60s/epoch - 40ms/step
Epoch 119/200
1493/1493 - 60s - loss: 2.3395e-04 - val_loss: 2.3667e-04 - 60s/epoch - 40ms/step
Epoch 120/200
1493/1493 - 60s - loss: 2.3187e-04 - val_loss: 2.2508e-04 - 60s/epoch - 40ms/step
Epoch 121/200
1493/1493 - 60s - loss: 2.3204e-04 - val_loss: 3.3195e-04 - 60s/epoch - 40ms/step
Epoch 122/200
1493/1493 - 60s - loss: 2.3604e-04 - val_loss: 2.4501e-04 - 60s/epoch - 40ms/step
Epoch 123/200
1493/1493 - 60s - loss: 2.3395e-04 - val_loss: 2.3508e-04 - 60s/epoch - 40ms/step
Epoch 124/200
1493/1493 - 60s - loss: 2.2706e-04 - val_loss: 3.2984e-04 - 60s/epoch - 40ms/step
Epoch 125/200
1493/1493 - 60s - loss: 2.3532e-04 - val_loss: 2.3464e-04 - 60s/epoch - 40ms/step
Epoch 126/200
1493/1493 - 60s - loss: 2.3112e-04 - val_loss: 3.2890e-04 - 60s/epoch - 40ms/step
Epoch 127/200
1493/1493 - 60s - loss: 2.4666e-04 - val_loss: 2.4850e-04 - 60s/epoch - 40ms/step
Epoch 128/200
1493/1493 - 60s - loss: 2.2845e-04 - val_loss: 2.2033e-04 - 60s/epoch - 40ms/step
Epoch 129/200
1493/1493 - 60s - loss: 2.2529e-04 - val_loss: 2.3588e-04 - 60s/epoch - 40ms/step
Epoch 130/200
1493/1493 - 60s - loss: 2.2474e-04 - val_loss: 2.2622e-04 - 60s/epoch - 40ms/step
Epoch 131/200
1493/1493 - 60s - loss: 2.2872e-04 - val_loss: 4.4481e-04 - 60s/epoch - 40ms/step
Epoch 132/200
1493/1493 - 60s - loss: 2.6875e-04 - val_loss: 2.1757e-04 - 60s/epoch - 40ms/step
Epoch 133/200
1493/1493 - 60s - loss: 2.2741e-04 - val_loss: 2.3235e-04 - 60s/epoch - 40ms/step
Epoch 134/200
1493/1493 - 60s - loss: 2.2573e-04 - val_loss: 2.1923e-04 - 60s/epoch - 40ms/step
Epoch 135/200
1493/1493 - 60s - loss: 2.2061e-04 - val_loss: 2.1790e-04 - 60s/epoch - 40ms/step
Epoch 136/200
1493/1493 - 60s - loss: 2.2497e-04 - val_loss: 3.7185e-04 - 60s/epoch - 40ms/step
Epoch 137/200
1493/1493 - 60s - loss: 2.7273e-04 - val_loss: 3.5933e-04 - 60s/epoch - 40ms/step
Epoch 138/200
1493/1493 - 60s - loss: 2.6227e-04 - val_loss: 2.3764e-04 - 60s/epoch - 40ms/step
Epoch 139/200
1493/1493 - 60s - loss: 2.2955e-04 - val_loss: 2.4916e-04 - 60s/epoch - 40ms/step
Epoch 140/200
1493/1493 - 60s - loss: 2.2970e-04 - val_loss: 2.1869e-04 - 60s/epoch - 40ms/step
Epoch 141/200
1493/1493 - 60s - loss: 2.2169e-04 - val_loss: 2.6975e-04 - 60s/epoch - 40ms/step
Epoch 142/200
1493/1493 - 60s - loss: 2.3081e-04 - val_loss: 2.1817e-04 - 60s/epoch - 40ms/step
Epoch 143/200
1493/1493 - 60s - loss: 2.1871e-04 - val_loss: 2.5418e-04 - 60s/epoch - 40ms/step
Epoch 144/200
1493/1493 - 60s - loss: 2.2426e-04 - val_loss: 2.2309e-04 - 60s/epoch - 40ms/step
Epoch 145/200
1493/1493 - 60s - loss: 2.1834e-04 - val_loss: 2.1081e-04 - 60s/epoch - 40ms/step
Epoch 146/200
1493/1493 - 60s - loss: 2.1693e-04 - val_loss: 2.0495e-04 - 60s/epoch - 40ms/step
Epoch 147/200
1493/1493 - 59s - loss: 2.1450e-04 - val_loss: 2.2883e-04 - 59s/epoch - 40ms/step
Epoch 148/200
1493/1493 - 60s - loss: 2.1565e-04 - val_loss: 2.8454e-04 - 60s/epoch - 40ms/step
Epoch 149/200
1493/1493 - 60s - loss: 2.1444e-04 - val_loss: 2.2386e-04 - 60s/epoch - 40ms/step
Epoch 150/200
1493/1493 - 60s - loss: 2.1244e-04 - val_loss: 2.1232e-04 - 60s/epoch - 40ms/step
Epoch 151/200
1493/1493 - 60s - loss: 2.1359e-04 - val_loss: 2.1370e-04 - 60s/epoch - 40ms/step
Epoch 152/200
1493/1493 - 60s - loss: 2.1341e-04 - val_loss: 2.9327e-04 - 60s/epoch - 40ms/step
Epoch 153/200
1493/1493 - 60s - loss: 2.2109e-04 - val_loss: 2.2020e-04 - 60s/epoch - 40ms/step
Epoch 154/200
1493/1493 - 60s - loss: 2.1319e-04 - val_loss: 2.8901e-04 - 60s/epoch - 40ms/step
Epoch 155/200
1493/1493 - 60s - loss: 2.3530e-04 - val_loss: 2.0965e-04 - 60s/epoch - 40ms/step
Epoch 156/200
1493/1493 - 60s - loss: 2.1602e-04 - val_loss: 2.1829e-04 - 60s/epoch - 40ms/step
Epoch 157/200
1493/1493 - 60s - loss: 2.0995e-04 - val_loss: 2.4241e-04 - 60s/epoch - 40ms/step
Epoch 158/200
1493/1493 - 60s - loss: 2.1482e-04 - val_loss: 2.1976e-04 - 60s/epoch - 40ms/step
Epoch 159/200
1493/1493 - 60s - loss: 2.0891e-04 - val_loss: 2.0646e-04 - 60s/epoch - 40ms/step
Epoch 160/200
1493/1493 - 60s - loss: 2.1250e-04 - val_loss: 2.8944e-04 - 60s/epoch - 40ms/step
Epoch 161/200
1493/1493 - 60s - loss: 2.4056e-04 - val_loss: 2.0428e-04 - 60s/epoch - 40ms/step
Epoch 162/200
1493/1493 - 60s - loss: 2.1139e-04 - val_loss: 2.1526e-04 - 60s/epoch - 40ms/step
Epoch 163/200
1493/1493 - 60s - loss: 2.1129e-04 - val_loss: 2.8057e-04 - 60s/epoch - 40ms/step
Epoch 164/200
1493/1493 - 60s - loss: 2.2211e-04 - val_loss: 2.1186e-04 - 60s/epoch - 40ms/step
Epoch 165/200
1493/1493 - 60s - loss: 2.2988e-04 - val_loss: 2.0967e-04 - 60s/epoch - 40ms/step
Epoch 166/200
1493/1493 - 60s - loss: 2.0927e-04 - val_loss: 2.2305e-04 - 60s/epoch - 40ms/step
Epoch 167/200
1493/1493 - 60s - loss: 2.0823e-04 - val_loss: 2.1194e-04 - 60s/epoch - 40ms/step
Epoch 168/200
1493/1493 - 60s - loss: 2.1237e-04 - val_loss: 2.0413e-04 - 60s/epoch - 40ms/step
Epoch 169/200
1493/1493 - 60s - loss: 2.0646e-04 - val_loss: 2.4582e-04 - 60s/epoch - 40ms/step
Epoch 170/200
1493/1493 - 60s - loss: 2.0483e-04 - val_loss: 2.4794e-04 - 60s/epoch - 40ms/step
Epoch 171/200
1493/1493 - 60s - loss: 2.1465e-04 - val_loss: 4.3184e-04 - 60s/epoch - 40ms/step
Epoch 172/200
1493/1493 - 60s - loss: 2.4023e-04 - val_loss: 2.6549e-04 - 60s/epoch - 40ms/step
Epoch 173/200
1493/1493 - 60s - loss: 2.1604e-04 - val_loss: 1.9497e-04 - 60s/epoch - 40ms/step
Epoch 174/200
1493/1493 - 60s - loss: 2.0498e-04 - val_loss: 2.0726e-04 - 60s/epoch - 40ms/step
Epoch 175/200
1493/1493 - 60s - loss: 2.0568e-04 - val_loss: 2.0529e-04 - 60s/epoch - 40ms/step
Epoch 176/200
1493/1493 - 60s - loss: 2.0690e-04 - val_loss: 2.4640e-04 - 60s/epoch - 40ms/step
Epoch 177/200
1493/1493 - 60s - loss: 2.1648e-04 - val_loss: 2.0241e-04 - 60s/epoch - 40ms/step
Epoch 178/200
1493/1493 - 60s - loss: 2.0356e-04 - val_loss: 1.9901e-04 - 60s/epoch - 40ms/step
Epoch 179/200
1493/1493 - 60s - loss: 2.0220e-04 - val_loss: 1.9967e-04 - 60s/epoch - 40ms/step
Epoch 180/200
1493/1493 - 60s - loss: 2.0449e-04 - val_loss: 3.2746e-04 - 60s/epoch - 40ms/step
Epoch 181/200
1493/1493 - 60s - loss: 2.2361e-04 - val_loss: 2.3271e-04 - 60s/epoch - 40ms/step
Epoch 182/200
1493/1493 - 60s - loss: 2.0593e-04 - val_loss: 2.2235e-04 - 60s/epoch - 40ms/step
Epoch 183/200
1493/1493 - 60s - loss: 2.0449e-04 - val_loss: 2.4743e-04 - 60s/epoch - 40ms/step
Epoch 184/200
1493/1493 - 60s - loss: 2.0636e-04 - val_loss: 1.9598e-04 - 60s/epoch - 40ms/step
Epoch 185/200
1493/1493 - 60s - loss: 1.9960e-04 - val_loss: 2.4086e-04 - 60s/epoch - 40ms/step
Epoch 186/200
1493/1493 - 60s - loss: 2.0061e-04 - val_loss: 2.1227e-04 - 60s/epoch - 40ms/step
Epoch 187/200
1493/1493 - 60s - loss: 1.9903e-04 - val_loss: 2.1648e-04 - 60s/epoch - 40ms/step
Epoch 188/200
1493/1493 - 60s - loss: 2.0534e-04 - val_loss: 3.5938e-04 - 60s/epoch - 40ms/step
Epoch 189/200
1493/1493 - 60s - loss: 2.0893e-04 - val_loss: 2.1044e-04 - 60s/epoch - 40ms/step
Epoch 190/200
1493/1493 - 60s - loss: 2.0136e-04 - val_loss: 2.1346e-04 - 60s/epoch - 40ms/step
Epoch 191/200
1493/1493 - 60s - loss: 2.0303e-04 - val_loss: 2.3051e-04 - 60s/epoch - 40ms/step
Epoch 192/200
1493/1493 - 60s - loss: 2.0308e-04 - val_loss: 2.1418e-04 - 60s/epoch - 40ms/step
Epoch 193/200
1493/1493 - 60s - loss: 2.0218e-04 - val_loss: 2.7832e-04 - 60s/epoch - 40ms/step
Epoch 194/200
1493/1493 - 60s - loss: 2.1483e-04 - val_loss: 3.6750e-04 - 60s/epoch - 40ms/step
Epoch 195/200
1493/1493 - 60s - loss: 2.2390e-04 - val_loss: 2.1663e-04 - 60s/epoch - 40ms/step
Epoch 196/200
1493/1493 - 60s - loss: 2.0165e-04 - val_loss: 2.1740e-04 - 60s/epoch - 40ms/step
Epoch 197/200
1493/1493 - 60s - loss: 1.9797e-04 - val_loss: 2.0314e-04 - 60s/epoch - 40ms/step
Epoch 198/200
1493/1493 - 60s - loss: 2.0037e-04 - val_loss: 2.5269e-04 - 60s/epoch - 40ms/step
Epoch 199/200
1493/1493 - 60s - loss: 2.0523e-04 - val_loss: 1.9355e-04 - 60s/epoch - 40ms/step
Epoch 200/200
1493/1493 - 60s - loss: 1.9709e-04 - val_loss: 2.1264e-04 - 60s/epoch - 40ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00021263776579871774
  1/332 [..............................] - ETA: 28s 10/332 [..............................] - ETA: 1s  19/332 [>.............................] - ETA: 1s 28/332 [=>............................] - ETA: 1s 37/332 [==>...........................] - ETA: 1s 46/332 [===>..........................] - ETA: 1s 55/332 [===>..........................] - ETA: 1s 64/332 [====>.........................] - ETA: 1s 73/332 [=====>........................] - ETA: 1s 82/332 [======>.......................] - ETA: 1s 91/332 [=======>......................] - ETA: 1s100/332 [========>.....................] - ETA: 1s109/332 [========>.....................] - ETA: 1s118/332 [=========>....................] - ETA: 1s127/332 [==========>...................] - ETA: 1s136/332 [===========>..................] - ETA: 1s145/332 [============>.................] - ETA: 1s154/332 [============>.................] - ETA: 1s163/332 [=============>................] - ETA: 1s172/332 [==============>...............] - ETA: 0s181/332 [===============>..............] - ETA: 0s190/332 [================>.............] - ETA: 0s199/332 [================>.............] - ETA: 0s208/332 [=================>............] - ETA: 0s217/332 [==================>...........] - ETA: 0s226/332 [===================>..........] - ETA: 0s235/332 [====================>.........] - ETA: 0s244/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s262/332 [======================>.......] - ETA: 0s271/332 [=======================>......] - ETA: 0s280/332 [========================>.....] - ETA: 0s289/332 [=========================>....] - ETA: 0s298/332 [=========================>....] - ETA: 0s307/332 [==========================>...] - ETA: 0s316/332 [===========================>..] - ETA: 0s325/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 6ms/step
correlation 0.0024438844877865645
cosine 0.001925442630373843
MAE: 0.007812967
RMSE: 0.0145820975
r2: 0.9862062739782043
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 2275)              2877875   
                                                                 
 batch_normalization_9 (Batc  (None, 2275)             9100      
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2275)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               573552    
                                                                 
 batch_normalization_10 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 252)               0         
                                                                 
 dense_10 (Dense)            (None, 2275)              575575    
                                                                 
 batch_normalization_11 (Bat  (None, 2275)             9100      
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2275)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              2876864   
                                                                 
=================================================================
Total params: 6,923,074
Trainable params: 6,913,470
Non-trainable params: 9,604
_________________________________________________________________
Encoder
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 2275)              2877875   
                                                                 
 batch_normalization_9 (Batc  (None, 2275)             9100      
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2275)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               573552    
                                                                 
=================================================================
Total params: 3,460,527
Trainable params: 3,455,977
Non-trainable params: 4,550
_________________________________________________________________
Decoder
Model: "model_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 252)]             0         
                                                                 
 batch_normalization_10 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 252)               0         
                                                                 
 dense_10 (Dense)            (None, 2275)              575575    
                                                                 
 batch_normalization_11 (Bat  (None, 2275)             9100      
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2275)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              2876864   
                                                                 
=================================================================
Total params: 3,462,547
Trainable params: 3,457,493
Non-trainable params: 5,054
_________________________________________________________________
['1.8custom_n_b', 'mse', 64, 200, 0.0005, 0.2, 252, 0.0001970883458852768, 0.00021263776579871774, 0.0024438844877865645, 0.001925442630373843, 0.007812966592609882, 0.01458209753036499, 0.9862062739782043, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_12 (Dense)            (None, 2401)              3037265   
                                                                 
 batch_normalization_12 (Bat  (None, 2401)             9604      
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2401)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               605304    
                                                                 
 batch_normalization_13 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 252)               0         
                                                                 
 dense_13 (Dense)            (None, 2401)              607453    
                                                                 
 batch_normalization_14 (Bat  (None, 2401)             9604      
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2401)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3036128   
                                                                 
=================================================================
Total params: 7,306,366
Trainable params: 7,296,258
Non-trainable params: 10,108
_________________________________________________________________
Epoch 1/200
1493/1493 - 64s - loss: 0.0101 - val_loss: 0.0057 - 64s/epoch - 43ms/step
Epoch 2/200
1493/1493 - 62s - loss: 0.0037 - val_loss: 0.0036 - 62s/epoch - 42ms/step
Epoch 3/200
1493/1493 - 63s - loss: 0.0025 - val_loss: 0.0023 - 63s/epoch - 42ms/step
Epoch 4/200
1493/1493 - 63s - loss: 0.0021 - val_loss: 0.0041 - 63s/epoch - 42ms/step
Epoch 5/200
1493/1493 - 63s - loss: 0.0019 - val_loss: 0.0017 - 63s/epoch - 42ms/step
Epoch 6/200
1493/1493 - 63s - loss: 0.0017 - val_loss: 0.0017 - 63s/epoch - 42ms/step
Epoch 7/200
1493/1493 - 63s - loss: 0.0017 - val_loss: 0.0013 - 63s/epoch - 42ms/step
Epoch 8/200
1493/1493 - 63s - loss: 0.0014 - val_loss: 0.0016 - 63s/epoch - 42ms/step
Epoch 9/200
1493/1493 - 63s - loss: 0.0013 - val_loss: 0.0017 - 63s/epoch - 42ms/step
Epoch 10/200
1493/1493 - 63s - loss: 0.0013 - val_loss: 0.0010 - 63s/epoch - 42ms/step
Epoch 11/200
1493/1493 - 63s - loss: 0.0012 - val_loss: 0.0011 - 63s/epoch - 42ms/step
Epoch 12/200
1493/1493 - 63s - loss: 9.9626e-04 - val_loss: 0.0012 - 63s/epoch - 42ms/step
Epoch 13/200
1493/1493 - 62s - loss: 9.5001e-04 - val_loss: 0.0017 - 62s/epoch - 42ms/step
Epoch 14/200
1493/1493 - 63s - loss: 9.7544e-04 - val_loss: 7.8322e-04 - 63s/epoch - 42ms/step
Epoch 15/200
1493/1493 - 63s - loss: 8.4180e-04 - val_loss: 8.9594e-04 - 63s/epoch - 42ms/step
Epoch 16/200
1493/1493 - 63s - loss: 7.7038e-04 - val_loss: 6.7276e-04 - 63s/epoch - 42ms/step
Epoch 17/200
1493/1493 - 63s - loss: 7.1571e-04 - val_loss: 0.0010 - 63s/epoch - 42ms/step
Epoch 18/200
1493/1493 - 62s - loss: 6.9871e-04 - val_loss: 8.8110e-04 - 62s/epoch - 42ms/step
Epoch 19/200
1493/1493 - 63s - loss: 6.7223e-04 - val_loss: 8.0054e-04 - 63s/epoch - 42ms/step
Epoch 20/200
1493/1493 - 63s - loss: 6.3232e-04 - val_loss: 5.8780e-04 - 63s/epoch - 42ms/step
Epoch 21/200
1493/1493 - 63s - loss: 5.8984e-04 - val_loss: 6.4932e-04 - 63s/epoch - 42ms/step
Epoch 22/200
1493/1493 - 63s - loss: 5.7430e-04 - val_loss: 0.0012 - 63s/epoch - 42ms/step
Epoch 23/200
1493/1493 - 63s - loss: 6.4862e-04 - val_loss: 7.0567e-04 - 63s/epoch - 42ms/step
Epoch 24/200
1493/1493 - 62s - loss: 5.6072e-04 - val_loss: 5.0695e-04 - 62s/epoch - 42ms/step
Epoch 25/200
1493/1493 - 63s - loss: 5.1600e-04 - val_loss: 4.9604e-04 - 63s/epoch - 42ms/step
Epoch 26/200
1493/1493 - 63s - loss: 4.9486e-04 - val_loss: 5.9364e-04 - 63s/epoch - 42ms/step
Epoch 27/200
1493/1493 - 63s - loss: 4.7884e-04 - val_loss: 4.6885e-04 - 63s/epoch - 42ms/step
Epoch 28/200
1493/1493 - 63s - loss: 4.5765e-04 - val_loss: 6.6260e-04 - 63s/epoch - 42ms/step
Epoch 29/200
1493/1493 - 63s - loss: 4.5686e-04 - val_loss: 5.6165e-04 - 63s/epoch - 42ms/step
Epoch 30/200
1493/1493 - 63s - loss: 4.5147e-04 - val_loss: 4.6012e-04 - 63s/epoch - 42ms/step
Epoch 31/200
1493/1493 - 63s - loss: 4.2642e-04 - val_loss: 6.2921e-04 - 63s/epoch - 42ms/step
Epoch 32/200
1493/1493 - 63s - loss: 4.4423e-04 - val_loss: 4.2815e-04 - 63s/epoch - 42ms/step
Epoch 33/200
1493/1493 - 63s - loss: 4.1568e-04 - val_loss: 4.2238e-04 - 63s/epoch - 42ms/step
Epoch 34/200
1493/1493 - 62s - loss: 4.0024e-04 - val_loss: 3.9039e-04 - 62s/epoch - 42ms/step
Epoch 35/200
1493/1493 - 63s - loss: 3.8665e-04 - val_loss: 6.2969e-04 - 63s/epoch - 42ms/step
Epoch 36/200
1493/1493 - 63s - loss: 4.1454e-04 - val_loss: 4.5177e-04 - 63s/epoch - 42ms/step
Epoch 37/200
1493/1493 - 63s - loss: 3.7981e-04 - val_loss: 4.7391e-04 - 63s/epoch - 42ms/step
Epoch 38/200
1493/1493 - 63s - loss: 3.9376e-04 - val_loss: 4.2465e-04 - 63s/epoch - 42ms/step
Epoch 39/200
1493/1493 - 63s - loss: 3.6571e-04 - val_loss: 3.4851e-04 - 63s/epoch - 42ms/step
Epoch 40/200
1493/1493 - 62s - loss: 3.5470e-04 - val_loss: 4.3145e-04 - 62s/epoch - 42ms/step
Epoch 41/200
1493/1493 - 63s - loss: 3.5409e-04 - val_loss: 3.2714e-04 - 63s/epoch - 42ms/step
Epoch 42/200
1493/1493 - 63s - loss: 3.4866e-04 - val_loss: 3.3840e-04 - 63s/epoch - 42ms/step
Epoch 43/200
1493/1493 - 63s - loss: 3.4582e-04 - val_loss: 4.2908e-04 - 63s/epoch - 42ms/step
Epoch 44/200
1493/1493 - 63s - loss: 3.4915e-04 - val_loss: 9.2809e-04 - 63s/epoch - 42ms/step
Epoch 45/200
1493/1493 - 62s - loss: 3.8726e-04 - val_loss: 2.9850e-04 - 62s/epoch - 42ms/step
Epoch 46/200
1493/1493 - 63s - loss: 3.3004e-04 - val_loss: 3.0721e-04 - 63s/epoch - 42ms/step
Epoch 47/200
1493/1493 - 63s - loss: 3.2681e-04 - val_loss: 3.4107e-04 - 63s/epoch - 42ms/step
Epoch 48/200
1493/1493 - 63s - loss: 3.2406e-04 - val_loss: 3.6391e-04 - 63s/epoch - 42ms/step
Epoch 49/200
1493/1493 - 63s - loss: 3.2194e-04 - val_loss: 0.0011 - 63s/epoch - 42ms/step
Epoch 50/200
1493/1493 - 63s - loss: 4.0991e-04 - val_loss: 0.0010 - 63s/epoch - 42ms/step
Epoch 51/200
1493/1493 - 63s - loss: 3.7286e-04 - val_loss: 3.0319e-04 - 63s/epoch - 42ms/step
Epoch 52/200
1493/1493 - 63s - loss: 3.0990e-04 - val_loss: 3.0861e-04 - 63s/epoch - 42ms/step
Epoch 53/200
1493/1493 - 63s - loss: 3.1230e-04 - val_loss: 2.9235e-04 - 63s/epoch - 42ms/step
Epoch 54/200
1493/1493 - 63s - loss: 3.0098e-04 - val_loss: 3.2084e-04 - 63s/epoch - 42ms/step
Epoch 55/200
1493/1493 - 63s - loss: 2.9547e-04 - val_loss: 2.9875e-04 - 63s/epoch - 42ms/step
Epoch 56/200
1493/1493 - 62s - loss: 2.9405e-04 - val_loss: 3.4041e-04 - 62s/epoch - 42ms/step
Epoch 57/200
1493/1493 - 63s - loss: 2.9120e-04 - val_loss: 3.1334e-04 - 63s/epoch - 42ms/step
Epoch 58/200
1493/1493 - 63s - loss: 2.8908e-04 - val_loss: 2.7662e-04 - 63s/epoch - 42ms/step
Epoch 59/200
1493/1493 - 63s - loss: 2.8431e-04 - val_loss: 2.8555e-04 - 63s/epoch - 42ms/step
Epoch 60/200
1493/1493 - 63s - loss: 2.8474e-04 - val_loss: 5.1162e-04 - 63s/epoch - 42ms/step
Epoch 61/200
1493/1493 - 62s - loss: 3.5065e-04 - val_loss: 2.7387e-04 - 62s/epoch - 42ms/step
Epoch 62/200
1493/1493 - 63s - loss: 2.8578e-04 - val_loss: 2.9972e-04 - 63s/epoch - 42ms/step
Epoch 63/200
1493/1493 - 63s - loss: 2.7664e-04 - val_loss: 3.2324e-04 - 63s/epoch - 42ms/step
Epoch 64/200
1493/1493 - 63s - loss: 2.7860e-04 - val_loss: 0.0015 - 63s/epoch - 42ms/step
Epoch 65/200
1493/1493 - 63s - loss: 3.6676e-04 - val_loss: 9.1873e-04 - 63s/epoch - 42ms/step
Epoch 66/200
1493/1493 - 63s - loss: 3.2639e-04 - val_loss: 2.6972e-04 - 63s/epoch - 42ms/step
Epoch 67/200
1493/1493 - 63s - loss: 2.7362e-04 - val_loss: 2.7230e-04 - 63s/epoch - 42ms/step
Epoch 68/200
1493/1493 - 63s - loss: 2.7129e-04 - val_loss: 5.5524e-04 - 63s/epoch - 42ms/step
Epoch 69/200
1493/1493 - 63s - loss: 3.2727e-04 - val_loss: 2.8072e-04 - 63s/epoch - 42ms/step
Epoch 70/200
1493/1493 - 63s - loss: 2.7096e-04 - val_loss: 3.9126e-04 - 63s/epoch - 42ms/step
Epoch 71/200
1493/1493 - 63s - loss: 2.9529e-04 - val_loss: 2.6254e-04 - 63s/epoch - 42ms/step
Epoch 72/200
1493/1493 - 62s - loss: 2.6922e-04 - val_loss: 2.8481e-04 - 62s/epoch - 42ms/step
Epoch 73/200
1493/1493 - 63s - loss: 2.6809e-04 - val_loss: 2.6462e-04 - 63s/epoch - 42ms/step
Epoch 74/200
1493/1493 - 63s - loss: 2.6329e-04 - val_loss: 2.5807e-04 - 63s/epoch - 42ms/step
Epoch 75/200
1493/1493 - 63s - loss: 2.5569e-04 - val_loss: 2.4225e-04 - 63s/epoch - 42ms/step
Epoch 76/200
1493/1493 - 63s - loss: 2.5221e-04 - val_loss: 2.6816e-04 - 63s/epoch - 42ms/step
Epoch 77/200
1493/1493 - 62s - loss: 2.4999e-04 - val_loss: 4.0343e-04 - 62s/epoch - 42ms/step
Epoch 78/200
1493/1493 - 63s - loss: 2.6137e-04 - val_loss: 2.5518e-04 - 63s/epoch - 42ms/step
Epoch 79/200
1493/1493 - 63s - loss: 2.5534e-04 - val_loss: 5.1568e-04 - 63s/epoch - 42ms/step
Epoch 80/200
1493/1493 - 63s - loss: 3.0265e-04 - val_loss: 2.6570e-04 - 63s/epoch - 42ms/step
Epoch 81/200
1493/1493 - 63s - loss: 2.5838e-04 - val_loss: 3.4474e-04 - 63s/epoch - 42ms/step
Epoch 82/200
1493/1493 - 63s - loss: 2.4786e-04 - val_loss: 2.3816e-04 - 63s/epoch - 42ms/step
Epoch 83/200
1493/1493 - 63s - loss: 2.4308e-04 - val_loss: 2.6388e-04 - 63s/epoch - 42ms/step
Epoch 84/200
1493/1493 - 63s - loss: 2.4355e-04 - val_loss: 2.5297e-04 - 63s/epoch - 42ms/step
Epoch 85/200
1493/1493 - 63s - loss: 2.5489e-04 - val_loss: 2.7771e-04 - 63s/epoch - 42ms/step
Epoch 86/200
1493/1493 - 63s - loss: 2.4810e-04 - val_loss: 2.3908e-04 - 63s/epoch - 42ms/step
Epoch 87/200
1493/1493 - 63s - loss: 2.3726e-04 - val_loss: 2.2406e-04 - 63s/epoch - 42ms/step
Epoch 88/200
1493/1493 - 62s - loss: 2.3550e-04 - val_loss: 3.0303e-04 - 62s/epoch - 42ms/step
Epoch 89/200
1493/1493 - 63s - loss: 2.5190e-04 - val_loss: 2.4941e-04 - 63s/epoch - 42ms/step
Epoch 90/200
1493/1493 - 63s - loss: 2.3325e-04 - val_loss: 2.5094e-04 - 63s/epoch - 42ms/step
Epoch 91/200
1493/1493 - 63s - loss: 2.3119e-04 - val_loss: 2.7570e-04 - 63s/epoch - 42ms/step
Epoch 92/200
1493/1493 - 63s - loss: 2.3390e-04 - val_loss: 6.8460e-04 - 63s/epoch - 42ms/step
Epoch 93/200
1493/1493 - 63s - loss: 2.9335e-04 - val_loss: 2.2525e-04 - 63s/epoch - 42ms/step
Epoch 94/200
1493/1493 - 63s - loss: 2.3219e-04 - val_loss: 2.1001e-04 - 63s/epoch - 42ms/step
Epoch 95/200
1493/1493 - 63s - loss: 2.3029e-04 - val_loss: 2.8032e-04 - 63s/epoch - 42ms/step
Epoch 96/200
1493/1493 - 63s - loss: 2.3145e-04 - val_loss: 2.5199e-04 - 63s/epoch - 42ms/step
Epoch 97/200
1493/1493 - 63s - loss: 2.2731e-04 - val_loss: 2.2495e-04 - 63s/epoch - 42ms/step
Epoch 98/200
1493/1493 - 63s - loss: 2.3269e-04 - val_loss: 4.5181e-04 - 63s/epoch - 42ms/step
Epoch 99/200
1493/1493 - 63s - loss: 2.8532e-04 - val_loss: 2.1316e-04 - 63s/epoch - 42ms/step
Epoch 100/200
1493/1493 - 63s - loss: 2.3540e-04 - val_loss: 2.2771e-04 - 63s/epoch - 42ms/step
Epoch 101/200
1493/1493 - 63s - loss: 2.2571e-04 - val_loss: 2.6178e-04 - 63s/epoch - 42ms/step
Epoch 102/200
1493/1493 - 63s - loss: 2.2904e-04 - val_loss: 2.6919e-04 - 63s/epoch - 42ms/step
Epoch 103/200
1493/1493 - 63s - loss: 2.2929e-04 - val_loss: 2.2051e-04 - 63s/epoch - 42ms/step
Epoch 104/200
1493/1493 - 63s - loss: 2.2229e-04 - val_loss: 4.2302e-04 - 63s/epoch - 42ms/step
Epoch 105/200
1493/1493 - 63s - loss: 2.3993e-04 - val_loss: 2.3831e-04 - 63s/epoch - 42ms/step
Epoch 106/200
1493/1493 - 63s - loss: 2.1948e-04 - val_loss: 2.2505e-04 - 63s/epoch - 42ms/step
Epoch 107/200
1493/1493 - 63s - loss: 2.1744e-04 - val_loss: 2.3502e-04 - 63s/epoch - 42ms/step
Epoch 108/200
1493/1493 - 63s - loss: 2.2384e-04 - val_loss: 2.1851e-04 - 63s/epoch - 42ms/step
Epoch 109/200
1493/1493 - 63s - loss: 2.2236e-04 - val_loss: 2.2916e-04 - 63s/epoch - 42ms/step
Epoch 110/200
1493/1493 - 63s - loss: 2.1575e-04 - val_loss: 2.3188e-04 - 63s/epoch - 42ms/step
Epoch 111/200
1493/1493 - 63s - loss: 2.1415e-04 - val_loss: 2.4734e-04 - 63s/epoch - 42ms/step
Epoch 112/200
1493/1493 - 63s - loss: 2.1869e-04 - val_loss: 2.1481e-04 - 63s/epoch - 42ms/step
Epoch 113/200
1493/1493 - 63s - loss: 2.1535e-04 - val_loss: 5.4583e-04 - 63s/epoch - 42ms/step
Epoch 114/200
1493/1493 - 63s - loss: 2.6404e-04 - val_loss: 2.7160e-04 - 63s/epoch - 42ms/step
Epoch 115/200
1493/1493 - 63s - loss: 2.2572e-04 - val_loss: 2.8933e-04 - 63s/epoch - 42ms/step
Epoch 116/200
1493/1493 - 63s - loss: 2.1462e-04 - val_loss: 2.2193e-04 - 63s/epoch - 42ms/step
Epoch 117/200
1493/1493 - 63s - loss: 2.1068e-04 - val_loss: 2.3419e-04 - 63s/epoch - 42ms/step
Epoch 118/200
1493/1493 - 63s - loss: 2.1354e-04 - val_loss: 2.1296e-04 - 63s/epoch - 42ms/step
Epoch 119/200
1493/1493 - 63s - loss: 2.0971e-04 - val_loss: 2.3801e-04 - 63s/epoch - 42ms/step
Epoch 120/200
1493/1493 - 63s - loss: 2.0831e-04 - val_loss: 2.1779e-04 - 63s/epoch - 42ms/step
Epoch 121/200
1493/1493 - 63s - loss: 2.0953e-04 - val_loss: 2.7928e-04 - 63s/epoch - 42ms/step
Epoch 122/200
1493/1493 - 63s - loss: 2.0681e-04 - val_loss: 2.1016e-04 - 63s/epoch - 42ms/step
Epoch 123/200
1493/1493 - 63s - loss: 2.0531e-04 - val_loss: 2.2514e-04 - 63s/epoch - 42ms/step
Epoch 124/200
1493/1493 - 63s - loss: 2.0804e-04 - val_loss: 4.2746e-04 - 63s/epoch - 42ms/step
Epoch 125/200
1493/1493 - 63s - loss: 2.5059e-04 - val_loss: 1.9943e-04 - 63s/epoch - 42ms/step
Epoch 126/200
1493/1493 - 63s - loss: 2.2624e-04 - val_loss: 7.0855e-04 - 63s/epoch - 42ms/step
Epoch 127/200
1493/1493 - 63s - loss: 2.5378e-04 - val_loss: 2.1503e-04 - 63s/epoch - 42ms/step
Epoch 128/200
1493/1493 - 63s - loss: 2.0913e-04 - val_loss: 1.9365e-04 - 63s/epoch - 42ms/step
Epoch 129/200
1493/1493 - 63s - loss: 2.0590e-04 - val_loss: 2.0077e-04 - 63s/epoch - 42ms/step
Epoch 130/200
1493/1493 - 63s - loss: 2.0438e-04 - val_loss: 2.0166e-04 - 63s/epoch - 42ms/step
Epoch 131/200
1493/1493 - 63s - loss: 2.0564e-04 - val_loss: 3.2437e-04 - 63s/epoch - 42ms/step
Epoch 132/200
1493/1493 - 63s - loss: 2.2967e-04 - val_loss: 2.1382e-04 - 63s/epoch - 42ms/step
Epoch 133/200
1493/1493 - 63s - loss: 2.0350e-04 - val_loss: 1.9934e-04 - 63s/epoch - 42ms/step
Epoch 134/200
1493/1493 - 63s - loss: 2.0014e-04 - val_loss: 2.0831e-04 - 63s/epoch - 42ms/step
Epoch 135/200
1493/1493 - 63s - loss: 1.9901e-04 - val_loss: 1.9839e-04 - 63s/epoch - 42ms/step
Epoch 136/200
1493/1493 - 63s - loss: 2.0266e-04 - val_loss: 3.8328e-04 - 63s/epoch - 42ms/step
Epoch 137/200
1493/1493 - 63s - loss: 2.5393e-04 - val_loss: 3.4729e-04 - 63s/epoch - 42ms/step
Epoch 138/200
1493/1493 - 63s - loss: 2.4899e-04 - val_loss: 3.4550e-04 - 63s/epoch - 42ms/step
Epoch 139/200
1493/1493 - 63s - loss: 2.5422e-04 - val_loss: 4.4102e-04 - 63s/epoch - 42ms/step
Epoch 140/200
1493/1493 - 63s - loss: 2.6461e-04 - val_loss: 1.9139e-04 - 63s/epoch - 42ms/step
Epoch 141/200
1493/1493 - 63s - loss: 2.0915e-04 - val_loss: 2.8916e-04 - 63s/epoch - 42ms/step
Epoch 142/200
1493/1493 - 63s - loss: 2.2339e-04 - val_loss: 2.0422e-04 - 63s/epoch - 42ms/step
Epoch 143/200
1493/1493 - 63s - loss: 2.0151e-04 - val_loss: 2.2708e-04 - 63s/epoch - 42ms/step
Epoch 144/200
1493/1493 - 63s - loss: 2.0577e-04 - val_loss: 2.1973e-04 - 63s/epoch - 42ms/step
Epoch 145/200
1493/1493 - 63s - loss: 2.0164e-04 - val_loss: 2.0391e-04 - 63s/epoch - 42ms/step
Epoch 146/200
1493/1493 - 63s - loss: 1.9843e-04 - val_loss: 2.0202e-04 - 63s/epoch - 42ms/step
Epoch 147/200
1493/1493 - 63s - loss: 1.9563e-04 - val_loss: 2.2958e-04 - 63s/epoch - 42ms/step
Epoch 148/200
1493/1493 - 63s - loss: 1.9525e-04 - val_loss: 2.7462e-04 - 63s/epoch - 42ms/step
Epoch 149/200
1493/1493 - 63s - loss: 1.9328e-04 - val_loss: 2.0363e-04 - 63s/epoch - 42ms/step
Epoch 150/200
1493/1493 - 63s - loss: 1.9259e-04 - val_loss: 1.8746e-04 - 63s/epoch - 42ms/step
Epoch 151/200
1493/1493 - 63s - loss: 1.9714e-04 - val_loss: 2.0289e-04 - 63s/epoch - 42ms/step
Epoch 152/200
1493/1493 - 62s - loss: 1.9506e-04 - val_loss: 3.1707e-04 - 62s/epoch - 42ms/step
Epoch 153/200
1493/1493 - 63s - loss: 2.1019e-04 - val_loss: 1.9742e-04 - 63s/epoch - 42ms/step
Epoch 154/200
1493/1493 - 63s - loss: 1.9542e-04 - val_loss: 2.8284e-04 - 63s/epoch - 42ms/step
Epoch 155/200
1493/1493 - 63s - loss: 2.1938e-04 - val_loss: 1.8867e-04 - 63s/epoch - 42ms/step
Epoch 156/200
1493/1493 - 63s - loss: 1.9445e-04 - val_loss: 1.9240e-04 - 63s/epoch - 42ms/step
Epoch 157/200
1493/1493 - 63s - loss: 1.9021e-04 - val_loss: 2.1451e-04 - 63s/epoch - 42ms/step
Epoch 158/200
1493/1493 - 63s - loss: 1.9207e-04 - val_loss: 2.1051e-04 - 63s/epoch - 42ms/step
Epoch 159/200
1493/1493 - 63s - loss: 1.8932e-04 - val_loss: 1.9018e-04 - 63s/epoch - 42ms/step
Epoch 160/200
1493/1493 - 63s - loss: 1.8883e-04 - val_loss: 2.0578e-04 - 63s/epoch - 42ms/step
Epoch 161/200
1493/1493 - 63s - loss: 2.0063e-04 - val_loss: 1.8751e-04 - 63s/epoch - 42ms/step
Epoch 162/200
1493/1493 - 63s - loss: 1.8822e-04 - val_loss: 1.8719e-04 - 63s/epoch - 42ms/step
Epoch 163/200
1493/1493 - 63s - loss: 1.8855e-04 - val_loss: 2.4856e-04 - 63s/epoch - 42ms/step
Epoch 164/200
1493/1493 - 63s - loss: 2.0352e-04 - val_loss: 1.8254e-04 - 63s/epoch - 42ms/step
Epoch 165/200
1493/1493 - 63s - loss: 1.9847e-04 - val_loss: 1.8262e-04 - 63s/epoch - 42ms/step
Epoch 166/200
1493/1493 - 63s - loss: 1.8773e-04 - val_loss: 2.0475e-04 - 63s/epoch - 42ms/step
Epoch 167/200
1493/1493 - 63s - loss: 1.8695e-04 - val_loss: 1.8462e-04 - 63s/epoch - 42ms/step
Epoch 168/200
1493/1493 - 62s - loss: 1.8629e-04 - val_loss: 1.9582e-04 - 62s/epoch - 42ms/step
Epoch 169/200
1493/1493 - 63s - loss: 1.8572e-04 - val_loss: 2.1941e-04 - 63s/epoch - 42ms/step
Epoch 170/200
1493/1493 - 63s - loss: 1.8608e-04 - val_loss: 4.1485e-04 - 63s/epoch - 42ms/step
Epoch 171/200
1493/1493 - 63s - loss: 2.1726e-04 - val_loss: 4.5765e-04 - 63s/epoch - 42ms/step
Epoch 172/200
1493/1493 - 63s - loss: 2.3923e-04 - val_loss: 2.2382e-04 - 63s/epoch - 42ms/step
Epoch 173/200
1493/1493 - 62s - loss: 1.9472e-04 - val_loss: 1.7335e-04 - 62s/epoch - 42ms/step
Epoch 174/200
1493/1493 - 63s - loss: 1.8595e-04 - val_loss: 1.7878e-04 - 63s/epoch - 42ms/step
Epoch 175/200
1493/1493 - 63s - loss: 1.8727e-04 - val_loss: 1.8986e-04 - 63s/epoch - 42ms/step
Epoch 176/200
1493/1493 - 63s - loss: 1.8720e-04 - val_loss: 1.9616e-04 - 63s/epoch - 42ms/step
Epoch 177/200
1493/1493 - 63s - loss: 1.9168e-04 - val_loss: 1.7418e-04 - 63s/epoch - 42ms/step
Epoch 178/200
1493/1493 - 63s - loss: 1.8386e-04 - val_loss: 1.8790e-04 - 63s/epoch - 42ms/step
Epoch 179/200
1493/1493 - 63s - loss: 1.8239e-04 - val_loss: 1.9032e-04 - 63s/epoch - 42ms/step
Epoch 180/200
1493/1493 - 63s - loss: 1.8328e-04 - val_loss: 2.1404e-04 - 63s/epoch - 42ms/step
Epoch 181/200
1493/1493 - 63s - loss: 1.8458e-04 - val_loss: 2.0459e-04 - 63s/epoch - 42ms/step
Epoch 182/200
1493/1493 - 63s - loss: 1.8179e-04 - val_loss: 2.0087e-04 - 63s/epoch - 42ms/step
Epoch 183/200
1493/1493 - 63s - loss: 1.8286e-04 - val_loss: 2.2221e-04 - 63s/epoch - 42ms/step
Epoch 184/200
1493/1493 - 62s - loss: 1.8342e-04 - val_loss: 1.8371e-04 - 62s/epoch - 42ms/step
Epoch 185/200
1493/1493 - 63s - loss: 1.7813e-04 - val_loss: 2.1502e-04 - 63s/epoch - 42ms/step
Epoch 186/200
1493/1493 - 63s - loss: 1.8009e-04 - val_loss: 1.8552e-04 - 63s/epoch - 42ms/step
Epoch 187/200
1493/1493 - 63s - loss: 1.7859e-04 - val_loss: 1.8925e-04 - 63s/epoch - 42ms/step
Epoch 188/200
1493/1493 - 63s - loss: 1.8444e-04 - val_loss: 2.9831e-04 - 63s/epoch - 42ms/step
Epoch 189/200
1493/1493 - 63s - loss: 1.8993e-04 - val_loss: 1.9167e-04 - 63s/epoch - 42ms/step
Epoch 190/200
1493/1493 - 63s - loss: 1.8281e-04 - val_loss: 2.3768e-04 - 63s/epoch - 42ms/step
Epoch 191/200
1493/1493 - 63s - loss: 1.8957e-04 - val_loss: 1.9794e-04 - 63s/epoch - 42ms/step
Epoch 192/200
1493/1493 - 63s - loss: 1.8068e-04 - val_loss: 2.0039e-04 - 63s/epoch - 42ms/step
Epoch 193/200
1493/1493 - 63s - loss: 1.8377e-04 - val_loss: 3.2077e-04 - 63s/epoch - 42ms/step
Epoch 194/200
1493/1493 - 62s - loss: 2.0926e-04 - val_loss: 4.4711e-04 - 62s/epoch - 42ms/step
Epoch 195/200
1493/1493 - 63s - loss: 2.4288e-04 - val_loss: 3.1485e-04 - 63s/epoch - 42ms/step
Epoch 196/200
1493/1493 - 63s - loss: 2.0143e-04 - val_loss: 1.6417e-04 - 63s/epoch - 42ms/step
Epoch 197/200
1493/1493 - 63s - loss: 1.8218e-04 - val_loss: 2.6219e-04 - 63s/epoch - 42ms/step
Epoch 198/200
1493/1493 - 63s - loss: 1.9078e-04 - val_loss: 2.1916e-04 - 63s/epoch - 42ms/step
Epoch 199/200
1493/1493 - 63s - loss: 1.8481e-04 - val_loss: 1.7432e-04 - 63s/epoch - 42ms/step
Epoch 200/200
1493/1493 - 63s - loss: 1.7894e-04 - val_loss: 1.8844e-04 - 63s/epoch - 42ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00018844372243620455
  1/332 [..............................] - ETA: 30s 10/332 [..............................] - ETA: 2s  19/332 [>.............................] - ETA: 1s 28/332 [=>............................] - ETA: 1s 37/332 [==>...........................] - ETA: 1s 46/332 [===>..........................] - ETA: 1s 55/332 [===>..........................] - ETA: 1s 64/332 [====>.........................] - ETA: 1s 73/332 [=====>........................] - ETA: 1s 82/332 [======>.......................] - ETA: 1s 91/332 [=======>......................] - ETA: 1s100/332 [========>.....................] - ETA: 1s109/332 [========>.....................] - ETA: 1s118/332 [=========>....................] - ETA: 1s127/332 [==========>...................] - ETA: 1s136/332 [===========>..................] - ETA: 1s145/332 [============>.................] - ETA: 1s153/332 [============>.................] - ETA: 1s162/332 [=============>................] - ETA: 1s171/332 [==============>...............] - ETA: 0s180/332 [===============>..............] - ETA: 0s189/332 [================>.............] - ETA: 0s198/332 [================>.............] - ETA: 0s207/332 [=================>............] - ETA: 0s216/332 [==================>...........] - ETA: 0s225/332 [===================>..........] - ETA: 0s234/332 [====================>.........] - ETA: 0s243/332 [====================>.........] - ETA: 0s252/332 [=====================>........] - ETA: 0s261/332 [======================>.......] - ETA: 0s270/332 [=======================>......] - ETA: 0s279/332 [========================>.....] - ETA: 0s288/332 [=========================>....] - ETA: 0s297/332 [=========================>....] - ETA: 0s306/332 [==========================>...] - ETA: 0s315/332 [===========================>..] - ETA: 0s324/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 6ms/step
correlation 0.002163571229396206
cosine 0.0017041182949369554
MAE: 0.0074844165
RMSE: 0.013727473
r2: 0.9877758070851974
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 2401)              3037265   
                                                                 
 batch_normalization_12 (Bat  (None, 2401)             9604      
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2401)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               605304    
                                                                 
 batch_normalization_13 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 252)               0         
                                                                 
 dense_13 (Dense)            (None, 2401)              607453    
                                                                 
 batch_normalization_14 (Bat  (None, 2401)             9604      
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2401)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3036128   
                                                                 
=================================================================
Total params: 7,306,366
Trainable params: 7,296,258
Non-trainable params: 10,108
_________________________________________________________________
Encoder
Model: "model_13"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_14 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 2401)              3037265   
                                                                 
 batch_normalization_12 (Bat  (None, 2401)             9604      
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2401)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               605304    
                                                                 
=================================================================
Total params: 3,652,173
Trainable params: 3,647,371
Non-trainable params: 4,802
_________________________________________________________________
Decoder
Model: "model_14"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_15 (InputLayer)       [(None, 252)]             0         
                                                                 
 batch_normalization_13 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 252)               0         
                                                                 
 dense_13 (Dense)            (None, 2401)              607453    
                                                                 
 batch_normalization_14 (Bat  (None, 2401)             9604      
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2401)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3036128   
                                                                 
=================================================================
Total params: 3,654,193
Trainable params: 3,648,887
Non-trainable params: 5,306
_________________________________________________________________
['1.9custom_n_b', 'mse', 64, 200, 0.0005, 0.2, 252, 0.00017894463962875307, 0.00018844372243620455, 0.002163571229396206, 0.0017041182949369554, 0.007484416477382183, 0.013727473095059395, 0.9877758070851974, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               637308    
                                                                 
 batch_normalization_16 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 252)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              639584    
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 7,692,700
Trainable params: 7,682,084
Non-trainable params: 10,616
_________________________________________________________________
Epoch 1/200
1493/1493 - 65s - loss: 0.0103 - val_loss: 0.0048 - 65s/epoch - 43ms/step
Epoch 2/200
1493/1493 - 64s - loss: 0.0035 - val_loss: 0.0034 - 64s/epoch - 43ms/step
Epoch 3/200
1493/1493 - 64s - loss: 0.0025 - val_loss: 0.0020 - 64s/epoch - 43ms/step
Epoch 4/200
1493/1493 - 64s - loss: 0.0021 - val_loss: 0.0032 - 64s/epoch - 43ms/step
Epoch 5/200
1493/1493 - 64s - loss: 0.0019 - val_loss: 0.0017 - 64s/epoch - 43ms/step
Epoch 6/200
1493/1493 - 64s - loss: 0.0017 - val_loss: 0.0017 - 64s/epoch - 43ms/step
Epoch 7/200
1493/1493 - 64s - loss: 0.0017 - val_loss: 0.0013 - 64s/epoch - 43ms/step
Epoch 8/200
1493/1493 - 64s - loss: 0.0014 - val_loss: 0.0017 - 64s/epoch - 43ms/step
Epoch 9/200
1493/1493 - 64s - loss: 0.0013 - val_loss: 0.0012 - 64s/epoch - 43ms/step
Epoch 10/200
1493/1493 - 64s - loss: 0.0012 - val_loss: 0.0010 - 64s/epoch - 43ms/step
Epoch 11/200
1493/1493 - 64s - loss: 0.0011 - val_loss: 0.0011 - 64s/epoch - 43ms/step
Epoch 12/200
1493/1493 - 64s - loss: 9.6892e-04 - val_loss: 0.0012 - 64s/epoch - 43ms/step
Epoch 13/200
1493/1493 - 64s - loss: 9.3507e-04 - val_loss: 0.0019 - 64s/epoch - 43ms/step
Epoch 14/200
1493/1493 - 64s - loss: 9.5290e-04 - val_loss: 0.0012 - 64s/epoch - 43ms/step
Epoch 15/200
1493/1493 - 64s - loss: 8.6919e-04 - val_loss: 7.4253e-04 - 64s/epoch - 43ms/step
Epoch 16/200
1493/1493 - 64s - loss: 7.4867e-04 - val_loss: 6.8483e-04 - 64s/epoch - 43ms/step
Epoch 17/200
1493/1493 - 64s - loss: 7.0837e-04 - val_loss: 0.0011 - 64s/epoch - 43ms/step
Epoch 18/200
1493/1493 - 64s - loss: 7.0087e-04 - val_loss: 0.0012 - 64s/epoch - 43ms/step
Epoch 19/200
1493/1493 - 64s - loss: 6.8808e-04 - val_loss: 7.1283e-04 - 64s/epoch - 43ms/step
Epoch 20/200
1493/1493 - 64s - loss: 6.1966e-04 - val_loss: 5.7052e-04 - 64s/epoch - 43ms/step
Epoch 21/200
1493/1493 - 64s - loss: 5.9250e-04 - val_loss: 6.5826e-04 - 64s/epoch - 43ms/step
Epoch 22/200
1493/1493 - 64s - loss: 5.7327e-04 - val_loss: 8.8303e-04 - 64s/epoch - 43ms/step
Epoch 23/200
1493/1493 - 64s - loss: 6.2935e-04 - val_loss: 0.0011 - 64s/epoch - 43ms/step
Epoch 24/200
1493/1493 - 64s - loss: 6.1327e-04 - val_loss: 5.2270e-04 - 64s/epoch - 43ms/step
Epoch 25/200
1493/1493 - 64s - loss: 5.2067e-04 - val_loss: 4.8435e-04 - 64s/epoch - 43ms/step
Epoch 26/200
1493/1493 - 64s - loss: 5.0009e-04 - val_loss: 7.3750e-04 - 64s/epoch - 43ms/step
Epoch 27/200
1493/1493 - 64s - loss: 4.8475e-04 - val_loss: 4.6582e-04 - 64s/epoch - 43ms/step
Epoch 28/200
1493/1493 - 64s - loss: 4.6070e-04 - val_loss: 7.0076e-04 - 64s/epoch - 43ms/step
Epoch 29/200
1493/1493 - 64s - loss: 4.5883e-04 - val_loss: 5.1512e-04 - 64s/epoch - 43ms/step
Epoch 30/200
1493/1493 - 64s - loss: 4.5165e-04 - val_loss: 4.7928e-04 - 64s/epoch - 43ms/step
Epoch 31/200
1493/1493 - 64s - loss: 4.3085e-04 - val_loss: 5.4281e-04 - 64s/epoch - 43ms/step
Epoch 32/200
1493/1493 - 64s - loss: 4.4154e-04 - val_loss: 4.0061e-04 - 64s/epoch - 43ms/step
Epoch 33/200
1493/1493 - 64s - loss: 4.1270e-04 - val_loss: 4.1293e-04 - 64s/epoch - 43ms/step
Epoch 34/200
1493/1493 - 64s - loss: 4.0486e-04 - val_loss: 3.8030e-04 - 64s/epoch - 43ms/step
Epoch 35/200
1493/1493 - 64s - loss: 3.9072e-04 - val_loss: 6.9957e-04 - 64s/epoch - 43ms/step
Epoch 36/200
1493/1493 - 64s - loss: 4.2314e-04 - val_loss: 4.2891e-04 - 64s/epoch - 43ms/step
Epoch 37/200
1493/1493 - 64s - loss: 3.8105e-04 - val_loss: 3.9944e-04 - 64s/epoch - 43ms/step
Epoch 38/200
1493/1493 - 64s - loss: 3.8097e-04 - val_loss: 3.7304e-04 - 64s/epoch - 43ms/step
Epoch 39/200
1493/1493 - 64s - loss: 3.6664e-04 - val_loss: 3.5529e-04 - 64s/epoch - 43ms/step
Epoch 40/200
1493/1493 - 64s - loss: 3.5513e-04 - val_loss: 4.1804e-04 - 64s/epoch - 43ms/step
Epoch 41/200
1493/1493 - 64s - loss: 3.5461e-04 - val_loss: 3.2684e-04 - 64s/epoch - 43ms/step
Epoch 42/200
1493/1493 - 64s - loss: 3.5004e-04 - val_loss: 3.3004e-04 - 64s/epoch - 43ms/step
Epoch 43/200
1493/1493 - 64s - loss: 3.5022e-04 - val_loss: 5.2577e-04 - 64s/epoch - 43ms/step
Epoch 44/200
1493/1493 - 64s - loss: 3.6641e-04 - val_loss: 6.1531e-04 - 64s/epoch - 43ms/step
Epoch 45/200
1493/1493 - 64s - loss: 3.5906e-04 - val_loss: 3.0867e-04 - 64s/epoch - 43ms/step
Epoch 46/200
1493/1493 - 64s - loss: 3.3246e-04 - val_loss: 3.0864e-04 - 64s/epoch - 43ms/step
Epoch 47/200
1493/1493 - 63s - loss: 3.3037e-04 - val_loss: 3.5082e-04 - 63s/epoch - 42ms/step
Epoch 48/200
1493/1493 - 64s - loss: 3.2651e-04 - val_loss: 4.0122e-04 - 64s/epoch - 43ms/step
Epoch 49/200
1493/1493 - 64s - loss: 3.3063e-04 - val_loss: 4.6403e-04 - 64s/epoch - 43ms/step
Epoch 50/200
1493/1493 - 64s - loss: 3.3219e-04 - val_loss: 4.0928e-04 - 64s/epoch - 43ms/step
Epoch 51/200
1493/1493 - 64s - loss: 3.2553e-04 - val_loss: 3.0155e-04 - 64s/epoch - 43ms/step
Epoch 52/200
1493/1493 - 64s - loss: 3.0697e-04 - val_loss: 3.1024e-04 - 64s/epoch - 43ms/step
Epoch 53/200
1493/1493 - 64s - loss: 3.0923e-04 - val_loss: 2.8081e-04 - 64s/epoch - 43ms/step
Epoch 54/200
1493/1493 - 64s - loss: 3.0505e-04 - val_loss: 3.2203e-04 - 64s/epoch - 43ms/step
Epoch 55/200
1493/1493 - 64s - loss: 2.9461e-04 - val_loss: 2.8958e-04 - 64s/epoch - 43ms/step
Epoch 56/200
1493/1493 - 64s - loss: 2.9379e-04 - val_loss: 3.5071e-04 - 64s/epoch - 43ms/step
Epoch 57/200
1493/1493 - 64s - loss: 3.0248e-04 - val_loss: 2.8715e-04 - 64s/epoch - 43ms/step
Epoch 58/200
1493/1493 - 64s - loss: 2.9014e-04 - val_loss: 2.7173e-04 - 64s/epoch - 43ms/step
Epoch 59/200
1493/1493 - 64s - loss: 2.8488e-04 - val_loss: 2.9199e-04 - 64s/epoch - 43ms/step
Epoch 60/200
1493/1493 - 64s - loss: 2.8335e-04 - val_loss: 4.0233e-04 - 64s/epoch - 43ms/step
Epoch 61/200
1493/1493 - 64s - loss: 3.0035e-04 - val_loss: 2.8509e-04 - 64s/epoch - 43ms/step
Epoch 62/200
1493/1493 - 64s - loss: 2.8509e-04 - val_loss: 3.0074e-04 - 64s/epoch - 43ms/step
Epoch 63/200
1493/1493 - 64s - loss: 2.7504e-04 - val_loss: 3.1105e-04 - 64s/epoch - 43ms/step
Epoch 64/200
1493/1493 - 64s - loss: 2.7882e-04 - val_loss: 0.0011 - 64s/epoch - 43ms/step
Epoch 65/200
1493/1493 - 64s - loss: 3.6152e-04 - val_loss: 7.9966e-04 - 64s/epoch - 43ms/step
Epoch 66/200
1493/1493 - 64s - loss: 3.2932e-04 - val_loss: 2.7668e-04 - 64s/epoch - 43ms/step
Epoch 67/200
1493/1493 - 64s - loss: 2.7375e-04 - val_loss: 2.6000e-04 - 64s/epoch - 43ms/step
Epoch 68/200
1493/1493 - 64s - loss: 2.7398e-04 - val_loss: 6.5368e-04 - 64s/epoch - 43ms/step
Epoch 69/200
1493/1493 - 64s - loss: 3.7167e-04 - val_loss: 2.7135e-04 - 64s/epoch - 43ms/step
Epoch 70/200
1493/1493 - 64s - loss: 2.7357e-04 - val_loss: 4.7220e-04 - 64s/epoch - 43ms/step
Epoch 71/200
1493/1493 - 64s - loss: 2.9009e-04 - val_loss: 2.7255e-04 - 64s/epoch - 43ms/step
Epoch 72/200
1493/1493 - 64s - loss: 2.7024e-04 - val_loss: 2.7349e-04 - 64s/epoch - 43ms/step
Epoch 73/200
1493/1493 - 64s - loss: 2.6804e-04 - val_loss: 2.8143e-04 - 64s/epoch - 43ms/step
Epoch 74/200
1493/1493 - 64s - loss: 2.6500e-04 - val_loss: 2.7316e-04 - 64s/epoch - 43ms/step
Epoch 75/200
1493/1493 - 64s - loss: 2.5737e-04 - val_loss: 2.5610e-04 - 64s/epoch - 43ms/step
Epoch 76/200
1493/1493 - 64s - loss: 2.5496e-04 - val_loss: 2.6460e-04 - 64s/epoch - 43ms/step
Epoch 77/200
1493/1493 - 64s - loss: 2.5072e-04 - val_loss: 3.5116e-04 - 64s/epoch - 43ms/step
Epoch 78/200
1493/1493 - 64s - loss: 2.6151e-04 - val_loss: 4.0164e-04 - 64s/epoch - 43ms/step
Epoch 79/200
1493/1493 - 64s - loss: 2.7492e-04 - val_loss: 3.7075e-04 - 64s/epoch - 43ms/step
Epoch 80/200
1493/1493 - 64s - loss: 2.6841e-04 - val_loss: 2.6941e-04 - 64s/epoch - 43ms/step
Epoch 81/200
1493/1493 - 64s - loss: 2.5558e-04 - val_loss: 3.9519e-04 - 64s/epoch - 43ms/step
Epoch 82/200
1493/1493 - 64s - loss: 2.4943e-04 - val_loss: 2.5068e-04 - 64s/epoch - 43ms/step
Epoch 83/200
1493/1493 - 64s - loss: 2.4369e-04 - val_loss: 2.5490e-04 - 64s/epoch - 43ms/step
Epoch 84/200
1493/1493 - 64s - loss: 2.4431e-04 - val_loss: 2.5659e-04 - 64s/epoch - 43ms/step
Epoch 85/200
1493/1493 - 64s - loss: 2.4729e-04 - val_loss: 2.7220e-04 - 64s/epoch - 43ms/step
Epoch 86/200
1493/1493 - 64s - loss: 2.4646e-04 - val_loss: 2.4960e-04 - 64s/epoch - 43ms/step
Epoch 87/200
1493/1493 - 64s - loss: 2.4004e-04 - val_loss: 2.5857e-04 - 64s/epoch - 43ms/step
Epoch 88/200
1493/1493 - 64s - loss: 2.3968e-04 - val_loss: 2.9736e-04 - 64s/epoch - 43ms/step
Epoch 89/200
1493/1493 - 64s - loss: 2.6082e-04 - val_loss: 2.4454e-04 - 64s/epoch - 43ms/step
Epoch 90/200
1493/1493 - 64s - loss: 2.3591e-04 - val_loss: 2.5226e-04 - 64s/epoch - 43ms/step
Epoch 91/200
1493/1493 - 64s - loss: 2.3312e-04 - val_loss: 2.5934e-04 - 64s/epoch - 43ms/step
Epoch 92/200
1493/1493 - 64s - loss: 2.3730e-04 - val_loss: 7.5047e-04 - 64s/epoch - 43ms/step
Epoch 93/200
1493/1493 - 64s - loss: 3.0422e-04 - val_loss: 2.6288e-04 - 64s/epoch - 43ms/step
Epoch 94/200
1493/1493 - 64s - loss: 2.4037e-04 - val_loss: 2.1374e-04 - 64s/epoch - 43ms/step
Epoch 95/200
1493/1493 - 64s - loss: 2.3350e-04 - val_loss: 2.5013e-04 - 64s/epoch - 43ms/step
Epoch 96/200
1493/1493 - 64s - loss: 2.3773e-04 - val_loss: 3.5597e-04 - 64s/epoch - 43ms/step
Epoch 97/200
1493/1493 - 64s - loss: 2.6513e-04 - val_loss: 2.3446e-04 - 64s/epoch - 43ms/step
Epoch 98/200
1493/1493 - 64s - loss: 2.3653e-04 - val_loss: 3.8065e-04 - 64s/epoch - 43ms/step
Epoch 99/200
1493/1493 - 64s - loss: 2.7055e-04 - val_loss: 2.1678e-04 - 64s/epoch - 43ms/step
Epoch 100/200
1493/1493 - 64s - loss: 2.3899e-04 - val_loss: 2.3094e-04 - 64s/epoch - 43ms/step
Epoch 101/200
1493/1493 - 64s - loss: 2.2866e-04 - val_loss: 2.5393e-04 - 64s/epoch - 43ms/step
Epoch 102/200
1493/1493 - 64s - loss: 2.3411e-04 - val_loss: 2.6967e-04 - 64s/epoch - 43ms/step
Epoch 103/200
1493/1493 - 64s - loss: 2.3276e-04 - val_loss: 2.3297e-04 - 64s/epoch - 43ms/step
Epoch 104/200
1493/1493 - 64s - loss: 2.2516e-04 - val_loss: 4.0732e-04 - 64s/epoch - 43ms/step
Epoch 105/200
1493/1493 - 64s - loss: 2.4136e-04 - val_loss: 2.2110e-04 - 64s/epoch - 43ms/step
Epoch 106/200
1493/1493 - 64s - loss: 2.2234e-04 - val_loss: 2.3158e-04 - 64s/epoch - 43ms/step
Epoch 107/200
1493/1493 - 64s - loss: 2.2179e-04 - val_loss: 2.5054e-04 - 64s/epoch - 43ms/step
Epoch 108/200
1493/1493 - 64s - loss: 2.3541e-04 - val_loss: 2.1962e-04 - 64s/epoch - 43ms/step
Epoch 109/200
1493/1493 - 64s - loss: 2.2136e-04 - val_loss: 2.4746e-04 - 64s/epoch - 43ms/step
Epoch 110/200
1493/1493 - 64s - loss: 2.1848e-04 - val_loss: 2.3305e-04 - 64s/epoch - 43ms/step
Epoch 111/200
1493/1493 - 64s - loss: 2.1816e-04 - val_loss: 2.8387e-04 - 64s/epoch - 43ms/step
Epoch 112/200
1493/1493 - 64s - loss: 2.2881e-04 - val_loss: 2.1702e-04 - 64s/epoch - 43ms/step
Epoch 113/200
1493/1493 - 64s - loss: 2.1745e-04 - val_loss: 4.5471e-04 - 64s/epoch - 43ms/step
Epoch 114/200
1493/1493 - 64s - loss: 2.5661e-04 - val_loss: 2.5416e-04 - 64s/epoch - 43ms/step
Epoch 115/200
1493/1493 - 64s - loss: 2.4745e-04 - val_loss: 2.4986e-04 - 64s/epoch - 43ms/step
Epoch 116/200
1493/1493 - 64s - loss: 2.1887e-04 - val_loss: 2.3326e-04 - 64s/epoch - 43ms/step
Epoch 117/200
1493/1493 - 64s - loss: 2.1584e-04 - val_loss: 2.6888e-04 - 64s/epoch - 43ms/step
Epoch 118/200
1493/1493 - 64s - loss: 2.2070e-04 - val_loss: 2.1960e-04 - 64s/epoch - 43ms/step
Epoch 119/200
1493/1493 - 64s - loss: 2.1433e-04 - val_loss: 2.3215e-04 - 64s/epoch - 43ms/step
Epoch 120/200
1493/1493 - 64s - loss: 2.1287e-04 - val_loss: 2.1870e-04 - 64s/epoch - 43ms/step
Epoch 121/200
1493/1493 - 64s - loss: 2.1307e-04 - val_loss: 3.1376e-04 - 64s/epoch - 43ms/step
Epoch 122/200
1493/1493 - 64s - loss: 2.1547e-04 - val_loss: 2.0892e-04 - 64s/epoch - 43ms/step
Epoch 123/200
1493/1493 - 64s - loss: 2.1097e-04 - val_loss: 2.2246e-04 - 64s/epoch - 43ms/step
Epoch 124/200
1493/1493 - 64s - loss: 2.1140e-04 - val_loss: 4.5830e-04 - 64s/epoch - 43ms/step
Epoch 125/200
1493/1493 - 64s - loss: 2.4157e-04 - val_loss: 2.6655e-04 - 64s/epoch - 43ms/step
Epoch 126/200
1493/1493 - 64s - loss: 2.2313e-04 - val_loss: 2.6993e-04 - 64s/epoch - 43ms/step
Epoch 127/200
1493/1493 - 64s - loss: 2.1725e-04 - val_loss: 3.0998e-04 - 64s/epoch - 43ms/step
Epoch 128/200
1493/1493 - 64s - loss: 2.1776e-04 - val_loss: 2.0167e-04 - 64s/epoch - 43ms/step
Epoch 129/200
1493/1493 - 64s - loss: 2.0741e-04 - val_loss: 2.2172e-04 - 64s/epoch - 43ms/step
Epoch 130/200
1493/1493 - 64s - loss: 2.0692e-04 - val_loss: 2.0444e-04 - 64s/epoch - 43ms/step
Epoch 131/200
1493/1493 - 64s - loss: 2.1077e-04 - val_loss: 3.8528e-04 - 64s/epoch - 43ms/step
Epoch 132/200
1493/1493 - 64s - loss: 2.4023e-04 - val_loss: 2.0147e-04 - 64s/epoch - 43ms/step
Epoch 133/200
1493/1493 - 64s - loss: 2.0743e-04 - val_loss: 2.1572e-04 - 64s/epoch - 43ms/step
Epoch 134/200
1493/1493 - 64s - loss: 2.0465e-04 - val_loss: 2.0279e-04 - 64s/epoch - 43ms/step
Epoch 135/200
1493/1493 - 64s - loss: 2.0224e-04 - val_loss: 1.9711e-04 - 64s/epoch - 43ms/step
Epoch 136/200
1493/1493 - 64s - loss: 2.0413e-04 - val_loss: 3.0413e-04 - 64s/epoch - 43ms/step
Epoch 137/200
1493/1493 - 64s - loss: 2.3817e-04 - val_loss: 3.1472e-04 - 64s/epoch - 43ms/step
Epoch 138/200
1493/1493 - 64s - loss: 2.3499e-04 - val_loss: 2.4378e-04 - 64s/epoch - 43ms/step
Epoch 139/200
1493/1493 - 64s - loss: 2.1726e-04 - val_loss: 2.3413e-04 - 64s/epoch - 43ms/step
Epoch 140/200
1493/1493 - 64s - loss: 2.0963e-04 - val_loss: 2.0314e-04 - 64s/epoch - 43ms/step
Epoch 141/200
1493/1493 - 64s - loss: 2.0389e-04 - val_loss: 2.6733e-04 - 64s/epoch - 43ms/step
Epoch 142/200
1493/1493 - 64s - loss: 2.1496e-04 - val_loss: 2.0960e-04 - 64s/epoch - 43ms/step
Epoch 143/200
1493/1493 - 64s - loss: 2.0028e-04 - val_loss: 2.3475e-04 - 64s/epoch - 43ms/step
Epoch 144/200
1493/1493 - 64s - loss: 2.0289e-04 - val_loss: 2.3407e-04 - 64s/epoch - 43ms/step
Epoch 145/200
1493/1493 - 64s - loss: 2.0240e-04 - val_loss: 2.0517e-04 - 64s/epoch - 43ms/step
Epoch 146/200
1493/1493 - 64s - loss: 1.9958e-04 - val_loss: 2.0550e-04 - 64s/epoch - 43ms/step
Epoch 147/200
1493/1493 - 64s - loss: 1.9644e-04 - val_loss: 2.0493e-04 - 64s/epoch - 43ms/step
Epoch 148/200
1493/1493 - 64s - loss: 1.9656e-04 - val_loss: 2.5483e-04 - 64s/epoch - 43ms/step
Epoch 149/200
1493/1493 - 64s - loss: 1.9525e-04 - val_loss: 1.9975e-04 - 64s/epoch - 43ms/step
Epoch 150/200
1493/1493 - 64s - loss: 1.9454e-04 - val_loss: 2.0615e-04 - 64s/epoch - 43ms/step
Epoch 151/200
1493/1493 - 64s - loss: 1.9673e-04 - val_loss: 1.9978e-04 - 64s/epoch - 43ms/step
Epoch 152/200
1493/1493 - 64s - loss: 1.9618e-04 - val_loss: 2.7976e-04 - 64s/epoch - 43ms/step
Epoch 153/200
1493/1493 - 64s - loss: 2.1012e-04 - val_loss: 1.9927e-04 - 64s/epoch - 43ms/step
Epoch 154/200
1493/1493 - 64s - loss: 1.9881e-04 - val_loss: 3.5668e-04 - 64s/epoch - 43ms/step
Epoch 155/200
1493/1493 - 64s - loss: 2.4208e-04 - val_loss: 2.0024e-04 - 64s/epoch - 43ms/step
Epoch 156/200
1493/1493 - 64s - loss: 1.9896e-04 - val_loss: 2.0320e-04 - 64s/epoch - 43ms/step
Epoch 157/200
1493/1493 - 64s - loss: 1.9399e-04 - val_loss: 2.4364e-04 - 64s/epoch - 43ms/step
Epoch 158/200
1493/1493 - 64s - loss: 2.0453e-04 - val_loss: 1.9734e-04 - 64s/epoch - 43ms/step
Epoch 159/200
1493/1493 - 64s - loss: 1.9316e-04 - val_loss: 1.9370e-04 - 64s/epoch - 43ms/step
Epoch 160/200
1493/1493 - 64s - loss: 1.9254e-04 - val_loss: 2.1400e-04 - 64s/epoch - 43ms/step
Epoch 161/200
1493/1493 - 64s - loss: 1.9864e-04 - val_loss: 1.9952e-04 - 64s/epoch - 43ms/step
Epoch 162/200
1493/1493 - 64s - loss: 1.9080e-04 - val_loss: 1.9791e-04 - 64s/epoch - 43ms/step
Epoch 163/200
1493/1493 - 64s - loss: 1.9313e-04 - val_loss: 3.3176e-04 - 64s/epoch - 43ms/step
Epoch 164/200
1493/1493 - 64s - loss: 2.2249e-04 - val_loss: 2.0140e-04 - 64s/epoch - 43ms/step
Epoch 165/200
1493/1493 - 64s - loss: 2.0642e-04 - val_loss: 1.8527e-04 - 64s/epoch - 43ms/step
Epoch 166/200
1493/1493 - 64s - loss: 1.9203e-04 - val_loss: 1.9593e-04 - 64s/epoch - 43ms/step
Epoch 167/200
1493/1493 - 64s - loss: 1.9074e-04 - val_loss: 1.9922e-04 - 64s/epoch - 43ms/step
Epoch 168/200
1493/1493 - 64s - loss: 1.8931e-04 - val_loss: 1.9889e-04 - 64s/epoch - 43ms/step
Epoch 169/200
1493/1493 - 64s - loss: 1.8914e-04 - val_loss: 2.3446e-04 - 64s/epoch - 43ms/step
Epoch 170/200
1493/1493 - 64s - loss: 1.9146e-04 - val_loss: 3.0485e-04 - 64s/epoch - 43ms/step
Epoch 171/200
1493/1493 - 64s - loss: 2.3924e-04 - val_loss: 4.2350e-04 - 64s/epoch - 43ms/step
Epoch 172/200
1493/1493 - 64s - loss: 2.3739e-04 - val_loss: 1.8534e-04 - 64s/epoch - 43ms/step
Epoch 173/200
1493/1493 - 64s - loss: 1.9476e-04 - val_loss: 1.8410e-04 - 64s/epoch - 43ms/step
Epoch 174/200
1493/1493 - 64s - loss: 1.8953e-04 - val_loss: 2.1507e-04 - 64s/epoch - 43ms/step
Epoch 175/200
1493/1493 - 64s - loss: 1.9034e-04 - val_loss: 1.8203e-04 - 64s/epoch - 43ms/step
Epoch 176/200
1493/1493 - 64s - loss: 1.9077e-04 - val_loss: 1.9817e-04 - 64s/epoch - 43ms/step
Epoch 177/200
1493/1493 - 64s - loss: 1.9232e-04 - val_loss: 1.8289e-04 - 64s/epoch - 43ms/step
Epoch 178/200
1493/1493 - 64s - loss: 1.8648e-04 - val_loss: 1.9270e-04 - 64s/epoch - 43ms/step
Epoch 179/200
1493/1493 - 64s - loss: 1.8713e-04 - val_loss: 1.9088e-04 - 64s/epoch - 43ms/step
Epoch 180/200
1493/1493 - 64s - loss: 1.8968e-04 - val_loss: 3.0256e-04 - 64s/epoch - 43ms/step
Epoch 181/200
1493/1493 - 64s - loss: 2.0930e-04 - val_loss: 2.0341e-04 - 64s/epoch - 43ms/step
Epoch 182/200
1493/1493 - 64s - loss: 1.8827e-04 - val_loss: 1.9768e-04 - 64s/epoch - 43ms/step
Epoch 183/200
1493/1493 - 64s - loss: 1.8675e-04 - val_loss: 2.0186e-04 - 64s/epoch - 43ms/step
Epoch 184/200
1493/1493 - 64s - loss: 1.8667e-04 - val_loss: 1.8397e-04 - 64s/epoch - 43ms/step
Epoch 185/200
1493/1493 - 64s - loss: 1.8242e-04 - val_loss: 2.0793e-04 - 64s/epoch - 43ms/step
Epoch 186/200
1493/1493 - 64s - loss: 1.8426e-04 - val_loss: 1.9116e-04 - 64s/epoch - 43ms/step
Epoch 187/200
1493/1493 - 64s - loss: 1.8179e-04 - val_loss: 1.9694e-04 - 64s/epoch - 43ms/step
Epoch 188/200
1493/1493 - 64s - loss: 1.8833e-04 - val_loss: 2.8529e-04 - 64s/epoch - 43ms/step
Epoch 189/200
1493/1493 - 64s - loss: 1.8602e-04 - val_loss: 1.9824e-04 - 64s/epoch - 43ms/step
Epoch 190/200
1493/1493 - 64s - loss: 1.8436e-04 - val_loss: 2.4873e-04 - 64s/epoch - 43ms/step
Epoch 191/200
1493/1493 - 64s - loss: 1.8915e-04 - val_loss: 2.4569e-04 - 64s/epoch - 43ms/step
Epoch 192/200
1493/1493 - 64s - loss: 1.9050e-04 - val_loss: 2.0310e-04 - 64s/epoch - 43ms/step
Epoch 193/200
1493/1493 - 64s - loss: 1.9211e-04 - val_loss: 3.5745e-04 - 64s/epoch - 43ms/step
Epoch 194/200
1493/1493 - 64s - loss: 2.1993e-04 - val_loss: 4.0592e-04 - 64s/epoch - 43ms/step
Epoch 195/200
1493/1493 - 64s - loss: 2.1770e-04 - val_loss: 2.7900e-04 - 64s/epoch - 43ms/step
Epoch 196/200
1493/1493 - 64s - loss: 2.0156e-04 - val_loss: 1.7223e-04 - 64s/epoch - 43ms/step
Epoch 197/200
1493/1493 - 64s - loss: 1.8410e-04 - val_loss: 1.9683e-04 - 64s/epoch - 43ms/step
Epoch 198/200
1493/1493 - 64s - loss: 1.8831e-04 - val_loss: 2.1823e-04 - 64s/epoch - 43ms/step
Epoch 199/200
1493/1493 - 64s - loss: 1.9086e-04 - val_loss: 1.7439e-04 - 64s/epoch - 43ms/step
Epoch 200/200
1493/1493 - 64s - loss: 1.8169e-04 - val_loss: 2.0078e-04 - 64s/epoch - 43ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00020078342640772462
  1/332 [..............................] - ETA: 26s 10/332 [..............................] - ETA: 1s  19/332 [>.............................] - ETA: 1s 28/332 [=>............................] - ETA: 1s 37/332 [==>...........................] - ETA: 1s 46/332 [===>..........................] - ETA: 1s 55/332 [===>..........................] - ETA: 1s 64/332 [====>.........................] - ETA: 1s 73/332 [=====>........................] - ETA: 1s 82/332 [======>.......................] - ETA: 1s 91/332 [=======>......................] - ETA: 1s100/332 [========>.....................] - ETA: 1s109/332 [========>.....................] - ETA: 1s118/332 [=========>....................] - ETA: 1s127/332 [==========>...................] - ETA: 1s136/332 [===========>..................] - ETA: 1s145/332 [============>.................] - ETA: 1s154/332 [============>.................] - ETA: 1s163/332 [=============>................] - ETA: 1s172/332 [==============>...............] - ETA: 0s181/332 [===============>..............] - ETA: 0s190/332 [================>.............] - ETA: 0s199/332 [================>.............] - ETA: 0s208/332 [=================>............] - ETA: 0s217/332 [==================>...........] - ETA: 0s226/332 [===================>..........] - ETA: 0s235/332 [====================>.........] - ETA: 0s244/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s262/332 [======================>.......] - ETA: 0s271/332 [=======================>......] - ETA: 0s280/332 [========================>.....] - ETA: 0s289/332 [=========================>....] - ETA: 0s298/332 [=========================>....] - ETA: 0s307/332 [==========================>...] - ETA: 0s316/332 [===========================>..] - ETA: 0s325/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 6ms/step
correlation 0.0022843231907261124
cosine 0.0017978932569487967
MAE: 0.0077361516
RMSE: 0.014169801
r2: 0.9869752388984494
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               637308    
                                                                 
 batch_normalization_16 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 252)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              639584    
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 7,692,700
Trainable params: 7,682,084
Non-trainable params: 10,616
_________________________________________________________________
Encoder
Model: "model_16"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_17 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               637308    
                                                                 
=================================================================
Total params: 3,845,340
Trainable params: 3,840,284
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_17"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_18 (InputLayer)       [(None, 252)]             0         
                                                                 
 batch_normalization_16 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 252)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              639584    
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 3,847,360
Trainable params: 3,841,800
Non-trainable params: 5,560
_________________________________________________________________
['2.0custom_n_b', 'mse', 64, 200, 0.0005, 0.2, 252, 0.00018169268150813878, 0.00020078342640772462, 0.0022843231907261124, 0.0017978932569487967, 0.007736151572316885, 0.014169801026582718, 0.9869752388984494, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_18 (Dense)            (None, 2654)              3357310   
                                                                 
 batch_normalization_18 (Bat  (None, 2654)             10616     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2654)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               669060    
                                                                 
 batch_normalization_19 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 252)               0         
                                                                 
 dense_19 (Dense)            (None, 2654)              671462    
                                                                 
 batch_normalization_20 (Bat  (None, 2654)             10616     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2654)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3355920   
                                                                 
=================================================================
Total params: 8,075,992
Trainable params: 8,064,872
Non-trainable params: 11,120
_________________________________________________________________
Epoch 1/200
1493/1493 - 70s - loss: 0.0102 - val_loss: 0.0046 - 70s/epoch - 47ms/step
Epoch 2/200
1493/1493 - 69s - loss: 0.0035 - val_loss: 0.0029 - 69s/epoch - 46ms/step
Epoch 3/200
1493/1493 - 69s - loss: 0.0026 - val_loss: 0.0027 - 69s/epoch - 46ms/step
Epoch 4/200
1493/1493 - 69s - loss: 0.0022 - val_loss: 0.0064 - 69s/epoch - 46ms/step
Epoch 5/200
1493/1493 - 69s - loss: 0.0022 - val_loss: 0.0016 - 69s/epoch - 46ms/step
Epoch 6/200
1493/1493 - 69s - loss: 0.0018 - val_loss: 0.0022 - 69s/epoch - 46ms/step
Epoch 7/200
1493/1493 - 69s - loss: 0.0017 - val_loss: 0.0015 - 69s/epoch - 46ms/step
Epoch 8/200
1493/1493 - 69s - loss: 0.0015 - val_loss: 0.0018 - 69s/epoch - 46ms/step
Epoch 9/200
1493/1493 - 69s - loss: 0.0014 - val_loss: 0.0014 - 69s/epoch - 47ms/step
Epoch 10/200
1493/1493 - 69s - loss: 0.0013 - val_loss: 0.0010 - 69s/epoch - 46ms/step
Epoch 11/200
1493/1493 - 69s - loss: 0.0012 - val_loss: 0.0011 - 69s/epoch - 47ms/step
Epoch 12/200
1493/1493 - 69s - loss: 9.9704e-04 - val_loss: 0.0010 - 69s/epoch - 47ms/step
Epoch 13/200
1493/1493 - 69s - loss: 9.4368e-04 - val_loss: 0.0015 - 69s/epoch - 46ms/step
Epoch 14/200
1493/1493 - 70s - loss: 9.3110e-04 - val_loss: 9.6646e-04 - 70s/epoch - 47ms/step
Epoch 15/200
1493/1493 - 69s - loss: 8.4063e-04 - val_loss: 7.5069e-04 - 69s/epoch - 46ms/step
Epoch 16/200
1493/1493 - 69s - loss: 7.5101e-04 - val_loss: 6.6206e-04 - 69s/epoch - 47ms/step
Epoch 17/200
1493/1493 - 69s - loss: 7.1350e-04 - val_loss: 8.0424e-04 - 69s/epoch - 47ms/step
Epoch 18/200
1493/1493 - 69s - loss: 6.9511e-04 - val_loss: 0.0012 - 69s/epoch - 46ms/step
Epoch 19/200
1493/1493 - 69s - loss: 6.9998e-04 - val_loss: 7.0629e-04 - 69s/epoch - 47ms/step
Epoch 20/200
1493/1493 - 69s - loss: 6.2504e-04 - val_loss: 5.4800e-04 - 69s/epoch - 46ms/step
Epoch 21/200
1493/1493 - 69s - loss: 5.9277e-04 - val_loss: 7.3054e-04 - 69s/epoch - 47ms/step
Epoch 22/200
1493/1493 - 69s - loss: 5.8013e-04 - val_loss: 6.4952e-04 - 69s/epoch - 47ms/step
Epoch 23/200
1493/1493 - 69s - loss: 5.6895e-04 - val_loss: 6.6736e-04 - 69s/epoch - 46ms/step
Epoch 24/200
1493/1493 - 69s - loss: 5.6577e-04 - val_loss: 4.8493e-04 - 69s/epoch - 47ms/step
Epoch 25/200
1493/1493 - 69s - loss: 5.1981e-04 - val_loss: 5.0064e-04 - 69s/epoch - 46ms/step
Epoch 26/200
1493/1493 - 69s - loss: 4.9985e-04 - val_loss: 7.1290e-04 - 69s/epoch - 46ms/step
Epoch 27/200
1493/1493 - 69s - loss: 4.9018e-04 - val_loss: 4.4900e-04 - 69s/epoch - 47ms/step
Epoch 28/200
1493/1493 - 69s - loss: 4.6426e-04 - val_loss: 5.5193e-04 - 69s/epoch - 46ms/step
Epoch 29/200
1493/1493 - 69s - loss: 4.5523e-04 - val_loss: 4.6820e-04 - 69s/epoch - 46ms/step
Epoch 30/200
1493/1493 - 69s - loss: 4.5052e-04 - val_loss: 4.4014e-04 - 69s/epoch - 46ms/step
Epoch 31/200
1493/1493 - 69s - loss: 4.3456e-04 - val_loss: 5.2971e-04 - 69s/epoch - 46ms/step
Epoch 32/200
1493/1493 - 69s - loss: 4.7473e-04 - val_loss: 3.8125e-04 - 69s/epoch - 47ms/step
Epoch 33/200
1493/1493 - 69s - loss: 4.2403e-04 - val_loss: 3.8284e-04 - 69s/epoch - 46ms/step
Epoch 34/200
1493/1493 - 69s - loss: 4.0948e-04 - val_loss: 3.9010e-04 - 69s/epoch - 46ms/step
Epoch 35/200
1493/1493 - 69s - loss: 3.9838e-04 - val_loss: 6.4655e-04 - 69s/epoch - 46ms/step
Epoch 36/200
1493/1493 - 69s - loss: 4.2918e-04 - val_loss: 4.3511e-04 - 69s/epoch - 47ms/step
Epoch 37/200
1493/1493 - 69s - loss: 3.9010e-04 - val_loss: 3.8360e-04 - 69s/epoch - 47ms/step
Epoch 38/200
1493/1493 - 69s - loss: 3.9242e-04 - val_loss: 3.7220e-04 - 69s/epoch - 46ms/step
Epoch 39/200
1493/1493 - 69s - loss: 3.7699e-04 - val_loss: 3.6698e-04 - 69s/epoch - 46ms/step
Epoch 40/200
1493/1493 - 69s - loss: 3.6519e-04 - val_loss: 3.7920e-04 - 69s/epoch - 47ms/step
Epoch 41/200
1493/1493 - 69s - loss: 3.6518e-04 - val_loss: 3.3835e-04 - 69s/epoch - 46ms/step
Epoch 42/200
1493/1493 - 69s - loss: 3.5850e-04 - val_loss: 3.2882e-04 - 69s/epoch - 46ms/step
Epoch 43/200
1493/1493 - 69s - loss: 3.6157e-04 - val_loss: 4.4134e-04 - 69s/epoch - 46ms/step
Epoch 44/200
1493/1493 - 69s - loss: 3.6236e-04 - val_loss: 0.0011 - 69s/epoch - 46ms/step
Epoch 45/200
1493/1493 - 69s - loss: 4.2131e-04 - val_loss: 3.1479e-04 - 69s/epoch - 46ms/step
Epoch 46/200
1493/1493 - 69s - loss: 3.4158e-04 - val_loss: 3.1135e-04 - 69s/epoch - 47ms/step
Epoch 47/200
1493/1493 - 69s - loss: 3.3946e-04 - val_loss: 3.3546e-04 - 69s/epoch - 46ms/step
Epoch 48/200
1493/1493 - 69s - loss: 3.3594e-04 - val_loss: 3.5580e-04 - 69s/epoch - 46ms/step
Epoch 49/200
1493/1493 - 69s - loss: 3.3372e-04 - val_loss: 7.8673e-04 - 69s/epoch - 46ms/step
Epoch 50/200
1493/1493 - 69s - loss: 3.9560e-04 - val_loss: 6.7830e-04 - 69s/epoch - 46ms/step
Epoch 51/200
1493/1493 - 69s - loss: 3.6325e-04 - val_loss: 3.0309e-04 - 69s/epoch - 46ms/step
Epoch 52/200
1493/1493 - 69s - loss: 3.2002e-04 - val_loss: 3.0976e-04 - 69s/epoch - 47ms/step
Epoch 53/200
1493/1493 - 69s - loss: 3.2007e-04 - val_loss: 3.0249e-04 - 69s/epoch - 46ms/step
Epoch 54/200
1493/1493 - 69s - loss: 3.1554e-04 - val_loss: 3.2206e-04 - 69s/epoch - 46ms/step
Epoch 55/200
1493/1493 - 69s - loss: 3.0792e-04 - val_loss: 3.1817e-04 - 69s/epoch - 46ms/step
Epoch 56/200
1493/1493 - 69s - loss: 3.0622e-04 - val_loss: 3.2020e-04 - 69s/epoch - 46ms/step
Epoch 57/200
1493/1493 - 69s - loss: 3.0221e-04 - val_loss: 2.9601e-04 - 69s/epoch - 46ms/step
Epoch 58/200
1493/1493 - 69s - loss: 3.0052e-04 - val_loss: 2.8545e-04 - 69s/epoch - 46ms/step
Epoch 59/200
1493/1493 - 69s - loss: 2.9539e-04 - val_loss: 3.2458e-04 - 69s/epoch - 46ms/step
Epoch 60/200
1493/1493 - 69s - loss: 2.9705e-04 - val_loss: 5.3813e-04 - 69s/epoch - 46ms/step
Epoch 61/200
1493/1493 - 69s - loss: 3.7164e-04 - val_loss: 2.8380e-04 - 69s/epoch - 46ms/step
Epoch 62/200
1493/1493 - 69s - loss: 2.9654e-04 - val_loss: 2.9927e-04 - 69s/epoch - 46ms/step
Epoch 63/200
1493/1493 - 69s - loss: 2.8836e-04 - val_loss: 2.9757e-04 - 69s/epoch - 46ms/step
Epoch 64/200
1493/1493 - 69s - loss: 2.8711e-04 - val_loss: 4.9472e-04 - 69s/epoch - 46ms/step
Epoch 65/200
1493/1493 - 69s - loss: 3.0842e-04 - val_loss: 3.4609e-04 - 69s/epoch - 46ms/step
Epoch 66/200
1493/1493 - 69s - loss: 2.9427e-04 - val_loss: 2.8233e-04 - 69s/epoch - 46ms/step
Epoch 67/200
1493/1493 - 69s - loss: 2.8077e-04 - val_loss: 2.8308e-04 - 69s/epoch - 46ms/step
Epoch 68/200
1493/1493 - 69s - loss: 2.8065e-04 - val_loss: 7.0281e-04 - 69s/epoch - 47ms/step
Epoch 69/200
1493/1493 - 69s - loss: 3.8294e-04 - val_loss: 2.9795e-04 - 69s/epoch - 46ms/step
Epoch 70/200
1493/1493 - 69s - loss: 2.9283e-04 - val_loss: 9.1462e-04 - 69s/epoch - 46ms/step
Epoch 71/200
1493/1493 - 69s - loss: 4.0673e-04 - val_loss: 2.7775e-04 - 69s/epoch - 46ms/step
Epoch 72/200
1493/1493 - 69s - loss: 2.9799e-04 - val_loss: 2.7588e-04 - 69s/epoch - 46ms/step
Epoch 73/200
1493/1493 - 69s - loss: 2.8441e-04 - val_loss: 2.7823e-04 - 69s/epoch - 46ms/step
Epoch 74/200
1493/1493 - 69s - loss: 2.8210e-04 - val_loss: 2.7340e-04 - 69s/epoch - 46ms/step
Epoch 75/200
1493/1493 - 69s - loss: 2.7270e-04 - val_loss: 2.7780e-04 - 69s/epoch - 46ms/step
Epoch 76/200
1493/1493 - 69s - loss: 2.6986e-04 - val_loss: 2.7654e-04 - 69s/epoch - 46ms/step
Epoch 77/200
1493/1493 - 69s - loss: 2.6929e-04 - val_loss: 3.7844e-04 - 69s/epoch - 46ms/step
Epoch 78/200
1493/1493 - 69s - loss: 2.7423e-04 - val_loss: 2.9852e-04 - 69s/epoch - 46ms/step
Epoch 79/200
1493/1493 - 69s - loss: 2.6911e-04 - val_loss: 3.6809e-04 - 69s/epoch - 46ms/step
Epoch 80/200
1493/1493 - 69s - loss: 2.7775e-04 - val_loss: 2.5974e-04 - 69s/epoch - 46ms/step
Epoch 81/200
1493/1493 - 69s - loss: 2.6362e-04 - val_loss: 3.1503e-04 - 69s/epoch - 46ms/step
Epoch 82/200
1493/1493 - 69s - loss: 2.5926e-04 - val_loss: 2.5682e-04 - 69s/epoch - 46ms/step
Epoch 83/200
1493/1493 - 69s - loss: 2.5480e-04 - val_loss: 2.7563e-04 - 69s/epoch - 46ms/step
Epoch 84/200
1493/1493 - 69s - loss: 2.5471e-04 - val_loss: 2.6109e-04 - 69s/epoch - 46ms/step
Epoch 85/200
1493/1493 - 69s - loss: 2.5600e-04 - val_loss: 2.8733e-04 - 69s/epoch - 46ms/step
Epoch 86/200
1493/1493 - 69s - loss: 2.5661e-04 - val_loss: 2.5763e-04 - 69s/epoch - 46ms/step
Epoch 87/200
1493/1493 - 69s - loss: 2.4942e-04 - val_loss: 2.7050e-04 - 69s/epoch - 46ms/step
Epoch 88/200
1493/1493 - 69s - loss: 2.4904e-04 - val_loss: 2.9301e-04 - 69s/epoch - 47ms/step
Epoch 89/200
1493/1493 - 69s - loss: 2.6615e-04 - val_loss: 2.4310e-04 - 69s/epoch - 46ms/step
Epoch 90/200
1493/1493 - 69s - loss: 2.4660e-04 - val_loss: 2.5282e-04 - 69s/epoch - 46ms/step
Epoch 91/200
1493/1493 - 69s - loss: 2.4478e-04 - val_loss: 2.7460e-04 - 69s/epoch - 46ms/step
Epoch 92/200
1493/1493 - 69s - loss: 2.4408e-04 - val_loss: 3.3917e-04 - 69s/epoch - 46ms/step
Epoch 93/200
1493/1493 - 70s - loss: 2.7240e-04 - val_loss: 2.5841e-04 - 70s/epoch - 47ms/step
Epoch 94/200
1493/1493 - 69s - loss: 2.4636e-04 - val_loss: 2.2572e-04 - 69s/epoch - 46ms/step
Epoch 95/200
1493/1493 - 69s - loss: 2.4220e-04 - val_loss: 2.5385e-04 - 69s/epoch - 46ms/step
Epoch 96/200
1493/1493 - 69s - loss: 2.4264e-04 - val_loss: 2.3549e-04 - 69s/epoch - 46ms/step
Epoch 97/200
1493/1493 - 69s - loss: 2.3945e-04 - val_loss: 2.3091e-04 - 69s/epoch - 47ms/step
Epoch 98/200
1493/1493 - 69s - loss: 2.4454e-04 - val_loss: 6.6444e-04 - 69s/epoch - 47ms/step
Epoch 99/200
1493/1493 - 69s - loss: 3.2450e-04 - val_loss: 2.2783e-04 - 69s/epoch - 46ms/step
Epoch 100/200
1493/1493 - 69s - loss: 2.5125e-04 - val_loss: 2.1573e-04 - 69s/epoch - 46ms/step
Epoch 101/200
1493/1493 - 69s - loss: 2.4012e-04 - val_loss: 2.4622e-04 - 69s/epoch - 47ms/step
Epoch 102/200
1493/1493 - 69s - loss: 2.4102e-04 - val_loss: 2.5330e-04 - 69s/epoch - 46ms/step
Epoch 103/200
1493/1493 - 69s - loss: 2.3968e-04 - val_loss: 2.2983e-04 - 69s/epoch - 47ms/step
Epoch 104/200
1493/1493 - 69s - loss: 2.3468e-04 - val_loss: 3.5645e-04 - 69s/epoch - 46ms/step
Epoch 105/200
1493/1493 - 69s - loss: 2.5501e-04 - val_loss: 2.2878e-04 - 69s/epoch - 46ms/step
Epoch 106/200
1493/1493 - 69s - loss: 2.3224e-04 - val_loss: 2.3892e-04 - 69s/epoch - 47ms/step
Epoch 107/200
1493/1493 - 69s - loss: 2.3186e-04 - val_loss: 2.3873e-04 - 69s/epoch - 46ms/step
Epoch 108/200
1493/1493 - 69s - loss: 2.3352e-04 - val_loss: 2.3163e-04 - 69s/epoch - 46ms/step
Epoch 109/200
1493/1493 - 69s - loss: 2.3029e-04 - val_loss: 2.4589e-04 - 69s/epoch - 46ms/step
Epoch 110/200
1493/1493 - 69s - loss: 2.2767e-04 - val_loss: 2.3818e-04 - 69s/epoch - 46ms/step
Epoch 111/200
1493/1493 - 69s - loss: 2.2816e-04 - val_loss: 3.1020e-04 - 69s/epoch - 47ms/step
Epoch 112/200
1493/1493 - 69s - loss: 2.5334e-04 - val_loss: 2.2665e-04 - 69s/epoch - 47ms/step
Epoch 113/200
1493/1493 - 70s - loss: 2.2838e-04 - val_loss: 4.3094e-04 - 70s/epoch - 47ms/step
Epoch 114/200
1493/1493 - 69s - loss: 2.6665e-04 - val_loss: 2.4293e-04 - 69s/epoch - 46ms/step
Epoch 115/200
1493/1493 - 69s - loss: 2.3459e-04 - val_loss: 2.6604e-04 - 69s/epoch - 46ms/step
Epoch 116/200
1493/1493 - 69s - loss: 2.2783e-04 - val_loss: 2.2227e-04 - 69s/epoch - 47ms/step
Epoch 117/200
1493/1493 - 69s - loss: 2.2417e-04 - val_loss: 2.4683e-04 - 69s/epoch - 46ms/step
Epoch 118/200
1493/1493 - 69s - loss: 2.2912e-04 - val_loss: 2.3042e-04 - 69s/epoch - 47ms/step
Epoch 119/200
1493/1493 - 69s - loss: 2.2322e-04 - val_loss: 2.3183e-04 - 69s/epoch - 46ms/step
Epoch 120/200
1493/1493 - 69s - loss: 2.2082e-04 - val_loss: 2.1599e-04 - 69s/epoch - 46ms/step
Epoch 121/200
1493/1493 - 69s - loss: 2.2210e-04 - val_loss: 2.6663e-04 - 69s/epoch - 47ms/step
Epoch 122/200
1493/1493 - 69s - loss: 2.2380e-04 - val_loss: 2.1346e-04 - 69s/epoch - 46ms/step
Epoch 123/200
1493/1493 - 69s - loss: 2.1822e-04 - val_loss: 2.2150e-04 - 69s/epoch - 46ms/step
Epoch 124/200
1493/1493 - 69s - loss: 2.2048e-04 - val_loss: 4.2587e-04 - 69s/epoch - 46ms/step
Epoch 125/200
1493/1493 - 69s - loss: 2.7113e-04 - val_loss: 2.1472e-04 - 69s/epoch - 47ms/step
Epoch 126/200
1493/1493 - 70s - loss: 2.3503e-04 - val_loss: 7.4861e-04 - 70s/epoch - 47ms/step
Epoch 127/200
1493/1493 - 69s - loss: 3.2986e-04 - val_loss: 4.3430e-04 - 69s/epoch - 46ms/step
Epoch 128/200
1493/1493 - 69s - loss: 2.4646e-04 - val_loss: 2.0848e-04 - 69s/epoch - 46ms/step
Epoch 129/200
1493/1493 - 69s - loss: 2.2448e-04 - val_loss: 2.1304e-04 - 69s/epoch - 46ms/step
Epoch 130/200
1493/1493 - 69s - loss: 2.2073e-04 - val_loss: 2.1462e-04 - 69s/epoch - 46ms/step
Epoch 131/200
1493/1493 - 69s - loss: 2.2442e-04 - val_loss: 3.7299e-04 - 69s/epoch - 47ms/step
Epoch 132/200
1493/1493 - 69s - loss: 2.6721e-04 - val_loss: 2.1296e-04 - 69s/epoch - 46ms/step
Epoch 133/200
1493/1493 - 69s - loss: 2.2168e-04 - val_loss: 2.0920e-04 - 69s/epoch - 46ms/step
Epoch 134/200
1493/1493 - 70s - loss: 2.1700e-04 - val_loss: 2.1086e-04 - 70s/epoch - 47ms/step
Epoch 135/200
1493/1493 - 69s - loss: 2.1445e-04 - val_loss: 2.2536e-04 - 69s/epoch - 46ms/step
Epoch 136/200
1493/1493 - 69s - loss: 2.1490e-04 - val_loss: 2.5154e-04 - 69s/epoch - 47ms/step
Epoch 137/200
1493/1493 - 69s - loss: 2.3614e-04 - val_loss: 3.4207e-04 - 69s/epoch - 46ms/step
Epoch 138/200
1493/1493 - 69s - loss: 2.4589e-04 - val_loss: 2.8490e-04 - 69s/epoch - 46ms/step
Epoch 139/200
1493/1493 - 69s - loss: 2.2809e-04 - val_loss: 2.5091e-04 - 69s/epoch - 47ms/step
Epoch 140/200
1493/1493 - 69s - loss: 2.3077e-04 - val_loss: 2.0993e-04 - 69s/epoch - 46ms/step
Epoch 141/200
1493/1493 - 69s - loss: 2.1390e-04 - val_loss: 2.6221e-04 - 69s/epoch - 46ms/step
Epoch 142/200
1493/1493 - 69s - loss: 2.2996e-04 - val_loss: 2.0464e-04 - 69s/epoch - 46ms/step
Epoch 143/200
1493/1493 - 69s - loss: 2.1133e-04 - val_loss: 2.3857e-04 - 69s/epoch - 46ms/step
Epoch 144/200
1493/1493 - 69s - loss: 2.1544e-04 - val_loss: 2.0847e-04 - 69s/epoch - 47ms/step
Epoch 145/200
1493/1493 - 69s - loss: 2.0919e-04 - val_loss: 2.0666e-04 - 69s/epoch - 46ms/step
Epoch 146/200
1493/1493 - 69s - loss: 2.0865e-04 - val_loss: 2.0108e-04 - 69s/epoch - 46ms/step
Epoch 147/200
1493/1493 - 69s - loss: 2.0692e-04 - val_loss: 2.2296e-04 - 69s/epoch - 47ms/step
Epoch 148/200
1493/1493 - 69s - loss: 2.0835e-04 - val_loss: 2.9815e-04 - 69s/epoch - 46ms/step
Epoch 149/200
1493/1493 - 69s - loss: 2.0546e-04 - val_loss: 2.0684e-04 - 69s/epoch - 47ms/step
Epoch 150/200
1493/1493 - 69s - loss: 2.0437e-04 - val_loss: 2.0437e-04 - 69s/epoch - 46ms/step
Epoch 151/200
1493/1493 - 69s - loss: 2.0523e-04 - val_loss: 2.0827e-04 - 69s/epoch - 46ms/step
Epoch 152/200
1493/1493 - 69s - loss: 2.0414e-04 - val_loss: 2.4041e-04 - 69s/epoch - 46ms/step
Epoch 153/200
1493/1493 - 69s - loss: 2.1128e-04 - val_loss: 2.1025e-04 - 69s/epoch - 46ms/step
Epoch 154/200
1493/1493 - 69s - loss: 2.0336e-04 - val_loss: 2.5868e-04 - 69s/epoch - 46ms/step
Epoch 155/200
1493/1493 - 69s - loss: 2.1818e-04 - val_loss: 2.0575e-04 - 69s/epoch - 46ms/step
Epoch 156/200
1493/1493 - 69s - loss: 2.0397e-04 - val_loss: 2.1145e-04 - 69s/epoch - 47ms/step
Epoch 157/200
1493/1493 - 69s - loss: 2.0116e-04 - val_loss: 2.2086e-04 - 69s/epoch - 46ms/step
Epoch 158/200
1493/1493 - 69s - loss: 2.0433e-04 - val_loss: 2.1391e-04 - 69s/epoch - 46ms/step
Epoch 159/200
1493/1493 - 69s - loss: 2.0030e-04 - val_loss: 2.0448e-04 - 69s/epoch - 47ms/step
Epoch 160/200
1493/1493 - 69s - loss: 2.0271e-04 - val_loss: 2.6405e-04 - 69s/epoch - 46ms/step
Epoch 161/200
1493/1493 - 69s - loss: 2.2975e-04 - val_loss: 1.9548e-04 - 69s/epoch - 46ms/step
Epoch 162/200
1493/1493 - 69s - loss: 2.0173e-04 - val_loss: 1.9889e-04 - 69s/epoch - 46ms/step
Epoch 163/200
1493/1493 - 69s - loss: 2.0359e-04 - val_loss: 2.2216e-04 - 69s/epoch - 46ms/step
Epoch 164/200
1493/1493 - 69s - loss: 2.0419e-04 - val_loss: 2.2258e-04 - 69s/epoch - 46ms/step
Epoch 165/200
1493/1493 - 69s - loss: 2.1855e-04 - val_loss: 1.9579e-04 - 69s/epoch - 47ms/step
Epoch 166/200
1493/1493 - 69s - loss: 2.0041e-04 - val_loss: 2.1844e-04 - 69s/epoch - 46ms/step
Epoch 167/200
1493/1493 - 69s - loss: 1.9912e-04 - val_loss: 1.9961e-04 - 69s/epoch - 46ms/step
Epoch 168/200
1493/1493 - 69s - loss: 1.9891e-04 - val_loss: 1.9833e-04 - 69s/epoch - 46ms/step
Epoch 169/200
1493/1493 - 69s - loss: 1.9785e-04 - val_loss: 2.3081e-04 - 69s/epoch - 46ms/step
Epoch 170/200
1493/1493 - 69s - loss: 1.9915e-04 - val_loss: 2.9393e-04 - 69s/epoch - 46ms/step
Epoch 171/200
1493/1493 - 69s - loss: 2.3305e-04 - val_loss: 4.0098e-04 - 69s/epoch - 46ms/step
Epoch 172/200
1493/1493 - 69s - loss: 2.4239e-04 - val_loss: 1.9310e-04 - 69s/epoch - 47ms/step
Epoch 173/200
1493/1493 - 69s - loss: 2.0141e-04 - val_loss: 1.9233e-04 - 69s/epoch - 47ms/step
Epoch 174/200
1493/1493 - 69s - loss: 1.9739e-04 - val_loss: 2.0682e-04 - 69s/epoch - 46ms/step
Epoch 175/200
1493/1493 - 69s - loss: 2.0017e-04 - val_loss: 1.9737e-04 - 69s/epoch - 47ms/step
Epoch 176/200
1493/1493 - 69s - loss: 1.9859e-04 - val_loss: 2.4009e-04 - 69s/epoch - 46ms/step
Epoch 177/200
1493/1493 - 69s - loss: 2.1010e-04 - val_loss: 1.8663e-04 - 69s/epoch - 47ms/step
Epoch 178/200
1493/1493 - 69s - loss: 1.9580e-04 - val_loss: 1.9837e-04 - 69s/epoch - 46ms/step
Epoch 179/200
1493/1493 - 69s - loss: 1.9406e-04 - val_loss: 1.9236e-04 - 69s/epoch - 46ms/step
Epoch 180/200
1493/1493 - 70s - loss: 1.9602e-04 - val_loss: 3.2883e-04 - 70s/epoch - 47ms/step
Epoch 181/200
1493/1493 - 69s - loss: 2.2099e-04 - val_loss: 2.0500e-04 - 69s/epoch - 46ms/step
Epoch 182/200
1493/1493 - 69s - loss: 1.9653e-04 - val_loss: 2.0356e-04 - 69s/epoch - 47ms/step
Epoch 183/200
1493/1493 - 69s - loss: 1.9488e-04 - val_loss: 2.0611e-04 - 69s/epoch - 46ms/step
Epoch 184/200
1493/1493 - 69s - loss: 1.9640e-04 - val_loss: 1.9379e-04 - 69s/epoch - 47ms/step
Epoch 185/200
1493/1493 - 69s - loss: 1.9109e-04 - val_loss: 2.1720e-04 - 69s/epoch - 46ms/step
Epoch 186/200
1493/1493 - 69s - loss: 1.9419e-04 - val_loss: 1.9640e-04 - 69s/epoch - 46ms/step
Epoch 187/200
1493/1493 - 69s - loss: 1.9071e-04 - val_loss: 1.9763e-04 - 69s/epoch - 47ms/step
Epoch 188/200
1493/1493 - 69s - loss: 1.9560e-04 - val_loss: 2.8565e-04 - 69s/epoch - 46ms/step
Epoch 189/200
1493/1493 - 69s - loss: 1.9966e-04 - val_loss: 1.9517e-04 - 69s/epoch - 46ms/step
Epoch 190/200
1493/1493 - 69s - loss: 1.9359e-04 - val_loss: 2.1643e-04 - 69s/epoch - 46ms/step
Epoch 191/200
1493/1493 - 69s - loss: 2.0142e-04 - val_loss: 2.1133e-04 - 69s/epoch - 46ms/step
Epoch 192/200
1493/1493 - 69s - loss: 1.9364e-04 - val_loss: 1.9872e-04 - 69s/epoch - 46ms/step
Epoch 193/200
1493/1493 - 69s - loss: 1.9723e-04 - val_loss: 3.6565e-04 - 69s/epoch - 46ms/step
Epoch 194/200
1493/1493 - 69s - loss: 2.3676e-04 - val_loss: 5.1048e-04 - 69s/epoch - 46ms/step
Epoch 195/200
1493/1493 - 69s - loss: 2.4110e-04 - val_loss: 2.7763e-04 - 69s/epoch - 46ms/step
Epoch 196/200
1493/1493 - 69s - loss: 2.1596e-04 - val_loss: 1.8616e-04 - 69s/epoch - 46ms/step
Epoch 197/200
1493/1493 - 69s - loss: 1.9670e-04 - val_loss: 1.9168e-04 - 69s/epoch - 46ms/step
Epoch 198/200
1493/1493 - 69s - loss: 1.9346e-04 - val_loss: 2.1574e-04 - 69s/epoch - 47ms/step
Epoch 199/200
1493/1493 - 69s - loss: 1.9502e-04 - val_loss: 1.8918e-04 - 69s/epoch - 46ms/step
Epoch 200/200
1493/1493 - 69s - loss: 1.9047e-04 - val_loss: 1.9990e-04 - 69s/epoch - 46ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00019990236614830792
  1/332 [..............................] - ETA: 29s  9/332 [..............................] - ETA: 2s  17/332 [>.............................] - ETA: 2s 25/332 [=>............................] - ETA: 2s 33/332 [=>............................] - ETA: 1s 41/332 [==>...........................] - ETA: 1s 49/332 [===>..........................] - ETA: 1s 57/332 [====>.........................] - ETA: 1s 65/332 [====>.........................] - ETA: 1s 73/332 [=====>........................] - ETA: 1s 81/332 [======>.......................] - ETA: 1s 89/332 [=======>......................] - ETA: 1s 97/332 [=======>......................] - ETA: 1s105/332 [========>.....................] - ETA: 1s113/332 [=========>....................] - ETA: 1s121/332 [=========>....................] - ETA: 1s129/332 [==========>...................] - ETA: 1s137/332 [===========>..................] - ETA: 1s145/332 [============>.................] - ETA: 1s153/332 [============>.................] - ETA: 1s161/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s177/332 [==============>...............] - ETA: 1s185/332 [===============>..............] - ETA: 0s193/332 [================>.............] - ETA: 0s201/332 [=================>............] - ETA: 0s209/332 [=================>............] - ETA: 0s217/332 [==================>...........] - ETA: 0s225/332 [===================>..........] - ETA: 0s233/332 [====================>.........] - ETA: 0s241/332 [====================>.........] - ETA: 0s249/332 [=====================>........] - ETA: 0s257/332 [======================>.......] - ETA: 0s265/332 [======================>.......] - ETA: 0s273/332 [=======================>......] - ETA: 0s281/332 [========================>.....] - ETA: 0s289/332 [=========================>....] - ETA: 0s297/332 [=========================>....] - ETA: 0s305/332 [==========================>...] - ETA: 0s313/332 [===========================>..] - ETA: 0s321/332 [============================>.] - ETA: 0s329/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 7ms/step
correlation 0.002302635235018969
cosine 0.0018122174011461249
MAE: 0.007565827
RMSE: 0.014138673
r2: 0.9870323999335023
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 2654)              3357310   
                                                                 
 batch_normalization_18 (Bat  (None, 2654)             10616     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2654)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               669060    
                                                                 
 batch_normalization_19 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 252)               0         
                                                                 
 dense_19 (Dense)            (None, 2654)              671462    
                                                                 
 batch_normalization_20 (Bat  (None, 2654)             10616     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2654)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3355920   
                                                                 
=================================================================
Total params: 8,075,992
Trainable params: 8,064,872
Non-trainable params: 11,120
_________________________________________________________________
Encoder
Model: "model_19"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_20 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 2654)              3357310   
                                                                 
 batch_normalization_18 (Bat  (None, 2654)             10616     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2654)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               669060    
                                                                 
=================================================================
Total params: 4,036,986
Trainable params: 4,031,678
Non-trainable params: 5,308
_________________________________________________________________
Decoder
Model: "model_20"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_21 (InputLayer)       [(None, 252)]             0         
                                                                 
 batch_normalization_19 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 252)               0         
                                                                 
 dense_19 (Dense)            (None, 2654)              671462    
                                                                 
 batch_normalization_20 (Bat  (None, 2654)             10616     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2654)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3355920   
                                                                 
=================================================================
Total params: 4,039,006
Trainable params: 4,033,194
Non-trainable params: 5,812
_________________________________________________________________
['2.1custom_n_b', 'mse', 64, 200, 0.0005, 0.2, 252, 0.00019047314708586782, 0.00019990236614830792, 0.002302635235018969, 0.0018122174011461249, 0.0075658271089196205, 0.014138673432171345, 0.9870323999335023, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_21 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_21 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               700812    
                                                                 
 batch_normalization_22 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 252)               0         
                                                                 
 dense_22 (Dense)            (None, 2780)              703340    
                                                                 
 batch_normalization_23 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2780)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 8,459,284
Trainable params: 8,447,660
Non-trainable params: 11,624
_________________________________________________________________
Epoch 1/200
1493/1493 - 73s - loss: 0.0104 - val_loss: 0.0055 - 73s/epoch - 49ms/step
Epoch 2/200
1493/1493 - 73s - loss: 0.0036 - val_loss: 0.0032 - 73s/epoch - 49ms/step
Epoch 3/200
1493/1493 - 73s - loss: 0.0026 - val_loss: 0.0033 - 73s/epoch - 49ms/step
Epoch 4/200
1493/1493 - 73s - loss: 0.0022 - val_loss: 0.0033 - 73s/epoch - 49ms/step
Epoch 5/200
1493/1493 - 73s - loss: 0.0020 - val_loss: 0.0017 - 73s/epoch - 49ms/step
Epoch 6/200
1493/1493 - 73s - loss: 0.0017 - val_loss: 0.0016 - 73s/epoch - 49ms/step
Epoch 7/200
1493/1493 - 73s - loss: 0.0017 - val_loss: 0.0016 - 73s/epoch - 49ms/step
Epoch 8/200
1493/1493 - 73s - loss: 0.0015 - val_loss: 0.0013 - 73s/epoch - 49ms/step
Epoch 9/200
1493/1493 - 73s - loss: 0.0013 - val_loss: 0.0010 - 73s/epoch - 49ms/step
Epoch 10/200
1493/1493 - 73s - loss: 0.0012 - val_loss: 9.8167e-04 - 73s/epoch - 49ms/step
Epoch 11/200
1493/1493 - 73s - loss: 0.0011 - val_loss: 8.9246e-04 - 73s/epoch - 49ms/step
Epoch 12/200
1493/1493 - 73s - loss: 9.4051e-04 - val_loss: 0.0017 - 73s/epoch - 49ms/step
Epoch 13/200
1493/1493 - 73s - loss: 9.2674e-04 - val_loss: 0.0012 - 73s/epoch - 49ms/step
Epoch 14/200
1493/1493 - 73s - loss: 8.5613e-04 - val_loss: 7.5756e-04 - 73s/epoch - 49ms/step
Epoch 15/200
1493/1493 - 73s - loss: 7.8647e-04 - val_loss: 9.1399e-04 - 73s/epoch - 49ms/step
Epoch 16/200
1493/1493 - 73s - loss: 7.5266e-04 - val_loss: 6.4830e-04 - 73s/epoch - 49ms/step
Epoch 17/200
1493/1493 - 73s - loss: 6.7652e-04 - val_loss: 7.8316e-04 - 73s/epoch - 49ms/step
Epoch 18/200
1493/1493 - 73s - loss: 6.5334e-04 - val_loss: 8.7647e-04 - 73s/epoch - 49ms/step
Epoch 19/200
1493/1493 - 73s - loss: 6.4491e-04 - val_loss: 5.9146e-04 - 73s/epoch - 49ms/step
Epoch 20/200
1493/1493 - 73s - loss: 6.0118e-04 - val_loss: 5.3053e-04 - 73s/epoch - 49ms/step
Epoch 21/200
1493/1493 - 73s - loss: 5.6736e-04 - val_loss: 6.6814e-04 - 73s/epoch - 49ms/step
Epoch 22/200
1493/1493 - 73s - loss: 5.6241e-04 - val_loss: 5.9464e-04 - 73s/epoch - 49ms/step
Epoch 23/200
1493/1493 - 73s - loss: 5.4231e-04 - val_loss: 6.3945e-04 - 73s/epoch - 49ms/step
Epoch 24/200
1493/1493 - 73s - loss: 5.3509e-04 - val_loss: 4.9318e-04 - 73s/epoch - 49ms/step
Epoch 25/200
1493/1493 - 73s - loss: 4.9750e-04 - val_loss: 4.9898e-04 - 73s/epoch - 49ms/step
Epoch 26/200
1493/1493 - 73s - loss: 4.8257e-04 - val_loss: 5.2262e-04 - 73s/epoch - 49ms/step
Epoch 27/200
1493/1493 - 73s - loss: 4.6424e-04 - val_loss: 4.5479e-04 - 73s/epoch - 49ms/step
Epoch 28/200
1493/1493 - 73s - loss: 4.5038e-04 - val_loss: 4.6818e-04 - 73s/epoch - 49ms/step
Epoch 29/200
1493/1493 - 73s - loss: 4.3682e-04 - val_loss: 4.7470e-04 - 73s/epoch - 49ms/step
Epoch 30/200
1493/1493 - 73s - loss: 4.4376e-04 - val_loss: 4.1140e-04 - 73s/epoch - 49ms/step
Epoch 31/200
1493/1493 - 73s - loss: 4.2113e-04 - val_loss: 5.2011e-04 - 73s/epoch - 49ms/step
Epoch 32/200
1493/1493 - 73s - loss: 4.2170e-04 - val_loss: 4.1270e-04 - 73s/epoch - 49ms/step
Epoch 33/200
1493/1493 - 73s - loss: 4.0789e-04 - val_loss: 3.7971e-04 - 73s/epoch - 49ms/step
Epoch 34/200
1493/1493 - 73s - loss: 3.9488e-04 - val_loss: 3.9175e-04 - 73s/epoch - 49ms/step
Epoch 35/200
1493/1493 - 73s - loss: 3.8460e-04 - val_loss: 8.8156e-04 - 73s/epoch - 49ms/step
Epoch 36/200
1493/1493 - 73s - loss: 4.3105e-04 - val_loss: 4.4251e-04 - 73s/epoch - 49ms/step
Epoch 37/200
1493/1493 - 73s - loss: 3.8523e-04 - val_loss: 4.0997e-04 - 73s/epoch - 49ms/step
Epoch 38/200
1493/1493 - 73s - loss: 3.7783e-04 - val_loss: 3.5804e-04 - 73s/epoch - 49ms/step
Epoch 39/200
1493/1493 - 73s - loss: 3.6335e-04 - val_loss: 3.6287e-04 - 73s/epoch - 49ms/step
Epoch 40/200
1493/1493 - 73s - loss: 3.5427e-04 - val_loss: 4.3716e-04 - 73s/epoch - 49ms/step
Epoch 41/200
1493/1493 - 73s - loss: 3.5200e-04 - val_loss: 3.2856e-04 - 73s/epoch - 49ms/step
Epoch 42/200
1493/1493 - 73s - loss: 3.5435e-04 - val_loss: 3.1604e-04 - 73s/epoch - 49ms/step
Epoch 43/200
1493/1493 - 73s - loss: 3.4257e-04 - val_loss: 4.6976e-04 - 73s/epoch - 49ms/step
Epoch 44/200
1493/1493 - 73s - loss: 3.6296e-04 - val_loss: 7.2830e-04 - 73s/epoch - 49ms/step
Epoch 45/200
1493/1493 - 73s - loss: 3.8902e-04 - val_loss: 3.0718e-04 - 73s/epoch - 49ms/step
Epoch 46/200
1493/1493 - 73s - loss: 3.3131e-04 - val_loss: 2.9156e-04 - 73s/epoch - 49ms/step
Epoch 47/200
1493/1493 - 73s - loss: 3.2908e-04 - val_loss: 3.4837e-04 - 73s/epoch - 49ms/step
Epoch 48/200
1493/1493 - 73s - loss: 3.2568e-04 - val_loss: 3.5728e-04 - 73s/epoch - 49ms/step
Epoch 49/200
1493/1493 - 73s - loss: 3.2091e-04 - val_loss: 5.7984e-04 - 73s/epoch - 49ms/step
Epoch 50/200
1493/1493 - 73s - loss: 3.4969e-04 - val_loss: 4.5341e-04 - 73s/epoch - 49ms/step
Epoch 51/200
1493/1493 - 73s - loss: 3.2926e-04 - val_loss: 3.1610e-04 - 73s/epoch - 49ms/step
Epoch 52/200
1493/1493 - 73s - loss: 3.0864e-04 - val_loss: 3.0880e-04 - 73s/epoch - 49ms/step
Epoch 53/200
1493/1493 - 73s - loss: 3.1077e-04 - val_loss: 2.8755e-04 - 73s/epoch - 49ms/step
Epoch 54/200
1493/1493 - 73s - loss: 3.0247e-04 - val_loss: 3.0642e-04 - 73s/epoch - 49ms/step
Epoch 55/200
1493/1493 - 73s - loss: 2.9931e-04 - val_loss: 2.8801e-04 - 73s/epoch - 49ms/step
Epoch 56/200
1493/1493 - 73s - loss: 2.9539e-04 - val_loss: 3.0332e-04 - 73s/epoch - 49ms/step
Epoch 57/200
1493/1493 - 73s - loss: 2.9291e-04 - val_loss: 3.0454e-04 - 73s/epoch - 49ms/step
Epoch 58/200
1493/1493 - 73s - loss: 2.9080e-04 - val_loss: 2.7578e-04 - 73s/epoch - 49ms/step
Epoch 59/200
1493/1493 - 73s - loss: 2.8695e-04 - val_loss: 2.9798e-04 - 73s/epoch - 49ms/step
Epoch 60/200
1493/1493 - 73s - loss: 2.8763e-04 - val_loss: 6.1141e-04 - 73s/epoch - 49ms/step
Epoch 61/200
1493/1493 - 73s - loss: 3.7972e-04 - val_loss: 2.7349e-04 - 73s/epoch - 49ms/step
Epoch 62/200
1493/1493 - 73s - loss: 2.9186e-04 - val_loss: 3.0429e-04 - 73s/epoch - 49ms/step
Epoch 63/200
1493/1493 - 73s - loss: 2.8511e-04 - val_loss: 3.0949e-04 - 73s/epoch - 49ms/step
Epoch 64/200
1493/1493 - 73s - loss: 2.8504e-04 - val_loss: 0.0012 - 73s/epoch - 49ms/step
Epoch 65/200
1493/1493 - 73s - loss: 4.1642e-04 - val_loss: 4.2462e-04 - 73s/epoch - 49ms/step
Epoch 66/200
1493/1493 - 73s - loss: 3.0619e-04 - val_loss: 2.9527e-04 - 73s/epoch - 49ms/step
Epoch 67/200
1493/1493 - 73s - loss: 2.8057e-04 - val_loss: 2.6517e-04 - 73s/epoch - 49ms/step
Epoch 68/200
1493/1493 - 73s - loss: 2.7732e-04 - val_loss: 5.0792e-04 - 73s/epoch - 49ms/step
Epoch 69/200
1493/1493 - 73s - loss: 3.2969e-04 - val_loss: 2.7560e-04 - 73s/epoch - 49ms/step
Epoch 70/200
1493/1493 - 73s - loss: 2.7592e-04 - val_loss: 5.7699e-04 - 73s/epoch - 49ms/step
Epoch 71/200
1493/1493 - 73s - loss: 3.3016e-04 - val_loss: 2.5391e-04 - 73s/epoch - 49ms/step
Epoch 72/200
1493/1493 - 73s - loss: 2.7608e-04 - val_loss: 2.9162e-04 - 73s/epoch - 49ms/step
Epoch 73/200
1493/1493 - 73s - loss: 2.7400e-04 - val_loss: 3.1581e-04 - 73s/epoch - 49ms/step
Epoch 74/200
1493/1493 - 73s - loss: 2.8056e-04 - val_loss: 2.6939e-04 - 73s/epoch - 49ms/step
Epoch 75/200
1493/1493 - 73s - loss: 2.6175e-04 - val_loss: 2.6371e-04 - 73s/epoch - 49ms/step
Epoch 76/200
1493/1493 - 73s - loss: 2.6091e-04 - val_loss: 2.6197e-04 - 73s/epoch - 49ms/step
Epoch 77/200
1493/1493 - 73s - loss: 2.5561e-04 - val_loss: 4.4482e-04 - 73s/epoch - 49ms/step
Epoch 78/200
1493/1493 - 73s - loss: 2.8619e-04 - val_loss: 2.4922e-04 - 73s/epoch - 49ms/step
Epoch 79/200
1493/1493 - 73s - loss: 2.5901e-04 - val_loss: 3.5165e-04 - 73s/epoch - 49ms/step
Epoch 80/200
1493/1493 - 73s - loss: 2.7801e-04 - val_loss: 2.6664e-04 - 73s/epoch - 49ms/step
Epoch 81/200
1493/1493 - 73s - loss: 2.5948e-04 - val_loss: 3.5200e-04 - 73s/epoch - 49ms/step
Epoch 82/200
1493/1493 - 73s - loss: 2.5520e-04 - val_loss: 2.4767e-04 - 73s/epoch - 49ms/step
Epoch 83/200
1493/1493 - 73s - loss: 2.4912e-04 - val_loss: 2.5664e-04 - 73s/epoch - 49ms/step
Epoch 84/200
1493/1493 - 73s - loss: 2.4754e-04 - val_loss: 2.5910e-04 - 73s/epoch - 49ms/step
Epoch 85/200
1493/1493 - 73s - loss: 2.4623e-04 - val_loss: 2.7405e-04 - 73s/epoch - 49ms/step
Epoch 86/200
1493/1493 - 73s - loss: 2.4738e-04 - val_loss: 2.5648e-04 - 73s/epoch - 49ms/step
Epoch 87/200
1493/1493 - 73s - loss: 2.4022e-04 - val_loss: 2.4863e-04 - 73s/epoch - 49ms/step
Epoch 88/200
1493/1493 - 73s - loss: 2.3914e-04 - val_loss: 2.9160e-04 - 73s/epoch - 49ms/step
Epoch 89/200
1493/1493 - 73s - loss: 2.6501e-04 - val_loss: 2.4428e-04 - 73s/epoch - 49ms/step
Epoch 90/200
1493/1493 - 73s - loss: 2.4108e-04 - val_loss: 2.4701e-04 - 73s/epoch - 49ms/step
Epoch 91/200
1493/1493 - 73s - loss: 2.3873e-04 - val_loss: 2.7054e-04 - 73s/epoch - 49ms/step
Epoch 92/200
1493/1493 - 73s - loss: 2.3843e-04 - val_loss: 3.9372e-04 - 73s/epoch - 49ms/step
Epoch 93/200
1493/1493 - 73s - loss: 2.5968e-04 - val_loss: 2.4546e-04 - 73s/epoch - 49ms/step
Epoch 94/200
1493/1493 - 73s - loss: 2.3735e-04 - val_loss: 2.2565e-04 - 73s/epoch - 49ms/step
Epoch 95/200
1493/1493 - 73s - loss: 2.3572e-04 - val_loss: 4.1045e-04 - 73s/epoch - 49ms/step
Epoch 96/200
1493/1493 - 73s - loss: 2.6258e-04 - val_loss: 2.3614e-04 - 73s/epoch - 49ms/step
Epoch 97/200
1493/1493 - 73s - loss: 2.3713e-04 - val_loss: 2.1694e-04 - 73s/epoch - 49ms/step
Epoch 98/200
1493/1493 - 73s - loss: 2.3708e-04 - val_loss: 4.8377e-04 - 73s/epoch - 49ms/step
Epoch 99/200
1493/1493 - 73s - loss: 2.9070e-04 - val_loss: 2.2439e-04 - 73s/epoch - 49ms/step
Epoch 100/200
1493/1493 - 73s - loss: 2.4391e-04 - val_loss: 2.1911e-04 - 73s/epoch - 49ms/step
Epoch 101/200
1493/1493 - 73s - loss: 2.3014e-04 - val_loss: 2.7525e-04 - 73s/epoch - 49ms/step
Epoch 102/200
1493/1493 - 73s - loss: 2.3392e-04 - val_loss: 3.1426e-04 - 73s/epoch - 49ms/step
Epoch 103/200
1493/1493 - 73s - loss: 2.3947e-04 - val_loss: 2.2671e-04 - 73s/epoch - 49ms/step
Epoch 104/200
1493/1493 - 73s - loss: 2.3052e-04 - val_loss: 6.2173e-04 - 73s/epoch - 49ms/step
Epoch 105/200
1493/1493 - 73s - loss: 2.9156e-04 - val_loss: 2.1735e-04 - 73s/epoch - 49ms/step
Epoch 106/200
1493/1493 - 73s - loss: 2.3092e-04 - val_loss: 2.2095e-04 - 73s/epoch - 49ms/step
Epoch 107/200
1493/1493 - 73s - loss: 2.2786e-04 - val_loss: 2.3283e-04 - 73s/epoch - 49ms/step
Epoch 108/200
1493/1493 - 73s - loss: 2.3474e-04 - val_loss: 2.3937e-04 - 73s/epoch - 49ms/step
Epoch 109/200
1493/1493 - 73s - loss: 2.2604e-04 - val_loss: 2.4364e-04 - 73s/epoch - 49ms/step
Epoch 110/200
1493/1493 - 73s - loss: 2.2383e-04 - val_loss: 2.3161e-04 - 73s/epoch - 49ms/step
Epoch 111/200
1493/1493 - 73s - loss: 2.2258e-04 - val_loss: 2.6419e-04 - 73s/epoch - 49ms/step
Epoch 112/200
1493/1493 - 73s - loss: 2.3386e-04 - val_loss: 2.2890e-04 - 73s/epoch - 49ms/step
Epoch 113/200
1493/1493 - 73s - loss: 2.1793e-04 - val_loss: 2.5343e-04 - 73s/epoch - 49ms/step
Epoch 114/200
1493/1493 - 73s - loss: 2.2580e-04 - val_loss: 2.6176e-04 - 73s/epoch - 49ms/step
Epoch 115/200
1493/1493 - 73s - loss: 2.3447e-04 - val_loss: 2.9623e-04 - 73s/epoch - 49ms/step
Epoch 116/200
1493/1493 - 73s - loss: 2.1987e-04 - val_loss: 2.2787e-04 - 73s/epoch - 49ms/step
Epoch 117/200
1493/1493 - 73s - loss: 2.1799e-04 - val_loss: 2.6551e-04 - 73s/epoch - 49ms/step
Epoch 118/200
1493/1493 - 73s - loss: 2.2208e-04 - val_loss: 2.3207e-04 - 73s/epoch - 49ms/step
Epoch 119/200
1493/1493 - 73s - loss: 2.1702e-04 - val_loss: 2.2116e-04 - 73s/epoch - 49ms/step
Epoch 120/200
1493/1493 - 73s - loss: 2.1412e-04 - val_loss: 2.1108e-04 - 73s/epoch - 49ms/step
Epoch 121/200
1493/1493 - 73s - loss: 2.1493e-04 - val_loss: 2.6283e-04 - 73s/epoch - 49ms/step
Epoch 122/200
1493/1493 - 73s - loss: 2.1373e-04 - val_loss: 2.1052e-04 - 73s/epoch - 49ms/step
Epoch 123/200
1493/1493 - 73s - loss: 2.1298e-04 - val_loss: 2.1735e-04 - 73s/epoch - 49ms/step
Epoch 124/200
1493/1493 - 73s - loss: 2.1289e-04 - val_loss: 3.5279e-04 - 73s/epoch - 49ms/step
Epoch 125/200
1493/1493 - 73s - loss: 2.4074e-04 - val_loss: 2.2596e-04 - 73s/epoch - 49ms/step
Epoch 126/200
1493/1493 - 73s - loss: 2.1673e-04 - val_loss: 3.4988e-04 - 73s/epoch - 49ms/step
Epoch 127/200
1493/1493 - 73s - loss: 2.2464e-04 - val_loss: 3.5035e-04 - 73s/epoch - 49ms/step
Epoch 128/200
1493/1493 - 73s - loss: 2.2926e-04 - val_loss: 2.0142e-04 - 73s/epoch - 49ms/step
Epoch 129/200
1493/1493 - 73s - loss: 2.1123e-04 - val_loss: 2.2203e-04 - 73s/epoch - 49ms/step
Epoch 130/200
1493/1493 - 73s - loss: 2.1024e-04 - val_loss: 2.0360e-04 - 73s/epoch - 49ms/step
Epoch 131/200
1493/1493 - 73s - loss: 2.1252e-04 - val_loss: 3.6305e-04 - 73s/epoch - 49ms/step
Epoch 132/200
1493/1493 - 73s - loss: 2.4799e-04 - val_loss: 2.0635e-04 - 73s/epoch - 49ms/step
Epoch 133/200
1493/1493 - 73s - loss: 2.1190e-04 - val_loss: 2.2255e-04 - 73s/epoch - 49ms/step
Epoch 134/200
1493/1493 - 73s - loss: 2.1119e-04 - val_loss: 2.1572e-04 - 73s/epoch - 49ms/step
Epoch 135/200
1493/1493 - 73s - loss: 2.0730e-04 - val_loss: 2.0384e-04 - 73s/epoch - 49ms/step
Epoch 136/200
1493/1493 - 73s - loss: 2.0717e-04 - val_loss: 2.5293e-04 - 73s/epoch - 49ms/step
Epoch 137/200
1493/1493 - 73s - loss: 2.2272e-04 - val_loss: 3.0303e-04 - 73s/epoch - 49ms/step
Epoch 138/200
1493/1493 - 73s - loss: 2.4133e-04 - val_loss: 2.3865e-04 - 73s/epoch - 49ms/step
Epoch 139/200
1493/1493 - 73s - loss: 2.1749e-04 - val_loss: 2.1619e-04 - 73s/epoch - 49ms/step
Epoch 140/200
1493/1493 - 73s - loss: 2.1269e-04 - val_loss: 1.9577e-04 - 73s/epoch - 49ms/step
Epoch 141/200
1493/1493 - 73s - loss: 2.0652e-04 - val_loss: 2.5893e-04 - 73s/epoch - 49ms/step
Epoch 142/200
1493/1493 - 73s - loss: 2.1378e-04 - val_loss: 2.0627e-04 - 73s/epoch - 49ms/step
Epoch 143/200
1493/1493 - 73s - loss: 2.0513e-04 - val_loss: 2.2161e-04 - 73s/epoch - 49ms/step
Epoch 144/200
1493/1493 - 73s - loss: 2.0903e-04 - val_loss: 2.2178e-04 - 73s/epoch - 49ms/step
Epoch 145/200
1493/1493 - 73s - loss: 2.0352e-04 - val_loss: 2.1237e-04 - 73s/epoch - 49ms/step
Epoch 146/200
1493/1493 - 73s - loss: 2.0225e-04 - val_loss: 1.9774e-04 - 73s/epoch - 49ms/step
Epoch 147/200
1493/1493 - 73s - loss: 1.9949e-04 - val_loss: 2.0635e-04 - 73s/epoch - 49ms/step
Epoch 148/200
1493/1493 - 73s - loss: 1.9962e-04 - val_loss: 3.5309e-04 - 73s/epoch - 49ms/step
Epoch 149/200
1493/1493 - 73s - loss: 1.9868e-04 - val_loss: 2.0826e-04 - 73s/epoch - 49ms/step
Epoch 150/200
1493/1493 - 73s - loss: 1.9763e-04 - val_loss: 1.9883e-04 - 73s/epoch - 49ms/step
Epoch 151/200
1493/1493 - 73s - loss: 2.0017e-04 - val_loss: 2.0960e-04 - 73s/epoch - 49ms/step
Epoch 152/200
1493/1493 - 73s - loss: 1.9795e-04 - val_loss: 2.3960e-04 - 73s/epoch - 49ms/step
Epoch 153/200
1493/1493 - 73s - loss: 2.0483e-04 - val_loss: 2.0346e-04 - 73s/epoch - 49ms/step
Epoch 154/200
1493/1493 - 73s - loss: 1.9983e-04 - val_loss: 3.3686e-04 - 73s/epoch - 49ms/step
Epoch 155/200
1493/1493 - 73s - loss: 2.4077e-04 - val_loss: 2.0115e-04 - 73s/epoch - 49ms/step
Epoch 156/200
1493/1493 - 73s - loss: 2.0053e-04 - val_loss: 2.1089e-04 - 73s/epoch - 49ms/step
Epoch 157/200
1493/1493 - 73s - loss: 1.9619e-04 - val_loss: 2.3171e-04 - 73s/epoch - 49ms/step
Epoch 158/200
1493/1493 - 73s - loss: 1.9933e-04 - val_loss: 2.1099e-04 - 73s/epoch - 49ms/step
Epoch 159/200
1493/1493 - 73s - loss: 1.9529e-04 - val_loss: 1.9732e-04 - 73s/epoch - 49ms/step
Epoch 160/200
1493/1493 - 73s - loss: 1.9742e-04 - val_loss: 2.7927e-04 - 73s/epoch - 49ms/step
Epoch 161/200
1493/1493 - 73s - loss: 2.4009e-04 - val_loss: 1.9253e-04 - 73s/epoch - 49ms/step
Epoch 162/200
1493/1493 - 73s - loss: 1.9664e-04 - val_loss: 2.0626e-04 - 73s/epoch - 49ms/step
Epoch 163/200
1493/1493 - 73s - loss: 1.9706e-04 - val_loss: 3.1895e-04 - 73s/epoch - 49ms/step
Epoch 164/200
1493/1493 - 73s - loss: 2.1796e-04 - val_loss: 2.0542e-04 - 73s/epoch - 49ms/step
Epoch 165/200
1493/1493 - 73s - loss: 2.0326e-04 - val_loss: 1.9954e-04 - 73s/epoch - 49ms/step
Epoch 166/200
1493/1493 - 73s - loss: 1.9530e-04 - val_loss: 1.9662e-04 - 73s/epoch - 49ms/step
Epoch 167/200
1493/1493 - 73s - loss: 1.9480e-04 - val_loss: 1.9329e-04 - 73s/epoch - 49ms/step
Epoch 168/200
1493/1493 - 73s - loss: 1.9365e-04 - val_loss: 2.0113e-04 - 73s/epoch - 49ms/step
Epoch 169/200
1493/1493 - 73s - loss: 1.9265e-04 - val_loss: 2.2853e-04 - 73s/epoch - 49ms/step
Epoch 170/200
1493/1493 - 73s - loss: 1.9872e-04 - val_loss: 4.2302e-04 - 73s/epoch - 49ms/step
Epoch 171/200
1493/1493 - 73s - loss: 2.5744e-04 - val_loss: 6.0247e-04 - 73s/epoch - 49ms/step
Epoch 172/200
1493/1493 - 73s - loss: 2.5527e-04 - val_loss: 2.2331e-04 - 73s/epoch - 49ms/step
Epoch 173/200
1493/1493 - 73s - loss: 2.0190e-04 - val_loss: 1.8976e-04 - 73s/epoch - 49ms/step
Epoch 174/200
1493/1493 - 72s - loss: 1.9399e-04 - val_loss: 2.1321e-04 - 72s/epoch - 49ms/step
Epoch 175/200
1493/1493 - 72s - loss: 1.9439e-04 - val_loss: 2.0110e-04 - 72s/epoch - 48ms/step
Epoch 176/200
1493/1493 - 73s - loss: 1.9437e-04 - val_loss: 1.9290e-04 - 73s/epoch - 49ms/step
Epoch 177/200
1493/1493 - 73s - loss: 1.9409e-04 - val_loss: 1.8759e-04 - 73s/epoch - 49ms/step
Epoch 178/200
1493/1493 - 73s - loss: 1.9088e-04 - val_loss: 1.9838e-04 - 73s/epoch - 49ms/step
Epoch 179/200
1493/1493 - 73s - loss: 1.8815e-04 - val_loss: 2.0209e-04 - 73s/epoch - 49ms/step
Epoch 180/200
1493/1493 - 72s - loss: 1.9046e-04 - val_loss: 2.2427e-04 - 72s/epoch - 49ms/step
Epoch 181/200
1493/1493 - 73s - loss: 1.9179e-04 - val_loss: 2.2123e-04 - 73s/epoch - 49ms/step
Epoch 182/200
1493/1493 - 73s - loss: 1.8955e-04 - val_loss: 2.0270e-04 - 73s/epoch - 49ms/step
Epoch 183/200
1493/1493 - 73s - loss: 1.9088e-04 - val_loss: 2.1356e-04 - 73s/epoch - 49ms/step
Epoch 184/200
1493/1493 - 72s - loss: 1.8972e-04 - val_loss: 1.8786e-04 - 72s/epoch - 48ms/step
Epoch 185/200
1493/1493 - 73s - loss: 1.8522e-04 - val_loss: 2.3515e-04 - 73s/epoch - 49ms/step
Epoch 186/200
1493/1493 - 73s - loss: 1.8759e-04 - val_loss: 1.9500e-04 - 73s/epoch - 49ms/step
Epoch 187/200
1493/1493 - 73s - loss: 1.8461e-04 - val_loss: 1.9712e-04 - 73s/epoch - 49ms/step
Epoch 188/200
1493/1493 - 73s - loss: 1.8932e-04 - val_loss: 2.7774e-04 - 73s/epoch - 49ms/step
Epoch 189/200
1493/1493 - 72s - loss: 1.9395e-04 - val_loss: 1.9435e-04 - 72s/epoch - 49ms/step
Epoch 190/200
1493/1493 - 73s - loss: 1.8825e-04 - val_loss: 2.0833e-04 - 73s/epoch - 49ms/step
Epoch 191/200
1493/1493 - 73s - loss: 1.9453e-04 - val_loss: 2.0082e-04 - 73s/epoch - 49ms/step
Epoch 192/200
1493/1493 - 73s - loss: 1.8626e-04 - val_loss: 2.0376e-04 - 73s/epoch - 49ms/step
Epoch 193/200
1493/1493 - 72s - loss: 1.9171e-04 - val_loss: 3.0531e-04 - 72s/epoch - 49ms/step
Epoch 194/200
1493/1493 - 73s - loss: 2.1630e-04 - val_loss: 3.2505e-04 - 73s/epoch - 49ms/step
Epoch 195/200
1493/1493 - 73s - loss: 2.1672e-04 - val_loss: 2.5996e-04 - 73s/epoch - 49ms/step
Epoch 196/200
1493/1493 - 73s - loss: 1.9821e-04 - val_loss: 1.8002e-04 - 73s/epoch - 49ms/step
Epoch 197/200
1493/1493 - 73s - loss: 1.8642e-04 - val_loss: 1.9120e-04 - 73s/epoch - 49ms/step
Epoch 198/200
1493/1493 - 72s - loss: 1.8764e-04 - val_loss: 2.5287e-04 - 72s/epoch - 49ms/step
Epoch 199/200
1493/1493 - 73s - loss: 2.0215e-04 - val_loss: 1.8455e-04 - 73s/epoch - 49ms/step
Epoch 200/200
1493/1493 - 73s - loss: 1.8525e-04 - val_loss: 1.8862e-04 - 73s/epoch - 49ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.0001886186801129952
  1/332 [..............................] - ETA: 32s  9/332 [..............................] - ETA: 2s  17/332 [>.............................] - ETA: 2s 25/332 [=>............................] - ETA: 2s 33/332 [=>............................] - ETA: 1s 41/332 [==>...........................] - ETA: 1s 49/332 [===>..........................] - ETA: 1s 57/332 [====>.........................] - ETA: 1s 65/332 [====>.........................] - ETA: 1s 73/332 [=====>........................] - ETA: 1s 81/332 [======>.......................] - ETA: 1s 89/332 [=======>......................] - ETA: 1s 97/332 [=======>......................] - ETA: 1s105/332 [========>.....................] - ETA: 1s113/332 [=========>....................] - ETA: 1s121/332 [=========>....................] - ETA: 1s129/332 [==========>...................] - ETA: 1s137/332 [===========>..................] - ETA: 1s145/332 [============>.................] - ETA: 1s153/332 [============>.................] - ETA: 1s161/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s177/332 [==============>...............] - ETA: 1s185/332 [===============>..............] - ETA: 0s193/332 [================>.............] - ETA: 0s201/332 [=================>............] - ETA: 0s209/332 [=================>............] - ETA: 0s217/332 [==================>...........] - ETA: 0s225/332 [===================>..........] - ETA: 0s233/332 [====================>.........] - ETA: 0s241/332 [====================>.........] - ETA: 0s249/332 [=====================>........] - ETA: 0s257/332 [======================>.......] - ETA: 0s265/332 [======================>.......] - ETA: 0s273/332 [=======================>......] - ETA: 0s281/332 [========================>.....] - ETA: 0s289/332 [=========================>....] - ETA: 0s297/332 [=========================>....] - ETA: 0s305/332 [==========================>...] - ETA: 0s313/332 [===========================>..] - ETA: 0s321/332 [============================>.] - ETA: 0s329/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 7ms/step
correlation 0.002171218563734789
cosine 0.0017083957482408767
MAE: 0.007432076
RMSE: 0.013733846
r2: 0.9877645635007061
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       multiple                  0         
                                                                 
 dense_21 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_21 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               700812    
                                                                 
 batch_normalization_22 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 252)               0         
                                                                 
 dense_22 (Dense)            (None, 2780)              703340    
                                                                 
 batch_normalization_23 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2780)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 8,459,284
Trainable params: 8,447,660
Non-trainable params: 11,624
_________________________________________________________________
Encoder
Model: "model_22"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_23 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_22 (InputLayer)       multiple                  0         
                                                                 
 dense_21 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_21 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               700812    
                                                                 
=================================================================
Total params: 4,228,632
Trainable params: 4,223,072
Non-trainable params: 5,560
_________________________________________________________________
Decoder
Model: "model_23"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_24 (InputLayer)       [(None, 252)]             0         
                                                                 
 batch_normalization_22 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 252)               0         
                                                                 
 dense_22 (Dense)            (None, 2780)              703340    
                                                                 
 batch_normalization_23 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2780)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 4,230,652
Trainable params: 4,224,588
Non-trainable params: 6,064
_________________________________________________________________
['2.2custom_n_b', 'mse', 64, 200, 0.0005, 0.2, 252, 0.00018525385530665517, 0.0001886186801129952, 0.002171218563734789, 0.0017083957482408767, 0.007432076148688793, 0.013733846135437489, 0.9877645635007061, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_24"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_25 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_24 (Dense)            (None, 2907)              3677355   
                                                                 
 batch_normalization_24 (Bat  (None, 2907)             11628     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2907)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               732816    
                                                                 
 batch_normalization_25 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 252)               0         
                                                                 
 dense_25 (Dense)            (None, 2907)              735471    
                                                                 
 batch_normalization_26 (Bat  (None, 2907)             11628     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2907)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3675712   
                                                                 
=================================================================
Total params: 8,845,618
Trainable params: 8,833,486
Non-trainable params: 12,132
_________________________________________________________________
Epoch 1/200
1493/1493 - 76s - loss: 0.0102 - val_loss: 0.0044 - 76s/epoch - 51ms/step
Epoch 2/200
1493/1493 - 75s - loss: 0.0036 - val_loss: 0.0035 - 75s/epoch - 50ms/step
Epoch 3/200
1493/1493 - 76s - loss: 0.0026 - val_loss: 0.0027 - 76s/epoch - 51ms/step
Epoch 4/200
1493/1493 - 76s - loss: 0.0022 - val_loss: 0.0032 - 76s/epoch - 51ms/step
Epoch 5/200
1493/1493 - 75s - loss: 0.0021 - val_loss: 0.0018 - 75s/epoch - 51ms/step
Epoch 6/200
1493/1493 - 75s - loss: 0.0019 - val_loss: 0.0018 - 75s/epoch - 50ms/step
Epoch 7/200
1493/1493 - 76s - loss: 0.0017 - val_loss: 0.0014 - 76s/epoch - 51ms/step
Epoch 8/200
1493/1493 - 76s - loss: 0.0015 - val_loss: 0.0016 - 76s/epoch - 51ms/step
Epoch 9/200
1493/1493 - 76s - loss: 0.0013 - val_loss: 0.0026 - 76s/epoch - 51ms/step
Epoch 10/200
1493/1493 - 75s - loss: 0.0014 - val_loss: 0.0010 - 75s/epoch - 51ms/step
Epoch 11/200
1493/1493 - 75s - loss: 0.0011 - val_loss: 9.5537e-04 - 75s/epoch - 50ms/step
Epoch 12/200
1493/1493 - 75s - loss: 9.5890e-04 - val_loss: 0.0012 - 75s/epoch - 51ms/step
Epoch 13/200
1493/1493 - 75s - loss: 9.2905e-04 - val_loss: 0.0016 - 75s/epoch - 51ms/step
Epoch 14/200
1493/1493 - 76s - loss: 8.6915e-04 - val_loss: 7.8325e-04 - 76s/epoch - 51ms/step
Epoch 15/200
1493/1493 - 75s - loss: 7.9933e-04 - val_loss: 7.7659e-04 - 75s/epoch - 50ms/step
Epoch 16/200
1493/1493 - 75s - loss: 7.2352e-04 - val_loss: 6.6206e-04 - 75s/epoch - 51ms/step
Epoch 17/200
1493/1493 - 76s - loss: 6.6936e-04 - val_loss: 8.2716e-04 - 76s/epoch - 51ms/step
Epoch 18/200
1493/1493 - 76s - loss: 6.5560e-04 - val_loss: 6.9710e-04 - 76s/epoch - 51ms/step
Epoch 19/200
1493/1493 - 75s - loss: 6.1784e-04 - val_loss: 8.0328e-04 - 75s/epoch - 50ms/step
Epoch 20/200
1493/1493 - 76s - loss: 6.0579e-04 - val_loss: 5.3550e-04 - 76s/epoch - 51ms/step
Epoch 21/200
1493/1493 - 76s - loss: 5.6379e-04 - val_loss: 6.8571e-04 - 76s/epoch - 51ms/step
Epoch 22/200
1493/1493 - 76s - loss: 5.5404e-04 - val_loss: 8.1285e-04 - 76s/epoch - 51ms/step
Epoch 23/200
1493/1493 - 75s - loss: 5.9563e-04 - val_loss: 0.0011 - 75s/epoch - 51ms/step
Epoch 24/200
1493/1493 - 75s - loss: 5.7284e-04 - val_loss: 4.8034e-04 - 75s/epoch - 50ms/step
Epoch 25/200
1493/1493 - 76s - loss: 5.0105e-04 - val_loss: 5.2579e-04 - 76s/epoch - 51ms/step
Epoch 26/200
1493/1493 - 76s - loss: 4.8206e-04 - val_loss: 6.2924e-04 - 76s/epoch - 51ms/step
Epoch 27/200
1493/1493 - 76s - loss: 4.6611e-04 - val_loss: 4.3333e-04 - 76s/epoch - 51ms/step
Epoch 28/200
1493/1493 - 75s - loss: 4.4696e-04 - val_loss: 5.5503e-04 - 75s/epoch - 50ms/step
Epoch 29/200
1493/1493 - 75s - loss: 4.4570e-04 - val_loss: 5.7525e-04 - 75s/epoch - 51ms/step
Epoch 30/200
1493/1493 - 75s - loss: 4.5852e-04 - val_loss: 4.7284e-04 - 75s/epoch - 51ms/step
Epoch 31/200
1493/1493 - 76s - loss: 4.2354e-04 - val_loss: 5.4995e-04 - 76s/epoch - 51ms/step
Epoch 32/200
1493/1493 - 75s - loss: 4.3038e-04 - val_loss: 4.9664e-04 - 75s/epoch - 50ms/step
Epoch 33/200
1493/1493 - 75s - loss: 4.1392e-04 - val_loss: 3.8087e-04 - 75s/epoch - 51ms/step
Epoch 34/200
1493/1493 - 76s - loss: 3.9357e-04 - val_loss: 3.8057e-04 - 76s/epoch - 51ms/step
Epoch 35/200
1493/1493 - 76s - loss: 3.8353e-04 - val_loss: 5.0430e-04 - 76s/epoch - 51ms/step
Epoch 36/200
1493/1493 - 75s - loss: 4.0129e-04 - val_loss: 4.3953e-04 - 75s/epoch - 51ms/step
Epoch 37/200
1493/1493 - 75s - loss: 3.7554e-04 - val_loss: 3.9932e-04 - 75s/epoch - 50ms/step
Epoch 38/200
1493/1493 - 75s - loss: 3.7823e-04 - val_loss: 3.6686e-04 - 75s/epoch - 51ms/step
Epoch 39/200
1493/1493 - 75s - loss: 3.6109e-04 - val_loss: 3.4056e-04 - 75s/epoch - 51ms/step
Epoch 40/200
1493/1493 - 75s - loss: 3.5078e-04 - val_loss: 4.1934e-04 - 75s/epoch - 51ms/step
Epoch 41/200
1493/1493 - 75s - loss: 3.5062e-04 - val_loss: 3.2015e-04 - 75s/epoch - 50ms/step
Epoch 42/200
1493/1493 - 75s - loss: 3.4512e-04 - val_loss: 3.4421e-04 - 75s/epoch - 51ms/step
Epoch 43/200
1493/1493 - 75s - loss: 3.4676e-04 - val_loss: 4.4227e-04 - 75s/epoch - 51ms/step
Epoch 44/200
1493/1493 - 76s - loss: 3.5061e-04 - val_loss: 0.0012 - 76s/epoch - 51ms/step
Epoch 45/200
1493/1493 - 75s - loss: 4.2509e-04 - val_loss: 2.9812e-04 - 75s/epoch - 50ms/step
Epoch 46/200
1493/1493 - 76s - loss: 3.3072e-04 - val_loss: 3.0191e-04 - 76s/epoch - 51ms/step
Epoch 47/200
1493/1493 - 76s - loss: 3.2708e-04 - val_loss: 3.4151e-04 - 76s/epoch - 51ms/step
Epoch 48/200
1493/1493 - 75s - loss: 3.2267e-04 - val_loss: 3.2601e-04 - 75s/epoch - 51ms/step
Epoch 49/200
1493/1493 - 76s - loss: 3.2061e-04 - val_loss: 6.1229e-04 - 76s/epoch - 51ms/step
Epoch 50/200
1493/1493 - 75s - loss: 3.7548e-04 - val_loss: 6.2435e-04 - 75s/epoch - 50ms/step
Epoch 51/200
1493/1493 - 75s - loss: 3.4125e-04 - val_loss: 3.0277e-04 - 75s/epoch - 51ms/step
Epoch 52/200
1493/1493 - 75s - loss: 3.0842e-04 - val_loss: 3.0171e-04 - 75s/epoch - 50ms/step
Epoch 53/200
1493/1493 - 75s - loss: 3.0780e-04 - val_loss: 2.9069e-04 - 75s/epoch - 51ms/step
Epoch 54/200
1493/1493 - 75s - loss: 3.0506e-04 - val_loss: 3.0533e-04 - 75s/epoch - 50ms/step
Epoch 55/200
1493/1493 - 75s - loss: 2.9640e-04 - val_loss: 2.8399e-04 - 75s/epoch - 51ms/step
Epoch 56/200
1493/1493 - 75s - loss: 2.9480e-04 - val_loss: 3.3002e-04 - 75s/epoch - 50ms/step
Epoch 57/200
1493/1493 - 75s - loss: 2.9869e-04 - val_loss: 2.8884e-04 - 75s/epoch - 50ms/step
Epoch 58/200
1493/1493 - 75s - loss: 2.8992e-04 - val_loss: 2.8230e-04 - 75s/epoch - 50ms/step
Epoch 59/200
1493/1493 - 75s - loss: 2.8430e-04 - val_loss: 2.8886e-04 - 75s/epoch - 50ms/step
Epoch 60/200
1493/1493 - 75s - loss: 2.8414e-04 - val_loss: 3.9924e-04 - 75s/epoch - 50ms/step
Epoch 61/200
1493/1493 - 75s - loss: 3.1378e-04 - val_loss: 2.7538e-04 - 75s/epoch - 51ms/step
Epoch 62/200
1493/1493 - 75s - loss: 2.8669e-04 - val_loss: 2.7515e-04 - 75s/epoch - 50ms/step
Epoch 63/200
1493/1493 - 75s - loss: 2.7739e-04 - val_loss: 2.9174e-04 - 75s/epoch - 50ms/step
Epoch 64/200
1493/1493 - 75s - loss: 2.8132e-04 - val_loss: 8.3329e-04 - 75s/epoch - 50ms/step
Epoch 65/200
1493/1493 - 75s - loss: 3.7364e-04 - val_loss: 6.5011e-04 - 75s/epoch - 50ms/step
Epoch 66/200
1493/1493 - 75s - loss: 3.1825e-04 - val_loss: 2.8470e-04 - 75s/epoch - 51ms/step
Epoch 67/200
1493/1493 - 75s - loss: 2.7652e-04 - val_loss: 2.5521e-04 - 75s/epoch - 50ms/step
Epoch 68/200
1493/1493 - 75s - loss: 2.7355e-04 - val_loss: 4.1972e-04 - 75s/epoch - 51ms/step
Epoch 69/200
1493/1493 - 75s - loss: 3.2418e-04 - val_loss: 2.8302e-04 - 75s/epoch - 51ms/step
Epoch 70/200
1493/1493 - 75s - loss: 2.7484e-04 - val_loss: 3.9979e-04 - 75s/epoch - 50ms/step
Epoch 71/200
1493/1493 - 75s - loss: 3.0235e-04 - val_loss: 2.5598e-04 - 75s/epoch - 50ms/step
Epoch 72/200
1493/1493 - 75s - loss: 2.7258e-04 - val_loss: 2.7651e-04 - 75s/epoch - 50ms/step
Epoch 73/200
1493/1493 - 75s - loss: 2.7171e-04 - val_loss: 3.7175e-04 - 75s/epoch - 51ms/step
Epoch 74/200
1493/1493 - 75s - loss: 2.8830e-04 - val_loss: 2.6135e-04 - 75s/epoch - 51ms/step
Epoch 75/200
1493/1493 - 75s - loss: 2.6152e-04 - val_loss: 2.5444e-04 - 75s/epoch - 50ms/step
Epoch 76/200
1493/1493 - 75s - loss: 2.5880e-04 - val_loss: 2.6415e-04 - 75s/epoch - 50ms/step
Epoch 77/200
1493/1493 - 75s - loss: 2.5521e-04 - val_loss: 3.7398e-04 - 75s/epoch - 51ms/step
Epoch 78/200
1493/1493 - 75s - loss: 2.7841e-04 - val_loss: 2.4068e-04 - 75s/epoch - 51ms/step
Epoch 79/200
1493/1493 - 75s - loss: 2.5412e-04 - val_loss: 2.6512e-04 - 75s/epoch - 50ms/step
Epoch 80/200
1493/1493 - 75s - loss: 2.5515e-04 - val_loss: 2.7976e-04 - 75s/epoch - 50ms/step
Epoch 81/200
1493/1493 - 75s - loss: 2.5561e-04 - val_loss: 3.3284e-04 - 75s/epoch - 50ms/step
Epoch 82/200
1493/1493 - 75s - loss: 2.5202e-04 - val_loss: 2.5035e-04 - 75s/epoch - 50ms/step
Epoch 83/200
1493/1493 - 75s - loss: 2.4584e-04 - val_loss: 2.4632e-04 - 75s/epoch - 50ms/step
Epoch 84/200
1493/1493 - 75s - loss: 2.4548e-04 - val_loss: 2.5502e-04 - 75s/epoch - 50ms/step
Epoch 85/200
1493/1493 - 75s - loss: 2.5070e-04 - val_loss: 2.8107e-04 - 75s/epoch - 50ms/step
Epoch 86/200
1493/1493 - 75s - loss: 2.5058e-04 - val_loss: 2.4515e-04 - 75s/epoch - 50ms/step
Epoch 87/200
1493/1493 - 75s - loss: 2.3952e-04 - val_loss: 2.3439e-04 - 75s/epoch - 50ms/step
Epoch 88/200
1493/1493 - 75s - loss: 2.3828e-04 - val_loss: 2.5322e-04 - 75s/epoch - 50ms/step
Epoch 89/200
1493/1493 - 75s - loss: 2.5141e-04 - val_loss: 2.4718e-04 - 75s/epoch - 50ms/step
Epoch 90/200
1493/1493 - 75s - loss: 2.3793e-04 - val_loss: 2.3456e-04 - 75s/epoch - 50ms/step
Epoch 91/200
1493/1493 - 75s - loss: 2.3629e-04 - val_loss: 2.7118e-04 - 75s/epoch - 50ms/step
Epoch 92/200
1493/1493 - 75s - loss: 2.3708e-04 - val_loss: 5.1583e-04 - 75s/epoch - 50ms/step
Epoch 93/200
1493/1493 - 75s - loss: 2.9038e-04 - val_loss: 2.3417e-04 - 75s/epoch - 50ms/step
Epoch 94/200
1493/1493 - 75s - loss: 2.3724e-04 - val_loss: 2.2665e-04 - 75s/epoch - 50ms/step
Epoch 95/200
1493/1493 - 75s - loss: 2.3470e-04 - val_loss: 2.6450e-04 - 75s/epoch - 50ms/step
Epoch 96/200
1493/1493 - 75s - loss: 2.3571e-04 - val_loss: 2.5375e-04 - 75s/epoch - 50ms/step
Epoch 97/200
1493/1493 - 75s - loss: 2.3180e-04 - val_loss: 2.3269e-04 - 75s/epoch - 50ms/step
Epoch 98/200
1493/1493 - 75s - loss: 2.3836e-04 - val_loss: 5.8543e-04 - 75s/epoch - 50ms/step
Epoch 99/200
1493/1493 - 75s - loss: 3.2243e-04 - val_loss: 2.2673e-04 - 75s/epoch - 50ms/step
Epoch 100/200
1493/1493 - 75s - loss: 2.4429e-04 - val_loss: 2.3258e-04 - 75s/epoch - 50ms/step
Epoch 101/200
1493/1493 - 75s - loss: 2.3518e-04 - val_loss: 2.6386e-04 - 75s/epoch - 51ms/step
Epoch 102/200
1493/1493 - 75s - loss: 2.3879e-04 - val_loss: 2.3907e-04 - 75s/epoch - 50ms/step
Epoch 103/200
1493/1493 - 75s - loss: 2.3286e-04 - val_loss: 2.2583e-04 - 75s/epoch - 50ms/step
Epoch 104/200
1493/1493 - 75s - loss: 2.2836e-04 - val_loss: 3.6817e-04 - 75s/epoch - 50ms/step
Epoch 105/200
1493/1493 - 75s - loss: 2.4733e-04 - val_loss: 2.1257e-04 - 75s/epoch - 50ms/step
Epoch 106/200
1493/1493 - 75s - loss: 2.2605e-04 - val_loss: 2.3651e-04 - 75s/epoch - 50ms/step
Epoch 107/200
1493/1493 - 75s - loss: 2.2457e-04 - val_loss: 2.3939e-04 - 75s/epoch - 50ms/step
Epoch 108/200
1493/1493 - 75s - loss: 2.2896e-04 - val_loss: 2.2562e-04 - 75s/epoch - 51ms/step
Epoch 109/200
1493/1493 - 75s - loss: 2.2408e-04 - val_loss: 2.3872e-04 - 75s/epoch - 50ms/step
Epoch 110/200
1493/1493 - 75s - loss: 2.2134e-04 - val_loss: 2.3314e-04 - 75s/epoch - 50ms/step
Epoch 111/200
1493/1493 - 75s - loss: 2.2094e-04 - val_loss: 2.5276e-04 - 75s/epoch - 50ms/step
Epoch 112/200
1493/1493 - 75s - loss: 2.2956e-04 - val_loss: 2.1547e-04 - 75s/epoch - 50ms/step
Epoch 113/200
1493/1493 - 75s - loss: 2.1663e-04 - val_loss: 2.5726e-04 - 75s/epoch - 50ms/step
Epoch 114/200
1493/1493 - 75s - loss: 2.2707e-04 - val_loss: 2.8296e-04 - 75s/epoch - 50ms/step
Epoch 115/200
1493/1493 - 75s - loss: 2.4227e-04 - val_loss: 2.8571e-04 - 75s/epoch - 50ms/step
Epoch 116/200
1493/1493 - 75s - loss: 2.2851e-04 - val_loss: 2.2508e-04 - 75s/epoch - 51ms/step
Epoch 117/200
1493/1493 - 75s - loss: 2.1709e-04 - val_loss: 2.5303e-04 - 75s/epoch - 51ms/step
Epoch 118/200
1493/1493 - 75s - loss: 2.2270e-04 - val_loss: 2.1639e-04 - 75s/epoch - 50ms/step
Epoch 119/200
1493/1493 - 75s - loss: 2.1600e-04 - val_loss: 2.2039e-04 - 75s/epoch - 50ms/step
Epoch 120/200
1493/1493 - 75s - loss: 2.1690e-04 - val_loss: 2.1779e-04 - 75s/epoch - 51ms/step
Epoch 121/200
1493/1493 - 75s - loss: 2.1529e-04 - val_loss: 2.8346e-04 - 75s/epoch - 51ms/step
Epoch 122/200
1493/1493 - 75s - loss: 2.1778e-04 - val_loss: 2.1245e-04 - 75s/epoch - 50ms/step
Epoch 123/200
1493/1493 - 75s - loss: 2.1105e-04 - val_loss: 2.1891e-04 - 75s/epoch - 50ms/step
Epoch 124/200
1493/1493 - 75s - loss: 2.1334e-04 - val_loss: 3.7414e-04 - 75s/epoch - 50ms/step
Epoch 125/200
1493/1493 - 75s - loss: 2.4454e-04 - val_loss: 2.0833e-04 - 75s/epoch - 50ms/step
Epoch 126/200
1493/1493 - 75s - loss: 2.1388e-04 - val_loss: 4.7990e-04 - 75s/epoch - 50ms/step
Epoch 127/200
1493/1493 - 75s - loss: 2.3582e-04 - val_loss: 2.4290e-04 - 75s/epoch - 51ms/step
Epoch 128/200
1493/1493 - 75s - loss: 2.1150e-04 - val_loss: 2.0823e-04 - 75s/epoch - 50ms/step
Epoch 129/200
1493/1493 - 75s - loss: 2.0993e-04 - val_loss: 2.1439e-04 - 75s/epoch - 50ms/step
Epoch 130/200
1493/1493 - 75s - loss: 2.0952e-04 - val_loss: 2.0828e-04 - 75s/epoch - 50ms/step
Epoch 131/200
1493/1493 - 75s - loss: 2.1744e-04 - val_loss: 5.2455e-04 - 75s/epoch - 50ms/step
Epoch 132/200
1493/1493 - 75s - loss: 2.7785e-04 - val_loss: 2.0425e-04 - 75s/epoch - 50ms/step
Epoch 133/200
1493/1493 - 75s - loss: 2.1411e-04 - val_loss: 2.0464e-04 - 75s/epoch - 50ms/step
Epoch 134/200
1493/1493 - 75s - loss: 2.0920e-04 - val_loss: 2.2580e-04 - 75s/epoch - 50ms/step
Epoch 135/200
1493/1493 - 75s - loss: 2.0745e-04 - val_loss: 2.0450e-04 - 75s/epoch - 50ms/step
Epoch 136/200
1493/1493 - 75s - loss: 2.0670e-04 - val_loss: 2.1559e-04 - 75s/epoch - 50ms/step
Epoch 137/200
1493/1493 - 75s - loss: 2.2102e-04 - val_loss: 4.1729e-04 - 75s/epoch - 50ms/step
Epoch 138/200
1493/1493 - 75s - loss: 2.5856e-04 - val_loss: 2.4037e-04 - 75s/epoch - 50ms/step
Epoch 139/200
1493/1493 - 75s - loss: 2.1930e-04 - val_loss: 3.1382e-04 - 75s/epoch - 50ms/step
Epoch 140/200
1493/1493 - 75s - loss: 2.3300e-04 - val_loss: 1.9832e-04 - 75s/epoch - 50ms/step
Epoch 141/200
1493/1493 - 75s - loss: 2.1156e-04 - val_loss: 3.3447e-04 - 75s/epoch - 50ms/step
Epoch 142/200
1493/1493 - 75s - loss: 2.3691e-04 - val_loss: 2.0397e-04 - 75s/epoch - 51ms/step
Epoch 143/200
1493/1493 - 75s - loss: 2.0782e-04 - val_loss: 2.2424e-04 - 75s/epoch - 50ms/step
Epoch 144/200
1493/1493 - 75s - loss: 2.0922e-04 - val_loss: 2.0970e-04 - 75s/epoch - 50ms/step
Epoch 145/200
1493/1493 - 75s - loss: 2.0510e-04 - val_loss: 2.1339e-04 - 75s/epoch - 50ms/step
Epoch 146/200
1493/1493 - 75s - loss: 2.0428e-04 - val_loss: 1.9984e-04 - 75s/epoch - 51ms/step
Epoch 147/200
1493/1493 - 75s - loss: 2.0110e-04 - val_loss: 2.0122e-04 - 75s/epoch - 50ms/step
Epoch 148/200
1493/1493 - 75s - loss: 2.0131e-04 - val_loss: 2.5348e-04 - 75s/epoch - 51ms/step
Epoch 149/200
1493/1493 - 75s - loss: 1.9925e-04 - val_loss: 2.0614e-04 - 75s/epoch - 50ms/step
Epoch 150/200
1493/1493 - 75s - loss: 1.9821e-04 - val_loss: 2.0427e-04 - 75s/epoch - 50ms/step
Epoch 151/200
1493/1493 - 75s - loss: 2.0723e-04 - val_loss: 2.0131e-04 - 75s/epoch - 51ms/step
Epoch 152/200
1493/1493 - 75s - loss: 2.0103e-04 - val_loss: 2.9939e-04 - 75s/epoch - 50ms/step
Epoch 153/200
1493/1493 - 75s - loss: 2.1949e-04 - val_loss: 2.0723e-04 - 75s/epoch - 51ms/step
Epoch 154/200
1493/1493 - 75s - loss: 2.0111e-04 - val_loss: 3.0712e-04 - 75s/epoch - 50ms/step
Epoch 155/200
1493/1493 - 75s - loss: 2.2560e-04 - val_loss: 2.0115e-04 - 75s/epoch - 50ms/step
Epoch 156/200
1493/1493 - 75s - loss: 2.0215e-04 - val_loss: 2.0024e-04 - 75s/epoch - 50ms/step
Epoch 157/200
1493/1493 - 75s - loss: 1.9790e-04 - val_loss: 2.4045e-04 - 75s/epoch - 51ms/step
Epoch 158/200
1493/1493 - 75s - loss: 2.0612e-04 - val_loss: 1.9396e-04 - 75s/epoch - 50ms/step
Epoch 159/200
1493/1493 - 75s - loss: 1.9646e-04 - val_loss: 1.9953e-04 - 75s/epoch - 51ms/step
Epoch 160/200
1493/1493 - 75s - loss: 1.9849e-04 - val_loss: 2.7024e-04 - 75s/epoch - 51ms/step
Epoch 161/200
1493/1493 - 75s - loss: 2.3450e-04 - val_loss: 1.9704e-04 - 75s/epoch - 51ms/step
Epoch 162/200
1493/1493 - 75s - loss: 1.9767e-04 - val_loss: 1.9732e-04 - 75s/epoch - 51ms/step
Epoch 163/200
1493/1493 - 75s - loss: 1.9725e-04 - val_loss: 2.7899e-04 - 75s/epoch - 50ms/step
Epoch 164/200
1493/1493 - 75s - loss: 2.0841e-04 - val_loss: 1.9953e-04 - 75s/epoch - 51ms/step
Epoch 165/200
1493/1493 - 75s - loss: 2.0255e-04 - val_loss: 1.9428e-04 - 75s/epoch - 51ms/step
Epoch 166/200
1493/1493 - 75s - loss: 1.9485e-04 - val_loss: 1.9325e-04 - 75s/epoch - 50ms/step
Epoch 167/200
1493/1493 - 75s - loss: 1.9683e-04 - val_loss: 1.9292e-04 - 75s/epoch - 50ms/step
Epoch 168/200
1493/1493 - 75s - loss: 1.9362e-04 - val_loss: 1.9410e-04 - 75s/epoch - 51ms/step
Epoch 169/200
1493/1493 - 75s - loss: 1.9359e-04 - val_loss: 2.3014e-04 - 75s/epoch - 51ms/step
Epoch 170/200
1493/1493 - 75s - loss: 1.9675e-04 - val_loss: 3.6428e-04 - 75s/epoch - 51ms/step
Epoch 171/200
1493/1493 - 75s - loss: 2.4244e-04 - val_loss: 4.2460e-04 - 75s/epoch - 50ms/step
Epoch 172/200
1493/1493 - 75s - loss: 2.3754e-04 - val_loss: 2.0991e-04 - 75s/epoch - 50ms/step
Epoch 173/200
1493/1493 - 75s - loss: 2.0029e-04 - val_loss: 1.8484e-04 - 75s/epoch - 51ms/step
Epoch 174/200
1493/1493 - 75s - loss: 1.9424e-04 - val_loss: 2.0366e-04 - 75s/epoch - 51ms/step
Epoch 175/200
1493/1493 - 75s - loss: 1.9367e-04 - val_loss: 1.9819e-04 - 75s/epoch - 50ms/step
Epoch 176/200
1493/1493 - 75s - loss: 1.9556e-04 - val_loss: 2.0782e-04 - 75s/epoch - 50ms/step
Epoch 177/200
1493/1493 - 75s - loss: 1.9622e-04 - val_loss: 1.9467e-04 - 75s/epoch - 51ms/step
Epoch 178/200
1493/1493 - 75s - loss: 1.9043e-04 - val_loss: 1.9819e-04 - 75s/epoch - 50ms/step
Epoch 179/200
1493/1493 - 75s - loss: 1.8897e-04 - val_loss: 1.9682e-04 - 75s/epoch - 51ms/step
Epoch 180/200
1493/1493 - 75s - loss: 1.9554e-04 - val_loss: 2.1451e-04 - 75s/epoch - 50ms/step
Epoch 181/200
1493/1493 - 75s - loss: 1.9987e-04 - val_loss: 2.0661e-04 - 75s/epoch - 51ms/step
Epoch 182/200
1493/1493 - 75s - loss: 1.9343e-04 - val_loss: 1.9280e-04 - 75s/epoch - 51ms/step
Epoch 183/200
1493/1493 - 75s - loss: 1.8971e-04 - val_loss: 2.0956e-04 - 75s/epoch - 51ms/step
Epoch 184/200
1493/1493 - 75s - loss: 1.9038e-04 - val_loss: 1.8815e-04 - 75s/epoch - 50ms/step
Epoch 185/200
1493/1493 - 75s - loss: 1.8629e-04 - val_loss: 2.3856e-04 - 75s/epoch - 51ms/step
Epoch 186/200
1493/1493 - 75s - loss: 1.9077e-04 - val_loss: 1.8801e-04 - 75s/epoch - 51ms/step
Epoch 187/200
1493/1493 - 75s - loss: 1.8656e-04 - val_loss: 1.9820e-04 - 75s/epoch - 51ms/step
Epoch 188/200
1493/1493 - 75s - loss: 1.9126e-04 - val_loss: 2.1390e-04 - 75s/epoch - 51ms/step
Epoch 189/200
1493/1493 - 75s - loss: 1.8960e-04 - val_loss: 1.9224e-04 - 75s/epoch - 50ms/step
Epoch 190/200
1493/1493 - 75s - loss: 1.8743e-04 - val_loss: 2.1198e-04 - 75s/epoch - 51ms/step
Epoch 191/200
1493/1493 - 75s - loss: 1.9068e-04 - val_loss: 2.2267e-04 - 75s/epoch - 50ms/step
Epoch 192/200
1493/1493 - 75s - loss: 1.8859e-04 - val_loss: 1.9322e-04 - 75s/epoch - 50ms/step
Epoch 193/200
1493/1493 - 75s - loss: 1.8944e-04 - val_loss: 3.4349e-04 - 75s/epoch - 50ms/step
Epoch 194/200
1493/1493 - 75s - loss: 2.2302e-04 - val_loss: 5.0559e-04 - 75s/epoch - 50ms/step
Epoch 195/200
1493/1493 - 75s - loss: 2.3448e-04 - val_loss: 2.8337e-04 - 75s/epoch - 50ms/step
Epoch 196/200
1493/1493 - 75s - loss: 2.1610e-04 - val_loss: 1.7867e-04 - 75s/epoch - 51ms/step
Epoch 197/200
1493/1493 - 75s - loss: 1.8985e-04 - val_loss: 1.9285e-04 - 75s/epoch - 50ms/step
Epoch 198/200
1493/1493 - 75s - loss: 1.9130e-04 - val_loss: 2.3775e-04 - 75s/epoch - 51ms/step
Epoch 199/200
1493/1493 - 75s - loss: 1.9665e-04 - val_loss: 1.7815e-04 - 75s/epoch - 50ms/step
Epoch 200/200
1493/1493 - 75s - loss: 1.8605e-04 - val_loss: 1.9439e-04 - 75s/epoch - 50ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00019438994058873504
  1/332 [..............................] - ETA: 28s  8/332 [..............................] - ETA: 2s  15/332 [>.............................] - ETA: 2s 22/332 [>.............................] - ETA: 2s 29/332 [=>............................] - ETA: 2s 36/332 [==>...........................] - ETA: 2s 43/332 [==>...........................] - ETA: 2s 50/332 [===>..........................] - ETA: 2s 57/332 [====>.........................] - ETA: 2s 64/332 [====>.........................] - ETA: 1s 71/332 [=====>........................] - ETA: 1s 78/332 [======>.......................] - ETA: 1s 85/332 [======>.......................] - ETA: 1s 92/332 [=======>......................] - ETA: 1s 99/332 [=======>......................] - ETA: 1s106/332 [========>.....................] - ETA: 1s113/332 [=========>....................] - ETA: 1s120/332 [=========>....................] - ETA: 1s127/332 [==========>...................] - ETA: 1s134/332 [===========>..................] - ETA: 1s141/332 [===========>..................] - ETA: 1s148/332 [============>.................] - ETA: 1s155/332 [=============>................] - ETA: 1s162/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s176/332 [==============>...............] - ETA: 1s183/332 [===============>..............] - ETA: 1s190/332 [================>.............] - ETA: 1s197/332 [================>.............] - ETA: 1s204/332 [=================>............] - ETA: 0s211/332 [==================>...........] - ETA: 0s218/332 [==================>...........] - ETA: 0s225/332 [===================>..........] - ETA: 0s232/332 [===================>..........] - ETA: 0s239/332 [====================>.........] - ETA: 0s246/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s260/332 [======================>.......] - ETA: 0s267/332 [=======================>......] - ETA: 0s274/332 [=======================>......] - ETA: 0s281/332 [========================>.....] - ETA: 0s288/332 [=========================>....] - ETA: 0s295/332 [=========================>....] - ETA: 0s302/332 [==========================>...] - ETA: 0s309/332 [==========================>...] - ETA: 0s316/332 [===========================>..] - ETA: 0s323/332 [============================>.] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 7ms/step
correlation 0.0022372606337137868
cosine 0.0017610585244700362
MAE: 0.0075353347
RMSE: 0.013942371
r2: 0.9873900752813356
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_24"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_25 (InputLayer)       multiple                  0         
                                                                 
 dense_24 (Dense)            (None, 2907)              3677355   
                                                                 
 batch_normalization_24 (Bat  (None, 2907)             11628     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2907)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               732816    
                                                                 
 batch_normalization_25 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 252)               0         
                                                                 
 dense_25 (Dense)            (None, 2907)              735471    
                                                                 
 batch_normalization_26 (Bat  (None, 2907)             11628     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2907)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3675712   
                                                                 
=================================================================
Total params: 8,845,618
Trainable params: 8,833,486
Non-trainable params: 12,132
_________________________________________________________________
Encoder
Model: "model_25"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_26 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_25 (InputLayer)       multiple                  0         
                                                                 
 dense_24 (Dense)            (None, 2907)              3677355   
                                                                 
 batch_normalization_24 (Bat  (None, 2907)             11628     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2907)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               732816    
                                                                 
=================================================================
Total params: 4,421,799
Trainable params: 4,415,985
Non-trainable params: 5,814
_________________________________________________________________
Decoder
Model: "model_26"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_27 (InputLayer)       [(None, 252)]             0         
                                                                 
 batch_normalization_25 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 252)               0         
                                                                 
 dense_25 (Dense)            (None, 2907)              735471    
                                                                 
 batch_normalization_26 (Bat  (None, 2907)             11628     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2907)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3675712   
                                                                 
=================================================================
Total params: 4,423,819
Trainable params: 4,417,501
Non-trainable params: 6,318
_________________________________________________________________
['2.3custom_n_b', 'mse', 64, 200, 0.0005, 0.2, 252, 0.00018605144578032196, 0.00019438994058873504, 0.0022372606337137868, 0.0017610585244700362, 0.007535334676504135, 0.013942371122539043, 0.9873900752813356, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_27"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_27 (Dense)            (None, 3033)              3836745   
                                                                 
 batch_normalization_27 (Bat  (None, 3033)             12132     
 chNormalization)                                                
                                                                 
 re_lu_27 (ReLU)             (None, 3033)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               764568    
                                                                 
 batch_normalization_28 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_28 (ReLU)             (None, 252)               0         
                                                                 
 dense_28 (Dense)            (None, 3033)              767349    
                                                                 
 batch_normalization_29 (Bat  (None, 3033)             12132     
 chNormalization)                                                
                                                                 
 re_lu_29 (ReLU)             (None, 3033)              0         
                                                                 
 dense_29 (Dense)            (None, 1264)              3834976   
                                                                 
=================================================================
Total params: 9,228,910
Trainable params: 9,216,274
Non-trainable params: 12,636
_________________________________________________________________
Epoch 1/200
1493/1493 - 80s - loss: 0.0101 - val_loss: 0.0048 - 80s/epoch - 53ms/step
Epoch 2/200
1493/1493 - 79s - loss: 0.0035 - val_loss: 0.0036 - 79s/epoch - 53ms/step
Epoch 3/200
1493/1493 - 79s - loss: 0.0026 - val_loss: 0.0027 - 79s/epoch - 53ms/step
Epoch 4/200
1493/1493 - 79s - loss: 0.0022 - val_loss: 0.0033 - 79s/epoch - 53ms/step
Epoch 5/200
1493/1493 - 79s - loss: 0.0021 - val_loss: 0.0018 - 79s/epoch - 53ms/step
Epoch 6/200
1493/1493 - 79s - loss: 0.0019 - val_loss: 0.0022 - 79s/epoch - 53ms/step
Epoch 7/200
1493/1493 - 79s - loss: 0.0017 - val_loss: 0.0014 - 79s/epoch - 53ms/step
Epoch 8/200
1493/1493 - 79s - loss: 0.0015 - val_loss: 0.0015 - 79s/epoch - 53ms/step
Epoch 9/200
1493/1493 - 79s - loss: 0.0014 - val_loss: 0.0016 - 79s/epoch - 53ms/step
Epoch 10/200
1493/1493 - 79s - loss: 0.0013 - val_loss: 9.8160e-04 - 79s/epoch - 53ms/step
Epoch 11/200
1493/1493 - 79s - loss: 0.0011 - val_loss: 9.0684e-04 - 79s/epoch - 53ms/step
Epoch 12/200
1493/1493 - 79s - loss: 9.6082e-04 - val_loss: 0.0012 - 79s/epoch - 53ms/step
Epoch 13/200
1493/1493 - 79s - loss: 9.1388e-04 - val_loss: 0.0015 - 79s/epoch - 53ms/step
Epoch 14/200
1493/1493 - 79s - loss: 9.1669e-04 - val_loss: 6.8810e-04 - 79s/epoch - 53ms/step
Epoch 15/200
1493/1493 - 79s - loss: 7.8317e-04 - val_loss: 7.3652e-04 - 79s/epoch - 53ms/step
Epoch 16/200
1493/1493 - 79s - loss: 7.1781e-04 - val_loss: 6.0547e-04 - 79s/epoch - 53ms/step
Epoch 17/200
1493/1493 - 79s - loss: 6.7645e-04 - val_loss: 8.3448e-04 - 79s/epoch - 53ms/step
Epoch 18/200
1493/1493 - 79s - loss: 6.6312e-04 - val_loss: 7.6046e-04 - 79s/epoch - 53ms/step
Epoch 19/200
1493/1493 - 79s - loss: 6.3360e-04 - val_loss: 8.0441e-04 - 79s/epoch - 53ms/step
Epoch 20/200
1493/1493 - 79s - loss: 6.0875e-04 - val_loss: 5.3558e-04 - 79s/epoch - 53ms/step
Epoch 21/200
1493/1493 - 79s - loss: 5.6934e-04 - val_loss: 6.1626e-04 - 79s/epoch - 53ms/step
Epoch 22/200
1493/1493 - 79s - loss: 5.5233e-04 - val_loss: 8.3788e-04 - 79s/epoch - 53ms/step
Epoch 23/200
1493/1493 - 79s - loss: 6.0564e-04 - val_loss: 0.0017 - 79s/epoch - 53ms/step
Epoch 24/200
1493/1493 - 79s - loss: 6.1542e-04 - val_loss: 4.8756e-04 - 79s/epoch - 53ms/step
Epoch 25/200
1493/1493 - 79s - loss: 5.0838e-04 - val_loss: 4.7391e-04 - 79s/epoch - 53ms/step
Epoch 26/200
1493/1493 - 79s - loss: 4.8861e-04 - val_loss: 6.2823e-04 - 79s/epoch - 53ms/step
Epoch 27/200
1493/1493 - 79s - loss: 4.7181e-04 - val_loss: 4.4988e-04 - 79s/epoch - 53ms/step
Epoch 28/200
1493/1493 - 79s - loss: 4.5145e-04 - val_loss: 7.4021e-04 - 79s/epoch - 53ms/step
Epoch 29/200
1493/1493 - 79s - loss: 4.6222e-04 - val_loss: 4.0825e-04 - 79s/epoch - 53ms/step
Epoch 30/200
1493/1493 - 79s - loss: 4.2831e-04 - val_loss: 4.3341e-04 - 79s/epoch - 53ms/step
Epoch 31/200
1493/1493 - 79s - loss: 4.2382e-04 - val_loss: 6.4379e-04 - 79s/epoch - 53ms/step
Epoch 32/200
1493/1493 - 79s - loss: 4.7208e-04 - val_loss: 4.6061e-04 - 79s/epoch - 53ms/step
Epoch 33/200
1493/1493 - 79s - loss: 4.2851e-04 - val_loss: 3.7330e-04 - 79s/epoch - 53ms/step
Epoch 34/200
1493/1493 - 79s - loss: 3.9967e-04 - val_loss: 3.7396e-04 - 79s/epoch - 53ms/step
Epoch 35/200
1493/1493 - 79s - loss: 3.8806e-04 - val_loss: 5.4486e-04 - 79s/epoch - 53ms/step
Epoch 36/200
1493/1493 - 79s - loss: 4.1090e-04 - val_loss: 4.7225e-04 - 79s/epoch - 53ms/step
Epoch 37/200
1493/1493 - 79s - loss: 3.8305e-04 - val_loss: 6.8173e-04 - 79s/epoch - 53ms/step
Epoch 38/200
1493/1493 - 79s - loss: 4.2106e-04 - val_loss: 3.4783e-04 - 79s/epoch - 53ms/step
Epoch 39/200
1493/1493 - 79s - loss: 3.6688e-04 - val_loss: 3.5600e-04 - 79s/epoch - 53ms/step
Epoch 40/200
1493/1493 - 79s - loss: 3.5820e-04 - val_loss: 3.6909e-04 - 79s/epoch - 53ms/step
Epoch 41/200
1493/1493 - 79s - loss: 3.5573e-04 - val_loss: 3.2395e-04 - 79s/epoch - 53ms/step
Epoch 42/200
1493/1493 - 79s - loss: 3.5184e-04 - val_loss: 3.3097e-04 - 79s/epoch - 53ms/step
Epoch 43/200
1493/1493 - 79s - loss: 3.4883e-04 - val_loss: 4.2188e-04 - 79s/epoch - 53ms/step
Epoch 44/200
1493/1493 - 79s - loss: 3.5650e-04 - val_loss: 7.5294e-04 - 79s/epoch - 53ms/step
Epoch 45/200
1493/1493 - 79s - loss: 3.9653e-04 - val_loss: 3.0313e-04 - 79s/epoch - 53ms/step
Epoch 46/200
1493/1493 - 79s - loss: 3.3428e-04 - val_loss: 2.9878e-04 - 79s/epoch - 53ms/step
Epoch 47/200
1493/1493 - 79s - loss: 3.2996e-04 - val_loss: 3.1559e-04 - 79s/epoch - 53ms/step
Epoch 48/200
1493/1493 - 79s - loss: 3.2510e-04 - val_loss: 3.5361e-04 - 79s/epoch - 53ms/step
Epoch 49/200
1493/1493 - 79s - loss: 3.2738e-04 - val_loss: 6.2712e-04 - 79s/epoch - 53ms/step
Epoch 50/200
1493/1493 - 79s - loss: 3.7798e-04 - val_loss: 7.5813e-04 - 79s/epoch - 53ms/step
Epoch 51/200
1493/1493 - 79s - loss: 3.6193e-04 - val_loss: 3.2468e-04 - 79s/epoch - 53ms/step
Epoch 52/200
1493/1493 - 79s - loss: 3.1415e-04 - val_loss: 3.1986e-04 - 79s/epoch - 53ms/step
Epoch 53/200
1493/1493 - 79s - loss: 3.1366e-04 - val_loss: 2.9068e-04 - 79s/epoch - 53ms/step
Epoch 54/200
1493/1493 - 79s - loss: 3.0603e-04 - val_loss: 3.0745e-04 - 79s/epoch - 53ms/step
Epoch 55/200
1493/1493 - 79s - loss: 2.9990e-04 - val_loss: 3.0188e-04 - 79s/epoch - 53ms/step
Epoch 56/200
1493/1493 - 79s - loss: 2.9737e-04 - val_loss: 3.1348e-04 - 79s/epoch - 53ms/step
Epoch 57/200
1493/1493 - 79s - loss: 2.9454e-04 - val_loss: 2.8984e-04 - 79s/epoch - 53ms/step
Epoch 58/200
1493/1493 - 79s - loss: 2.9268e-04 - val_loss: 2.8178e-04 - 79s/epoch - 53ms/step
Epoch 59/200
1493/1493 - 79s - loss: 2.8708e-04 - val_loss: 2.9725e-04 - 79s/epoch - 53ms/step
Epoch 60/200
1493/1493 - 79s - loss: 2.8884e-04 - val_loss: 5.1925e-04 - 79s/epoch - 53ms/step
Epoch 61/200
1493/1493 - 79s - loss: 3.6770e-04 - val_loss: 2.7482e-04 - 79s/epoch - 53ms/step
Epoch 62/200
1493/1493 - 79s - loss: 2.9452e-04 - val_loss: 2.8555e-04 - 79s/epoch - 53ms/step
Epoch 63/200
1493/1493 - 79s - loss: 2.8179e-04 - val_loss: 2.8934e-04 - 79s/epoch - 53ms/step
Epoch 64/200
1493/1493 - 79s - loss: 2.8228e-04 - val_loss: 8.5052e-04 - 79s/epoch - 53ms/step
Epoch 65/200
1493/1493 - 79s - loss: 3.4918e-04 - val_loss: 5.3701e-04 - 79s/epoch - 53ms/step
Epoch 66/200
1493/1493 - 79s - loss: 3.0829e-04 - val_loss: 2.6745e-04 - 79s/epoch - 53ms/step
Epoch 67/200
1493/1493 - 79s - loss: 2.7841e-04 - val_loss: 2.5873e-04 - 79s/epoch - 53ms/step
Epoch 68/200
1493/1493 - 79s - loss: 2.7369e-04 - val_loss: 3.2159e-04 - 79s/epoch - 53ms/step
Epoch 69/200
1493/1493 - 79s - loss: 2.8430e-04 - val_loss: 2.6749e-04 - 79s/epoch - 53ms/step
Epoch 70/200
1493/1493 - 79s - loss: 2.7145e-04 - val_loss: 3.7313e-04 - 79s/epoch - 53ms/step
Epoch 71/200
1493/1493 - 79s - loss: 3.2855e-04 - val_loss: 2.6722e-04 - 79s/epoch - 53ms/step
Epoch 72/200
1493/1493 - 79s - loss: 2.7380e-04 - val_loss: 2.6568e-04 - 79s/epoch - 53ms/step
Epoch 73/200
1493/1493 - 79s - loss: 2.7109e-04 - val_loss: 2.6993e-04 - 79s/epoch - 53ms/step
Epoch 74/200
1493/1493 - 79s - loss: 2.7327e-04 - val_loss: 2.6222e-04 - 79s/epoch - 53ms/step
Epoch 75/200
1493/1493 - 79s - loss: 2.6136e-04 - val_loss: 2.5380e-04 - 79s/epoch - 53ms/step
Epoch 76/200
1493/1493 - 79s - loss: 2.5875e-04 - val_loss: 2.6565e-04 - 79s/epoch - 53ms/step
Epoch 77/200
1493/1493 - 79s - loss: 2.5780e-04 - val_loss: 3.5984e-04 - 79s/epoch - 53ms/step
Epoch 78/200
1493/1493 - 79s - loss: 2.7027e-04 - val_loss: 2.5720e-04 - 79s/epoch - 53ms/step
Epoch 79/200
1493/1493 - 79s - loss: 2.5906e-04 - val_loss: 3.4045e-04 - 79s/epoch - 53ms/step
Epoch 80/200
1493/1493 - 79s - loss: 2.6960e-04 - val_loss: 2.6051e-04 - 79s/epoch - 53ms/step
Epoch 81/200
1493/1493 - 79s - loss: 2.5597e-04 - val_loss: 3.0538e-04 - 79s/epoch - 53ms/step
Epoch 82/200
1493/1493 - 79s - loss: 2.5079e-04 - val_loss: 2.5648e-04 - 79s/epoch - 53ms/step
Epoch 83/200
1493/1493 - 79s - loss: 2.4785e-04 - val_loss: 2.4966e-04 - 79s/epoch - 53ms/step
Epoch 84/200
1493/1493 - 79s - loss: 2.4696e-04 - val_loss: 2.5901e-04 - 79s/epoch - 53ms/step
Epoch 85/200
1493/1493 - 79s - loss: 2.6230e-04 - val_loss: 2.7273e-04 - 79s/epoch - 53ms/step
Epoch 86/200
1493/1493 - 79s - loss: 2.5304e-04 - val_loss: 2.3637e-04 - 79s/epoch - 53ms/step
Epoch 87/200
1493/1493 - 79s - loss: 2.4284e-04 - val_loss: 2.3815e-04 - 79s/epoch - 53ms/step
Epoch 88/200
1493/1493 - 79s - loss: 2.4117e-04 - val_loss: 2.9386e-04 - 79s/epoch - 53ms/step
Epoch 89/200
1493/1493 - 79s - loss: 2.6255e-04 - val_loss: 2.5131e-04 - 79s/epoch - 53ms/step
Epoch 90/200
1493/1493 - 79s - loss: 2.3979e-04 - val_loss: 2.4256e-04 - 79s/epoch - 53ms/step
Epoch 91/200
1493/1493 - 79s - loss: 2.3811e-04 - val_loss: 2.5829e-04 - 79s/epoch - 53ms/step
Epoch 92/200
1493/1493 - 79s - loss: 2.3962e-04 - val_loss: 4.6021e-04 - 79s/epoch - 53ms/step
Epoch 93/200
1493/1493 - 79s - loss: 2.8506e-04 - val_loss: 2.4352e-04 - 79s/epoch - 53ms/step
Epoch 94/200
1493/1493 - 79s - loss: 2.4022e-04 - val_loss: 2.2389e-04 - 79s/epoch - 53ms/step
Epoch 95/200
1493/1493 - 79s - loss: 2.3665e-04 - val_loss: 2.5805e-04 - 79s/epoch - 53ms/step
Epoch 96/200
1493/1493 - 79s - loss: 2.3568e-04 - val_loss: 2.4474e-04 - 79s/epoch - 53ms/step
Epoch 97/200
1493/1493 - 79s - loss: 2.3368e-04 - val_loss: 2.2685e-04 - 79s/epoch - 53ms/step
Epoch 98/200
1493/1493 - 79s - loss: 2.4690e-04 - val_loss: 6.8386e-04 - 79s/epoch - 53ms/step
Epoch 99/200
1493/1493 - 79s - loss: 3.6183e-04 - val_loss: 2.2269e-04 - 79s/epoch - 53ms/step
Epoch 100/200
1493/1493 - 79s - loss: 2.5163e-04 - val_loss: 2.2737e-04 - 79s/epoch - 53ms/step
Epoch 101/200
1493/1493 - 79s - loss: 2.3738e-04 - val_loss: 2.3790e-04 - 79s/epoch - 53ms/step
Epoch 102/200
1493/1493 - 79s - loss: 2.3970e-04 - val_loss: 3.8196e-04 - 79s/epoch - 53ms/step
Epoch 103/200
1493/1493 - 79s - loss: 2.5039e-04 - val_loss: 2.2215e-04 - 79s/epoch - 53ms/step
Epoch 104/200
1493/1493 - 79s - loss: 2.3272e-04 - val_loss: 3.8886e-04 - 79s/epoch - 53ms/step
Epoch 105/200
1493/1493 - 79s - loss: 2.5186e-04 - val_loss: 2.2279e-04 - 79s/epoch - 53ms/step
Epoch 106/200
1493/1493 - 79s - loss: 2.2929e-04 - val_loss: 2.2616e-04 - 79s/epoch - 53ms/step
Epoch 107/200
1493/1493 - 79s - loss: 2.2788e-04 - val_loss: 2.4883e-04 - 79s/epoch - 53ms/step
Epoch 108/200
1493/1493 - 79s - loss: 2.3573e-04 - val_loss: 2.1977e-04 - 79s/epoch - 53ms/step
Epoch 109/200
1493/1493 - 79s - loss: 2.2751e-04 - val_loss: 2.2998e-04 - 79s/epoch - 53ms/step
Epoch 110/200
1493/1493 - 79s - loss: 2.2372e-04 - val_loss: 2.2675e-04 - 79s/epoch - 53ms/step
Epoch 111/200
1493/1493 - 79s - loss: 2.2364e-04 - val_loss: 2.9183e-04 - 79s/epoch - 53ms/step
Epoch 112/200
1493/1493 - 79s - loss: 2.4191e-04 - val_loss: 2.1831e-04 - 79s/epoch - 53ms/step
Epoch 113/200
1493/1493 - 79s - loss: 2.2823e-04 - val_loss: 5.6457e-04 - 79s/epoch - 53ms/step
Epoch 114/200
1493/1493 - 79s - loss: 2.8918e-04 - val_loss: 2.2440e-04 - 79s/epoch - 53ms/step
Epoch 115/200
1493/1493 - 79s - loss: 2.3252e-04 - val_loss: 2.9512e-04 - 79s/epoch - 53ms/step
Epoch 116/200
1493/1493 - 79s - loss: 2.2670e-04 - val_loss: 2.2222e-04 - 79s/epoch - 53ms/step
Epoch 117/200
1493/1493 - 79s - loss: 2.2216e-04 - val_loss: 2.7966e-04 - 79s/epoch - 53ms/step
Epoch 118/200
1493/1493 - 79s - loss: 2.3145e-04 - val_loss: 2.3337e-04 - 79s/epoch - 53ms/step
Epoch 119/200
1493/1493 - 79s - loss: 2.2192e-04 - val_loss: 2.3031e-04 - 79s/epoch - 53ms/step
Epoch 120/200
1493/1493 - 79s - loss: 2.1833e-04 - val_loss: 2.2461e-04 - 79s/epoch - 53ms/step
Epoch 121/200
1493/1493 - 79s - loss: 2.1903e-04 - val_loss: 2.8143e-04 - 79s/epoch - 53ms/step
Epoch 122/200
1493/1493 - 79s - loss: 2.2242e-04 - val_loss: 2.1472e-04 - 79s/epoch - 53ms/step
Epoch 123/200
1493/1493 - 79s - loss: 2.1490e-04 - val_loss: 2.2457e-04 - 79s/epoch - 53ms/step
Epoch 124/200
1493/1493 - 79s - loss: 2.1848e-04 - val_loss: 3.8786e-04 - 79s/epoch - 53ms/step
Epoch 125/200
1493/1493 - 79s - loss: 2.6265e-04 - val_loss: 2.1780e-04 - 79s/epoch - 53ms/step
Epoch 126/200
1493/1493 - 79s - loss: 2.3777e-04 - val_loss: 3.4205e-04 - 79s/epoch - 53ms/step
Epoch 127/200
1493/1493 - 79s - loss: 2.3394e-04 - val_loss: 3.0197e-04 - 79s/epoch - 53ms/step
Epoch 128/200
1493/1493 - 79s - loss: 2.3347e-04 - val_loss: 2.0823e-04 - 79s/epoch - 53ms/step
Epoch 129/200
1493/1493 - 79s - loss: 2.1582e-04 - val_loss: 2.1552e-04 - 79s/epoch - 53ms/step
Epoch 130/200
1493/1493 - 79s - loss: 2.1357e-04 - val_loss: 2.1220e-04 - 79s/epoch - 53ms/step
Epoch 131/200
1493/1493 - 79s - loss: 2.1890e-04 - val_loss: 3.7001e-04 - 79s/epoch - 53ms/step
Epoch 132/200
1493/1493 - 79s - loss: 2.7127e-04 - val_loss: 2.1450e-04 - 79s/epoch - 53ms/step
Epoch 133/200
1493/1493 - 79s - loss: 2.1940e-04 - val_loss: 2.2199e-04 - 79s/epoch - 53ms/step
Epoch 134/200
1493/1493 - 79s - loss: 2.1564e-04 - val_loss: 2.2289e-04 - 79s/epoch - 53ms/step
Epoch 135/200
1493/1493 - 79s - loss: 2.1245e-04 - val_loss: 2.0548e-04 - 79s/epoch - 53ms/step
Epoch 136/200
1493/1493 - 79s - loss: 2.1013e-04 - val_loss: 2.2613e-04 - 79s/epoch - 53ms/step
Epoch 137/200
1493/1493 - 79s - loss: 2.1861e-04 - val_loss: 2.8082e-04 - 79s/epoch - 53ms/step
Epoch 138/200
1493/1493 - 79s - loss: 2.3090e-04 - val_loss: 2.1212e-04 - 79s/epoch - 53ms/step
Epoch 139/200
1493/1493 - 79s - loss: 2.1191e-04 - val_loss: 2.9864e-04 - 79s/epoch - 53ms/step
Epoch 140/200
1493/1493 - 79s - loss: 2.3126e-04 - val_loss: 1.9936e-04 - 79s/epoch - 53ms/step
Epoch 141/200
1493/1493 - 79s - loss: 2.1379e-04 - val_loss: 3.4126e-04 - 79s/epoch - 53ms/step
Epoch 142/200
1493/1493 - 79s - loss: 2.4574e-04 - val_loss: 2.0847e-04 - 79s/epoch - 53ms/step
Epoch 143/200
1493/1493 - 79s - loss: 2.0986e-04 - val_loss: 2.1871e-04 - 79s/epoch - 53ms/step
Epoch 144/200
1493/1493 - 79s - loss: 2.1389e-04 - val_loss: 2.1427e-04 - 79s/epoch - 53ms/step
Epoch 145/200
1493/1493 - 79s - loss: 2.0707e-04 - val_loss: 2.0532e-04 - 79s/epoch - 53ms/step
Epoch 146/200
1493/1493 - 79s - loss: 2.0536e-04 - val_loss: 2.0034e-04 - 79s/epoch - 53ms/step
Epoch 147/200
1493/1493 - 79s - loss: 2.0391e-04 - val_loss: 2.1215e-04 - 79s/epoch - 53ms/step
Epoch 148/200
1493/1493 - 79s - loss: 2.0353e-04 - val_loss: 2.6648e-04 - 79s/epoch - 53ms/step
Epoch 149/200
1493/1493 - 79s - loss: 2.0174e-04 - val_loss: 2.1521e-04 - 79s/epoch - 53ms/step
Epoch 150/200
1493/1493 - 79s - loss: 2.0063e-04 - val_loss: 2.0135e-04 - 79s/epoch - 53ms/step
Epoch 151/200
1493/1493 - 79s - loss: 2.0502e-04 - val_loss: 2.0899e-04 - 79s/epoch - 53ms/step
Epoch 152/200
1493/1493 - 79s - loss: 2.0203e-04 - val_loss: 2.6977e-04 - 79s/epoch - 53ms/step
Epoch 153/200
1493/1493 - 79s - loss: 2.1080e-04 - val_loss: 2.1343e-04 - 79s/epoch - 53ms/step
Epoch 154/200
1493/1493 - 79s - loss: 1.9940e-04 - val_loss: 2.3013e-04 - 79s/epoch - 53ms/step
Epoch 155/200
1493/1493 - 78s - loss: 2.0345e-04 - val_loss: 2.1263e-04 - 78s/epoch - 52ms/step
Epoch 156/200
1493/1493 - 79s - loss: 2.0146e-04 - val_loss: 2.0964e-04 - 79s/epoch - 53ms/step
Epoch 157/200
1493/1493 - 79s - loss: 2.0024e-04 - val_loss: 3.5080e-04 - 79s/epoch - 53ms/step
Epoch 158/200
1493/1493 - 79s - loss: 2.4016e-04 - val_loss: 2.0113e-04 - 79s/epoch - 53ms/step
Epoch 159/200
1493/1493 - 79s - loss: 2.0132e-04 - val_loss: 2.0846e-04 - 79s/epoch - 53ms/step
Epoch 160/200
1493/1493 - 79s - loss: 1.9837e-04 - val_loss: 2.0458e-04 - 79s/epoch - 53ms/step
Epoch 161/200
1493/1493 - 79s - loss: 2.0260e-04 - val_loss: 2.0990e-04 - 79s/epoch - 53ms/step
Epoch 162/200
1493/1493 - 79s - loss: 1.9669e-04 - val_loss: 2.0076e-04 - 79s/epoch - 53ms/step
Epoch 163/200
1493/1493 - 79s - loss: 1.9759e-04 - val_loss: 2.5507e-04 - 79s/epoch - 53ms/step
Epoch 164/200
1493/1493 - 79s - loss: 2.0954e-04 - val_loss: 2.0677e-04 - 79s/epoch - 53ms/step
Epoch 165/200
1493/1493 - 79s - loss: 2.1259e-04 - val_loss: 2.0296e-04 - 79s/epoch - 53ms/step
Epoch 166/200
1493/1493 - 79s - loss: 1.9678e-04 - val_loss: 2.2168e-04 - 79s/epoch - 53ms/step
Epoch 167/200
1493/1493 - 79s - loss: 1.9596e-04 - val_loss: 1.9767e-04 - 79s/epoch - 53ms/step
Epoch 168/200
1493/1493 - 79s - loss: 1.9531e-04 - val_loss: 1.9495e-04 - 79s/epoch - 53ms/step
Epoch 169/200
1493/1493 - 79s - loss: 1.9499e-04 - val_loss: 2.1442e-04 - 79s/epoch - 53ms/step
Epoch 170/200
1493/1493 - 79s - loss: 1.9567e-04 - val_loss: 2.8294e-04 - 79s/epoch - 53ms/step
Epoch 171/200
1493/1493 - 79s - loss: 2.2488e-04 - val_loss: 3.7557e-04 - 79s/epoch - 53ms/step
Epoch 172/200
1493/1493 - 79s - loss: 2.3161e-04 - val_loss: 2.2125e-04 - 79s/epoch - 53ms/step
Epoch 173/200
1493/1493 - 79s - loss: 2.0088e-04 - val_loss: 1.9451e-04 - 79s/epoch - 53ms/step
Epoch 174/200
1493/1493 - 79s - loss: 1.9447e-04 - val_loss: 1.9500e-04 - 79s/epoch - 53ms/step
Epoch 175/200
1493/1493 - 79s - loss: 1.9405e-04 - val_loss: 2.0026e-04 - 79s/epoch - 53ms/step
Epoch 176/200
1493/1493 - 79s - loss: 1.9708e-04 - val_loss: 2.1613e-04 - 79s/epoch - 53ms/step
Epoch 177/200
1493/1493 - 79s - loss: 1.9973e-04 - val_loss: 1.9610e-04 - 79s/epoch - 53ms/step
Epoch 178/200
1493/1493 - 79s - loss: 1.9203e-04 - val_loss: 2.0499e-04 - 79s/epoch - 53ms/step
Epoch 179/200
1493/1493 - 79s - loss: 1.9008e-04 - val_loss: 1.9504e-04 - 79s/epoch - 53ms/step
Epoch 180/200
1493/1493 - 79s - loss: 1.9028e-04 - val_loss: 1.9241e-04 - 79s/epoch - 53ms/step
Epoch 181/200
1493/1493 - 79s - loss: 1.9026e-04 - val_loss: 2.1308e-04 - 79s/epoch - 53ms/step
Epoch 182/200
1493/1493 - 79s - loss: 1.9086e-04 - val_loss: 2.0736e-04 - 79s/epoch - 53ms/step
Epoch 183/200
1493/1493 - 79s - loss: 1.9232e-04 - val_loss: 2.4230e-04 - 79s/epoch - 53ms/step
Epoch 184/200
1493/1493 - 79s - loss: 1.9607e-04 - val_loss: 1.8629e-04 - 79s/epoch - 53ms/step
Epoch 185/200
1493/1493 - 79s - loss: 1.8697e-04 - val_loss: 2.0796e-04 - 79s/epoch - 53ms/step
Epoch 186/200
1493/1493 - 79s - loss: 1.8818e-04 - val_loss: 2.0433e-04 - 79s/epoch - 53ms/step
Epoch 187/200
1493/1493 - 79s - loss: 1.8735e-04 - val_loss: 2.3438e-04 - 79s/epoch - 53ms/step
Epoch 188/200
1493/1493 - 79s - loss: 1.9996e-04 - val_loss: 2.1399e-04 - 79s/epoch - 53ms/step
Epoch 189/200
1493/1493 - 79s - loss: 1.8994e-04 - val_loss: 1.9842e-04 - 79s/epoch - 53ms/step
Epoch 190/200
1493/1493 - 79s - loss: 1.8945e-04 - val_loss: 2.2720e-04 - 79s/epoch - 53ms/step
Epoch 191/200
1493/1493 - 79s - loss: 1.9158e-04 - val_loss: 2.4533e-04 - 79s/epoch - 53ms/step
Epoch 192/200
1493/1493 - 79s - loss: 1.9660e-04 - val_loss: 1.9961e-04 - 79s/epoch - 53ms/step
Epoch 193/200
1493/1493 - 79s - loss: 1.9421e-04 - val_loss: 3.7883e-04 - 79s/epoch - 53ms/step
Epoch 194/200
1493/1493 - 79s - loss: 2.5266e-04 - val_loss: 6.6793e-04 - 79s/epoch - 53ms/step
Epoch 195/200
1493/1493 - 79s - loss: 2.8936e-04 - val_loss: 4.2314e-04 - 79s/epoch - 53ms/step
Epoch 196/200
1493/1493 - 79s - loss: 2.4905e-04 - val_loss: 1.9035e-04 - 79s/epoch - 53ms/step
Epoch 197/200
1493/1493 - 79s - loss: 2.0125e-04 - val_loss: 3.0749e-04 - 79s/epoch - 53ms/step
Epoch 198/200
1493/1493 - 79s - loss: 2.1984e-04 - val_loss: 2.1564e-04 - 79s/epoch - 53ms/step
Epoch 199/200
1493/1493 - 79s - loss: 1.9424e-04 - val_loss: 1.8547e-04 - 79s/epoch - 53ms/step
Epoch 200/200
1493/1493 - 79s - loss: 1.9028e-04 - val_loss: 2.0167e-04 - 79s/epoch - 53ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00020166551985312253
  1/332 [..............................] - ETA: 28s  8/332 [..............................] - ETA: 2s  15/332 [>.............................] - ETA: 2s 22/332 [>.............................] - ETA: 2s 29/332 [=>............................] - ETA: 2s 36/332 [==>...........................] - ETA: 2s 43/332 [==>...........................] - ETA: 2s 50/332 [===>..........................] - ETA: 2s 57/332 [====>.........................] - ETA: 2s 64/332 [====>.........................] - ETA: 2s 71/332 [=====>........................] - ETA: 1s 78/332 [======>.......................] - ETA: 1s 85/332 [======>.......................] - ETA: 1s 92/332 [=======>......................] - ETA: 1s 99/332 [=======>......................] - ETA: 1s106/332 [========>.....................] - ETA: 1s113/332 [=========>....................] - ETA: 1s120/332 [=========>....................] - ETA: 1s127/332 [==========>...................] - ETA: 1s134/332 [===========>..................] - ETA: 1s141/332 [===========>..................] - ETA: 1s148/332 [============>.................] - ETA: 1s155/332 [=============>................] - ETA: 1s162/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s176/332 [==============>...............] - ETA: 1s183/332 [===============>..............] - ETA: 1s190/332 [================>.............] - ETA: 1s197/332 [================>.............] - ETA: 1s204/332 [=================>............] - ETA: 0s211/332 [==================>...........] - ETA: 0s218/332 [==================>...........] - ETA: 0s225/332 [===================>..........] - ETA: 0s232/332 [===================>..........] - ETA: 0s239/332 [====================>.........] - ETA: 0s246/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s260/332 [======================>.......] - ETA: 0s267/332 [=======================>......] - ETA: 0s274/332 [=======================>......] - ETA: 0s281/332 [========================>.....] - ETA: 0s288/332 [=========================>....] - ETA: 0s295/332 [=========================>....] - ETA: 0s302/332 [==========================>...] - ETA: 0s309/332 [==========================>...] - ETA: 0s316/332 [===========================>..] - ETA: 0s323/332 [============================>.] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 8ms/step
correlation 0.0023221182364856706
cosine 0.001826972213053856
MAE: 0.00763182
RMSE: 0.014200894
r2: 0.9869182968825797
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_27"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       multiple                  0         
                                                                 
 dense_27 (Dense)            (None, 3033)              3836745   
                                                                 
 batch_normalization_27 (Bat  (None, 3033)             12132     
 chNormalization)                                                
                                                                 
 re_lu_27 (ReLU)             (None, 3033)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               764568    
                                                                 
 batch_normalization_28 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_28 (ReLU)             (None, 252)               0         
                                                                 
 dense_28 (Dense)            (None, 3033)              767349    
                                                                 
 batch_normalization_29 (Bat  (None, 3033)             12132     
 chNormalization)                                                
                                                                 
 re_lu_29 (ReLU)             (None, 3033)              0         
                                                                 
 dense_29 (Dense)            (None, 1264)              3834976   
                                                                 
=================================================================
Total params: 9,228,910
Trainable params: 9,216,274
Non-trainable params: 12,636
_________________________________________________________________
Encoder
Model: "model_28"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_29 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_28 (InputLayer)       multiple                  0         
                                                                 
 dense_27 (Dense)            (None, 3033)              3836745   
                                                                 
 batch_normalization_27 (Bat  (None, 3033)             12132     
 chNormalization)                                                
                                                                 
 re_lu_27 (ReLU)             (None, 3033)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               764568    
                                                                 
=================================================================
Total params: 4,613,445
Trainable params: 4,607,379
Non-trainable params: 6,066
_________________________________________________________________
Decoder
Model: "model_29"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_30 (InputLayer)       [(None, 252)]             0         
                                                                 
 batch_normalization_28 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_28 (ReLU)             (None, 252)               0         
                                                                 
 dense_28 (Dense)            (None, 3033)              767349    
                                                                 
 batch_normalization_29 (Bat  (None, 3033)             12132     
 chNormalization)                                                
                                                                 
 re_lu_29 (ReLU)             (None, 3033)              0         
                                                                 
 dense_29 (Dense)            (None, 1264)              3834976   
                                                                 
=================================================================
Total params: 4,615,465
Trainable params: 4,608,895
Non-trainable params: 6,570
_________________________________________________________________
['2.4custom_n_b', 'mse', 64, 200, 0.0005, 0.2, 252, 0.00019027822418138385, 0.00020166551985312253, 0.0023221182364856706, 0.001826972213053856, 0.007631820160895586, 0.01420089416205883, 0.9869182968825797, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_30"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_31 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_30 (Dense)            (None, 3160)              3997400   
                                                                 
 batch_normalization_30 (Bat  (None, 3160)             12640     
 chNormalization)                                                
                                                                 
 re_lu_30 (ReLU)             (None, 3160)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               796572    
                                                                 
 batch_normalization_31 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_31 (ReLU)             (None, 252)               0         
                                                                 
 dense_31 (Dense)            (None, 3160)              799480    
                                                                 
 batch_normalization_32 (Bat  (None, 3160)             12640     
 chNormalization)                                                
                                                                 
 re_lu_32 (ReLU)             (None, 3160)              0         
                                                                 
 dense_32 (Dense)            (None, 1264)              3995504   
                                                                 
=================================================================
Total params: 9,615,244
Trainable params: 9,602,100
Non-trainable params: 13,144
_________________________________________________________________
Epoch 1/200
1493/1493 - 82s - loss: 0.0103 - val_loss: 0.0044 - 82s/epoch - 55ms/step
Epoch 2/200
1493/1493 - 81s - loss: 0.0036 - val_loss: 0.0037 - 81s/epoch - 54ms/step
Epoch 3/200
1493/1493 - 81s - loss: 0.0026 - val_loss: 0.0021 - 81s/epoch - 55ms/step
Epoch 4/200
1493/1493 - 81s - loss: 0.0022 - val_loss: 0.0029 - 81s/epoch - 54ms/step
Epoch 5/200
1493/1493 - 81s - loss: 0.0021 - val_loss: 0.0015 - 81s/epoch - 54ms/step
Epoch 6/200
1493/1493 - 81s - loss: 0.0018 - val_loss: 0.0018 - 81s/epoch - 54ms/step
Epoch 7/200
1493/1493 - 81s - loss: 0.0017 - val_loss: 0.0015 - 81s/epoch - 54ms/step
Epoch 8/200
1493/1493 - 81s - loss: 0.0015 - val_loss: 0.0013 - 81s/epoch - 54ms/step
Epoch 9/200
1493/1493 - 81s - loss: 0.0013 - val_loss: 0.0012 - 81s/epoch - 54ms/step
Epoch 10/200
1493/1493 - 81s - loss: 0.0012 - val_loss: 9.8685e-04 - 81s/epoch - 54ms/step
Epoch 11/200
1493/1493 - 81s - loss: 0.0011 - val_loss: 9.5406e-04 - 81s/epoch - 54ms/step
Epoch 12/200
1493/1493 - 81s - loss: 9.1310e-04 - val_loss: 9.4900e-04 - 81s/epoch - 54ms/step
Epoch 13/200
1493/1493 - 81s - loss: 8.4778e-04 - val_loss: 0.0014 - 81s/epoch - 54ms/step
Epoch 14/200
1493/1493 - 81s - loss: 8.5408e-04 - val_loss: 6.3798e-04 - 81s/epoch - 54ms/step
Epoch 15/200
1493/1493 - 81s - loss: 7.4781e-04 - val_loss: 6.7152e-04 - 81s/epoch - 54ms/step
Epoch 16/200
1493/1493 - 81s - loss: 6.8507e-04 - val_loss: 5.9155e-04 - 81s/epoch - 54ms/step
Epoch 17/200
1493/1493 - 81s - loss: 6.5164e-04 - val_loss: 8.0466e-04 - 81s/epoch - 54ms/step
Epoch 18/200
1493/1493 - 81s - loss: 6.2426e-04 - val_loss: 7.8471e-04 - 81s/epoch - 54ms/step
Epoch 19/200
1493/1493 - 81s - loss: 6.1217e-04 - val_loss: 6.5039e-04 - 81s/epoch - 54ms/step
Epoch 20/200
1493/1493 - 81s - loss: 5.6766e-04 - val_loss: 5.4976e-04 - 81s/epoch - 54ms/step
Epoch 21/200
1493/1493 - 81s - loss: 5.4399e-04 - val_loss: 6.3834e-04 - 81s/epoch - 54ms/step
Epoch 22/200
1493/1493 - 81s - loss: 5.3203e-04 - val_loss: 7.7280e-04 - 81s/epoch - 54ms/step
Epoch 23/200
1493/1493 - 81s - loss: 5.5459e-04 - val_loss: 6.6017e-04 - 81s/epoch - 54ms/step
Epoch 24/200
1493/1493 - 81s - loss: 5.2250e-04 - val_loss: 4.5203e-04 - 81s/epoch - 54ms/step
Epoch 25/200
1493/1493 - 81s - loss: 4.7538e-04 - val_loss: 4.7722e-04 - 81s/epoch - 54ms/step
Epoch 26/200
1493/1493 - 81s - loss: 4.6152e-04 - val_loss: 6.1774e-04 - 81s/epoch - 54ms/step
Epoch 27/200
1493/1493 - 81s - loss: 4.4628e-04 - val_loss: 4.4700e-04 - 81s/epoch - 54ms/step
Epoch 28/200
1493/1493 - 81s - loss: 4.2928e-04 - val_loss: 6.0154e-04 - 81s/epoch - 54ms/step
Epoch 29/200
1493/1493 - 81s - loss: 4.3474e-04 - val_loss: 5.1330e-04 - 81s/epoch - 54ms/step
Epoch 30/200
1493/1493 - 81s - loss: 4.3858e-04 - val_loss: 4.2645e-04 - 81s/epoch - 54ms/step
Epoch 31/200
1493/1493 - 81s - loss: 4.0814e-04 - val_loss: 4.7816e-04 - 81s/epoch - 54ms/step
Epoch 32/200
1493/1493 - 81s - loss: 4.2455e-04 - val_loss: 4.1048e-04 - 81s/epoch - 55ms/step
Epoch 33/200
1493/1493 - 81s - loss: 4.0013e-04 - val_loss: 3.6984e-04 - 81s/epoch - 54ms/step
Epoch 34/200
1493/1493 - 81s - loss: 3.8561e-04 - val_loss: 3.6643e-04 - 81s/epoch - 54ms/step
Epoch 35/200
1493/1493 - 81s - loss: 3.6976e-04 - val_loss: 5.3514e-04 - 81s/epoch - 54ms/step
Epoch 36/200
1493/1493 - 81s - loss: 3.9061e-04 - val_loss: 4.6724e-04 - 81s/epoch - 54ms/step
Epoch 37/200
1493/1493 - 81s - loss: 3.6456e-04 - val_loss: 3.8576e-04 - 81s/epoch - 55ms/step
Epoch 38/200
1493/1493 - 81s - loss: 3.7132e-04 - val_loss: 3.7602e-04 - 81s/epoch - 54ms/step
Epoch 39/200
1493/1493 - 81s - loss: 3.5091e-04 - val_loss: 3.6767e-04 - 81s/epoch - 54ms/step
Epoch 40/200
1493/1493 - 81s - loss: 3.4102e-04 - val_loss: 4.3160e-04 - 81s/epoch - 55ms/step
Epoch 41/200
1493/1493 - 81s - loss: 3.4013e-04 - val_loss: 3.1232e-04 - 81s/epoch - 54ms/step
Epoch 42/200
1493/1493 - 81s - loss: 3.3681e-04 - val_loss: 3.2338e-04 - 81s/epoch - 54ms/step
Epoch 43/200
1493/1493 - 81s - loss: 3.2908e-04 - val_loss: 3.7436e-04 - 81s/epoch - 54ms/step
Epoch 44/200
1493/1493 - 81s - loss: 3.3154e-04 - val_loss: 8.2491e-04 - 81s/epoch - 55ms/step
Epoch 45/200
1493/1493 - 81s - loss: 3.6647e-04 - val_loss: 3.0239e-04 - 81s/epoch - 54ms/step
Epoch 46/200
1493/1493 - 81s - loss: 3.1817e-04 - val_loss: 3.0945e-04 - 81s/epoch - 54ms/step
Epoch 47/200
1493/1493 - 81s - loss: 3.1345e-04 - val_loss: 3.1726e-04 - 81s/epoch - 54ms/step
Epoch 48/200
1493/1493 - 81s - loss: 3.1131e-04 - val_loss: 3.5193e-04 - 81s/epoch - 54ms/step
Epoch 49/200
1493/1493 - 81s - loss: 3.1113e-04 - val_loss: 7.0175e-04 - 81s/epoch - 54ms/step
Epoch 50/200
1493/1493 - 81s - loss: 3.5977e-04 - val_loss: 4.7949e-04 - 81s/epoch - 54ms/step
Epoch 51/200
1493/1493 - 81s - loss: 3.2354e-04 - val_loss: 3.0256e-04 - 81s/epoch - 54ms/step
Epoch 52/200
1493/1493 - 81s - loss: 2.9865e-04 - val_loss: 2.8657e-04 - 81s/epoch - 54ms/step
Epoch 53/200
1493/1493 - 81s - loss: 2.9783e-04 - val_loss: 2.8754e-04 - 81s/epoch - 54ms/step
Epoch 54/200
1493/1493 - 81s - loss: 2.9064e-04 - val_loss: 3.0609e-04 - 81s/epoch - 55ms/step
Epoch 55/200
1493/1493 - 81s - loss: 2.8682e-04 - val_loss: 2.9409e-04 - 81s/epoch - 54ms/step
Epoch 56/200
1493/1493 - 81s - loss: 2.8467e-04 - val_loss: 3.0026e-04 - 81s/epoch - 54ms/step
Epoch 57/200
1493/1493 - 81s - loss: 2.8275e-04 - val_loss: 3.1464e-04 - 81s/epoch - 55ms/step
Epoch 58/200
1493/1493 - 81s - loss: 2.7990e-04 - val_loss: 2.6756e-04 - 81s/epoch - 54ms/step
Epoch 59/200
1493/1493 - 81s - loss: 2.7611e-04 - val_loss: 2.9443e-04 - 81s/epoch - 54ms/step
Epoch 60/200
1493/1493 - 81s - loss: 2.7475e-04 - val_loss: 4.3675e-04 - 81s/epoch - 54ms/step
Epoch 61/200
1493/1493 - 81s - loss: 3.2903e-04 - val_loss: 2.7320e-04 - 81s/epoch - 54ms/step
Epoch 62/200
1493/1493 - 81s - loss: 2.8014e-04 - val_loss: 2.8965e-04 - 81s/epoch - 54ms/step
Epoch 63/200
1493/1493 - 81s - loss: 2.7082e-04 - val_loss: 3.2555e-04 - 81s/epoch - 54ms/step
Epoch 64/200
1493/1493 - 81s - loss: 2.7302e-04 - val_loss: 0.0011 - 81s/epoch - 54ms/step
Epoch 65/200
1493/1493 - 81s - loss: 3.7056e-04 - val_loss: 6.6845e-04 - 81s/epoch - 54ms/step
Epoch 66/200
1493/1493 - 81s - loss: 3.0747e-04 - val_loss: 2.7612e-04 - 81s/epoch - 54ms/step
Epoch 67/200
1493/1493 - 81s - loss: 2.6781e-04 - val_loss: 2.5421e-04 - 81s/epoch - 54ms/step
Epoch 68/200
1493/1493 - 81s - loss: 2.6368e-04 - val_loss: 4.2816e-04 - 81s/epoch - 54ms/step
Epoch 69/200
1493/1493 - 81s - loss: 2.9516e-04 - val_loss: 2.8081e-04 - 81s/epoch - 54ms/step
Epoch 70/200
1493/1493 - 81s - loss: 2.6320e-04 - val_loss: 4.4430e-04 - 81s/epoch - 54ms/step
Epoch 71/200
1493/1493 - 81s - loss: 3.0435e-04 - val_loss: 2.7366e-04 - 81s/epoch - 55ms/step
Epoch 72/200
1493/1493 - 81s - loss: 2.6384e-04 - val_loss: 2.5951e-04 - 81s/epoch - 54ms/step
Epoch 73/200
1493/1493 - 81s - loss: 2.5959e-04 - val_loss: 2.6905e-04 - 81s/epoch - 55ms/step
Epoch 74/200
1493/1493 - 81s - loss: 2.5709e-04 - val_loss: 2.6312e-04 - 81s/epoch - 55ms/step
Epoch 75/200
1493/1493 - 81s - loss: 2.5032e-04 - val_loss: 2.5566e-04 - 81s/epoch - 54ms/step
Epoch 76/200
1493/1493 - 81s - loss: 2.4758e-04 - val_loss: 2.6570e-04 - 81s/epoch - 54ms/step
Epoch 77/200
1493/1493 - 81s - loss: 2.4635e-04 - val_loss: 3.8095e-04 - 81s/epoch - 55ms/step
Epoch 78/200
1493/1493 - 81s - loss: 2.5836e-04 - val_loss: 2.4457e-04 - 81s/epoch - 55ms/step
Epoch 79/200
1493/1493 - 81s - loss: 2.4614e-04 - val_loss: 3.6263e-04 - 81s/epoch - 55ms/step
Epoch 80/200
1493/1493 - 81s - loss: 2.7522e-04 - val_loss: 2.6134e-04 - 81s/epoch - 54ms/step
Epoch 81/200
1493/1493 - 81s - loss: 2.5617e-04 - val_loss: 3.0423e-04 - 81s/epoch - 54ms/step
Epoch 82/200
1493/1493 - 81s - loss: 2.4541e-04 - val_loss: 2.4330e-04 - 81s/epoch - 55ms/step
Epoch 83/200
1493/1493 - 81s - loss: 2.3927e-04 - val_loss: 2.4487e-04 - 81s/epoch - 55ms/step
Epoch 84/200
1493/1493 - 81s - loss: 2.3890e-04 - val_loss: 2.5161e-04 - 81s/epoch - 54ms/step
Epoch 85/200
1493/1493 - 81s - loss: 2.4487e-04 - val_loss: 2.7230e-04 - 81s/epoch - 54ms/step
Epoch 86/200
1493/1493 - 81s - loss: 2.4011e-04 - val_loss: 2.4854e-04 - 81s/epoch - 54ms/step
Epoch 87/200
1493/1493 - 81s - loss: 2.3232e-04 - val_loss: 2.4285e-04 - 81s/epoch - 55ms/step
Epoch 88/200
1493/1493 - 81s - loss: 2.3034e-04 - val_loss: 2.5752e-04 - 81s/epoch - 54ms/step
Epoch 89/200
1493/1493 - 81s - loss: 2.5168e-04 - val_loss: 2.4181e-04 - 81s/epoch - 55ms/step
Epoch 90/200
1493/1493 - 81s - loss: 2.3026e-04 - val_loss: 2.3731e-04 - 81s/epoch - 55ms/step
Epoch 91/200
1493/1493 - 81s - loss: 2.2830e-04 - val_loss: 2.7499e-04 - 81s/epoch - 54ms/step
Epoch 92/200
1493/1493 - 81s - loss: 2.2809e-04 - val_loss: 4.3899e-04 - 81s/epoch - 54ms/step
Epoch 93/200
1493/1493 - 81s - loss: 2.6020e-04 - val_loss: 2.5154e-04 - 81s/epoch - 55ms/step
Epoch 94/200
1493/1493 - 81s - loss: 2.2944e-04 - val_loss: 2.1431e-04 - 81s/epoch - 54ms/step
Epoch 95/200
1493/1493 - 81s - loss: 2.2636e-04 - val_loss: 2.7378e-04 - 81s/epoch - 54ms/step
Epoch 96/200
1493/1493 - 81s - loss: 2.2724e-04 - val_loss: 2.5619e-04 - 81s/epoch - 54ms/step
Epoch 97/200
1493/1493 - 81s - loss: 2.2403e-04 - val_loss: 2.2496e-04 - 81s/epoch - 55ms/step
Epoch 98/200
1493/1493 - 81s - loss: 2.3105e-04 - val_loss: 6.1257e-04 - 81s/epoch - 54ms/step
Epoch 99/200
1493/1493 - 81s - loss: 3.1248e-04 - val_loss: 2.2341e-04 - 81s/epoch - 55ms/step
Epoch 100/200
1493/1493 - 81s - loss: 2.3744e-04 - val_loss: 2.2678e-04 - 81s/epoch - 54ms/step
Epoch 101/200
1493/1493 - 81s - loss: 2.2587e-04 - val_loss: 2.3288e-04 - 81s/epoch - 55ms/step
Epoch 102/200
1493/1493 - 81s - loss: 2.2570e-04 - val_loss: 2.5763e-04 - 81s/epoch - 55ms/step
Epoch 103/200
1493/1493 - 81s - loss: 2.2727e-04 - val_loss: 2.3353e-04 - 81s/epoch - 55ms/step
Epoch 104/200
1493/1493 - 81s - loss: 2.2007e-04 - val_loss: 3.8145e-04 - 81s/epoch - 54ms/step
Epoch 105/200
1493/1493 - 81s - loss: 2.4057e-04 - val_loss: 2.1345e-04 - 81s/epoch - 54ms/step
Epoch 106/200
1493/1493 - 82s - loss: 2.1870e-04 - val_loss: 2.2548e-04 - 82s/epoch - 55ms/step
Epoch 107/200
1493/1493 - 81s - loss: 2.1690e-04 - val_loss: 2.5833e-04 - 81s/epoch - 55ms/step
Epoch 108/200
1493/1493 - 81s - loss: 2.2068e-04 - val_loss: 2.2671e-04 - 81s/epoch - 54ms/step
Epoch 109/200
1493/1493 - 81s - loss: 2.1542e-04 - val_loss: 2.2944e-04 - 81s/epoch - 54ms/step
Epoch 110/200
1493/1493 - 82s - loss: 2.1322e-04 - val_loss: 2.2983e-04 - 82s/epoch - 55ms/step
Epoch 111/200
1493/1493 - 81s - loss: 2.1363e-04 - val_loss: 3.1541e-04 - 81s/epoch - 54ms/step
Epoch 112/200
1493/1493 - 81s - loss: 2.3994e-04 - val_loss: 2.1507e-04 - 81s/epoch - 54ms/step
Epoch 113/200
1493/1493 - 81s - loss: 2.1279e-04 - val_loss: 3.7826e-04 - 81s/epoch - 54ms/step
Epoch 114/200
1493/1493 - 81s - loss: 2.4100e-04 - val_loss: 2.4896e-04 - 81s/epoch - 54ms/step
Epoch 115/200
1493/1493 - 81s - loss: 2.1980e-04 - val_loss: 2.8252e-04 - 81s/epoch - 55ms/step
Epoch 116/200
1493/1493 - 81s - loss: 2.1711e-04 - val_loss: 2.2295e-04 - 81s/epoch - 55ms/step
Epoch 117/200
1493/1493 - 81s - loss: 2.0937e-04 - val_loss: 2.3635e-04 - 81s/epoch - 54ms/step
Epoch 118/200
1493/1493 - 81s - loss: 2.1314e-04 - val_loss: 2.1648e-04 - 81s/epoch - 55ms/step
Epoch 119/200
1493/1493 - 81s - loss: 2.0806e-04 - val_loss: 2.3479e-04 - 81s/epoch - 55ms/step
Epoch 120/200
1493/1493 - 81s - loss: 2.0723e-04 - val_loss: 2.1619e-04 - 81s/epoch - 55ms/step
Epoch 121/200
1493/1493 - 81s - loss: 2.0770e-04 - val_loss: 2.6119e-04 - 81s/epoch - 54ms/step
Epoch 122/200
1493/1493 - 81s - loss: 2.0867e-04 - val_loss: 2.1731e-04 - 81s/epoch - 55ms/step
Epoch 123/200
1493/1493 - 81s - loss: 2.0575e-04 - val_loss: 2.2719e-04 - 81s/epoch - 55ms/step
Epoch 124/200
1493/1493 - 81s - loss: 2.0297e-04 - val_loss: 2.5450e-04 - 81s/epoch - 54ms/step
Epoch 125/200
1493/1493 - 81s - loss: 2.1194e-04 - val_loss: 2.1464e-04 - 81s/epoch - 54ms/step
Epoch 126/200
1493/1493 - 81s - loss: 2.1232e-04 - val_loss: 2.6196e-04 - 81s/epoch - 55ms/step
Epoch 127/200
1493/1493 - 81s - loss: 2.1763e-04 - val_loss: 3.3792e-04 - 81s/epoch - 55ms/step
Epoch 128/200
1493/1493 - 81s - loss: 2.3005e-04 - val_loss: 1.9757e-04 - 81s/epoch - 55ms/step
Epoch 129/200
1493/1493 - 81s - loss: 2.0508e-04 - val_loss: 2.1890e-04 - 81s/epoch - 54ms/step
Epoch 130/200
1493/1493 - 81s - loss: 2.0281e-04 - val_loss: 2.0716e-04 - 81s/epoch - 55ms/step
Epoch 131/200
1493/1493 - 81s - loss: 2.0371e-04 - val_loss: 3.1543e-04 - 81s/epoch - 55ms/step
Epoch 132/200
1493/1493 - 81s - loss: 2.1923e-04 - val_loss: 2.1816e-04 - 81s/epoch - 55ms/step
Epoch 133/200
1493/1493 - 81s - loss: 2.0071e-04 - val_loss: 2.2110e-04 - 81s/epoch - 54ms/step
Epoch 134/200
1493/1493 - 81s - loss: 2.0116e-04 - val_loss: 2.2685e-04 - 81s/epoch - 54ms/step
Epoch 135/200
1493/1493 - 81s - loss: 1.9996e-04 - val_loss: 2.0890e-04 - 81s/epoch - 55ms/step
Epoch 136/200
1493/1493 - 81s - loss: 1.9898e-04 - val_loss: 2.7814e-04 - 81s/epoch - 55ms/step
Epoch 137/200
1493/1493 - 81s - loss: 2.3297e-04 - val_loss: 3.5710e-04 - 81s/epoch - 54ms/step
Epoch 138/200
1493/1493 - 81s - loss: 2.3630e-04 - val_loss: 1.9850e-04 - 81s/epoch - 54ms/step
Epoch 139/200
1493/1493 - 81s - loss: 2.0857e-04 - val_loss: 4.1176e-04 - 81s/epoch - 55ms/step
Epoch 140/200
1493/1493 - 81s - loss: 2.5197e-04 - val_loss: 2.0022e-04 - 81s/epoch - 55ms/step
Epoch 141/200
1493/1493 - 81s - loss: 2.0678e-04 - val_loss: 2.5875e-04 - 81s/epoch - 55ms/step
Epoch 142/200
1493/1493 - 81s - loss: 2.1602e-04 - val_loss: 2.0132e-04 - 81s/epoch - 54ms/step
Epoch 143/200
1493/1493 - 81s - loss: 1.9983e-04 - val_loss: 2.4502e-04 - 81s/epoch - 55ms/step
Epoch 144/200
1493/1493 - 81s - loss: 2.0853e-04 - val_loss: 2.0331e-04 - 81s/epoch - 55ms/step
Epoch 145/200
1493/1493 - 81s - loss: 1.9632e-04 - val_loss: 2.0193e-04 - 81s/epoch - 54ms/step
Epoch 146/200
1493/1493 - 81s - loss: 1.9616e-04 - val_loss: 1.9333e-04 - 81s/epoch - 54ms/step
Epoch 147/200
1493/1493 - 81s - loss: 1.9420e-04 - val_loss: 2.1310e-04 - 81s/epoch - 55ms/step
Epoch 148/200
1493/1493 - 81s - loss: 1.9387e-04 - val_loss: 2.6565e-04 - 81s/epoch - 55ms/step
Epoch 149/200
1493/1493 - 81s - loss: 1.9237e-04 - val_loss: 2.0605e-04 - 81s/epoch - 54ms/step
Epoch 150/200
1493/1493 - 81s - loss: 1.9154e-04 - val_loss: 1.9810e-04 - 81s/epoch - 54ms/step
Epoch 151/200
1493/1493 - 81s - loss: 1.9302e-04 - val_loss: 1.9430e-04 - 81s/epoch - 55ms/step
Epoch 152/200
1493/1493 - 81s - loss: 1.9154e-04 - val_loss: 2.6679e-04 - 81s/epoch - 54ms/step
Epoch 153/200
1493/1493 - 81s - loss: 2.0054e-04 - val_loss: 2.0790e-04 - 81s/epoch - 55ms/step
Epoch 154/200
1493/1493 - 81s - loss: 1.9143e-04 - val_loss: 2.6415e-04 - 81s/epoch - 54ms/step
Epoch 155/200
1493/1493 - 81s - loss: 2.0798e-04 - val_loss: 2.0844e-04 - 81s/epoch - 55ms/step
Epoch 156/200
1493/1493 - 81s - loss: 1.9386e-04 - val_loss: 2.0628e-04 - 81s/epoch - 55ms/step
Epoch 157/200
1493/1493 - 81s - loss: 1.8972e-04 - val_loss: 2.6160e-04 - 81s/epoch - 55ms/step
Epoch 158/200
1493/1493 - 81s - loss: 2.1230e-04 - val_loss: 2.0003e-04 - 81s/epoch - 54ms/step
Epoch 159/200
1493/1493 - 81s - loss: 1.9050e-04 - val_loss: 2.0508e-04 - 81s/epoch - 55ms/step
Epoch 160/200
1493/1493 - 81s - loss: 1.8964e-04 - val_loss: 2.4018e-04 - 81s/epoch - 55ms/step
Epoch 161/200
1493/1493 - 81s - loss: 2.0469e-04 - val_loss: 1.9126e-04 - 81s/epoch - 54ms/step
Epoch 162/200
1493/1493 - 81s - loss: 1.8872e-04 - val_loss: 2.0815e-04 - 81s/epoch - 54ms/step
Epoch 163/200
1493/1493 - 81s - loss: 1.8871e-04 - val_loss: 2.6938e-04 - 81s/epoch - 54ms/step
Epoch 164/200
1493/1493 - 81s - loss: 2.0297e-04 - val_loss: 2.0088e-04 - 81s/epoch - 55ms/step
Epoch 165/200
1493/1493 - 81s - loss: 2.0100e-04 - val_loss: 1.9296e-04 - 81s/epoch - 55ms/step
Epoch 166/200
1493/1493 - 81s - loss: 1.8836e-04 - val_loss: 2.0132e-04 - 81s/epoch - 54ms/step
Epoch 167/200
1493/1493 - 81s - loss: 1.8809e-04 - val_loss: 1.8361e-04 - 81s/epoch - 55ms/step
Epoch 168/200
1493/1493 - 81s - loss: 1.8587e-04 - val_loss: 1.8837e-04 - 81s/epoch - 55ms/step
Epoch 169/200
1493/1493 - 81s - loss: 1.8620e-04 - val_loss: 2.3163e-04 - 81s/epoch - 55ms/step
Epoch 170/200
1493/1493 - 81s - loss: 1.8972e-04 - val_loss: 3.5906e-04 - 81s/epoch - 54ms/step
Epoch 171/200
1493/1493 - 81s - loss: 2.4144e-04 - val_loss: 4.4461e-04 - 81s/epoch - 55ms/step
Epoch 172/200
1493/1493 - 81s - loss: 2.3335e-04 - val_loss: 1.9365e-04 - 81s/epoch - 55ms/step
Epoch 173/200
1493/1493 - 81s - loss: 1.9232e-04 - val_loss: 1.9095e-04 - 81s/epoch - 55ms/step
Epoch 174/200
1493/1493 - 81s - loss: 1.8788e-04 - val_loss: 2.0650e-04 - 81s/epoch - 54ms/step
Epoch 175/200
1493/1493 - 81s - loss: 1.8719e-04 - val_loss: 1.9682e-04 - 81s/epoch - 54ms/step
Epoch 176/200
1493/1493 - 81s - loss: 1.8777e-04 - val_loss: 1.9415e-04 - 81s/epoch - 55ms/step
Epoch 177/200
1493/1493 - 81s - loss: 1.9364e-04 - val_loss: 1.8526e-04 - 81s/epoch - 55ms/step
Epoch 178/200
1493/1493 - 81s - loss: 1.8401e-04 - val_loss: 1.8775e-04 - 81s/epoch - 55ms/step
Epoch 179/200
1493/1493 - 81s - loss: 1.8275e-04 - val_loss: 2.0118e-04 - 81s/epoch - 54ms/step
Epoch 180/200
1493/1493 - 81s - loss: 1.8341e-04 - val_loss: 2.0020e-04 - 81s/epoch - 55ms/step
Epoch 181/200
1493/1493 - 81s - loss: 1.8188e-04 - val_loss: 2.2551e-04 - 81s/epoch - 54ms/step
Epoch 182/200
1493/1493 - 81s - loss: 1.8444e-04 - val_loss: 1.8618e-04 - 81s/epoch - 55ms/step
Epoch 183/200
1493/1493 - 81s - loss: 1.8276e-04 - val_loss: 2.1086e-04 - 81s/epoch - 54ms/step
Epoch 184/200
1493/1493 - 81s - loss: 1.8436e-04 - val_loss: 1.8312e-04 - 81s/epoch - 54ms/step
Epoch 185/200
1493/1493 - 81s - loss: 1.7863e-04 - val_loss: 2.0799e-04 - 81s/epoch - 55ms/step
Epoch 186/200
1493/1493 - 81s - loss: 1.8077e-04 - val_loss: 1.9255e-04 - 81s/epoch - 54ms/step
Epoch 187/200
1493/1493 - 81s - loss: 1.7873e-04 - val_loss: 1.9721e-04 - 81s/epoch - 54ms/step
Epoch 188/200
1493/1493 - 81s - loss: 1.8226e-04 - val_loss: 2.0547e-04 - 81s/epoch - 54ms/step
Epoch 189/200
1493/1493 - 81s - loss: 1.7919e-04 - val_loss: 2.0465e-04 - 81s/epoch - 55ms/step
Epoch 190/200
1493/1493 - 81s - loss: 1.8075e-04 - val_loss: 2.0290e-04 - 81s/epoch - 55ms/step
Epoch 191/200
1493/1493 - 81s - loss: 1.8178e-04 - val_loss: 2.1109e-04 - 81s/epoch - 54ms/step
Epoch 192/200
1493/1493 - 81s - loss: 1.8162e-04 - val_loss: 1.9850e-04 - 81s/epoch - 55ms/step
Epoch 193/200
1493/1493 - 81s - loss: 1.8397e-04 - val_loss: 3.3503e-04 - 81s/epoch - 54ms/step
Epoch 194/200
1493/1493 - 81s - loss: 2.2697e-04 - val_loss: 3.2211e-04 - 81s/epoch - 54ms/step
Epoch 195/200
1493/1493 - 81s - loss: 2.2696e-04 - val_loss: 1.8472e-04 - 81s/epoch - 54ms/step
Epoch 196/200
1493/1493 - 81s - loss: 1.8516e-04 - val_loss: 1.8223e-04 - 81s/epoch - 55ms/step
Epoch 197/200
1493/1493 - 81s - loss: 1.8240e-04 - val_loss: 2.1758e-04 - 81s/epoch - 55ms/step
Epoch 198/200
1493/1493 - 81s - loss: 1.9211e-04 - val_loss: 2.1240e-04 - 81s/epoch - 54ms/step
Epoch 199/200
1493/1493 - 81s - loss: 1.8406e-04 - val_loss: 1.8790e-04 - 81s/epoch - 54ms/step
Epoch 200/200
1493/1493 - 81s - loss: 1.7889e-04 - val_loss: 1.9096e-04 - 81s/epoch - 54ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00019095685274805874
  1/332 [..............................] - ETA: 28s  8/332 [..............................] - ETA: 2s  15/332 [>.............................] - ETA: 2s 22/332 [>.............................] - ETA: 2s 29/332 [=>............................] - ETA: 2s 36/332 [==>...........................] - ETA: 2s 43/332 [==>...........................] - ETA: 2s 50/332 [===>..........................] - ETA: 2s 57/332 [====>.........................] - ETA: 2s 64/332 [====>.........................] - ETA: 1s 71/332 [=====>........................] - ETA: 1s 78/332 [======>.......................] - ETA: 1s 85/332 [======>.......................] - ETA: 1s 92/332 [=======>......................] - ETA: 1s 99/332 [=======>......................] - ETA: 1s106/332 [========>.....................] - ETA: 1s113/332 [=========>....................] - ETA: 1s120/332 [=========>....................] - ETA: 1s127/332 [==========>...................] - ETA: 1s134/332 [===========>..................] - ETA: 1s141/332 [===========>..................] - ETA: 1s148/332 [============>.................] - ETA: 1s155/332 [=============>................] - ETA: 1s162/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s176/332 [==============>...............] - ETA: 1s183/332 [===============>..............] - ETA: 1s190/332 [================>.............] - ETA: 1s197/332 [================>.............] - ETA: 0s204/332 [=================>............] - ETA: 0s211/332 [==================>...........] - ETA: 0s218/332 [==================>...........] - ETA: 0s225/332 [===================>..........] - ETA: 0s232/332 [===================>..........] - ETA: 0s239/332 [====================>.........] - ETA: 0s246/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s260/332 [======================>.......] - ETA: 0s267/332 [=======================>......] - ETA: 0s274/332 [=======================>......] - ETA: 0s281/332 [========================>.....] - ETA: 0s288/332 [=========================>....] - ETA: 0s295/332 [=========================>....] - ETA: 0s302/332 [==========================>...] - ETA: 0s309/332 [==========================>...] - ETA: 0s316/332 [===========================>..] - ETA: 0s323/332 [============================>.] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 7ms/step
correlation 0.0021925380925974037
cosine 0.0017250049374843157
MAE: 0.007505121
RMSE: 0.013818707
r2: 0.987612958529356
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_30"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_31 (InputLayer)       multiple                  0         
                                                                 
 dense_30 (Dense)            (None, 3160)              3997400   
                                                                 
 batch_normalization_30 (Bat  (None, 3160)             12640     
 chNormalization)                                                
                                                                 
 re_lu_30 (ReLU)             (None, 3160)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               796572    
                                                                 
 batch_normalization_31 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_31 (ReLU)             (None, 252)               0         
                                                                 
 dense_31 (Dense)            (None, 3160)              799480    
                                                                 
 batch_normalization_32 (Bat  (None, 3160)             12640     
 chNormalization)                                                
                                                                 
 re_lu_32 (ReLU)             (None, 3160)              0         
                                                                 
 dense_32 (Dense)            (None, 1264)              3995504   
                                                                 
=================================================================
Total params: 9,615,244
Trainable params: 9,602,100
Non-trainable params: 13,144
_________________________________________________________________
Encoder
Model: "model_31"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_32 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_31 (InputLayer)       multiple                  0         
                                                                 
 dense_30 (Dense)            (None, 3160)              3997400   
                                                                 
 batch_normalization_30 (Bat  (None, 3160)             12640     
 chNormalization)                                                
                                                                 
 re_lu_30 (ReLU)             (None, 3160)              0         
                                                                 
 bottleneck (Dense)          (None, 252)               796572    
                                                                 
=================================================================
Total params: 4,806,612
Trainable params: 4,800,292
Non-trainable params: 6,320
_________________________________________________________________
Decoder
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 252)]             0         
                                                                 
 batch_normalization_31 (Bat  (None, 252)              1008      
 chNormalization)                                                
                                                                 
 re_lu_31 (ReLU)             (None, 252)               0         
                                                                 
 dense_31 (Dense)            (None, 3160)              799480    
                                                                 
 batch_normalization_32 (Bat  (None, 3160)             12640     
 chNormalization)                                                
                                                                 
 re_lu_32 (ReLU)             (None, 3160)              0         
                                                                 
 dense_32 (Dense)            (None, 1264)              3995504   
                                                                 
=================================================================
Total params: 4,808,632
Trainable params: 4,801,808
Non-trainable params: 6,824
_________________________________________________________________
['2.5custom_n_b', 'mse', 64, 200, 0.0005, 0.2, 252, 0.00017889247101265937, 0.00019095685274805874, 0.0021925380925974037, 0.0017250049374843157, 0.007505121175199747, 0.013818707317113876, 0.987612958529356, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_33"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_34 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_33 (Dense)            (None, 1896)              2398440   
                                                                 
 batch_normalization_33 (Bat  (None, 1896)             7584      
 chNormalization)                                                
                                                                 
 re_lu_33 (ReLU)             (None, 1896)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1198904   
                                                                 
 batch_normalization_34 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_34 (ReLU)             (None, 632)               0         
                                                                 
 dense_34 (Dense)            (None, 1896)              1200168   
                                                                 
 batch_normalization_35 (Bat  (None, 1896)             7584      
 chNormalization)                                                
                                                                 
 re_lu_35 (ReLU)             (None, 1896)              0         
                                                                 
 dense_35 (Dense)            (None, 1264)              2397808   
                                                                 
=================================================================
Total params: 7,213,016
Trainable params: 7,204,168
Non-trainable params: 8,848
_________________________________________________________________
Epoch 1/200
1493/1493 - 63s - loss: 0.0093 - val_loss: 0.0043 - 63s/epoch - 42ms/step
Epoch 2/200
1493/1493 - 62s - loss: 0.0030 - val_loss: 0.0027 - 62s/epoch - 41ms/step
Epoch 3/200
1493/1493 - 62s - loss: 0.0020 - val_loss: 0.0018 - 62s/epoch - 41ms/step
Epoch 4/200
1493/1493 - 62s - loss: 0.0017 - val_loss: 0.0035 - 62s/epoch - 41ms/step
Epoch 5/200
1493/1493 - 62s - loss: 0.0016 - val_loss: 0.0012 - 62s/epoch - 41ms/step
Epoch 6/200
1493/1493 - 62s - loss: 0.0013 - val_loss: 0.0014 - 62s/epoch - 41ms/step
Epoch 7/200
1493/1493 - 62s - loss: 0.0012 - val_loss: 9.8368e-04 - 62s/epoch - 41ms/step
Epoch 8/200
1493/1493 - 62s - loss: 0.0011 - val_loss: 0.0014 - 62s/epoch - 41ms/step
Epoch 9/200
1493/1493 - 62s - loss: 0.0010 - val_loss: 0.0018 - 62s/epoch - 41ms/step
Epoch 10/200
1493/1493 - 62s - loss: 0.0011 - val_loss: 8.9982e-04 - 62s/epoch - 41ms/step
Epoch 11/200
1493/1493 - 62s - loss: 9.1123e-04 - val_loss: 9.1794e-04 - 62s/epoch - 41ms/step
Epoch 12/200
1493/1493 - 62s - loss: 8.1599e-04 - val_loss: 0.0016 - 62s/epoch - 41ms/step
Epoch 13/200
1493/1493 - 62s - loss: 8.6207e-04 - val_loss: 0.0011 - 62s/epoch - 41ms/step
Epoch 14/200
1493/1493 - 62s - loss: 7.8196e-04 - val_loss: 8.8315e-04 - 62s/epoch - 41ms/step
Epoch 15/200
1493/1493 - 62s - loss: 7.4833e-04 - val_loss: 6.6855e-04 - 62s/epoch - 41ms/step
Epoch 16/200
1493/1493 - 62s - loss: 6.5299e-04 - val_loss: 6.4670e-04 - 62s/epoch - 41ms/step
Epoch 17/200
1493/1493 - 62s - loss: 6.2263e-04 - val_loss: 9.1164e-04 - 62s/epoch - 41ms/step
Epoch 18/200
1493/1493 - 62s - loss: 6.2872e-04 - val_loss: 8.2853e-04 - 62s/epoch - 41ms/step
Epoch 19/200
1493/1493 - 62s - loss: 6.0059e-04 - val_loss: 6.4664e-04 - 62s/epoch - 41ms/step
Epoch 20/200
1493/1493 - 62s - loss: 5.5427e-04 - val_loss: 5.7206e-04 - 62s/epoch - 41ms/step
Epoch 21/200
1493/1493 - 62s - loss: 5.2895e-04 - val_loss: 7.2009e-04 - 62s/epoch - 41ms/step
Epoch 22/200
1493/1493 - 62s - loss: 5.0902e-04 - val_loss: 6.2642e-04 - 62s/epoch - 41ms/step
Epoch 23/200
1493/1493 - 62s - loss: 4.8773e-04 - val_loss: 0.0013 - 62s/epoch - 41ms/step
Epoch 24/200
1493/1493 - 62s - loss: 5.1347e-04 - val_loss: 4.4400e-04 - 62s/epoch - 41ms/step
Epoch 25/200
1493/1493 - 62s - loss: 4.5571e-04 - val_loss: 4.4231e-04 - 62s/epoch - 41ms/step
Epoch 26/200
1493/1493 - 62s - loss: 4.3252e-04 - val_loss: 6.1776e-04 - 62s/epoch - 41ms/step
Epoch 27/200
1493/1493 - 62s - loss: 4.1576e-04 - val_loss: 4.4337e-04 - 62s/epoch - 41ms/step
Epoch 28/200
1493/1493 - 62s - loss: 3.9774e-04 - val_loss: 7.5644e-04 - 62s/epoch - 41ms/step
Epoch 29/200
1493/1493 - 62s - loss: 3.9465e-04 - val_loss: 5.4630e-04 - 62s/epoch - 41ms/step
Epoch 30/200
1493/1493 - 62s - loss: 3.9136e-04 - val_loss: 4.2633e-04 - 62s/epoch - 41ms/step
Epoch 31/200
1493/1493 - 62s - loss: 3.6629e-04 - val_loss: 4.5280e-04 - 62s/epoch - 41ms/step
Epoch 32/200
1493/1493 - 62s - loss: 3.9628e-04 - val_loss: 3.8111e-04 - 62s/epoch - 41ms/step
Epoch 33/200
1493/1493 - 62s - loss: 3.5383e-04 - val_loss: 3.4839e-04 - 62s/epoch - 41ms/step
Epoch 34/200
1493/1493 - 62s - loss: 3.3986e-04 - val_loss: 3.3110e-04 - 62s/epoch - 41ms/step
Epoch 35/200
1493/1493 - 62s - loss: 3.2402e-04 - val_loss: 9.3902e-04 - 62s/epoch - 41ms/step
Epoch 36/200
1493/1493 - 62s - loss: 3.5463e-04 - val_loss: 4.6605e-04 - 62s/epoch - 41ms/step
Epoch 37/200
1493/1493 - 62s - loss: 3.1729e-04 - val_loss: 3.6116e-04 - 62s/epoch - 41ms/step
Epoch 38/200
1493/1493 - 62s - loss: 3.1596e-04 - val_loss: 3.0740e-04 - 62s/epoch - 41ms/step
Epoch 39/200
1493/1493 - 62s - loss: 3.0093e-04 - val_loss: 2.9276e-04 - 62s/epoch - 41ms/step
Epoch 40/200
1493/1493 - 62s - loss: 2.8806e-04 - val_loss: 3.9818e-04 - 62s/epoch - 41ms/step
Epoch 41/200
1493/1493 - 62s - loss: 2.9015e-04 - val_loss: 2.5332e-04 - 62s/epoch - 41ms/step
Epoch 42/200
1493/1493 - 62s - loss: 2.8024e-04 - val_loss: 2.7581e-04 - 62s/epoch - 41ms/step
Epoch 43/200
1493/1493 - 62s - loss: 2.7542e-04 - val_loss: 3.3777e-04 - 62s/epoch - 41ms/step
Epoch 44/200
1493/1493 - 62s - loss: 2.7959e-04 - val_loss: 9.5895e-04 - 62s/epoch - 41ms/step
Epoch 45/200
1493/1493 - 62s - loss: 3.1221e-04 - val_loss: 2.1705e-04 - 62s/epoch - 41ms/step
Epoch 46/200
1493/1493 - 62s - loss: 2.6018e-04 - val_loss: 2.2408e-04 - 62s/epoch - 41ms/step
Epoch 47/200
1493/1493 - 62s - loss: 2.5550e-04 - val_loss: 2.4096e-04 - 62s/epoch - 41ms/step
Epoch 48/200
1493/1493 - 62s - loss: 2.5441e-04 - val_loss: 4.9412e-04 - 62s/epoch - 41ms/step
Epoch 49/200
1493/1493 - 62s - loss: 2.5134e-04 - val_loss: 5.9062e-04 - 62s/epoch - 41ms/step
Epoch 50/200
1493/1493 - 62s - loss: 2.6987e-04 - val_loss: 3.6838e-04 - 62s/epoch - 41ms/step
Epoch 51/200
1493/1493 - 62s - loss: 2.5650e-04 - val_loss: 2.5367e-04 - 62s/epoch - 41ms/step
Epoch 52/200
1493/1493 - 62s - loss: 2.3625e-04 - val_loss: 2.4146e-04 - 62s/epoch - 41ms/step
Epoch 53/200
1493/1493 - 62s - loss: 2.3821e-04 - val_loss: 3.0991e-04 - 62s/epoch - 41ms/step
Epoch 54/200
1493/1493 - 62s - loss: 2.3772e-04 - val_loss: 2.5892e-04 - 62s/epoch - 41ms/step
Epoch 55/200
1493/1493 - 62s - loss: 2.2477e-04 - val_loss: 2.3092e-04 - 62s/epoch - 41ms/step
Epoch 56/200
1493/1493 - 62s - loss: 2.2489e-04 - val_loss: 2.9459e-04 - 62s/epoch - 41ms/step
Epoch 57/200
1493/1493 - 62s - loss: 2.1860e-04 - val_loss: 2.6306e-04 - 62s/epoch - 41ms/step
Epoch 58/200
1493/1493 - 62s - loss: 2.1886e-04 - val_loss: 2.0983e-04 - 62s/epoch - 41ms/step
Epoch 59/200
1493/1493 - 62s - loss: 2.1424e-04 - val_loss: 2.1951e-04 - 62s/epoch - 41ms/step
Epoch 60/200
1493/1493 - 62s - loss: 2.1437e-04 - val_loss: 6.4799e-04 - 62s/epoch - 41ms/step
Epoch 61/200
1493/1493 - 62s - loss: 2.6675e-04 - val_loss: 2.0037e-04 - 62s/epoch - 41ms/step
Epoch 62/200
1493/1493 - 62s - loss: 2.1126e-04 - val_loss: 2.5855e-04 - 62s/epoch - 41ms/step
Epoch 63/200
1493/1493 - 62s - loss: 2.0577e-04 - val_loss: 2.7396e-04 - 62s/epoch - 41ms/step
Epoch 64/200
1493/1493 - 62s - loss: 2.0522e-04 - val_loss: 0.0012 - 62s/epoch - 41ms/step
Epoch 65/200
1493/1493 - 62s - loss: 2.6367e-04 - val_loss: 2.7246e-04 - 62s/epoch - 41ms/step
Epoch 66/200
1493/1493 - 62s - loss: 2.1338e-04 - val_loss: 2.2698e-04 - 62s/epoch - 41ms/step
Epoch 67/200
1493/1493 - 62s - loss: 2.0051e-04 - val_loss: 2.1103e-04 - 62s/epoch - 41ms/step
Epoch 68/200
1493/1493 - 62s - loss: 1.9820e-04 - val_loss: 6.9784e-04 - 62s/epoch - 41ms/step
Epoch 69/200
1493/1493 - 62s - loss: 2.6745e-04 - val_loss: 2.2865e-04 - 62s/epoch - 41ms/step
Epoch 70/200
1493/1493 - 62s - loss: 2.0035e-04 - val_loss: 0.0011 - 62s/epoch - 41ms/step
Epoch 71/200
1493/1493 - 62s - loss: 2.5175e-04 - val_loss: 1.8365e-04 - 62s/epoch - 41ms/step
Epoch 72/200
1493/1493 - 62s - loss: 1.9752e-04 - val_loss: 2.3021e-04 - 62s/epoch - 41ms/step
Epoch 73/200
1493/1493 - 62s - loss: 1.9804e-04 - val_loss: 2.2823e-04 - 62s/epoch - 41ms/step
Epoch 74/200
1493/1493 - 62s - loss: 1.9400e-04 - val_loss: 2.1088e-04 - 62s/epoch - 41ms/step
Epoch 75/200
1493/1493 - 62s - loss: 1.8559e-04 - val_loss: 2.0246e-04 - 62s/epoch - 41ms/step
Epoch 76/200
1493/1493 - 62s - loss: 1.8604e-04 - val_loss: 1.7615e-04 - 62s/epoch - 41ms/step
Epoch 77/200
1493/1493 - 62s - loss: 1.8505e-04 - val_loss: 4.5355e-04 - 62s/epoch - 41ms/step
Epoch 78/200
1493/1493 - 62s - loss: 1.8658e-04 - val_loss: 2.3559e-04 - 62s/epoch - 41ms/step
Epoch 79/200
1493/1493 - 62s - loss: 1.8329e-04 - val_loss: 2.2452e-04 - 62s/epoch - 41ms/step
Epoch 80/200
1493/1493 - 62s - loss: 1.8209e-04 - val_loss: 1.7617e-04 - 62s/epoch - 41ms/step
Epoch 81/200
1493/1493 - 62s - loss: 1.7734e-04 - val_loss: 4.1649e-04 - 62s/epoch - 41ms/step
Epoch 82/200
1493/1493 - 62s - loss: 1.7505e-04 - val_loss: 1.9965e-04 - 62s/epoch - 41ms/step
Epoch 83/200
1493/1493 - 62s - loss: 1.7019e-04 - val_loss: 2.0414e-04 - 62s/epoch - 41ms/step
Epoch 84/200
1493/1493 - 62s - loss: 1.6964e-04 - val_loss: 2.0522e-04 - 62s/epoch - 41ms/step
Epoch 85/200
1493/1493 - 62s - loss: 1.7476e-04 - val_loss: 2.9970e-04 - 62s/epoch - 41ms/step
Epoch 86/200
1493/1493 - 62s - loss: 1.8451e-04 - val_loss: 1.6863e-04 - 62s/epoch - 41ms/step
Epoch 87/200
1493/1493 - 62s - loss: 1.6651e-04 - val_loss: 1.7663e-04 - 62s/epoch - 41ms/step
Epoch 88/200
1493/1493 - 62s - loss: 1.6577e-04 - val_loss: 2.8918e-04 - 62s/epoch - 41ms/step
Epoch 89/200
1493/1493 - 62s - loss: 1.7568e-04 - val_loss: 1.7731e-04 - 62s/epoch - 41ms/step
Epoch 90/200
1493/1493 - 62s - loss: 1.6368e-04 - val_loss: 2.0403e-04 - 62s/epoch - 41ms/step
Epoch 91/200
1493/1493 - 62s - loss: 1.6189e-04 - val_loss: 1.9718e-04 - 62s/epoch - 41ms/step
Epoch 92/200
1493/1493 - 62s - loss: 1.6140e-04 - val_loss: 3.9264e-04 - 62s/epoch - 41ms/step
Epoch 93/200
1493/1493 - 62s - loss: 1.9570e-04 - val_loss: 2.3224e-04 - 62s/epoch - 41ms/step
Epoch 94/200
1493/1493 - 62s - loss: 1.6719e-04 - val_loss: 1.4007e-04 - 62s/epoch - 41ms/step
Epoch 95/200
1493/1493 - 62s - loss: 1.6028e-04 - val_loss: 2.6911e-04 - 62s/epoch - 41ms/step
Epoch 96/200
1493/1493 - 62s - loss: 1.6748e-04 - val_loss: 1.9507e-04 - 62s/epoch - 41ms/step
Epoch 97/200
1493/1493 - 62s - loss: 1.5818e-04 - val_loss: 1.6438e-04 - 62s/epoch - 41ms/step
Epoch 98/200
1493/1493 - 62s - loss: 1.6089e-04 - val_loss: 9.9682e-04 - 62s/epoch - 41ms/step
Epoch 99/200
1493/1493 - 62s - loss: 2.2483e-04 - val_loss: 1.5128e-04 - 62s/epoch - 41ms/step
Epoch 100/200
1493/1493 - 62s - loss: 1.6161e-04 - val_loss: 1.5929e-04 - 62s/epoch - 41ms/step
Epoch 101/200
1493/1493 - 62s - loss: 1.5672e-04 - val_loss: 4.7063e-04 - 62s/epoch - 41ms/step
Epoch 102/200
1493/1493 - 62s - loss: 1.8253e-04 - val_loss: 2.3982e-04 - 62s/epoch - 41ms/step
Epoch 103/200
1493/1493 - 62s - loss: 1.6158e-04 - val_loss: 1.7217e-04 - 62s/epoch - 41ms/step
Epoch 104/200
1493/1493 - 62s - loss: 1.5226e-04 - val_loss: 2.9251e-04 - 62s/epoch - 41ms/step
Epoch 105/200
1493/1493 - 62s - loss: 1.6217e-04 - val_loss: 1.6382e-04 - 62s/epoch - 41ms/step
Epoch 106/200
1493/1493 - 62s - loss: 1.5040e-04 - val_loss: 1.6555e-04 - 62s/epoch - 41ms/step
Epoch 107/200
1493/1493 - 62s - loss: 1.4914e-04 - val_loss: 1.9381e-04 - 62s/epoch - 41ms/step
Epoch 108/200
1493/1493 - 62s - loss: 1.5095e-04 - val_loss: 1.7153e-04 - 62s/epoch - 41ms/step
Epoch 109/200
1493/1493 - 62s - loss: 1.4838e-04 - val_loss: 1.6484e-04 - 62s/epoch - 41ms/step
Epoch 110/200
1493/1493 - 62s - loss: 1.4623e-04 - val_loss: 1.5558e-04 - 62s/epoch - 41ms/step
Epoch 111/200
1493/1493 - 62s - loss: 1.4690e-04 - val_loss: 3.0335e-04 - 62s/epoch - 41ms/step
Epoch 112/200
1493/1493 - 62s - loss: 1.6121e-04 - val_loss: 1.3286e-04 - 62s/epoch - 41ms/step
Epoch 113/200
1493/1493 - 62s - loss: 1.4706e-04 - val_loss: 6.4878e-04 - 62s/epoch - 41ms/step
Epoch 114/200
1493/1493 - 62s - loss: 2.0160e-04 - val_loss: 1.5207e-04 - 62s/epoch - 41ms/step
Epoch 115/200
1493/1493 - 62s - loss: 1.4851e-04 - val_loss: 2.3967e-04 - 62s/epoch - 41ms/step
Epoch 116/200
1493/1493 - 62s - loss: 1.4608e-04 - val_loss: 2.0635e-04 - 62s/epoch - 41ms/step
Epoch 117/200
1493/1493 - 62s - loss: 1.4304e-04 - val_loss: 2.0241e-04 - 62s/epoch - 41ms/step
Epoch 118/200
1493/1493 - 62s - loss: 1.5066e-04 - val_loss: 1.7737e-04 - 62s/epoch - 41ms/step
Epoch 119/200
1493/1493 - 62s - loss: 1.4192e-04 - val_loss: 1.5865e-04 - 62s/epoch - 41ms/step
Epoch 120/200
1493/1493 - 62s - loss: 1.4012e-04 - val_loss: 1.3375e-04 - 62s/epoch - 41ms/step
Epoch 121/200
1493/1493 - 62s - loss: 1.4034e-04 - val_loss: 3.3892e-04 - 62s/epoch - 41ms/step
Epoch 122/200
1493/1493 - 62s - loss: 1.5909e-04 - val_loss: 1.3292e-04 - 62s/epoch - 41ms/step
Epoch 123/200
1493/1493 - 62s - loss: 1.3961e-04 - val_loss: 1.5083e-04 - 62s/epoch - 41ms/step
Epoch 124/200
1493/1493 - 61s - loss: 1.3841e-04 - val_loss: 5.9975e-04 - 61s/epoch - 41ms/step
Epoch 125/200
1493/1493 - 62s - loss: 1.7036e-04 - val_loss: 1.3351e-04 - 62s/epoch - 41ms/step
Epoch 126/200
1493/1493 - 61s - loss: 1.4115e-04 - val_loss: 7.2574e-04 - 61s/epoch - 41ms/step
Epoch 127/200
1493/1493 - 62s - loss: 1.8229e-04 - val_loss: 4.2671e-04 - 62s/epoch - 41ms/step
Epoch 128/200
1493/1493 - 62s - loss: 1.6655e-04 - val_loss: 1.2174e-04 - 62s/epoch - 41ms/step
Epoch 129/200
1493/1493 - 61s - loss: 1.3815e-04 - val_loss: 1.3438e-04 - 61s/epoch - 41ms/step
Epoch 130/200
1493/1493 - 62s - loss: 1.3624e-04 - val_loss: 1.2705e-04 - 62s/epoch - 41ms/step
Epoch 131/200
1493/1493 - 62s - loss: 1.3814e-04 - val_loss: 3.7795e-04 - 62s/epoch - 41ms/step
Epoch 132/200
1493/1493 - 62s - loss: 1.6193e-04 - val_loss: 1.2136e-04 - 62s/epoch - 41ms/step
Epoch 133/200
1493/1493 - 62s - loss: 1.3483e-04 - val_loss: 1.3642e-04 - 62s/epoch - 41ms/step
Epoch 134/200
1493/1493 - 62s - loss: 1.3310e-04 - val_loss: 1.3150e-04 - 62s/epoch - 41ms/step
Epoch 135/200
1493/1493 - 61s - loss: 1.3126e-04 - val_loss: 1.4301e-04 - 61s/epoch - 41ms/step
Epoch 136/200
1493/1493 - 62s - loss: 1.3229e-04 - val_loss: 2.1883e-04 - 62s/epoch - 41ms/step
Epoch 137/200
1493/1493 - 62s - loss: 1.5172e-04 - val_loss: 3.6964e-04 - 62s/epoch - 41ms/step
Epoch 138/200
1493/1493 - 62s - loss: 1.6673e-04 - val_loss: 1.4252e-04 - 62s/epoch - 41ms/step
Epoch 139/200
1493/1493 - 62s - loss: 1.4148e-04 - val_loss: 4.4512e-04 - 62s/epoch - 41ms/step
Epoch 140/200
1493/1493 - 61s - loss: 1.8388e-04 - val_loss: 1.2067e-04 - 61s/epoch - 41ms/step
Epoch 141/200
1493/1493 - 62s - loss: 1.3882e-04 - val_loss: 1.6104e-04 - 62s/epoch - 41ms/step
Epoch 142/200
1493/1493 - 62s - loss: 1.3726e-04 - val_loss: 1.3639e-04 - 62s/epoch - 41ms/step
Epoch 143/200
1493/1493 - 62s - loss: 1.3075e-04 - val_loss: 1.4913e-04 - 62s/epoch - 41ms/step
Epoch 144/200
1493/1493 - 62s - loss: 1.3538e-04 - val_loss: 1.3722e-04 - 62s/epoch - 41ms/step
Epoch 145/200
1493/1493 - 61s - loss: 1.2956e-04 - val_loss: 1.3020e-04 - 61s/epoch - 41ms/step
Epoch 146/200
1493/1493 - 62s - loss: 1.2872e-04 - val_loss: 1.3497e-04 - 62s/epoch - 41ms/step
Epoch 147/200
1493/1493 - 62s - loss: 1.2650e-04 - val_loss: 1.4190e-04 - 62s/epoch - 41ms/step
Epoch 148/200
1493/1493 - 62s - loss: 1.2647e-04 - val_loss: 2.5361e-04 - 62s/epoch - 41ms/step
Epoch 149/200
1493/1493 - 62s - loss: 1.2631e-04 - val_loss: 1.2777e-04 - 62s/epoch - 41ms/step
Epoch 150/200
1493/1493 - 62s - loss: 1.2476e-04 - val_loss: 1.1958e-04 - 62s/epoch - 41ms/step
Epoch 151/200
1493/1493 - 61s - loss: 1.2877e-04 - val_loss: 1.7002e-04 - 61s/epoch - 41ms/step
Epoch 152/200
1493/1493 - 62s - loss: 1.2577e-04 - val_loss: 1.8512e-04 - 62s/epoch - 41ms/step
Epoch 153/200
1493/1493 - 62s - loss: 1.2701e-04 - val_loss: 1.5995e-04 - 62s/epoch - 41ms/step
Epoch 154/200
1493/1493 - 62s - loss: 1.2240e-04 - val_loss: 1.4887e-04 - 62s/epoch - 41ms/step
Epoch 155/200
1493/1493 - 62s - loss: 1.2307e-04 - val_loss: 1.3102e-04 - 62s/epoch - 41ms/step
Epoch 156/200
1493/1493 - 61s - loss: 1.2581e-04 - val_loss: 1.4258e-04 - 61s/epoch - 41ms/step
Epoch 157/200
1493/1493 - 62s - loss: 1.2103e-04 - val_loss: 1.4435e-04 - 62s/epoch - 41ms/step
Epoch 158/200
1493/1493 - 61s - loss: 1.2364e-04 - val_loss: 1.3113e-04 - 61s/epoch - 41ms/step
Epoch 159/200
1493/1493 - 62s - loss: 1.2094e-04 - val_loss: 1.4048e-04 - 62s/epoch - 41ms/step
Epoch 160/200
1493/1493 - 62s - loss: 1.2734e-04 - val_loss: 2.2020e-04 - 62s/epoch - 41ms/step
Epoch 161/200
1493/1493 - 62s - loss: 1.5014e-04 - val_loss: 1.2690e-04 - 62s/epoch - 41ms/step
Epoch 162/200
1493/1493 - 61s - loss: 1.2261e-04 - val_loss: 1.2089e-04 - 61s/epoch - 41ms/step
Epoch 163/200
1493/1493 - 62s - loss: 1.2170e-04 - val_loss: 2.2323e-04 - 62s/epoch - 41ms/step
Epoch 164/200
1493/1493 - 62s - loss: 1.3119e-04 - val_loss: 1.2497e-04 - 62s/epoch - 41ms/step
Epoch 165/200
1493/1493 - 62s - loss: 1.2806e-04 - val_loss: 1.2939e-04 - 62s/epoch - 41ms/step
Epoch 166/200
1493/1493 - 62s - loss: 1.2119e-04 - val_loss: 1.3071e-04 - 62s/epoch - 41ms/step
Epoch 167/200
1493/1493 - 61s - loss: 1.1984e-04 - val_loss: 1.1752e-04 - 61s/epoch - 41ms/step
Epoch 168/200
1493/1493 - 61s - loss: 1.2108e-04 - val_loss: 1.3025e-04 - 61s/epoch - 41ms/step
Epoch 169/200
1493/1493 - 62s - loss: 1.1886e-04 - val_loss: 1.4959e-04 - 62s/epoch - 41ms/step
Epoch 170/200
1493/1493 - 62s - loss: 1.2048e-04 - val_loss: 4.4454e-04 - 62s/epoch - 41ms/step
Epoch 171/200
1493/1493 - 62s - loss: 1.5284e-04 - val_loss: 3.8880e-04 - 62s/epoch - 41ms/step
Epoch 172/200
1493/1493 - 62s - loss: 1.4088e-04 - val_loss: 1.5509e-04 - 62s/epoch - 41ms/step
Epoch 173/200
1493/1493 - 61s - loss: 1.2255e-04 - val_loss: 1.1214e-04 - 61s/epoch - 41ms/step
Epoch 174/200
1493/1493 - 62s - loss: 1.1779e-04 - val_loss: 1.4523e-04 - 62s/epoch - 41ms/step
Epoch 175/200
1493/1493 - 62s - loss: 1.1938e-04 - val_loss: 1.5099e-04 - 62s/epoch - 41ms/step
Epoch 176/200
1493/1493 - 62s - loss: 1.1963e-04 - val_loss: 1.1446e-04 - 62s/epoch - 41ms/step
Epoch 177/200
1493/1493 - 62s - loss: 1.1953e-04 - val_loss: 1.1278e-04 - 62s/epoch - 41ms/step
Epoch 178/200
1493/1493 - 61s - loss: 1.1593e-04 - val_loss: 1.2514e-04 - 61s/epoch - 41ms/step
Epoch 179/200
1493/1493 - 62s - loss: 1.1703e-04 - val_loss: 1.3164e-04 - 62s/epoch - 41ms/step
Epoch 180/200
1493/1493 - 62s - loss: 1.1617e-04 - val_loss: 1.5116e-04 - 62s/epoch - 41ms/step
Epoch 181/200
1493/1493 - 62s - loss: 1.1796e-04 - val_loss: 1.4957e-04 - 62s/epoch - 41ms/step
Epoch 182/200
1493/1493 - 61s - loss: 1.1641e-04 - val_loss: 1.4553e-04 - 61s/epoch - 41ms/step
Epoch 183/200
1493/1493 - 62s - loss: 1.1376e-04 - val_loss: 1.5308e-04 - 62s/epoch - 41ms/step
Epoch 184/200
1493/1493 - 61s - loss: 1.1565e-04 - val_loss: 1.1307e-04 - 61s/epoch - 41ms/step
Epoch 185/200
1493/1493 - 62s - loss: 1.1375e-04 - val_loss: 3.1438e-04 - 62s/epoch - 41ms/step
Epoch 186/200
1493/1493 - 62s - loss: 1.3303e-04 - val_loss: 1.0666e-04 - 62s/epoch - 41ms/step
Epoch 187/200
1493/1493 - 62s - loss: 1.1337e-04 - val_loss: 1.4674e-04 - 62s/epoch - 41ms/step
Epoch 188/200
1493/1493 - 62s - loss: 1.3237e-04 - val_loss: 1.8298e-04 - 62s/epoch - 41ms/step
Epoch 189/200
1493/1493 - 61s - loss: 1.1925e-04 - val_loss: 1.4320e-04 - 61s/epoch - 41ms/step
Epoch 190/200
1493/1493 - 62s - loss: 1.1672e-04 - val_loss: 1.3967e-04 - 62s/epoch - 41ms/step
Epoch 191/200
1493/1493 - 62s - loss: 1.1589e-04 - val_loss: 1.4585e-04 - 62s/epoch - 41ms/step
Epoch 192/200
1493/1493 - 62s - loss: 1.1481e-04 - val_loss: 1.4899e-04 - 62s/epoch - 41ms/step
Epoch 193/200
1493/1493 - 62s - loss: 1.1693e-04 - val_loss: 2.6906e-04 - 62s/epoch - 41ms/step
Epoch 194/200
1493/1493 - 61s - loss: 1.3578e-04 - val_loss: 4.2526e-04 - 61s/epoch - 41ms/step
Epoch 195/200
1493/1493 - 62s - loss: 1.5133e-04 - val_loss: 1.3255e-04 - 62s/epoch - 41ms/step
Epoch 196/200
1493/1493 - 62s - loss: 1.1836e-04 - val_loss: 1.2182e-04 - 62s/epoch - 41ms/step
Epoch 197/200
1493/1493 - 62s - loss: 1.1508e-04 - val_loss: 1.5290e-04 - 62s/epoch - 41ms/step
Epoch 198/200
1493/1493 - 62s - loss: 1.1739e-04 - val_loss: 1.8076e-04 - 62s/epoch - 41ms/step
Epoch 199/200
1493/1493 - 62s - loss: 1.2072e-04 - val_loss: 1.1254e-04 - 62s/epoch - 41ms/step
Epoch 200/200
1493/1493 - 61s - loss: 1.1211e-04 - val_loss: 1.1858e-04 - 61s/epoch - 41ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0001185842847917229
  1/332 [..............................] - ETA: 26s 10/332 [..............................] - ETA: 1s  19/332 [>.............................] - ETA: 1s 28/332 [=>............................] - ETA: 1s 37/332 [==>...........................] - ETA: 1s 46/332 [===>..........................] - ETA: 1s 55/332 [===>..........................] - ETA: 1s 64/332 [====>.........................] - ETA: 1s 73/332 [=====>........................] - ETA: 1s 82/332 [======>.......................] - ETA: 1s 91/332 [=======>......................] - ETA: 1s100/332 [========>.....................] - ETA: 1s109/332 [========>.....................] - ETA: 1s118/332 [=========>....................] - ETA: 1s127/332 [==========>...................] - ETA: 1s136/332 [===========>..................] - ETA: 1s145/332 [============>.................] - ETA: 1s154/332 [============>.................] - ETA: 1s163/332 [=============>................] - ETA: 0s172/332 [==============>...............] - ETA: 0s181/332 [===============>..............] - ETA: 0s190/332 [================>.............] - ETA: 0s199/332 [================>.............] - ETA: 0s208/332 [=================>............] - ETA: 0s217/332 [==================>...........] - ETA: 0s226/332 [===================>..........] - ETA: 0s235/332 [====================>.........] - ETA: 0s244/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s262/332 [======================>.......] - ETA: 0s271/332 [=======================>......] - ETA: 0s280/332 [========================>.....] - ETA: 0s289/332 [=========================>....] - ETA: 0s298/332 [=========================>....] - ETA: 0s307/332 [==========================>...] - ETA: 0s316/332 [===========================>..] - ETA: 0s325/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 6ms/step
correlation 0.001328335987187043
cosine 0.0010460179806025217
MAE: 0.0062503666
RMSE: 0.010889637
r2: 0.9923075367946292
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_33"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_34 (InputLayer)       multiple                  0         
                                                                 
 dense_33 (Dense)            (None, 1896)              2398440   
                                                                 
 batch_normalization_33 (Bat  (None, 1896)             7584      
 chNormalization)                                                
                                                                 
 re_lu_33 (ReLU)             (None, 1896)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1198904   
                                                                 
 batch_normalization_34 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_34 (ReLU)             (None, 632)               0         
                                                                 
 dense_34 (Dense)            (None, 1896)              1200168   
                                                                 
 batch_normalization_35 (Bat  (None, 1896)             7584      
 chNormalization)                                                
                                                                 
 re_lu_35 (ReLU)             (None, 1896)              0         
                                                                 
 dense_35 (Dense)            (None, 1264)              2397808   
                                                                 
=================================================================
Total params: 7,213,016
Trainable params: 7,204,168
Non-trainable params: 8,848
_________________________________________________________________
Encoder
Model: "model_34"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_35 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_34 (InputLayer)       multiple                  0         
                                                                 
 dense_33 (Dense)            (None, 1896)              2398440   
                                                                 
 batch_normalization_33 (Bat  (None, 1896)             7584      
 chNormalization)                                                
                                                                 
 re_lu_33 (ReLU)             (None, 1896)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1198904   
                                                                 
=================================================================
Total params: 3,604,928
Trainable params: 3,601,136
Non-trainable params: 3,792
_________________________________________________________________
Decoder
Model: "model_35"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_36 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_34 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_34 (ReLU)             (None, 632)               0         
                                                                 
 dense_34 (Dense)            (None, 1896)              1200168   
                                                                 
 batch_normalization_35 (Bat  (None, 1896)             7584      
 chNormalization)                                                
                                                                 
 re_lu_35 (ReLU)             (None, 1896)              0         
                                                                 
 dense_35 (Dense)            (None, 1264)              2397808   
                                                                 
=================================================================
Total params: 3,608,088
Trainable params: 3,603,032
Non-trainable params: 5,056
_________________________________________________________________
['1.5custom_n_b', 'mse', 64, 200, 0.0005, 0.5, 632, 0.00011210996308363974, 0.0001185842847917229, 0.001328335987187043, 0.0010460179806025217, 0.006250366568565369, 0.010889637283980846, 0.9923075367946292, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_36"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_37 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_36 (Dense)            (None, 2022)              2557830   
                                                                 
 batch_normalization_36 (Bat  (None, 2022)             8088      
 chNormalization)                                                
                                                                 
 re_lu_36 (ReLU)             (None, 2022)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1278536   
                                                                 
 batch_normalization_37 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_37 (ReLU)             (None, 632)               0         
                                                                 
 dense_37 (Dense)            (None, 2022)              1279926   
                                                                 
 batch_normalization_38 (Bat  (None, 2022)             8088      
 chNormalization)                                                
                                                                 
 re_lu_38 (ReLU)             (None, 2022)              0         
                                                                 
 dense_38 (Dense)            (None, 1264)              2557072   
                                                                 
=================================================================
Total params: 7,692,068
Trainable params: 7,682,716
Non-trainable params: 9,352
_________________________________________________________________
Epoch 1/200
1493/1493 - 68s - loss: 0.0093 - val_loss: 0.0048 - 68s/epoch - 45ms/step
Epoch 2/200
1493/1493 - 67s - loss: 0.0031 - val_loss: 0.0030 - 67s/epoch - 45ms/step
Epoch 3/200
1493/1493 - 67s - loss: 0.0021 - val_loss: 0.0018 - 67s/epoch - 45ms/step
Epoch 4/200
1493/1493 - 67s - loss: 0.0017 - val_loss: 0.0019 - 67s/epoch - 45ms/step
Epoch 5/200
1493/1493 - 67s - loss: 0.0015 - val_loss: 0.0015 - 67s/epoch - 45ms/step
Epoch 6/200
1493/1493 - 67s - loss: 0.0013 - val_loss: 0.0014 - 67s/epoch - 45ms/step
Epoch 7/200
1493/1493 - 67s - loss: 0.0013 - val_loss: 0.0010 - 67s/epoch - 45ms/step
Epoch 8/200
1493/1493 - 67s - loss: 0.0011 - val_loss: 0.0016 - 67s/epoch - 45ms/step
Epoch 9/200
1493/1493 - 67s - loss: 0.0010 - val_loss: 9.9944e-04 - 67s/epoch - 45ms/step
Epoch 10/200
1493/1493 - 67s - loss: 9.7097e-04 - val_loss: 0.0011 - 67s/epoch - 45ms/step
Epoch 11/200
1493/1493 - 67s - loss: 8.9967e-04 - val_loss: 8.4991e-04 - 67s/epoch - 45ms/step
Epoch 12/200
1493/1493 - 67s - loss: 8.0524e-04 - val_loss: 0.0015 - 67s/epoch - 45ms/step
Epoch 13/200
1493/1493 - 67s - loss: 7.9222e-04 - val_loss: 0.0023 - 67s/epoch - 45ms/step
Epoch 14/200
1493/1493 - 67s - loss: 9.1844e-04 - val_loss: 8.9249e-04 - 67s/epoch - 45ms/step
Epoch 15/200
1493/1493 - 67s - loss: 7.3334e-04 - val_loss: 6.1150e-04 - 67s/epoch - 45ms/step
Epoch 16/200
1493/1493 - 67s - loss: 6.4335e-04 - val_loss: 6.2524e-04 - 67s/epoch - 45ms/step
Epoch 17/200
1493/1493 - 64s - loss: 6.1108e-04 - val_loss: 7.9461e-04 - 64s/epoch - 43ms/step
Epoch 18/200
1493/1493 - 63s - loss: 6.0336e-04 - val_loss: 0.0015 - 63s/epoch - 42ms/step
Epoch 19/200
1493/1493 - 63s - loss: 6.3003e-04 - val_loss: 6.4721e-04 - 63s/epoch - 42ms/step
Epoch 20/200
1493/1493 - 63s - loss: 5.4768e-04 - val_loss: 5.3048e-04 - 63s/epoch - 42ms/step
Epoch 21/200
1493/1493 - 63s - loss: 5.1545e-04 - val_loss: 6.8281e-04 - 63s/epoch - 42ms/step
Epoch 22/200
1493/1493 - 63s - loss: 4.9204e-04 - val_loss: 6.7746e-04 - 63s/epoch - 42ms/step
Epoch 23/200
1493/1493 - 63s - loss: 4.9418e-04 - val_loss: 8.1652e-04 - 63s/epoch - 42ms/step
Epoch 24/200
1493/1493 - 63s - loss: 4.6491e-04 - val_loss: 4.7215e-04 - 63s/epoch - 42ms/step
Epoch 25/200
1493/1493 - 63s - loss: 4.4248e-04 - val_loss: 4.5160e-04 - 63s/epoch - 42ms/step
Epoch 26/200
1493/1493 - 63s - loss: 4.1420e-04 - val_loss: 5.5125e-04 - 63s/epoch - 42ms/step
Epoch 27/200
1493/1493 - 63s - loss: 3.9666e-04 - val_loss: 4.1303e-04 - 63s/epoch - 42ms/step
Epoch 28/200
1493/1493 - 63s - loss: 3.7830e-04 - val_loss: 6.9640e-04 - 63s/epoch - 42ms/step
Epoch 29/200
1493/1493 - 63s - loss: 3.8036e-04 - val_loss: 5.2513e-04 - 63s/epoch - 42ms/step
Epoch 30/200
1493/1493 - 63s - loss: 3.7085e-04 - val_loss: 4.2453e-04 - 63s/epoch - 42ms/step
Epoch 31/200
1493/1493 - 63s - loss: 3.4829e-04 - val_loss: 5.4522e-04 - 63s/epoch - 42ms/step
Epoch 32/200
1493/1493 - 63s - loss: 3.8479e-04 - val_loss: 3.4752e-04 - 63s/epoch - 42ms/step
Epoch 33/200
1493/1493 - 63s - loss: 3.3902e-04 - val_loss: 3.6506e-04 - 63s/epoch - 42ms/step
Epoch 34/200
1493/1493 - 63s - loss: 3.2456e-04 - val_loss: 3.0656e-04 - 63s/epoch - 42ms/step
Epoch 35/200
1493/1493 - 62s - loss: 3.1067e-04 - val_loss: 9.5163e-04 - 62s/epoch - 42ms/step
Epoch 36/200
1493/1493 - 58s - loss: 3.3868e-04 - val_loss: 4.7186e-04 - 58s/epoch - 39ms/step
Epoch 37/200
1493/1493 - 59s - loss: 3.0646e-04 - val_loss: 6.8552e-04 - 59s/epoch - 39ms/step
Epoch 38/200
1493/1493 - 59s - loss: 3.3442e-04 - val_loss: 2.7805e-04 - 59s/epoch - 39ms/step
Epoch 39/200
1493/1493 - 58s - loss: 2.8574e-04 - val_loss: 2.6867e-04 - 58s/epoch - 39ms/step
Epoch 40/200
1493/1493 - 58s - loss: 2.7676e-04 - val_loss: 4.1259e-04 - 58s/epoch - 39ms/step
Epoch 41/200
1493/1493 - 58s - loss: 2.7960e-04 - val_loss: 2.8422e-04 - 58s/epoch - 39ms/step
Epoch 42/200
1493/1493 - 58s - loss: 2.6903e-04 - val_loss: 2.3783e-04 - 58s/epoch - 39ms/step
Epoch 43/200
1493/1493 - 58s - loss: 2.6555e-04 - val_loss: 4.1465e-04 - 58s/epoch - 39ms/step
Epoch 44/200
1493/1493 - 58s - loss: 2.7164e-04 - val_loss: 3.8339e-04 - 58s/epoch - 39ms/step
Epoch 45/200
1493/1493 - 58s - loss: 2.6769e-04 - val_loss: 2.2973e-04 - 58s/epoch - 39ms/step
Epoch 46/200
1493/1493 - 59s - loss: 2.5171e-04 - val_loss: 2.5378e-04 - 59s/epoch - 39ms/step
Epoch 47/200
1493/1493 - 58s - loss: 2.4474e-04 - val_loss: 2.7166e-04 - 58s/epoch - 39ms/step
Epoch 48/200
1493/1493 - 58s - loss: 2.5424e-04 - val_loss: 3.0525e-04 - 58s/epoch - 39ms/step
Epoch 49/200
1493/1493 - 58s - loss: 2.4325e-04 - val_loss: 5.3447e-04 - 58s/epoch - 39ms/step
Epoch 50/200
1493/1493 - 58s - loss: 2.5148e-04 - val_loss: 3.6867e-04 - 58s/epoch - 39ms/step
Epoch 51/200
1493/1493 - 58s - loss: 2.4403e-04 - val_loss: 2.4458e-04 - 58s/epoch - 39ms/step
Epoch 52/200
1493/1493 - 58s - loss: 2.2647e-04 - val_loss: 2.2314e-04 - 58s/epoch - 39ms/step
Epoch 53/200
1493/1493 - 59s - loss: 2.2927e-04 - val_loss: 4.0869e-04 - 59s/epoch - 39ms/step
Epoch 54/200
1493/1493 - 59s - loss: 2.3853e-04 - val_loss: 2.4474e-04 - 59s/epoch - 39ms/step
Epoch 55/200
1493/1493 - 58s - loss: 2.1674e-04 - val_loss: 2.1614e-04 - 58s/epoch - 39ms/step
Epoch 56/200
1493/1493 - 59s - loss: 2.1763e-04 - val_loss: 3.2217e-04 - 59s/epoch - 39ms/step
Epoch 57/200
1493/1493 - 59s - loss: 2.1137e-04 - val_loss: 2.6157e-04 - 59s/epoch - 39ms/step
Epoch 58/200
1493/1493 - 59s - loss: 2.1020e-04 - val_loss: 1.9389e-04 - 59s/epoch - 39ms/step
Epoch 59/200
1493/1493 - 59s - loss: 2.0813e-04 - val_loss: 2.2980e-04 - 59s/epoch - 39ms/step
Epoch 60/200
1493/1493 - 59s - loss: 2.0874e-04 - val_loss: 5.8567e-04 - 59s/epoch - 39ms/step
Epoch 61/200
1493/1493 - 59s - loss: 2.8103e-04 - val_loss: 1.9846e-04 - 59s/epoch - 39ms/step
Epoch 62/200
1493/1493 - 58s - loss: 2.0463e-04 - val_loss: 2.3527e-04 - 58s/epoch - 39ms/step
Epoch 63/200
1493/1493 - 59s - loss: 1.9944e-04 - val_loss: 2.9759e-04 - 59s/epoch - 39ms/step
Epoch 64/200
1493/1493 - 58s - loss: 2.0031e-04 - val_loss: 0.0012 - 58s/epoch - 39ms/step
Epoch 65/200
1493/1493 - 58s - loss: 2.6020e-04 - val_loss: 2.4048e-04 - 58s/epoch - 39ms/step
Epoch 66/200
1493/1493 - 59s - loss: 2.0997e-04 - val_loss: 2.0707e-04 - 59s/epoch - 39ms/step
Epoch 67/200
1493/1493 - 58s - loss: 1.9475e-04 - val_loss: 2.0019e-04 - 58s/epoch - 39ms/step
Epoch 68/200
1493/1493 - 59s - loss: 1.9198e-04 - val_loss: 6.8125e-04 - 59s/epoch - 39ms/step
Epoch 69/200
1493/1493 - 59s - loss: 2.6651e-04 - val_loss: 1.9846e-04 - 59s/epoch - 39ms/step
Epoch 70/200
1493/1493 - 59s - loss: 1.9332e-04 - val_loss: 5.8276e-04 - 59s/epoch - 39ms/step
Epoch 71/200
1493/1493 - 59s - loss: 2.4067e-04 - val_loss: 1.7881e-04 - 59s/epoch - 39ms/step
Epoch 72/200
1493/1493 - 59s - loss: 1.9405e-04 - val_loss: 2.3892e-04 - 59s/epoch - 39ms/step
Epoch 73/200
1493/1493 - 59s - loss: 1.9367e-04 - val_loss: 2.2480e-04 - 59s/epoch - 39ms/step
Epoch 74/200
1493/1493 - 58s - loss: 1.9048e-04 - val_loss: 1.9423e-04 - 58s/epoch - 39ms/step
Epoch 75/200
1493/1493 - 58s - loss: 1.8092e-04 - val_loss: 1.9216e-04 - 58s/epoch - 39ms/step
Epoch 76/200
1493/1493 - 59s - loss: 1.8003e-04 - val_loss: 1.7644e-04 - 59s/epoch - 39ms/step
Epoch 77/200
1493/1493 - 58s - loss: 1.7690e-04 - val_loss: 4.1392e-04 - 58s/epoch - 39ms/step
Epoch 78/200
1493/1493 - 58s - loss: 1.8314e-04 - val_loss: 4.3081e-04 - 58s/epoch - 39ms/step
Epoch 79/200
1493/1493 - 59s - loss: 2.0126e-04 - val_loss: 4.2593e-04 - 59s/epoch - 39ms/step
Epoch 80/200
1493/1493 - 58s - loss: 2.0301e-04 - val_loss: 1.5820e-04 - 58s/epoch - 39ms/step
Epoch 81/200
1493/1493 - 58s - loss: 1.7536e-04 - val_loss: 4.2295e-04 - 58s/epoch - 39ms/step
Epoch 82/200
1493/1493 - 58s - loss: 1.7052e-04 - val_loss: 1.8791e-04 - 58s/epoch - 39ms/step
Epoch 83/200
1493/1493 - 58s - loss: 1.6743e-04 - val_loss: 1.8169e-04 - 58s/epoch - 39ms/step
Epoch 84/200
1493/1493 - 58s - loss: 1.7105e-04 - val_loss: 1.8701e-04 - 58s/epoch - 39ms/step
Epoch 85/200
1493/1493 - 59s - loss: 1.6699e-04 - val_loss: 2.2046e-04 - 59s/epoch - 39ms/step
Epoch 86/200
1493/1493 - 59s - loss: 1.6686e-04 - val_loss: 1.8509e-04 - 59s/epoch - 39ms/step
Epoch 87/200
1493/1493 - 58s - loss: 1.6368e-04 - val_loss: 1.8449e-04 - 58s/epoch - 39ms/step
Epoch 88/200
1493/1493 - 58s - loss: 1.6282e-04 - val_loss: 2.8054e-04 - 58s/epoch - 39ms/step
Epoch 89/200
1493/1493 - 58s - loss: 1.6972e-04 - val_loss: 1.7966e-04 - 58s/epoch - 39ms/step
Epoch 90/200
1493/1493 - 58s - loss: 1.5830e-04 - val_loss: 1.9005e-04 - 58s/epoch - 39ms/step
Epoch 91/200
1493/1493 - 58s - loss: 1.5757e-04 - val_loss: 1.9021e-04 - 58s/epoch - 39ms/step
Epoch 92/200
1493/1493 - 58s - loss: 1.5716e-04 - val_loss: 5.8519e-04 - 58s/epoch - 39ms/step
Epoch 93/200
1493/1493 - 58s - loss: 2.0958e-04 - val_loss: 2.2640e-04 - 58s/epoch - 39ms/step
Epoch 94/200
1493/1493 - 58s - loss: 1.6765e-04 - val_loss: 1.3180e-04 - 58s/epoch - 39ms/step
Epoch 95/200
1493/1493 - 58s - loss: 1.5551e-04 - val_loss: 2.4201e-04 - 58s/epoch - 39ms/step
Epoch 96/200
1493/1493 - 58s - loss: 1.5799e-04 - val_loss: 1.9650e-04 - 58s/epoch - 39ms/step
Epoch 97/200
1493/1493 - 58s - loss: 1.5342e-04 - val_loss: 1.6763e-04 - 58s/epoch - 39ms/step
Epoch 98/200
1493/1493 - 58s - loss: 1.5707e-04 - val_loss: 8.5631e-04 - 58s/epoch - 39ms/step
Epoch 99/200
1493/1493 - 58s - loss: 2.2844e-04 - val_loss: 1.5142e-04 - 58s/epoch - 39ms/step
Epoch 100/200
1493/1493 - 58s - loss: 1.6125e-04 - val_loss: 1.5396e-04 - 58s/epoch - 39ms/step
Epoch 101/200
1493/1493 - 58s - loss: 1.5243e-04 - val_loss: 5.5537e-04 - 58s/epoch - 39ms/step
Epoch 102/200
1493/1493 - 58s - loss: 1.7542e-04 - val_loss: 2.1494e-04 - 58s/epoch - 39ms/step
Epoch 103/200
1493/1493 - 59s - loss: 1.5399e-04 - val_loss: 1.5598e-04 - 59s/epoch - 39ms/step
Epoch 104/200
1493/1493 - 59s - loss: 1.4742e-04 - val_loss: 2.3024e-04 - 59s/epoch - 39ms/step
Epoch 105/200
1493/1493 - 59s - loss: 1.5298e-04 - val_loss: 1.6415e-04 - 59s/epoch - 39ms/step
Epoch 106/200
1493/1493 - 59s - loss: 1.4549e-04 - val_loss: 1.7599e-04 - 59s/epoch - 39ms/step
Epoch 107/200
1493/1493 - 59s - loss: 1.4384e-04 - val_loss: 1.6730e-04 - 59s/epoch - 39ms/step
Epoch 108/200
1493/1493 - 59s - loss: 1.4601e-04 - val_loss: 1.6462e-04 - 59s/epoch - 39ms/step
Epoch 109/200
1493/1493 - 59s - loss: 1.4363e-04 - val_loss: 1.7507e-04 - 59s/epoch - 39ms/step
Epoch 110/200
1493/1493 - 59s - loss: 1.4124e-04 - val_loss: 1.4543e-04 - 59s/epoch - 39ms/step
Epoch 111/200
1493/1493 - 59s - loss: 1.4174e-04 - val_loss: 4.5845e-04 - 59s/epoch - 39ms/step
Epoch 112/200
1493/1493 - 59s - loss: 1.6949e-04 - val_loss: 1.3100e-04 - 59s/epoch - 39ms/step
Epoch 113/200
1493/1493 - 59s - loss: 1.4388e-04 - val_loss: 5.4704e-04 - 59s/epoch - 39ms/step
Epoch 114/200
1493/1493 - 59s - loss: 2.0280e-04 - val_loss: 1.4104e-04 - 59s/epoch - 39ms/step
Epoch 115/200
1493/1493 - 59s - loss: 1.4519e-04 - val_loss: 2.0702e-04 - 59s/epoch - 39ms/step
Epoch 116/200
1493/1493 - 59s - loss: 1.4126e-04 - val_loss: 1.7900e-04 - 59s/epoch - 39ms/step
Epoch 117/200
1493/1493 - 59s - loss: 1.3884e-04 - val_loss: 1.8463e-04 - 59s/epoch - 39ms/step
Epoch 118/200
1493/1493 - 59s - loss: 1.4384e-04 - val_loss: 1.5654e-04 - 59s/epoch - 39ms/step
Epoch 119/200
1493/1493 - 59s - loss: 1.3695e-04 - val_loss: 1.3632e-04 - 59s/epoch - 39ms/step
Epoch 120/200
1493/1493 - 58s - loss: 1.3497e-04 - val_loss: 1.4918e-04 - 58s/epoch - 39ms/step
Epoch 121/200
1493/1493 - 58s - loss: 1.3523e-04 - val_loss: 2.3736e-04 - 58s/epoch - 39ms/step
Epoch 122/200
1493/1493 - 58s - loss: 1.4400e-04 - val_loss: 1.2968e-04 - 58s/epoch - 39ms/step
Epoch 123/200
1493/1493 - 58s - loss: 1.3508e-04 - val_loss: 1.4233e-04 - 58s/epoch - 39ms/step
Epoch 124/200
1493/1493 - 59s - loss: 1.3344e-04 - val_loss: 5.4775e-04 - 59s/epoch - 39ms/step
Epoch 125/200
1493/1493 - 59s - loss: 1.6086e-04 - val_loss: 1.4829e-04 - 59s/epoch - 39ms/step
Epoch 126/200
1493/1493 - 59s - loss: 1.3458e-04 - val_loss: 3.8938e-04 - 59s/epoch - 39ms/step
Epoch 127/200
1493/1493 - 59s - loss: 1.4709e-04 - val_loss: 2.1807e-04 - 59s/epoch - 39ms/step
Epoch 128/200
1493/1493 - 59s - loss: 1.3935e-04 - val_loss: 1.2319e-04 - 59s/epoch - 39ms/step
Epoch 129/200
1493/1493 - 59s - loss: 1.2969e-04 - val_loss: 1.3567e-04 - 59s/epoch - 39ms/step
Epoch 130/200
1493/1493 - 58s - loss: 1.2884e-04 - val_loss: 1.2788e-04 - 58s/epoch - 39ms/step
Epoch 131/200
1493/1493 - 58s - loss: 1.3228e-04 - val_loss: 5.6726e-04 - 58s/epoch - 39ms/step
Epoch 132/200
slurmstepd: error: *** JOB 35395968 ON mb-icg101 CANCELLED AT 2022-12-29T23:12:59 ***
