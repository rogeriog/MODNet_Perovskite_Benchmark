start
Mon Dec 26 21:38:32 CET 2022
2022-12-26 21:38:33.354805: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-26 21:38:33.434163: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-12-26 21:39:04,283 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f1efe58b760> object, created with modnet version 0.1.12
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
./DATAFILES/MP_GapFeats_n_b already created.
Shape of dataset to encode: (106113, 1264)
2022-12-26 21:39:06.155871: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense (Dense)               (None, 1264)              1598960   
                                                                 
 batch_normalization (BatchN  (None, 1264)             5056      
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 1264)              800112    
                                                                 
 batch_normalization_2 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 1264)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Epoch 1/100
5969/5969 - 105s - loss: 0.0074 - val_loss: 0.0030 - 105s/epoch - 18ms/step
Epoch 2/100
5969/5969 - 104s - loss: 0.0033 - val_loss: 0.0021 - 104s/epoch - 17ms/step
Epoch 3/100
5969/5969 - 103s - loss: 0.0024 - val_loss: 0.0020 - 103s/epoch - 17ms/step
Epoch 4/100
5969/5969 - 103s - loss: 0.0018 - val_loss: 0.0012 - 103s/epoch - 17ms/step
Epoch 5/100
5969/5969 - 103s - loss: 0.0015 - val_loss: 0.0010 - 103s/epoch - 17ms/step
Epoch 6/100
5969/5969 - 103s - loss: 0.0013 - val_loss: 9.5210e-04 - 103s/epoch - 17ms/step
Epoch 7/100
5969/5969 - 103s - loss: 0.0011 - val_loss: 8.5627e-04 - 103s/epoch - 17ms/step
Epoch 8/100
5969/5969 - 103s - loss: 0.0010 - val_loss: 7.9782e-04 - 103s/epoch - 17ms/step
Epoch 9/100
5969/5969 - 103s - loss: 9.8299e-04 - val_loss: 6.9396e-04 - 103s/epoch - 17ms/step
Epoch 10/100
5969/5969 - 103s - loss: 9.1539e-04 - val_loss: 6.9562e-04 - 103s/epoch - 17ms/step
Epoch 11/100
5969/5969 - 103s - loss: 8.6572e-04 - val_loss: 6.7470e-04 - 103s/epoch - 17ms/step
Epoch 12/100
5969/5969 - 103s - loss: 8.2182e-04 - val_loss: 6.0623e-04 - 103s/epoch - 17ms/step
Epoch 13/100
5969/5969 - 103s - loss: 8.0106e-04 - val_loss: 5.9358e-04 - 103s/epoch - 17ms/step
Epoch 14/100
5969/5969 - 103s - loss: 7.7502e-04 - val_loss: 5.6960e-04 - 103s/epoch - 17ms/step
Epoch 15/100
5969/5969 - 103s - loss: 7.4803e-04 - val_loss: 6.0548e-04 - 103s/epoch - 17ms/step
Epoch 16/100
5969/5969 - 103s - loss: 7.2293e-04 - val_loss: 6.1693e-04 - 103s/epoch - 17ms/step
Epoch 17/100
5969/5969 - 103s - loss: 7.0681e-04 - val_loss: 5.4688e-04 - 103s/epoch - 17ms/step
Epoch 18/100
5969/5969 - 103s - loss: 6.9833e-04 - val_loss: 5.1918e-04 - 103s/epoch - 17ms/step
Epoch 19/100
5969/5969 - 103s - loss: 6.6730e-04 - val_loss: 5.1998e-04 - 103s/epoch - 17ms/step
Epoch 20/100
5969/5969 - 103s - loss: 6.5692e-04 - val_loss: 5.1147e-04 - 103s/epoch - 17ms/step
Epoch 21/100
5969/5969 - 103s - loss: 6.5683e-04 - val_loss: 4.8100e-04 - 103s/epoch - 17ms/step
Epoch 22/100
5969/5969 - 103s - loss: 6.3707e-04 - val_loss: 4.9368e-04 - 103s/epoch - 17ms/step
Epoch 23/100
5969/5969 - 103s - loss: 6.1627e-04 - val_loss: 4.7156e-04 - 103s/epoch - 17ms/step
Epoch 24/100
5969/5969 - 103s - loss: 6.1118e-04 - val_loss: 4.7472e-04 - 103s/epoch - 17ms/step
Epoch 25/100
5969/5969 - 103s - loss: 6.0170e-04 - val_loss: 4.9144e-04 - 103s/epoch - 17ms/step
Epoch 26/100
5969/5969 - 103s - loss: 6.0027e-04 - val_loss: 4.3485e-04 - 103s/epoch - 17ms/step
Epoch 27/100
5969/5969 - 103s - loss: 5.8594e-04 - val_loss: 4.5439e-04 - 103s/epoch - 17ms/step
Epoch 28/100
5969/5969 - 103s - loss: 5.7916e-04 - val_loss: 4.2212e-04 - 103s/epoch - 17ms/step
Epoch 29/100
5969/5969 - 103s - loss: 5.7157e-04 - val_loss: 5.1388e-04 - 103s/epoch - 17ms/step
Epoch 30/100
5969/5969 - 103s - loss: 5.5661e-04 - val_loss: 4.2520e-04 - 103s/epoch - 17ms/step
Epoch 31/100
5969/5969 - 103s - loss: 5.6549e-04 - val_loss: 4.1695e-04 - 103s/epoch - 17ms/step
Epoch 32/100
5969/5969 - 103s - loss: 5.4230e-04 - val_loss: 4.2216e-04 - 103s/epoch - 17ms/step
Epoch 33/100
5969/5969 - 103s - loss: 5.3962e-04 - val_loss: 3.9138e-04 - 103s/epoch - 17ms/step
Epoch 34/100
5969/5969 - 103s - loss: 5.2918e-04 - val_loss: 3.9335e-04 - 103s/epoch - 17ms/step
Epoch 35/100
5969/5969 - 103s - loss: 5.2892e-04 - val_loss: 3.9716e-04 - 103s/epoch - 17ms/step
Epoch 36/100
5969/5969 - 104s - loss: 5.1986e-04 - val_loss: 4.1821e-04 - 104s/epoch - 17ms/step
Epoch 37/100
5969/5969 - 104s - loss: 5.2280e-04 - val_loss: 4.0073e-04 - 104s/epoch - 18ms/step
Epoch 38/100
5969/5969 - 104s - loss: 5.1477e-04 - val_loss: 3.9306e-04 - 104s/epoch - 17ms/step
Epoch 39/100
5969/5969 - 104s - loss: 5.0867e-04 - val_loss: 3.8221e-04 - 104s/epoch - 17ms/step
Epoch 40/100
5969/5969 - 104s - loss: 5.0502e-04 - val_loss: 3.8870e-04 - 104s/epoch - 17ms/step
Epoch 41/100
5969/5969 - 104s - loss: 5.0746e-04 - val_loss: 3.6580e-04 - 104s/epoch - 17ms/step
Epoch 42/100
5969/5969 - 104s - loss: 5.0175e-04 - val_loss: 3.6811e-04 - 104s/epoch - 17ms/step
Epoch 43/100
5969/5969 - 104s - loss: 4.8577e-04 - val_loss: 3.5866e-04 - 104s/epoch - 17ms/step
Epoch 44/100
5969/5969 - 104s - loss: 4.9001e-04 - val_loss: 3.8010e-04 - 104s/epoch - 17ms/step
Epoch 45/100
5969/5969 - 104s - loss: 4.8632e-04 - val_loss: 3.5520e-04 - 104s/epoch - 17ms/step
Epoch 46/100
5969/5969 - 104s - loss: 4.8141e-04 - val_loss: 3.7332e-04 - 104s/epoch - 17ms/step
Epoch 47/100
5969/5969 - 104s - loss: 4.8481e-04 - val_loss: 3.5821e-04 - 104s/epoch - 17ms/step
Epoch 48/100
5969/5969 - 104s - loss: 4.7634e-04 - val_loss: 3.8389e-04 - 104s/epoch - 17ms/step
Epoch 49/100
5969/5969 - 104s - loss: 4.7621e-04 - val_loss: 3.6267e-04 - 104s/epoch - 17ms/step
Epoch 50/100
5969/5969 - 104s - loss: 4.7094e-04 - val_loss: 3.8570e-04 - 104s/epoch - 17ms/step
Epoch 51/100
5969/5969 - 104s - loss: 4.6443e-04 - val_loss: 3.4511e-04 - 104s/epoch - 17ms/step
Epoch 52/100
5969/5969 - 104s - loss: 4.5981e-04 - val_loss: 3.3948e-04 - 104s/epoch - 17ms/step
Epoch 53/100
5969/5969 - 104s - loss: 4.6142e-04 - val_loss: 3.3890e-04 - 104s/epoch - 17ms/step
Epoch 54/100
5969/5969 - 104s - loss: 4.6187e-04 - val_loss: 3.7522e-04 - 104s/epoch - 17ms/step
Epoch 55/100
5969/5969 - 104s - loss: 4.5760e-04 - val_loss: 3.3463e-04 - 104s/epoch - 17ms/step
Epoch 56/100
5969/5969 - 104s - loss: 4.5321e-04 - val_loss: 3.3878e-04 - 104s/epoch - 17ms/step
Epoch 57/100
5969/5969 - 104s - loss: 4.5272e-04 - val_loss: 3.3737e-04 - 104s/epoch - 17ms/step
Epoch 58/100
5969/5969 - 103s - loss: 4.4562e-04 - val_loss: 3.4522e-04 - 103s/epoch - 17ms/step
Epoch 59/100
5969/5969 - 103s - loss: 4.4291e-04 - val_loss: 3.3302e-04 - 103s/epoch - 17ms/step
Epoch 60/100
5969/5969 - 103s - loss: 4.4610e-04 - val_loss: 3.2496e-04 - 103s/epoch - 17ms/step
Epoch 61/100
5969/5969 - 103s - loss: 4.4155e-04 - val_loss: 3.5621e-04 - 103s/epoch - 17ms/step
Epoch 62/100
5969/5969 - 103s - loss: 4.4379e-04 - val_loss: 3.5140e-04 - 103s/epoch - 17ms/step
Epoch 63/100
5969/5969 - 103s - loss: 4.4411e-04 - val_loss: 3.2879e-04 - 103s/epoch - 17ms/step
Epoch 64/100
5969/5969 - 103s - loss: 4.3968e-04 - val_loss: 3.2080e-04 - 103s/epoch - 17ms/step
Epoch 65/100
5969/5969 - 103s - loss: 4.3931e-04 - val_loss: 3.3860e-04 - 103s/epoch - 17ms/step
Epoch 66/100
5969/5969 - 103s - loss: 4.3137e-04 - val_loss: 3.3077e-04 - 103s/epoch - 17ms/step
Epoch 67/100
5969/5969 - 103s - loss: 4.3485e-04 - val_loss: 3.2576e-04 - 103s/epoch - 17ms/step
Epoch 68/100
5969/5969 - 103s - loss: 4.3083e-04 - val_loss: 3.2977e-04 - 103s/epoch - 17ms/step
Epoch 69/100
5969/5969 - 103s - loss: 4.2902e-04 - val_loss: 3.2271e-04 - 103s/epoch - 17ms/step
Epoch 70/100
5969/5969 - 103s - loss: 4.2217e-04 - val_loss: 3.1298e-04 - 103s/epoch - 17ms/step
Epoch 71/100
5969/5969 - 103s - loss: 4.2842e-04 - val_loss: 3.3794e-04 - 103s/epoch - 17ms/step
Epoch 72/100
5969/5969 - 103s - loss: 4.2559e-04 - val_loss: 3.2761e-04 - 103s/epoch - 17ms/step
Epoch 73/100
5969/5969 - 103s - loss: 4.2379e-04 - val_loss: 3.2280e-04 - 103s/epoch - 17ms/step
Epoch 74/100
5969/5969 - 103s - loss: 4.2359e-04 - val_loss: 3.2613e-04 - 103s/epoch - 17ms/step
Epoch 75/100
5969/5969 - 103s - loss: 4.1855e-04 - val_loss: 3.2662e-04 - 103s/epoch - 17ms/step
Epoch 76/100
5969/5969 - 103s - loss: 4.1058e-04 - val_loss: 3.1016e-04 - 103s/epoch - 17ms/step
Epoch 77/100
5969/5969 - 103s - loss: 4.1896e-04 - val_loss: 3.0855e-04 - 103s/epoch - 17ms/step
Epoch 78/100
5969/5969 - 103s - loss: 4.1650e-04 - val_loss: 3.3993e-04 - 103s/epoch - 17ms/step
Epoch 79/100
5969/5969 - 103s - loss: 4.1329e-04 - val_loss: 3.1442e-04 - 103s/epoch - 17ms/step
Epoch 80/100
5969/5969 - 103s - loss: 4.1329e-04 - val_loss: 3.0561e-04 - 103s/epoch - 17ms/step
Epoch 81/100
5969/5969 - 103s - loss: 4.0781e-04 - val_loss: 3.1275e-04 - 103s/epoch - 17ms/step
Epoch 82/100
5969/5969 - 104s - loss: 4.0845e-04 - val_loss: 3.0737e-04 - 104s/epoch - 17ms/step
Epoch 83/100
5969/5969 - 103s - loss: 4.1497e-04 - val_loss: 3.4053e-04 - 103s/epoch - 17ms/step
Epoch 84/100
5969/5969 - 104s - loss: 4.0649e-04 - val_loss: 3.0431e-04 - 104s/epoch - 17ms/step
Epoch 85/100
5969/5969 - 104s - loss: 4.0431e-04 - val_loss: 3.0925e-04 - 104s/epoch - 17ms/step
Epoch 86/100
5969/5969 - 103s - loss: 4.0237e-04 - val_loss: 3.1798e-04 - 103s/epoch - 17ms/step
Epoch 87/100
5969/5969 - 104s - loss: 4.0181e-04 - val_loss: 3.2484e-04 - 104s/epoch - 17ms/step
Epoch 88/100
5969/5969 - 104s - loss: 3.9957e-04 - val_loss: 2.9192e-04 - 104s/epoch - 17ms/step
Epoch 89/100
5969/5969 - 103s - loss: 4.0848e-04 - val_loss: 3.1177e-04 - 103s/epoch - 17ms/step
Epoch 90/100
5969/5969 - 103s - loss: 3.9977e-04 - val_loss: 2.9157e-04 - 103s/epoch - 17ms/step
Epoch 91/100
5969/5969 - 103s - loss: 3.9509e-04 - val_loss: 4.0297e-04 - 103s/epoch - 17ms/step
Epoch 92/100
5969/5969 - 103s - loss: 3.9471e-04 - val_loss: 3.0470e-04 - 103s/epoch - 17ms/step
Epoch 93/100
5969/5969 - 103s - loss: 3.9549e-04 - val_loss: 3.0749e-04 - 103s/epoch - 17ms/step
Epoch 94/100
5969/5969 - 104s - loss: 3.9476e-04 - val_loss: 2.9380e-04 - 104s/epoch - 17ms/step
Epoch 95/100
5969/5969 - 103s - loss: 3.9490e-04 - val_loss: 2.9551e-04 - 103s/epoch - 17ms/step
Epoch 96/100
5969/5969 - 104s - loss: 3.9732e-04 - val_loss: 3.0843e-04 - 104s/epoch - 17ms/step
Epoch 97/100
5969/5969 - 104s - loss: 3.9977e-04 - val_loss: 3.0126e-04 - 104s/epoch - 17ms/step
Epoch 98/100
5969/5969 - 103s - loss: 3.9101e-04 - val_loss: 2.9929e-04 - 103s/epoch - 17ms/step
Epoch 99/100
5969/5969 - 103s - loss: 3.9378e-04 - val_loss: 2.9098e-04 - 103s/epoch - 17ms/step
Epoch 100/100
5969/5969 - 103s - loss: 3.8830e-04 - val_loss: 2.9435e-04 - 103s/epoch - 17ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00029435346368700266
  1/332 [..............................] - ETA: 36s 15/332 [>.............................] - ETA: 1s  31/332 [=>............................] - ETA: 1s 46/332 [===>..........................] - ETA: 0s 62/332 [====>.........................] - ETA: 0s 78/332 [======>.......................] - ETA: 0s 93/332 [=======>......................] - ETA: 0s108/332 [========>.....................] - ETA: 0s123/332 [==========>...................] - ETA: 0s139/332 [===========>..................] - ETA: 0s154/332 [============>.................] - ETA: 0s170/332 [==============>...............] - ETA: 0s186/332 [===============>..............] - ETA: 0s202/332 [=================>............] - ETA: 0s217/332 [==================>...........] - ETA: 0s232/332 [===================>..........] - ETA: 0s248/332 [=====================>........] - ETA: 0s264/332 [======================>.......] - ETA: 0s280/332 [========================>.....] - ETA: 0s296/332 [=========================>....] - ETA: 0s312/332 [===========================>..] - ETA: 0s328/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 3ms/step
correlation 0.003199858611321117
cosine 0.0025708302209568705
MAE: 0.009135129
RMSE: 0.017156724
r2: 0.9809047135866523
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 1264)              1598960   
                                                                 
 batch_normalization (BatchN  (None, 1264)             5056      
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 1264)              800112    
                                                                 
 batch_normalization_2 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 1264)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Encoder
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 1264)              1598960   
                                                                 
 batch_normalization (BatchN  (None, 1264)             5056      
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
=================================================================
Total params: 2,403,496
Trainable params: 2,400,968
Non-trainable params: 2,528
_________________________________________________________________
Decoder
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 1264)              800112    
                                                                 
 batch_normalization_2 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 1264)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 2,406,656
Trainable params: 2,402,864
Non-trainable params: 3,792
_________________________________________________________________
['n_b', 'mse', 16, 100, 0.0005, 0.5, 632, 0.00038830123958177865, 0.00029435346368700266, 0.003199858611321117, 0.0025708302209568705, 0.009135128930211067, 0.017156723886728287, 0.9809047135866523, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_3 (Dense)             (None, 1264)              1598960   
                                                                 
 batch_normalization_3 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 1264)              800112    
                                                                 
 batch_normalization_5 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 1264)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Epoch 1/100
5969/5969 - 109s - loss: 0.0065 - val_loss: 0.0025 - 109s/epoch - 18ms/step
Epoch 2/100
5969/5969 - 109s - loss: 0.0027 - val_loss: 0.0018 - 109s/epoch - 18ms/step
Epoch 3/100
5969/5969 - 109s - loss: 0.0018 - val_loss: 0.0015 - 109s/epoch - 18ms/step
Epoch 4/100
5969/5969 - 108s - loss: 0.0015 - val_loss: 0.0011 - 108s/epoch - 18ms/step
Epoch 5/100
5969/5969 - 109s - loss: 0.0012 - val_loss: 0.0010 - 109s/epoch - 18ms/step
Epoch 6/100
5969/5969 - 109s - loss: 0.0011 - val_loss: 9.0170e-04 - 109s/epoch - 18ms/step
Epoch 7/100
5969/5969 - 108s - loss: 0.0010 - val_loss: 8.3396e-04 - 108s/epoch - 18ms/step
Epoch 8/100
5969/5969 - 109s - loss: 9.8714e-04 - val_loss: 7.7997e-04 - 109s/epoch - 18ms/step
Epoch 9/100
5969/5969 - 109s - loss: 9.4735e-04 - val_loss: 7.2245e-04 - 109s/epoch - 18ms/step
Epoch 10/100
5969/5969 - 108s - loss: 9.0626e-04 - val_loss: 7.1229e-04 - 108s/epoch - 18ms/step
Epoch 11/100
5969/5969 - 109s - loss: 8.7234e-04 - val_loss: 6.8199e-04 - 109s/epoch - 18ms/step
Epoch 12/100
5969/5969 - 109s - loss: 8.4452e-04 - val_loss: 6.5428e-04 - 109s/epoch - 18ms/step
Epoch 13/100
5969/5969 - 108s - loss: 8.0826e-04 - val_loss: 6.4193e-04 - 108s/epoch - 18ms/step
Epoch 14/100
5969/5969 - 109s - loss: 7.8311e-04 - val_loss: 6.4623e-04 - 109s/epoch - 18ms/step
Epoch 15/100
5969/5969 - 109s - loss: 7.7051e-04 - val_loss: 6.4174e-04 - 109s/epoch - 18ms/step
Epoch 16/100
5969/5969 - 108s - loss: 7.5058e-04 - val_loss: 7.2158e-04 - 108s/epoch - 18ms/step
Epoch 17/100
5969/5969 - 109s - loss: 7.4624e-04 - val_loss: 6.1199e-04 - 109s/epoch - 18ms/step
Epoch 18/100
5969/5969 - 108s - loss: 7.2235e-04 - val_loss: 5.9750e-04 - 108s/epoch - 18ms/step
Epoch 19/100
5969/5969 - 109s - loss: 7.0298e-04 - val_loss: 5.6299e-04 - 109s/epoch - 18ms/step
Epoch 20/100
5969/5969 - 109s - loss: 7.0715e-04 - val_loss: 5.7459e-04 - 109s/epoch - 18ms/step
Epoch 21/100
5969/5969 - 108s - loss: 6.9077e-04 - val_loss: 5.3499e-04 - 108s/epoch - 18ms/step
Epoch 22/100
5969/5969 - 109s - loss: 6.8436e-04 - val_loss: 5.8722e-04 - 109s/epoch - 18ms/step
Epoch 23/100
5969/5969 - 109s - loss: 6.6168e-04 - val_loss: 5.7247e-04 - 109s/epoch - 18ms/step
Epoch 24/100
5969/5969 - 108s - loss: 6.6118e-04 - val_loss: 5.4670e-04 - 108s/epoch - 18ms/step
Epoch 25/100
5969/5969 - 109s - loss: 6.4529e-04 - val_loss: 5.0990e-04 - 109s/epoch - 18ms/step
Epoch 26/100
5969/5969 - 109s - loss: 6.4172e-04 - val_loss: 5.4029e-04 - 109s/epoch - 18ms/step
Epoch 27/100
5969/5969 - 108s - loss: 6.2807e-04 - val_loss: 5.0445e-04 - 108s/epoch - 18ms/step
Epoch 28/100
5969/5969 - 109s - loss: 6.4401e-04 - val_loss: 4.9808e-04 - 109s/epoch - 18ms/step
Epoch 29/100
5969/5969 - 109s - loss: 6.3130e-04 - val_loss: 5.6858e-04 - 109s/epoch - 18ms/step
Epoch 30/100
5969/5969 - 108s - loss: 6.1047e-04 - val_loss: 5.2371e-04 - 108s/epoch - 18ms/step
Epoch 31/100
5969/5969 - 109s - loss: 6.1511e-04 - val_loss: 4.7719e-04 - 109s/epoch - 18ms/step
Epoch 32/100
5969/5969 - 109s - loss: 5.9840e-04 - val_loss: 4.7948e-04 - 109s/epoch - 18ms/step
Epoch 33/100
5969/5969 - 108s - loss: 5.9678e-04 - val_loss: 4.7193e-04 - 108s/epoch - 18ms/step
Epoch 34/100
5969/5969 - 108s - loss: 5.9108e-04 - val_loss: 4.9648e-04 - 108s/epoch - 18ms/step
Epoch 35/100
5969/5969 - 108s - loss: 5.8601e-04 - val_loss: 4.5524e-04 - 108s/epoch - 18ms/step
Epoch 36/100
5969/5969 - 108s - loss: 5.9295e-04 - val_loss: 5.0188e-04 - 108s/epoch - 18ms/step
Epoch 37/100
5969/5969 - 108s - loss: 5.8149e-04 - val_loss: 4.9495e-04 - 108s/epoch - 18ms/step
Epoch 38/100
5969/5969 - 107s - loss: 5.8458e-04 - val_loss: 4.7419e-04 - 107s/epoch - 18ms/step
Epoch 39/100
5969/5969 - 108s - loss: 5.7253e-04 - val_loss: 4.7415e-04 - 108s/epoch - 18ms/step
Epoch 40/100
5969/5969 - 108s - loss: 5.6925e-04 - val_loss: 4.3210e-04 - 108s/epoch - 18ms/step
Epoch 41/100
5969/5969 - 107s - loss: 5.6224e-04 - val_loss: 4.4111e-04 - 107s/epoch - 18ms/step
Epoch 42/100
5969/5969 - 108s - loss: 5.6443e-04 - val_loss: 4.4396e-04 - 108s/epoch - 18ms/step
Epoch 43/100
5969/5969 - 108s - loss: 5.5959e-04 - val_loss: 4.5319e-04 - 108s/epoch - 18ms/step
Epoch 44/100
5969/5969 - 107s - loss: 5.4953e-04 - val_loss: 4.2271e-04 - 107s/epoch - 18ms/step
Epoch 45/100
5969/5969 - 108s - loss: 5.6041e-04 - val_loss: 4.2804e-04 - 108s/epoch - 18ms/step
Epoch 46/100
5969/5969 - 108s - loss: 5.3775e-04 - val_loss: 4.5278e-04 - 108s/epoch - 18ms/step
Epoch 47/100
5969/5969 - 107s - loss: 5.5028e-04 - val_loss: 4.3760e-04 - 107s/epoch - 18ms/step
Epoch 48/100
5969/5969 - 108s - loss: 5.3865e-04 - val_loss: 4.5247e-04 - 108s/epoch - 18ms/step
Epoch 49/100
5969/5969 - 108s - loss: 5.3608e-04 - val_loss: 4.8279e-04 - 108s/epoch - 18ms/step
Epoch 50/100
5969/5969 - 107s - loss: 5.3121e-04 - val_loss: 4.6753e-04 - 107s/epoch - 18ms/step
Epoch 51/100
5969/5969 - 108s - loss: 5.3418e-04 - val_loss: 4.4830e-04 - 108s/epoch - 18ms/step
Epoch 52/100
5969/5969 - 108s - loss: 5.2715e-04 - val_loss: 4.5193e-04 - 108s/epoch - 18ms/step
Epoch 53/100
5969/5969 - 107s - loss: 5.3570e-04 - val_loss: 4.5409e-04 - 107s/epoch - 18ms/step
Epoch 54/100
5969/5969 - 108s - loss: 5.2292e-04 - val_loss: 4.4841e-04 - 108s/epoch - 18ms/step
Epoch 55/100
5969/5969 - 108s - loss: 5.1861e-04 - val_loss: 4.2778e-04 - 108s/epoch - 18ms/step
Epoch 56/100
5969/5969 - 107s - loss: 5.1893e-04 - val_loss: 4.5549e-04 - 107s/epoch - 18ms/step
Epoch 57/100
5969/5969 - 108s - loss: 5.1121e-04 - val_loss: 4.2708e-04 - 108s/epoch - 18ms/step
Epoch 58/100
5969/5969 - 108s - loss: 5.2233e-04 - val_loss: 4.3427e-04 - 108s/epoch - 18ms/step
Epoch 59/100
5969/5969 - 107s - loss: 5.0464e-04 - val_loss: 3.8071e-04 - 107s/epoch - 18ms/step
Epoch 60/100
5969/5969 - 108s - loss: 5.0979e-04 - val_loss: 3.7994e-04 - 108s/epoch - 18ms/step
Epoch 61/100
5969/5969 - 107s - loss: 5.0792e-04 - val_loss: 4.3554e-04 - 107s/epoch - 18ms/step
Epoch 62/100
5969/5969 - 108s - loss: 5.0528e-04 - val_loss: 4.4609e-04 - 108s/epoch - 18ms/step
Epoch 63/100
5969/5969 - 108s - loss: 5.0023e-04 - val_loss: 4.0960e-04 - 108s/epoch - 18ms/step
Epoch 64/100
5969/5969 - 107s - loss: 5.0645e-04 - val_loss: 4.1298e-04 - 107s/epoch - 18ms/step
Epoch 65/100
5969/5969 - 108s - loss: 5.0023e-04 - val_loss: 3.9750e-04 - 108s/epoch - 18ms/step
Epoch 66/100
5969/5969 - 108s - loss: 5.0544e-04 - val_loss: 4.0892e-04 - 108s/epoch - 18ms/step
Epoch 67/100
5969/5969 - 107s - loss: 4.9770e-04 - val_loss: 4.1583e-04 - 107s/epoch - 18ms/step
Epoch 68/100
5969/5969 - 108s - loss: 4.9414e-04 - val_loss: 3.8133e-04 - 108s/epoch - 18ms/step
Epoch 69/100
5969/5969 - 108s - loss: 4.9354e-04 - val_loss: 4.1101e-04 - 108s/epoch - 18ms/step
Epoch 70/100
5969/5969 - 107s - loss: 4.8729e-04 - val_loss: 3.9304e-04 - 107s/epoch - 18ms/step
Epoch 71/100
5969/5969 - 108s - loss: 4.8619e-04 - val_loss: 4.1351e-04 - 108s/epoch - 18ms/step
Epoch 72/100
5969/5969 - 108s - loss: 4.9230e-04 - val_loss: 4.2425e-04 - 108s/epoch - 18ms/step
Epoch 73/100
5969/5969 - 107s - loss: 4.8220e-04 - val_loss: 4.0239e-04 - 107s/epoch - 18ms/step
Epoch 74/100
5969/5969 - 108s - loss: 4.8076e-04 - val_loss: 3.8100e-04 - 108s/epoch - 18ms/step
Epoch 75/100
5969/5969 - 108s - loss: 4.7884e-04 - val_loss: 3.9019e-04 - 108s/epoch - 18ms/step
Epoch 76/100
5969/5969 - 107s - loss: 4.7431e-04 - val_loss: 4.0648e-04 - 107s/epoch - 18ms/step
Epoch 77/100
5969/5969 - 108s - loss: 4.7248e-04 - val_loss: 3.7750e-04 - 108s/epoch - 18ms/step
Epoch 78/100
5969/5969 - 108s - loss: 4.7402e-04 - val_loss: 4.7526e-04 - 108s/epoch - 18ms/step
Epoch 79/100
5969/5969 - 107s - loss: 4.8491e-04 - val_loss: 3.8714e-04 - 107s/epoch - 18ms/step
Epoch 80/100
5969/5969 - 108s - loss: 4.7227e-04 - val_loss: 3.8233e-04 - 108s/epoch - 18ms/step
Epoch 81/100
5969/5969 - 108s - loss: 4.6904e-04 - val_loss: 3.7447e-04 - 108s/epoch - 18ms/step
Epoch 82/100
5969/5969 - 107s - loss: 4.8129e-04 - val_loss: 3.9435e-04 - 107s/epoch - 18ms/step
Epoch 83/100
5969/5969 - 108s - loss: 4.6899e-04 - val_loss: 4.0000e-04 - 108s/epoch - 18ms/step
Epoch 84/100
5969/5969 - 107s - loss: 4.7410e-04 - val_loss: 3.6859e-04 - 107s/epoch - 18ms/step
Epoch 85/100
5969/5969 - 108s - loss: 4.7006e-04 - val_loss: 3.6893e-04 - 108s/epoch - 18ms/step
Epoch 86/100
5969/5969 - 108s - loss: 4.6533e-04 - val_loss: 3.9905e-04 - 108s/epoch - 18ms/step
Epoch 87/100
5969/5969 - 107s - loss: 4.6514e-04 - val_loss: 3.8179e-04 - 107s/epoch - 18ms/step
Epoch 88/100
5969/5969 - 108s - loss: 4.6538e-04 - val_loss: 3.5372e-04 - 108s/epoch - 18ms/step
Epoch 89/100
5969/5969 - 108s - loss: 4.6641e-04 - val_loss: 4.3657e-04 - 108s/epoch - 18ms/step
Epoch 90/100
5969/5969 - 107s - loss: 4.5834e-04 - val_loss: 3.6293e-04 - 107s/epoch - 18ms/step
Epoch 91/100
5969/5969 - 108s - loss: 4.6640e-04 - val_loss: 5.8429e-04 - 108s/epoch - 18ms/step
Epoch 92/100
5969/5969 - 108s - loss: 4.6443e-04 - val_loss: 4.0672e-04 - 108s/epoch - 18ms/step
Epoch 93/100
5969/5969 - 107s - loss: 4.5335e-04 - val_loss: 4.0813e-04 - 107s/epoch - 18ms/step
Epoch 94/100
5969/5969 - 108s - loss: 4.6135e-04 - val_loss: 3.5975e-04 - 108s/epoch - 18ms/step
Epoch 95/100
5969/5969 - 108s - loss: 4.5470e-04 - val_loss: 3.6397e-04 - 108s/epoch - 18ms/step
Epoch 96/100
5969/5969 - 107s - loss: 4.5194e-04 - val_loss: 3.7246e-04 - 107s/epoch - 18ms/step
Epoch 97/100
5969/5969 - 108s - loss: 4.7333e-04 - val_loss: 3.5551e-04 - 108s/epoch - 18ms/step
Epoch 98/100
5969/5969 - 108s - loss: 4.5461e-04 - val_loss: 3.4945e-04 - 108s/epoch - 18ms/step
Epoch 99/100
5969/5969 - 107s - loss: 4.5088e-04 - val_loss: 3.4725e-04 - 107s/epoch - 18ms/step
Epoch 100/100
5969/5969 - 108s - loss: 4.5478e-04 - val_loss: 3.5695e-04 - 108s/epoch - 18ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00035694599500857294
  1/332 [..............................] - ETA: 29s 16/332 [>.............................] - ETA: 1s  32/332 [=>............................] - ETA: 1s 48/332 [===>..........................] - ETA: 0s 64/332 [====>.........................] - ETA: 0s 80/332 [======>.......................] - ETA: 0s 96/332 [=======>......................] - ETA: 0s112/332 [=========>....................] - ETA: 0s128/332 [==========>...................] - ETA: 0s144/332 [============>.................] - ETA: 0s160/332 [=============>................] - ETA: 0s176/332 [==============>...............] - ETA: 0s192/332 [================>.............] - ETA: 0s208/332 [=================>............] - ETA: 0s224/332 [===================>..........] - ETA: 0s240/332 [====================>.........] - ETA: 0s256/332 [======================>.......] - ETA: 0s272/332 [=======================>......] - ETA: 0s288/332 [=========================>....] - ETA: 0s304/332 [==========================>...] - ETA: 0s320/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 3ms/step
correlation 0.0038278833329340577
cosine 0.003088130867350924
MAE: 0.009607003
RMSE: 0.018893
r2: 0.9768442124460471
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 1264)              1598960   
                                                                 
 batch_normalization_3 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 1264)              800112    
                                                                 
 batch_normalization_5 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 1264)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Encoder
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 1264)              1598960   
                                                                 
 batch_normalization_3 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
=================================================================
Total params: 2,403,496
Trainable params: 2,400,968
Non-trainable params: 2,528
_________________________________________________________________
Decoder
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 1264)              800112    
                                                                 
 batch_normalization_5 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 1264)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 2,406,656
Trainable params: 2,402,864
Non-trainable params: 3,792
_________________________________________________________________
['n_b', 'mse', 16, 100, 0.001, 0.5, 632, 0.00045478343963623047, 0.00035694599500857294, 0.0038278833329340577, 0.003088130867350924, 0.009607003070414066, 0.01889299973845482, 0.9768442124460471, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_6 (Dense)             (None, 1264)              1598960   
                                                                 
 batch_normalization_6 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 1264)              800112    
                                                                 
 batch_normalization_8 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 1264)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Epoch 1/100
5969/5969 - 107s - loss: 0.0061 - val_loss: 0.0024 - 107s/epoch - 18ms/step
Epoch 2/100
5969/5969 - 106s - loss: 0.0024 - val_loss: 0.0018 - 106s/epoch - 18ms/step
Epoch 3/100
5969/5969 - 107s - loss: 0.0018 - val_loss: 0.0014 - 107s/epoch - 18ms/step
Epoch 4/100
5969/5969 - 107s - loss: 0.0015 - val_loss: 0.0012 - 107s/epoch - 18ms/step
Epoch 5/100
5969/5969 - 106s - loss: 0.0014 - val_loss: 0.0011 - 106s/epoch - 18ms/step
Epoch 6/100
5969/5969 - 107s - loss: 0.0013 - val_loss: 0.0010 - 107s/epoch - 18ms/step
Epoch 7/100
5969/5969 - 106s - loss: 0.0012 - val_loss: 9.7581e-04 - 106s/epoch - 18ms/step
Epoch 8/100
5969/5969 - 106s - loss: 0.0011 - val_loss: 8.8394e-04 - 106s/epoch - 18ms/step
Epoch 9/100
5969/5969 - 106s - loss: 0.0011 - val_loss: 8.8712e-04 - 106s/epoch - 18ms/step
Epoch 10/100
5969/5969 - 106s - loss: 0.0010 - val_loss: 8.1873e-04 - 106s/epoch - 18ms/step
Epoch 11/100
5969/5969 - 106s - loss: 0.0010 - val_loss: 8.1511e-04 - 106s/epoch - 18ms/step
Epoch 12/100
5969/5969 - 106s - loss: 9.8805e-04 - val_loss: 8.3993e-04 - 106s/epoch - 18ms/step
Epoch 13/100
5969/5969 - 106s - loss: 9.5669e-04 - val_loss: 7.7885e-04 - 106s/epoch - 18ms/step
Epoch 14/100
5969/5969 - 106s - loss: 9.6557e-04 - val_loss: 7.6156e-04 - 106s/epoch - 18ms/step
Epoch 15/100
5969/5969 - 106s - loss: 9.3299e-04 - val_loss: 7.3507e-04 - 106s/epoch - 18ms/step
Epoch 16/100
5969/5969 - 106s - loss: 9.0217e-04 - val_loss: 7.6966e-04 - 106s/epoch - 18ms/step
Epoch 17/100
5969/5969 - 106s - loss: 8.8746e-04 - val_loss: 7.0599e-04 - 106s/epoch - 18ms/step
Epoch 18/100
5969/5969 - 106s - loss: 8.9973e-04 - val_loss: 7.3108e-04 - 106s/epoch - 18ms/step
Epoch 19/100
5969/5969 - 106s - loss: 8.5524e-04 - val_loss: 6.9164e-04 - 106s/epoch - 18ms/step
Epoch 20/100
5969/5969 - 106s - loss: 8.5105e-04 - val_loss: 6.7416e-04 - 106s/epoch - 18ms/step
Epoch 21/100
5969/5969 - 106s - loss: 8.2877e-04 - val_loss: 6.4726e-04 - 106s/epoch - 18ms/step
Epoch 22/100
5969/5969 - 106s - loss: 8.3317e-04 - val_loss: 6.4320e-04 - 106s/epoch - 18ms/step
Epoch 23/100
5969/5969 - 106s - loss: 8.1099e-04 - val_loss: 6.4448e-04 - 106s/epoch - 18ms/step
Epoch 24/100
5969/5969 - 106s - loss: 8.0094e-04 - val_loss: 6.4902e-04 - 106s/epoch - 18ms/step
Epoch 25/100
5969/5969 - 106s - loss: 7.9804e-04 - val_loss: 6.4382e-04 - 106s/epoch - 18ms/step
Epoch 26/100
5969/5969 - 106s - loss: 8.1479e-04 - val_loss: 6.0103e-04 - 106s/epoch - 18ms/step
Epoch 27/100
5969/5969 - 106s - loss: 7.9062e-04 - val_loss: 6.0471e-04 - 106s/epoch - 18ms/step
Epoch 28/100
5969/5969 - 106s - loss: 8.1634e-04 - val_loss: 6.2317e-04 - 106s/epoch - 18ms/step
Epoch 29/100
5969/5969 - 106s - loss: 7.8519e-04 - val_loss: 6.1075e-04 - 106s/epoch - 18ms/step
Epoch 30/100
5969/5969 - 106s - loss: 7.5416e-04 - val_loss: 5.6479e-04 - 106s/epoch - 18ms/step
Epoch 31/100
5969/5969 - 106s - loss: 7.5550e-04 - val_loss: 5.4660e-04 - 106s/epoch - 18ms/step
Epoch 32/100
5969/5969 - 106s - loss: 8.0930e-04 - val_loss: 6.1104e-04 - 106s/epoch - 18ms/step
Epoch 33/100
5969/5969 - 106s - loss: 7.6554e-04 - val_loss: 5.9669e-04 - 106s/epoch - 18ms/step
Epoch 34/100
5969/5969 - 106s - loss: 7.5172e-04 - val_loss: 5.6139e-04 - 106s/epoch - 18ms/step
Epoch 35/100
5969/5969 - 106s - loss: 7.4851e-04 - val_loss: 5.5345e-04 - 106s/epoch - 18ms/step
Epoch 36/100
5969/5969 - 107s - loss: 7.2754e-04 - val_loss: 5.9428e-04 - 107s/epoch - 18ms/step
Epoch 37/100
5969/5969 - 106s - loss: 7.4851e-04 - val_loss: 5.6788e-04 - 106s/epoch - 18ms/step
Epoch 38/100
5969/5969 - 106s - loss: 7.5046e-04 - val_loss: 5.5646e-04 - 106s/epoch - 18ms/step
Epoch 39/100
5969/5969 - 106s - loss: 7.2308e-04 - val_loss: 5.3529e-04 - 106s/epoch - 18ms/step
Epoch 40/100
5969/5969 - 106s - loss: 7.3236e-04 - val_loss: 5.2974e-04 - 106s/epoch - 18ms/step
Epoch 41/100
5969/5969 - 106s - loss: 7.1048e-04 - val_loss: 5.4031e-04 - 106s/epoch - 18ms/step
Epoch 42/100
5969/5969 - 106s - loss: 7.1520e-04 - val_loss: 5.2389e-04 - 106s/epoch - 18ms/step
Epoch 43/100
5969/5969 - 106s - loss: 6.9295e-04 - val_loss: 5.2935e-04 - 106s/epoch - 18ms/step
Epoch 44/100
5969/5969 - 107s - loss: 6.9374e-04 - val_loss: 5.6723e-04 - 107s/epoch - 18ms/step
Epoch 45/100
5969/5969 - 106s - loss: 7.1438e-04 - val_loss: 5.3758e-04 - 106s/epoch - 18ms/step
Epoch 46/100
5969/5969 - 106s - loss: 6.9609e-04 - val_loss: 5.1907e-04 - 106s/epoch - 18ms/step
Epoch 47/100
5969/5969 - 106s - loss: 6.9688e-04 - val_loss: 5.3856e-04 - 106s/epoch - 18ms/step
Epoch 48/100
5969/5969 - 106s - loss: 6.8554e-04 - val_loss: 5.1472e-04 - 106s/epoch - 18ms/step
Epoch 49/100
5969/5969 - 106s - loss: 6.9464e-04 - val_loss: 5.2984e-04 - 106s/epoch - 18ms/step
Epoch 50/100
5969/5969 - 106s - loss: 6.8341e-04 - val_loss: 5.2171e-04 - 106s/epoch - 18ms/step
Epoch 51/100
5969/5969 - 106s - loss: 6.8626e-04 - val_loss: 5.2819e-04 - 106s/epoch - 18ms/step
Epoch 52/100
5969/5969 - 106s - loss: 6.7022e-04 - val_loss: 5.1307e-04 - 106s/epoch - 18ms/step
Epoch 53/100
5969/5969 - 106s - loss: 6.7390e-04 - val_loss: 5.2023e-04 - 106s/epoch - 18ms/step
Epoch 54/100
5969/5969 - 106s - loss: 6.7248e-04 - val_loss: 5.3551e-04 - 106s/epoch - 18ms/step
Epoch 55/100
5969/5969 - 107s - loss: 6.6530e-04 - val_loss: 4.9985e-04 - 107s/epoch - 18ms/step
Epoch 56/100
5969/5969 - 106s - loss: 6.6991e-04 - val_loss: 4.9650e-04 - 106s/epoch - 18ms/step
Epoch 57/100
5969/5969 - 106s - loss: 6.6676e-04 - val_loss: 5.0943e-04 - 106s/epoch - 18ms/step
Epoch 58/100
5969/5969 - 106s - loss: 6.5053e-04 - val_loss: 4.9661e-04 - 106s/epoch - 18ms/step
Epoch 59/100
5969/5969 - 106s - loss: 6.6504e-04 - val_loss: 5.0211e-04 - 106s/epoch - 18ms/step
Epoch 60/100
5969/5969 - 106s - loss: 6.4906e-04 - val_loss: 4.8264e-04 - 106s/epoch - 18ms/step
Epoch 61/100
5969/5969 - 106s - loss: 6.4967e-04 - val_loss: 4.9092e-04 - 106s/epoch - 18ms/step
Epoch 62/100
5969/5969 - 106s - loss: 6.5161e-04 - val_loss: 5.2959e-04 - 106s/epoch - 18ms/step
Epoch 63/100
5969/5969 - 106s - loss: 6.6098e-04 - val_loss: 4.9127e-04 - 106s/epoch - 18ms/step
Epoch 64/100
5969/5969 - 106s - loss: 6.7456e-04 - val_loss: 5.4041e-04 - 106s/epoch - 18ms/step
Epoch 65/100
5969/5969 - 106s - loss: 6.5916e-04 - val_loss: 4.8831e-04 - 106s/epoch - 18ms/step
Epoch 66/100
5969/5969 - 106s - loss: 6.3697e-04 - val_loss: 4.8449e-04 - 106s/epoch - 18ms/step
Epoch 67/100
5969/5969 - 106s - loss: 6.4646e-04 - val_loss: 4.9431e-04 - 106s/epoch - 18ms/step
Epoch 68/100
5969/5969 - 106s - loss: 6.3284e-04 - val_loss: 4.8001e-04 - 106s/epoch - 18ms/step
Epoch 69/100
5969/5969 - 106s - loss: 6.2138e-04 - val_loss: 4.5508e-04 - 106s/epoch - 18ms/step
Epoch 70/100
5969/5969 - 106s - loss: 6.2936e-04 - val_loss: 4.6343e-04 - 106s/epoch - 18ms/step
Epoch 71/100
5969/5969 - 106s - loss: 6.2423e-04 - val_loss: 4.8019e-04 - 106s/epoch - 18ms/step
Epoch 72/100
5969/5969 - 106s - loss: 6.2716e-04 - val_loss: 4.7508e-04 - 106s/epoch - 18ms/step
Epoch 73/100
5969/5969 - 106s - loss: 6.2359e-04 - val_loss: 4.8224e-04 - 106s/epoch - 18ms/step
Epoch 74/100
5969/5969 - 106s - loss: 6.2361e-04 - val_loss: 4.7810e-04 - 106s/epoch - 18ms/step
Epoch 75/100
5969/5969 - 106s - loss: 6.2849e-04 - val_loss: 5.0860e-04 - 106s/epoch - 18ms/step
Epoch 76/100
5969/5969 - 106s - loss: 6.1555e-04 - val_loss: 4.6924e-04 - 106s/epoch - 18ms/step
Epoch 77/100
5969/5969 - 106s - loss: 6.1342e-04 - val_loss: 4.5210e-04 - 106s/epoch - 18ms/step
Epoch 78/100
5969/5969 - 106s - loss: 6.2907e-04 - val_loss: 4.7272e-04 - 106s/epoch - 18ms/step
Epoch 79/100
5969/5969 - 106s - loss: 6.1969e-04 - val_loss: 4.9090e-04 - 106s/epoch - 18ms/step
Epoch 80/100
5969/5969 - 105s - loss: 6.1976e-04 - val_loss: 4.7891e-04 - 105s/epoch - 18ms/step
Epoch 81/100
5969/5969 - 106s - loss: 6.3276e-04 - val_loss: 5.0689e-04 - 106s/epoch - 18ms/step
Epoch 82/100
5969/5969 - 107s - loss: 6.1194e-04 - val_loss: 4.5901e-04 - 107s/epoch - 18ms/step
Epoch 83/100
5969/5969 - 106s - loss: 6.1469e-04 - val_loss: 4.8929e-04 - 106s/epoch - 18ms/step
Epoch 84/100
5969/5969 - 107s - loss: 6.1473e-04 - val_loss: 4.8014e-04 - 107s/epoch - 18ms/step
Epoch 85/100
5969/5969 - 107s - loss: 6.0679e-04 - val_loss: 4.6269e-04 - 107s/epoch - 18ms/step
Epoch 86/100
5969/5969 - 106s - loss: 6.0329e-04 - val_loss: 4.7487e-04 - 106s/epoch - 18ms/step
Epoch 87/100
5969/5969 - 107s - loss: 6.0422e-04 - val_loss: 4.8988e-04 - 107s/epoch - 18ms/step
Epoch 88/100
5969/5969 - 106s - loss: 6.0016e-04 - val_loss: 4.6046e-04 - 106s/epoch - 18ms/step
Epoch 89/100
5969/5969 - 106s - loss: 6.0409e-04 - val_loss: 4.6528e-04 - 106s/epoch - 18ms/step
Epoch 90/100
5969/5969 - 106s - loss: 5.9720e-04 - val_loss: 4.7650e-04 - 106s/epoch - 18ms/step
Epoch 91/100
5969/5969 - 107s - loss: 6.1277e-04 - val_loss: 4.4472e-04 - 107s/epoch - 18ms/step
Epoch 92/100
5969/5969 - 106s - loss: 5.9223e-04 - val_loss: 4.7978e-04 - 106s/epoch - 18ms/step
Epoch 93/100
5969/5969 - 107s - loss: 6.0771e-04 - val_loss: 4.9681e-04 - 107s/epoch - 18ms/step
Epoch 94/100
5969/5969 - 106s - loss: 5.9442e-04 - val_loss: 4.5827e-04 - 106s/epoch - 18ms/step
Epoch 95/100
5969/5969 - 106s - loss: 5.9385e-04 - val_loss: 4.6242e-04 - 106s/epoch - 18ms/step
Epoch 96/100
5969/5969 - 106s - loss: 6.2884e-04 - val_loss: 5.1496e-04 - 106s/epoch - 18ms/step
Epoch 97/100
5969/5969 - 106s - loss: 5.9627e-04 - val_loss: 4.5584e-04 - 106s/epoch - 18ms/step
Epoch 98/100
5969/5969 - 106s - loss: 5.9469e-04 - val_loss: 4.4904e-04 - 106s/epoch - 18ms/step
Epoch 99/100
5969/5969 - 107s - loss: 6.3857e-04 - val_loss: 4.8658e-04 - 107s/epoch - 18ms/step
Epoch 100/100
5969/5969 - 107s - loss: 5.9290e-04 - val_loss: 4.7459e-04 - 107s/epoch - 18ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00047459342749789357
  1/332 [..............................] - ETA: 27s 16/332 [>.............................] - ETA: 1s  31/332 [=>............................] - ETA: 1s 47/332 [===>..........................] - ETA: 0s 63/332 [====>.........................] - ETA: 0s 79/332 [======>.......................] - ETA: 0s 95/332 [=======>......................] - ETA: 0s111/332 [=========>....................] - ETA: 0s127/332 [==========>...................] - ETA: 0s143/332 [===========>..................] - ETA: 0s159/332 [=============>................] - ETA: 0s175/332 [==============>...............] - ETA: 0s191/332 [================>.............] - ETA: 0s207/332 [=================>............] - ETA: 0s223/332 [===================>..........] - ETA: 0s239/332 [====================>.........] - ETA: 0s255/332 [======================>.......] - ETA: 0s271/332 [=======================>......] - ETA: 0s287/332 [========================>.....] - ETA: 0s303/332 [==========================>...] - ETA: 0s319/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 3ms/step
correlation 0.005252051378168163
cosine 0.004196755568396133
MAE: 0.011989096
RMSE: 0.021785151
r2: 0.9692125447573285
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 1264)              1598960   
                                                                 
 batch_normalization_6 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 1264)              800112    
                                                                 
 batch_normalization_8 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 1264)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Encoder
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 1264)              1598960   
                                                                 
 batch_normalization_6 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
=================================================================
Total params: 2,403,496
Trainable params: 2,400,968
Non-trainable params: 2,528
_________________________________________________________________
Decoder
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 1264)              800112    
                                                                 
 batch_normalization_8 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 1264)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 2,406,656
Trainable params: 2,402,864
Non-trainable params: 3,792
_________________________________________________________________
['n_b', 'mse', 16, 100, 0.002, 0.5, 632, 0.0005928959581069648, 0.00047459342749789357, 0.005252051378168163, 0.004196755568396133, 0.01198909617960453, 0.021785151213407516, 0.9692125447573285, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_9 (Dense)             (None, 1264)              1598960   
                                                                 
 batch_normalization_9 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 1264)              800112    
                                                                 
 batch_normalization_11 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 1264)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Epoch 1/200
5969/5969 - 108s - loss: 0.0073 - val_loss: 0.0028 - 108s/epoch - 18ms/step
Epoch 2/200
5969/5969 - 108s - loss: 0.0033 - val_loss: 0.0019 - 108s/epoch - 18ms/step
Epoch 3/200
5969/5969 - 108s - loss: 0.0024 - val_loss: 0.0016 - 108s/epoch - 18ms/step
Epoch 4/200
5969/5969 - 108s - loss: 0.0018 - val_loss: 0.0012 - 108s/epoch - 18ms/step
Epoch 5/200
5969/5969 - 108s - loss: 0.0015 - val_loss: 0.0010 - 108s/epoch - 18ms/step
Epoch 6/200
5969/5969 - 108s - loss: 0.0013 - val_loss: 9.1703e-04 - 108s/epoch - 18ms/step
Epoch 7/200
5969/5969 - 108s - loss: 0.0012 - val_loss: 8.7906e-04 - 108s/epoch - 18ms/step
Epoch 8/200
5969/5969 - 108s - loss: 0.0011 - val_loss: 7.7727e-04 - 108s/epoch - 18ms/step
Epoch 9/200
5969/5969 - 108s - loss: 9.8673e-04 - val_loss: 7.5542e-04 - 108s/epoch - 18ms/step
Epoch 10/200
5969/5969 - 108s - loss: 9.5526e-04 - val_loss: 7.1003e-04 - 108s/epoch - 18ms/step
Epoch 11/200
5969/5969 - 108s - loss: 8.7306e-04 - val_loss: 6.6394e-04 - 108s/epoch - 18ms/step
Epoch 12/200
5969/5969 - 108s - loss: 8.3145e-04 - val_loss: 6.5084e-04 - 108s/epoch - 18ms/step
Epoch 13/200
5969/5969 - 108s - loss: 8.0872e-04 - val_loss: 6.1245e-04 - 108s/epoch - 18ms/step
Epoch 14/200
5969/5969 - 108s - loss: 7.9400e-04 - val_loss: 5.8144e-04 - 108s/epoch - 18ms/step
Epoch 15/200
5969/5969 - 108s - loss: 7.5270e-04 - val_loss: 5.7411e-04 - 108s/epoch - 18ms/step
Epoch 16/200
5969/5969 - 108s - loss: 7.2332e-04 - val_loss: 5.8601e-04 - 108s/epoch - 18ms/step
Epoch 17/200
5969/5969 - 108s - loss: 7.1562e-04 - val_loss: 5.6634e-04 - 108s/epoch - 18ms/step
Epoch 18/200
5969/5969 - 108s - loss: 6.9258e-04 - val_loss: 5.4554e-04 - 108s/epoch - 18ms/step
Epoch 19/200
5969/5969 - 108s - loss: 6.7352e-04 - val_loss: 5.3961e-04 - 108s/epoch - 18ms/step
Epoch 20/200
5969/5969 - 108s - loss: 6.6082e-04 - val_loss: 5.1549e-04 - 108s/epoch - 18ms/step
Epoch 21/200
5969/5969 - 108s - loss: 6.5493e-04 - val_loss: 5.5306e-04 - 108s/epoch - 18ms/step
Epoch 22/200
5969/5969 - 108s - loss: 6.3230e-04 - val_loss: 4.8605e-04 - 108s/epoch - 18ms/step
Epoch 23/200
5969/5969 - 108s - loss: 6.2962e-04 - val_loss: 4.8697e-04 - 108s/epoch - 18ms/step
Epoch 24/200
5969/5969 - 108s - loss: 6.0635e-04 - val_loss: 5.0003e-04 - 108s/epoch - 18ms/step
Epoch 25/200
5969/5969 - 108s - loss: 6.1111e-04 - val_loss: 4.6124e-04 - 108s/epoch - 18ms/step
Epoch 26/200
5969/5969 - 108s - loss: 5.9817e-04 - val_loss: 4.5615e-04 - 108s/epoch - 18ms/step
Epoch 27/200
5969/5969 - 108s - loss: 5.8558e-04 - val_loss: 4.6511e-04 - 108s/epoch - 18ms/step
Epoch 28/200
5969/5969 - 108s - loss: 5.8062e-04 - val_loss: 4.4867e-04 - 108s/epoch - 18ms/step
Epoch 29/200
5969/5969 - 108s - loss: 5.7126e-04 - val_loss: 4.4872e-04 - 108s/epoch - 18ms/step
Epoch 30/200
5969/5969 - 107s - loss: 5.6250e-04 - val_loss: 4.1459e-04 - 107s/epoch - 18ms/step
Epoch 31/200
5969/5969 - 108s - loss: 5.6760e-04 - val_loss: 4.1267e-04 - 108s/epoch - 18ms/step
Epoch 32/200
5969/5969 - 108s - loss: 5.4531e-04 - val_loss: 4.1945e-04 - 108s/epoch - 18ms/step
Epoch 33/200
5969/5969 - 107s - loss: 5.4188e-04 - val_loss: 4.1846e-04 - 107s/epoch - 18ms/step
Epoch 34/200
5969/5969 - 108s - loss: 5.3556e-04 - val_loss: 4.0653e-04 - 108s/epoch - 18ms/step
Epoch 35/200
5969/5969 - 108s - loss: 5.3759e-04 - val_loss: 3.9585e-04 - 108s/epoch - 18ms/step
Epoch 36/200
5969/5969 - 107s - loss: 5.2892e-04 - val_loss: 4.4115e-04 - 107s/epoch - 18ms/step
Epoch 37/200
5969/5969 - 108s - loss: 5.2876e-04 - val_loss: 4.1854e-04 - 108s/epoch - 18ms/step
Epoch 38/200
5969/5969 - 108s - loss: 5.1466e-04 - val_loss: 3.8993e-04 - 108s/epoch - 18ms/step
Epoch 39/200
5969/5969 - 107s - loss: 5.1382e-04 - val_loss: 4.9125e-04 - 107s/epoch - 18ms/step
Epoch 40/200
5969/5969 - 108s - loss: 5.0438e-04 - val_loss: 3.9652e-04 - 108s/epoch - 18ms/step
Epoch 41/200
5969/5969 - 108s - loss: 5.0279e-04 - val_loss: 3.6138e-04 - 108s/epoch - 18ms/step
Epoch 42/200
5969/5969 - 107s - loss: 5.0543e-04 - val_loss: 3.7134e-04 - 107s/epoch - 18ms/step
Epoch 43/200
5969/5969 - 108s - loss: 4.9471e-04 - val_loss: 3.7251e-04 - 108s/epoch - 18ms/step
Epoch 44/200
5969/5969 - 108s - loss: 4.9216e-04 - val_loss: 4.2325e-04 - 108s/epoch - 18ms/step
Epoch 45/200
5969/5969 - 107s - loss: 4.8783e-04 - val_loss: 3.5467e-04 - 107s/epoch - 18ms/step
Epoch 46/200
5969/5969 - 108s - loss: 4.8165e-04 - val_loss: 3.7447e-04 - 108s/epoch - 18ms/step
Epoch 47/200
5969/5969 - 108s - loss: 4.8835e-04 - val_loss: 3.7521e-04 - 108s/epoch - 18ms/step
Epoch 48/200
5969/5969 - 107s - loss: 4.7970e-04 - val_loss: 3.5221e-04 - 107s/epoch - 18ms/step
Epoch 49/200
5969/5969 - 108s - loss: 4.7631e-04 - val_loss: 3.7027e-04 - 108s/epoch - 18ms/step
Epoch 50/200
5969/5969 - 108s - loss: 4.7592e-04 - val_loss: 3.6234e-04 - 108s/epoch - 18ms/step
Epoch 51/200
5969/5969 - 107s - loss: 4.7018e-04 - val_loss: 3.5093e-04 - 107s/epoch - 18ms/step
Epoch 52/200
5969/5969 - 108s - loss: 4.6167e-04 - val_loss: 4.0084e-04 - 108s/epoch - 18ms/step
Epoch 53/200
5969/5969 - 108s - loss: 4.6342e-04 - val_loss: 3.4280e-04 - 108s/epoch - 18ms/step
Epoch 54/200
5969/5969 - 107s - loss: 4.6183e-04 - val_loss: 3.8217e-04 - 107s/epoch - 18ms/step
Epoch 55/200
5969/5969 - 108s - loss: 4.5529e-04 - val_loss: 3.3238e-04 - 108s/epoch - 18ms/step
Epoch 56/200
5969/5969 - 108s - loss: 4.5062e-04 - val_loss: 3.6351e-04 - 108s/epoch - 18ms/step
Epoch 57/200
5969/5969 - 107s - loss: 4.5970e-04 - val_loss: 3.3902e-04 - 107s/epoch - 18ms/step
Epoch 58/200
5969/5969 - 108s - loss: 4.4386e-04 - val_loss: 3.4410e-04 - 108s/epoch - 18ms/step
Epoch 59/200
5969/5969 - 108s - loss: 4.4800e-04 - val_loss: 3.4441e-04 - 108s/epoch - 18ms/step
Epoch 60/200
5969/5969 - 107s - loss: 4.4335e-04 - val_loss: 3.4332e-04 - 107s/epoch - 18ms/step
Epoch 61/200
5969/5969 - 108s - loss: 4.4658e-04 - val_loss: 3.3775e-04 - 108s/epoch - 18ms/step
Epoch 62/200
5969/5969 - 108s - loss: 4.3936e-04 - val_loss: 3.6292e-04 - 108s/epoch - 18ms/step
Epoch 63/200
5969/5969 - 107s - loss: 4.4064e-04 - val_loss: 3.2437e-04 - 107s/epoch - 18ms/step
Epoch 64/200
5969/5969 - 108s - loss: 4.4536e-04 - val_loss: 3.1097e-04 - 108s/epoch - 18ms/step
Epoch 65/200
5969/5969 - 108s - loss: 4.4538e-04 - val_loss: 3.2293e-04 - 108s/epoch - 18ms/step
Epoch 66/200
5969/5969 - 107s - loss: 4.3144e-04 - val_loss: 3.2788e-04 - 107s/epoch - 18ms/step
Epoch 67/200
5969/5969 - 108s - loss: 4.3938e-04 - val_loss: 3.2581e-04 - 108s/epoch - 18ms/step
Epoch 68/200
5969/5969 - 108s - loss: 4.2989e-04 - val_loss: 3.3509e-04 - 108s/epoch - 18ms/step
Epoch 69/200
5969/5969 - 108s - loss: 4.3727e-04 - val_loss: 3.1711e-04 - 108s/epoch - 18ms/step
Epoch 70/200
5969/5969 - 108s - loss: 4.2448e-04 - val_loss: 3.0984e-04 - 108s/epoch - 18ms/step
Epoch 71/200
5969/5969 - 108s - loss: 4.2246e-04 - val_loss: 3.3570e-04 - 108s/epoch - 18ms/step
Epoch 72/200
5969/5969 - 107s - loss: 4.2810e-04 - val_loss: 3.1218e-04 - 107s/epoch - 18ms/step
Epoch 73/200
5969/5969 - 108s - loss: 4.2455e-04 - val_loss: 3.1406e-04 - 108s/epoch - 18ms/step
Epoch 74/200
5969/5969 - 108s - loss: 4.2107e-04 - val_loss: 3.3254e-04 - 108s/epoch - 18ms/step
Epoch 75/200
5969/5969 - 107s - loss: 4.1923e-04 - val_loss: 3.2436e-04 - 107s/epoch - 18ms/step
Epoch 76/200
5969/5969 - 108s - loss: 4.2019e-04 - val_loss: 3.0721e-04 - 108s/epoch - 18ms/step
Epoch 77/200
5969/5969 - 108s - loss: 4.1588e-04 - val_loss: 3.3357e-04 - 108s/epoch - 18ms/step
Epoch 78/200
5969/5969 - 107s - loss: 4.1154e-04 - val_loss: 3.1817e-04 - 107s/epoch - 18ms/step
Epoch 79/200
5969/5969 - 108s - loss: 4.1430e-04 - val_loss: 3.1628e-04 - 108s/epoch - 18ms/step
Epoch 80/200
5969/5969 - 108s - loss: 4.1314e-04 - val_loss: 2.9864e-04 - 108s/epoch - 18ms/step
Epoch 81/200
5969/5969 - 107s - loss: 4.0811e-04 - val_loss: 3.3602e-04 - 107s/epoch - 18ms/step
Epoch 82/200
5969/5969 - 108s - loss: 4.1552e-04 - val_loss: 3.0081e-04 - 108s/epoch - 18ms/step
Epoch 83/200
5969/5969 - 107s - loss: 4.2042e-04 - val_loss: 3.1568e-04 - 107s/epoch - 18ms/step
Epoch 84/200
5969/5969 - 108s - loss: 4.1515e-04 - val_loss: 3.0299e-04 - 108s/epoch - 18ms/step
Epoch 85/200
5969/5969 - 108s - loss: 4.0552e-04 - val_loss: 2.9184e-04 - 108s/epoch - 18ms/step
Epoch 86/200
5969/5969 - 107s - loss: 4.0261e-04 - val_loss: 2.9449e-04 - 107s/epoch - 18ms/step
Epoch 87/200
5969/5969 - 108s - loss: 4.1130e-04 - val_loss: 2.9764e-04 - 108s/epoch - 18ms/step
Epoch 88/200
5969/5969 - 108s - loss: 4.0161e-04 - val_loss: 2.9117e-04 - 108s/epoch - 18ms/step
Epoch 89/200
5969/5969 - 108s - loss: 4.0118e-04 - val_loss: 3.0114e-04 - 108s/epoch - 18ms/step
Epoch 90/200
5969/5969 - 108s - loss: 4.0136e-04 - val_loss: 2.9249e-04 - 108s/epoch - 18ms/step
Epoch 91/200
5969/5969 - 108s - loss: 4.0164e-04 - val_loss: 2.9977e-04 - 108s/epoch - 18ms/step
Epoch 92/200
5969/5969 - 107s - loss: 4.0063e-04 - val_loss: 3.2142e-04 - 107s/epoch - 18ms/step
Epoch 93/200
5969/5969 - 108s - loss: 4.0256e-04 - val_loss: 3.0022e-04 - 108s/epoch - 18ms/step
Epoch 94/200
5969/5969 - 108s - loss: 3.9455e-04 - val_loss: 2.8896e-04 - 108s/epoch - 18ms/step
Epoch 95/200
5969/5969 - 107s - loss: 3.9341e-04 - val_loss: 2.9172e-04 - 107s/epoch - 18ms/step
Epoch 96/200
5969/5969 - 108s - loss: 3.9433e-04 - val_loss: 3.0318e-04 - 108s/epoch - 18ms/step
Epoch 97/200
5969/5969 - 108s - loss: 3.9425e-04 - val_loss: 2.8968e-04 - 108s/epoch - 18ms/step
Epoch 98/200
5969/5969 - 107s - loss: 3.9608e-04 - val_loss: 2.8656e-04 - 107s/epoch - 18ms/step
Epoch 99/200
5969/5969 - 108s - loss: 3.9578e-04 - val_loss: 2.8470e-04 - 108s/epoch - 18ms/step
Epoch 100/200
5969/5969 - 108s - loss: 3.9673e-04 - val_loss: 2.9785e-04 - 108s/epoch - 18ms/step
Epoch 101/200
5969/5969 - 107s - loss: 3.8662e-04 - val_loss: 3.0237e-04 - 107s/epoch - 18ms/step
Epoch 102/200
5969/5969 - 108s - loss: 3.9208e-04 - val_loss: 2.8781e-04 - 108s/epoch - 18ms/step
Epoch 103/200
5969/5969 - 108s - loss: 3.9058e-04 - val_loss: 3.0955e-04 - 108s/epoch - 18ms/step
Epoch 104/200
5969/5969 - 108s - loss: 3.8964e-04 - val_loss: 3.0132e-04 - 108s/epoch - 18ms/step
Epoch 105/200
5969/5969 - 108s - loss: 3.8652e-04 - val_loss: 2.9481e-04 - 108s/epoch - 18ms/step
Epoch 106/200
5969/5969 - 108s - loss: 3.8286e-04 - val_loss: 2.8723e-04 - 108s/epoch - 18ms/step
Epoch 107/200
5969/5969 - 107s - loss: 3.9058e-04 - val_loss: 3.0674e-04 - 107s/epoch - 18ms/step
Epoch 108/200
5969/5969 - 108s - loss: 3.8440e-04 - val_loss: 2.8262e-04 - 108s/epoch - 18ms/step
Epoch 109/200
5969/5969 - 108s - loss: 3.8658e-04 - val_loss: 2.9329e-04 - 108s/epoch - 18ms/step
Epoch 110/200
5969/5969 - 107s - loss: 3.8824e-04 - val_loss: 2.9536e-04 - 107s/epoch - 18ms/step
Epoch 111/200
5969/5969 - 108s - loss: 3.7845e-04 - val_loss: 2.9571e-04 - 108s/epoch - 18ms/step
Epoch 112/200
5969/5969 - 108s - loss: 3.8449e-04 - val_loss: 2.9938e-04 - 108s/epoch - 18ms/step
Epoch 113/200
5969/5969 - 108s - loss: 3.7752e-04 - val_loss: 2.9562e-04 - 108s/epoch - 18ms/step
Epoch 114/200
5969/5969 - 108s - loss: 3.7818e-04 - val_loss: 2.8125e-04 - 108s/epoch - 18ms/step
Epoch 115/200
5969/5969 - 108s - loss: 3.7705e-04 - val_loss: 2.8792e-04 - 108s/epoch - 18ms/step
Epoch 116/200
5969/5969 - 107s - loss: 3.8561e-04 - val_loss: 2.8521e-04 - 107s/epoch - 18ms/step
Epoch 117/200
5969/5969 - 108s - loss: 3.7396e-04 - val_loss: 2.9086e-04 - 108s/epoch - 18ms/step
Epoch 118/200
5969/5969 - 108s - loss: 3.7862e-04 - val_loss: 2.9251e-04 - 108s/epoch - 18ms/step
Epoch 119/200
5969/5969 - 108s - loss: 3.7652e-04 - val_loss: 2.8839e-04 - 108s/epoch - 18ms/step
Epoch 120/200
5969/5969 - 108s - loss: 3.8190e-04 - val_loss: 2.9724e-04 - 108s/epoch - 18ms/step
Epoch 121/200
5969/5969 - 108s - loss: 3.7190e-04 - val_loss: 2.7333e-04 - 108s/epoch - 18ms/step
Epoch 122/200
5969/5969 - 107s - loss: 3.7145e-04 - val_loss: 2.7235e-04 - 107s/epoch - 18ms/step
Epoch 123/200
5969/5969 - 108s - loss: 3.7047e-04 - val_loss: 2.6481e-04 - 108s/epoch - 18ms/step
Epoch 124/200
5969/5969 - 108s - loss: 3.7554e-04 - val_loss: 2.8940e-04 - 108s/epoch - 18ms/step
Epoch 125/200
5969/5969 - 107s - loss: 3.7242e-04 - val_loss: 2.8278e-04 - 107s/epoch - 18ms/step
Epoch 126/200
5969/5969 - 108s - loss: 3.7913e-04 - val_loss: 3.1070e-04 - 108s/epoch - 18ms/step
Epoch 127/200
5969/5969 - 108s - loss: 3.7428e-04 - val_loss: 2.6689e-04 - 108s/epoch - 18ms/step
Epoch 128/200
5969/5969 - 107s - loss: 3.6717e-04 - val_loss: 2.7196e-04 - 107s/epoch - 18ms/step
Epoch 129/200
5969/5969 - 108s - loss: 3.7074e-04 - val_loss: 2.7072e-04 - 108s/epoch - 18ms/step
Epoch 130/200
5969/5969 - 108s - loss: 3.6892e-04 - val_loss: 2.8001e-04 - 108s/epoch - 18ms/step
Epoch 131/200
5969/5969 - 107s - loss: 3.6656e-04 - val_loss: 2.7239e-04 - 107s/epoch - 18ms/step
Epoch 132/200
5969/5969 - 108s - loss: 3.6569e-04 - val_loss: 2.6787e-04 - 108s/epoch - 18ms/step
Epoch 133/200
5969/5969 - 108s - loss: 3.7273e-04 - val_loss: 2.8061e-04 - 108s/epoch - 18ms/step
Epoch 134/200
5969/5969 - 107s - loss: 3.6348e-04 - val_loss: 2.6308e-04 - 107s/epoch - 18ms/step
Epoch 135/200
5969/5969 - 108s - loss: 3.6538e-04 - val_loss: 2.8694e-04 - 108s/epoch - 18ms/step
Epoch 136/200
5969/5969 - 108s - loss: 3.7112e-04 - val_loss: 2.7320e-04 - 108s/epoch - 18ms/step
Epoch 137/200
5969/5969 - 108s - loss: 3.6819e-04 - val_loss: 3.1638e-04 - 108s/epoch - 18ms/step
Epoch 138/200
5969/5969 - 108s - loss: 3.6270e-04 - val_loss: 2.8457e-04 - 108s/epoch - 18ms/step
Epoch 139/200
5969/5969 - 108s - loss: 3.6458e-04 - val_loss: 2.8760e-04 - 108s/epoch - 18ms/step
Epoch 140/200
5969/5969 - 108s - loss: 3.6232e-04 - val_loss: 2.7401e-04 - 108s/epoch - 18ms/step
Epoch 141/200
5969/5969 - 108s - loss: 3.6444e-04 - val_loss: 2.6457e-04 - 108s/epoch - 18ms/step
Epoch 142/200
5969/5969 - 107s - loss: 3.6134e-04 - val_loss: 2.9000e-04 - 107s/epoch - 18ms/step
Epoch 143/200
5969/5969 - 108s - loss: 3.6861e-04 - val_loss: 2.8120e-04 - 108s/epoch - 18ms/step
Epoch 144/200
5969/5969 - 108s - loss: 3.6218e-04 - val_loss: 2.5678e-04 - 108s/epoch - 18ms/step
Epoch 145/200
5969/5969 - 107s - loss: 3.5990e-04 - val_loss: 2.7067e-04 - 107s/epoch - 18ms/step
Epoch 146/200
5969/5969 - 108s - loss: 3.6421e-04 - val_loss: 2.8772e-04 - 108s/epoch - 18ms/step
Epoch 147/200
5969/5969 - 108s - loss: 3.5928e-04 - val_loss: 2.7053e-04 - 108s/epoch - 18ms/step
Epoch 148/200
5969/5969 - 107s - loss: 3.6367e-04 - val_loss: 2.8388e-04 - 107s/epoch - 18ms/step
Epoch 149/200
5969/5969 - 108s - loss: 3.6839e-04 - val_loss: 2.7379e-04 - 108s/epoch - 18ms/step
Epoch 150/200
5969/5969 - 108s - loss: 3.5957e-04 - val_loss: 2.5474e-04 - 108s/epoch - 18ms/step
Epoch 151/200
5969/5969 - 108s - loss: 3.5649e-04 - val_loss: 2.6925e-04 - 108s/epoch - 18ms/step
Epoch 152/200
5969/5969 - 108s - loss: 3.5811e-04 - val_loss: 2.6665e-04 - 108s/epoch - 18ms/step
Epoch 153/200
5969/5969 - 108s - loss: 3.5573e-04 - val_loss: 2.6748e-04 - 108s/epoch - 18ms/step
Epoch 154/200
5969/5969 - 107s - loss: 3.5355e-04 - val_loss: 2.7086e-04 - 107s/epoch - 18ms/step
Epoch 155/200
5969/5969 - 108s - loss: 3.5812e-04 - val_loss: 2.5493e-04 - 108s/epoch - 18ms/step
Epoch 156/200
5969/5969 - 108s - loss: 3.5972e-04 - val_loss: 2.6480e-04 - 108s/epoch - 18ms/step
Epoch 157/200
5969/5969 - 107s - loss: 3.5981e-04 - val_loss: 2.5483e-04 - 107s/epoch - 18ms/step
Epoch 158/200
5969/5969 - 108s - loss: 3.5420e-04 - val_loss: 2.7883e-04 - 108s/epoch - 18ms/step
Epoch 159/200
5969/5969 - 108s - loss: 3.5993e-04 - val_loss: 2.5861e-04 - 108s/epoch - 18ms/step
Epoch 160/200
5969/5969 - 107s - loss: 3.5469e-04 - val_loss: 2.5972e-04 - 107s/epoch - 18ms/step
Epoch 161/200
5969/5969 - 108s - loss: 3.6035e-04 - val_loss: 2.6158e-04 - 108s/epoch - 18ms/step
Epoch 162/200
5969/5969 - 108s - loss: 3.5427e-04 - val_loss: 2.8226e-04 - 108s/epoch - 18ms/step
Epoch 163/200
5969/5969 - 108s - loss: 3.5709e-04 - val_loss: 2.8114e-04 - 108s/epoch - 18ms/step
Epoch 164/200
5969/5969 - 107s - loss: 3.5588e-04 - val_loss: 2.8376e-04 - 107s/epoch - 18ms/step
Epoch 165/200
5969/5969 - 108s - loss: 3.5280e-04 - val_loss: 2.7469e-04 - 108s/epoch - 18ms/step
Epoch 166/200
5969/5969 - 107s - loss: 3.5077e-04 - val_loss: 2.6931e-04 - 107s/epoch - 18ms/step
Epoch 167/200
5969/5969 - 108s - loss: 3.5520e-04 - val_loss: 2.9058e-04 - 108s/epoch - 18ms/step
Epoch 168/200
5969/5969 - 107s - loss: 3.5250e-04 - val_loss: 2.6683e-04 - 107s/epoch - 18ms/step
Epoch 169/200
5969/5969 - 107s - loss: 3.5360e-04 - val_loss: 2.6195e-04 - 107s/epoch - 18ms/step
Epoch 170/200
5969/5969 - 108s - loss: 3.4940e-04 - val_loss: 2.7034e-04 - 108s/epoch - 18ms/step
Epoch 171/200
5969/5969 - 108s - loss: 3.5507e-04 - val_loss: 2.9560e-04 - 108s/epoch - 18ms/step
Epoch 172/200
5969/5969 - 108s - loss: 3.4624e-04 - val_loss: 2.9437e-04 - 108s/epoch - 18ms/step
Epoch 173/200
5969/5969 - 109s - loss: 3.4543e-04 - val_loss: 2.8069e-04 - 109s/epoch - 18ms/step
Epoch 174/200
5969/5969 - 107s - loss: 3.4923e-04 - val_loss: 2.6356e-04 - 107s/epoch - 18ms/step
Epoch 175/200
5969/5969 - 108s - loss: 3.4746e-04 - val_loss: 2.6208e-04 - 108s/epoch - 18ms/step
Epoch 176/200
5969/5969 - 108s - loss: 3.4813e-04 - val_loss: 2.6766e-04 - 108s/epoch - 18ms/step
Epoch 177/200
5969/5969 - 108s - loss: 3.5435e-04 - val_loss: 2.5257e-04 - 108s/epoch - 18ms/step
Epoch 178/200
5969/5969 - 107s - loss: 3.5194e-04 - val_loss: 2.8159e-04 - 107s/epoch - 18ms/step
Epoch 179/200
5969/5969 - 108s - loss: 3.4674e-04 - val_loss: 2.5879e-04 - 108s/epoch - 18ms/step
Epoch 180/200
5969/5969 - 108s - loss: 3.4810e-04 - val_loss: 2.5761e-04 - 108s/epoch - 18ms/step
Epoch 181/200
5969/5969 - 108s - loss: 3.4740e-04 - val_loss: 2.5580e-04 - 108s/epoch - 18ms/step
Epoch 182/200
5969/5969 - 108s - loss: 3.5585e-04 - val_loss: 2.6300e-04 - 108s/epoch - 18ms/step
Epoch 183/200
5969/5969 - 108s - loss: 3.4475e-04 - val_loss: 2.7589e-04 - 108s/epoch - 18ms/step
Epoch 184/200
5969/5969 - 107s - loss: 3.4419e-04 - val_loss: 2.7780e-04 - 107s/epoch - 18ms/step
Epoch 185/200
5969/5969 - 108s - loss: 3.4519e-04 - val_loss: 2.6606e-04 - 108s/epoch - 18ms/step
Epoch 186/200
5969/5969 - 108s - loss: 3.4691e-04 - val_loss: 2.6066e-04 - 108s/epoch - 18ms/step
Epoch 187/200
5969/5969 - 108s - loss: 3.4453e-04 - val_loss: 2.8986e-04 - 108s/epoch - 18ms/step
Epoch 188/200
5969/5969 - 108s - loss: 3.4135e-04 - val_loss: 2.7328e-04 - 108s/epoch - 18ms/step
Epoch 189/200
5969/5969 - 108s - loss: 3.4427e-04 - val_loss: 2.5601e-04 - 108s/epoch - 18ms/step
Epoch 190/200
5969/5969 - 107s - loss: 3.4365e-04 - val_loss: 2.6306e-04 - 107s/epoch - 18ms/step
Epoch 191/200
5969/5969 - 108s - loss: 3.4813e-04 - val_loss: 2.6627e-04 - 108s/epoch - 18ms/step
Epoch 192/200
5969/5969 - 108s - loss: 3.4318e-04 - val_loss: 2.9150e-04 - 108s/epoch - 18ms/step
Epoch 193/200
5969/5969 - 107s - loss: 3.4308e-04 - val_loss: 2.7333e-04 - 107s/epoch - 18ms/step
Epoch 194/200
5969/5969 - 108s - loss: 3.4330e-04 - val_loss: 2.7164e-04 - 108s/epoch - 18ms/step
Epoch 195/200
5969/5969 - 108s - loss: 3.5338e-04 - val_loss: 2.6203e-04 - 108s/epoch - 18ms/step
Epoch 196/200
5969/5969 - 107s - loss: 3.4159e-04 - val_loss: 2.7712e-04 - 107s/epoch - 18ms/step
Epoch 197/200
5969/5969 - 108s - loss: 3.4254e-04 - val_loss: 2.5862e-04 - 108s/epoch - 18ms/step
Epoch 198/200
5969/5969 - 108s - loss: 3.4149e-04 - val_loss: 2.6348e-04 - 108s/epoch - 18ms/step
Epoch 199/200
5969/5969 - 107s - loss: 3.3751e-04 - val_loss: 2.5218e-04 - 107s/epoch - 18ms/step
Epoch 200/200
5969/5969 - 107s - loss: 3.4409e-04 - val_loss: 2.5644e-04 - 107s/epoch - 18ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0002564385358709842
  1/332 [..............................] - ETA: 27s 16/332 [>.............................] - ETA: 1s  32/332 [=>............................] - ETA: 1s 48/332 [===>..........................] - ETA: 0s 64/332 [====>.........................] - ETA: 0s 80/332 [======>.......................] - ETA: 0s 96/332 [=======>......................] - ETA: 0s112/332 [=========>....................] - ETA: 0s128/332 [==========>...................] - ETA: 0s143/332 [===========>..................] - ETA: 0s159/332 [=============>................] - ETA: 0s175/332 [==============>...............] - ETA: 0s191/332 [================>.............] - ETA: 0s207/332 [=================>............] - ETA: 0s223/332 [===================>..........] - ETA: 0s239/332 [====================>.........] - ETA: 0s255/332 [======================>.......] - ETA: 0s271/332 [=======================>......] - ETA: 0s287/332 [========================>.....] - ETA: 0s303/332 [==========================>...] - ETA: 0s319/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 3ms/step
correlation 0.0028258740418909004
cosine 0.002276285151648063
MAE: 0.008363578
RMSE: 0.01601369
r2: 0.9833644451546447
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 1264)              1598960   
                                                                 
 batch_normalization_9 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 1264)              800112    
                                                                 
 batch_normalization_11 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 1264)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Encoder
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 1264)              1598960   
                                                                 
 batch_normalization_9 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
=================================================================
Total params: 2,403,496
Trainable params: 2,400,968
Non-trainable params: 2,528
_________________________________________________________________
Decoder
Model: "model_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 1264)              800112    
                                                                 
 batch_normalization_11 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 1264)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 2,406,656
Trainable params: 2,402,864
Non-trainable params: 3,792
_________________________________________________________________
['n_b', 'mse', 16, 200, 0.0005, 0.5, 632, 0.00034408693318255246, 0.0002564385358709842, 0.0028258740418909004, 0.002276285151648063, 0.008363577537238598, 0.01601368933916092, 0.9833644451546447, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_12 (Dense)            (None, 1264)              1598960   
                                                                 
 batch_normalization_12 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 1264)              800112    
                                                                 
 batch_normalization_14 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 1264)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Epoch 1/200
5969/5969 - 107s - loss: 0.0066 - val_loss: 0.0025 - 107s/epoch - 18ms/step
Epoch 2/200
5969/5969 - 106s - loss: 0.0028 - val_loss: 0.0018 - 106s/epoch - 18ms/step
Epoch 3/200
5969/5969 - 106s - loss: 0.0018 - val_loss: 0.0015 - 106s/epoch - 18ms/step
Epoch 4/200
5969/5969 - 106s - loss: 0.0015 - val_loss: 0.0011 - 106s/epoch - 18ms/step
Epoch 5/200
5969/5969 - 106s - loss: 0.0013 - val_loss: 9.7927e-04 - 106s/epoch - 18ms/step
Epoch 6/200
5969/5969 - 106s - loss: 0.0011 - val_loss: 9.1732e-04 - 106s/epoch - 18ms/step
Epoch 7/200
5969/5969 - 106s - loss: 0.0010 - val_loss: 8.4329e-04 - 106s/epoch - 18ms/step
Epoch 8/200
5969/5969 - 106s - loss: 9.8516e-04 - val_loss: 7.8857e-04 - 106s/epoch - 18ms/step
Epoch 9/200
5969/5969 - 106s - loss: 9.3657e-04 - val_loss: 7.3467e-04 - 106s/epoch - 18ms/step
Epoch 10/200
5969/5969 - 106s - loss: 8.9330e-04 - val_loss: 7.0534e-04 - 106s/epoch - 18ms/step
Epoch 11/200
5969/5969 - 106s - loss: 8.6824e-04 - val_loss: 6.9467e-04 - 106s/epoch - 18ms/step
Epoch 12/200
5969/5969 - 106s - loss: 8.2858e-04 - val_loss: 6.5975e-04 - 106s/epoch - 18ms/step
Epoch 13/200
5969/5969 - 106s - loss: 8.1042e-04 - val_loss: 6.1932e-04 - 106s/epoch - 18ms/step
Epoch 14/200
5969/5969 - 106s - loss: 7.8267e-04 - val_loss: 6.2424e-04 - 106s/epoch - 18ms/step
Epoch 15/200
5969/5969 - 106s - loss: 7.7058e-04 - val_loss: 6.1480e-04 - 106s/epoch - 18ms/step
Epoch 16/200
5969/5969 - 106s - loss: 7.4087e-04 - val_loss: 6.0016e-04 - 106s/epoch - 18ms/step
Epoch 17/200
5969/5969 - 106s - loss: 7.5118e-04 - val_loss: 5.7552e-04 - 106s/epoch - 18ms/step
Epoch 18/200
5969/5969 - 106s - loss: 7.2225e-04 - val_loss: 5.8537e-04 - 106s/epoch - 18ms/step
Epoch 19/200
5969/5969 - 106s - loss: 7.1052e-04 - val_loss: 5.6842e-04 - 106s/epoch - 18ms/step
Epoch 20/200
5969/5969 - 106s - loss: 6.9171e-04 - val_loss: 5.3224e-04 - 106s/epoch - 18ms/step
Epoch 21/200
5969/5969 - 106s - loss: 6.8336e-04 - val_loss: 5.3468e-04 - 106s/epoch - 18ms/step
Epoch 22/200
5969/5969 - 106s - loss: 6.7568e-04 - val_loss: 5.2560e-04 - 106s/epoch - 18ms/step
Epoch 23/200
5969/5969 - 106s - loss: 6.5732e-04 - val_loss: 5.1530e-04 - 106s/epoch - 18ms/step
Epoch 24/200
5969/5969 - 106s - loss: 6.5283e-04 - val_loss: 5.1773e-04 - 106s/epoch - 18ms/step
Epoch 25/200
5969/5969 - 106s - loss: 6.7335e-04 - val_loss: 5.0369e-04 - 106s/epoch - 18ms/step
Epoch 26/200
5969/5969 - 106s - loss: 6.4072e-04 - val_loss: 4.9190e-04 - 106s/epoch - 18ms/step
Epoch 27/200
5969/5969 - 106s - loss: 6.2388e-04 - val_loss: 4.8886e-04 - 106s/epoch - 18ms/step
Epoch 28/200
5969/5969 - 106s - loss: 6.4398e-04 - val_loss: 4.6528e-04 - 106s/epoch - 18ms/step
Epoch 29/200
5969/5969 - 106s - loss: 6.3469e-04 - val_loss: 5.4188e-04 - 106s/epoch - 18ms/step
Epoch 30/200
5969/5969 - 106s - loss: 6.1197e-04 - val_loss: 4.7525e-04 - 106s/epoch - 18ms/step
Epoch 31/200
5969/5969 - 106s - loss: 6.0957e-04 - val_loss: 4.5344e-04 - 106s/epoch - 18ms/step
Epoch 32/200
5969/5969 - 106s - loss: 6.0044e-04 - val_loss: 4.7223e-04 - 106s/epoch - 18ms/step
Epoch 33/200
5969/5969 - 106s - loss: 6.0098e-04 - val_loss: 4.4674e-04 - 106s/epoch - 18ms/step
Epoch 34/200
5969/5969 - 106s - loss: 5.8755e-04 - val_loss: 4.4113e-04 - 106s/epoch - 18ms/step
Epoch 35/200
5969/5969 - 106s - loss: 5.9202e-04 - val_loss: 4.2813e-04 - 106s/epoch - 18ms/step
Epoch 36/200
5969/5969 - 106s - loss: 5.7726e-04 - val_loss: 4.4534e-04 - 106s/epoch - 18ms/step
Epoch 37/200
5969/5969 - 106s - loss: 5.7548e-04 - val_loss: 4.5215e-04 - 106s/epoch - 18ms/step
Epoch 38/200
5969/5969 - 106s - loss: 5.7857e-04 - val_loss: 4.4806e-04 - 106s/epoch - 18ms/step
Epoch 39/200
5969/5969 - 106s - loss: 5.6706e-04 - val_loss: 4.2904e-04 - 106s/epoch - 18ms/step
Epoch 40/200
5969/5969 - 106s - loss: 5.6254e-04 - val_loss: 4.4137e-04 - 106s/epoch - 18ms/step
Epoch 41/200
5969/5969 - 106s - loss: 5.6582e-04 - val_loss: 4.3024e-04 - 106s/epoch - 18ms/step
Epoch 42/200
5969/5969 - 106s - loss: 5.6187e-04 - val_loss: 4.0074e-04 - 106s/epoch - 18ms/step
Epoch 43/200
5969/5969 - 106s - loss: 5.4729e-04 - val_loss: 4.2797e-04 - 106s/epoch - 18ms/step
Epoch 44/200
5969/5969 - 106s - loss: 5.4363e-04 - val_loss: 4.3770e-04 - 106s/epoch - 18ms/step
Epoch 45/200
5969/5969 - 106s - loss: 5.5031e-04 - val_loss: 4.0450e-04 - 106s/epoch - 18ms/step
Epoch 46/200
5969/5969 - 106s - loss: 5.3740e-04 - val_loss: 4.1849e-04 - 106s/epoch - 18ms/step
Epoch 47/200
5969/5969 - 106s - loss: 5.3846e-04 - val_loss: 4.1888e-04 - 106s/epoch - 18ms/step
Epoch 48/200
5969/5969 - 106s - loss: 5.3630e-04 - val_loss: 4.5148e-04 - 106s/epoch - 18ms/step
Epoch 49/200
5969/5969 - 106s - loss: 5.3430e-04 - val_loss: 4.0755e-04 - 106s/epoch - 18ms/step
Epoch 50/200
5969/5969 - 106s - loss: 5.2493e-04 - val_loss: 3.9244e-04 - 106s/epoch - 18ms/step
Epoch 51/200
5969/5969 - 106s - loss: 5.2787e-04 - val_loss: 4.0407e-04 - 106s/epoch - 18ms/step
Epoch 52/200
5969/5969 - 106s - loss: 5.3200e-04 - val_loss: 4.0018e-04 - 106s/epoch - 18ms/step
Epoch 53/200
5969/5969 - 106s - loss: 5.2790e-04 - val_loss: 4.4079e-04 - 106s/epoch - 18ms/step
Epoch 54/200
5969/5969 - 106s - loss: 5.1898e-04 - val_loss: 4.4546e-04 - 106s/epoch - 18ms/step
Epoch 55/200
5969/5969 - 106s - loss: 5.1241e-04 - val_loss: 3.7360e-04 - 106s/epoch - 18ms/step
Epoch 56/200
5969/5969 - 106s - loss: 5.1937e-04 - val_loss: 3.8363e-04 - 106s/epoch - 18ms/step
Epoch 57/200
5969/5969 - 106s - loss: 5.0955e-04 - val_loss: 3.7945e-04 - 106s/epoch - 18ms/step
Epoch 58/200
5969/5969 - 106s - loss: 5.0370e-04 - val_loss: 3.7791e-04 - 106s/epoch - 18ms/step
Epoch 59/200
5969/5969 - 106s - loss: 5.0729e-04 - val_loss: 3.5641e-04 - 106s/epoch - 18ms/step
Epoch 60/200
5969/5969 - 106s - loss: 5.0073e-04 - val_loss: 3.6547e-04 - 106s/epoch - 18ms/step
Epoch 61/200
5969/5969 - 106s - loss: 5.0604e-04 - val_loss: 3.9421e-04 - 106s/epoch - 18ms/step
Epoch 62/200
5969/5969 - 106s - loss: 4.9918e-04 - val_loss: 4.0338e-04 - 106s/epoch - 18ms/step
Epoch 63/200
5969/5969 - 106s - loss: 4.9733e-04 - val_loss: 3.7373e-04 - 106s/epoch - 18ms/step
Epoch 64/200
5969/5969 - 105s - loss: 4.9190e-04 - val_loss: 3.6977e-04 - 105s/epoch - 18ms/step
Epoch 65/200
5969/5969 - 106s - loss: 4.9747e-04 - val_loss: 3.7809e-04 - 106s/epoch - 18ms/step
Epoch 66/200
5969/5969 - 106s - loss: 4.9326e-04 - val_loss: 3.6302e-04 - 106s/epoch - 18ms/step
Epoch 67/200
5969/5969 - 106s - loss: 4.9886e-04 - val_loss: 3.6990e-04 - 106s/epoch - 18ms/step
Epoch 68/200
5969/5969 - 106s - loss: 4.8799e-04 - val_loss: 3.7302e-04 - 106s/epoch - 18ms/step
Epoch 69/200
5969/5969 - 106s - loss: 4.8294e-04 - val_loss: 3.8505e-04 - 106s/epoch - 18ms/step
Epoch 70/200
5969/5969 - 106s - loss: 4.8278e-04 - val_loss: 3.6821e-04 - 106s/epoch - 18ms/step
Epoch 71/200
5969/5969 - 106s - loss: 4.8035e-04 - val_loss: 3.7421e-04 - 106s/epoch - 18ms/step
Epoch 72/200
5969/5969 - 106s - loss: 4.8111e-04 - val_loss: 3.7959e-04 - 106s/epoch - 18ms/step
Epoch 73/200
5969/5969 - 106s - loss: 4.8741e-04 - val_loss: 3.6617e-04 - 106s/epoch - 18ms/step
Epoch 74/200
5969/5969 - 106s - loss: 4.7510e-04 - val_loss: 3.7245e-04 - 106s/epoch - 18ms/step
Epoch 75/200
5969/5969 - 106s - loss: 4.7719e-04 - val_loss: 3.7287e-04 - 106s/epoch - 18ms/step
Epoch 76/200
5969/5969 - 105s - loss: 4.7139e-04 - val_loss: 3.7220e-04 - 105s/epoch - 18ms/step
Epoch 77/200
5969/5969 - 106s - loss: 4.7900e-04 - val_loss: 3.6922e-04 - 106s/epoch - 18ms/step
Epoch 78/200
5969/5969 - 106s - loss: 4.7588e-04 - val_loss: 3.8265e-04 - 106s/epoch - 18ms/step
Epoch 79/200
5969/5969 - 106s - loss: 4.6868e-04 - val_loss: 3.6153e-04 - 106s/epoch - 18ms/step
Epoch 80/200
5969/5969 - 106s - loss: 4.6901e-04 - val_loss: 3.5151e-04 - 106s/epoch - 18ms/step
Epoch 81/200
5969/5969 - 106s - loss: 4.6323e-04 - val_loss: 3.5846e-04 - 106s/epoch - 18ms/step
Epoch 82/200
5969/5969 - 106s - loss: 4.7122e-04 - val_loss: 3.4877e-04 - 106s/epoch - 18ms/step
Epoch 83/200
5969/5969 - 106s - loss: 4.7620e-04 - val_loss: 3.6657e-04 - 106s/epoch - 18ms/step
Epoch 84/200
5969/5969 - 106s - loss: 4.6951e-04 - val_loss: 3.5090e-04 - 106s/epoch - 18ms/step
Epoch 85/200
5969/5969 - 106s - loss: 4.7402e-04 - val_loss: 3.6364e-04 - 106s/epoch - 18ms/step
Epoch 86/200
5969/5969 - 106s - loss: 4.6328e-04 - val_loss: 3.6254e-04 - 106s/epoch - 18ms/step
Epoch 87/200
5969/5969 - 106s - loss: 4.6353e-04 - val_loss: 3.6629e-04 - 106s/epoch - 18ms/step
Epoch 88/200
5969/5969 - 106s - loss: 4.6447e-04 - val_loss: 3.4196e-04 - 106s/epoch - 18ms/step
Epoch 89/200
5969/5969 - 106s - loss: 4.7274e-04 - val_loss: 3.5167e-04 - 106s/epoch - 18ms/step
Epoch 90/200
5969/5969 - 106s - loss: 4.5738e-04 - val_loss: 3.3092e-04 - 106s/epoch - 18ms/step
Epoch 91/200
5969/5969 - 106s - loss: 4.5571e-04 - val_loss: 3.4906e-04 - 106s/epoch - 18ms/step
Epoch 92/200
5969/5969 - 106s - loss: 4.6623e-04 - val_loss: 3.7613e-04 - 106s/epoch - 18ms/step
Epoch 93/200
5969/5969 - 106s - loss: 4.6126e-04 - val_loss: 3.3986e-04 - 106s/epoch - 18ms/step
Epoch 94/200
5969/5969 - 106s - loss: 4.5482e-04 - val_loss: 3.5188e-04 - 106s/epoch - 18ms/step
Epoch 95/200
5969/5969 - 106s - loss: 4.5393e-04 - val_loss: 3.4086e-04 - 106s/epoch - 18ms/step
Epoch 96/200
5969/5969 - 106s - loss: 4.5730e-04 - val_loss: 3.5448e-04 - 106s/epoch - 18ms/step
Epoch 97/200
5969/5969 - 106s - loss: 4.5738e-04 - val_loss: 3.5502e-04 - 106s/epoch - 18ms/step
Epoch 98/200
5969/5969 - 106s - loss: 4.4979e-04 - val_loss: 3.4525e-04 - 106s/epoch - 18ms/step
Epoch 99/200
5969/5969 - 106s - loss: 4.4475e-04 - val_loss: 3.4724e-04 - 106s/epoch - 18ms/step
Epoch 100/200
5969/5969 - 106s - loss: 4.4931e-04 - val_loss: 3.4104e-04 - 106s/epoch - 18ms/step
Epoch 101/200
5969/5969 - 106s - loss: 4.4384e-04 - val_loss: 3.2723e-04 - 106s/epoch - 18ms/step
Epoch 102/200
5969/5969 - 106s - loss: 4.4340e-04 - val_loss: 3.3494e-04 - 106s/epoch - 18ms/step
Epoch 103/200
5969/5969 - 106s - loss: 4.4583e-04 - val_loss: 3.5864e-04 - 106s/epoch - 18ms/step
Epoch 104/200
5969/5969 - 106s - loss: 4.4525e-04 - val_loss: 3.5025e-04 - 106s/epoch - 18ms/step
Epoch 105/200
5969/5969 - 106s - loss: 4.4513e-04 - val_loss: 3.3252e-04 - 106s/epoch - 18ms/step
Epoch 106/200
5969/5969 - 106s - loss: 4.4038e-04 - val_loss: 3.3338e-04 - 106s/epoch - 18ms/step
Epoch 107/200
5969/5969 - 106s - loss: 4.3823e-04 - val_loss: 3.6759e-04 - 106s/epoch - 18ms/step
Epoch 108/200
5969/5969 - 106s - loss: 4.5193e-04 - val_loss: 3.3046e-04 - 106s/epoch - 18ms/step
Epoch 109/200
5969/5969 - 106s - loss: 4.3823e-04 - val_loss: 3.5486e-04 - 106s/epoch - 18ms/step
Epoch 110/200
5969/5969 - 106s - loss: 4.3876e-04 - val_loss: 3.2641e-04 - 106s/epoch - 18ms/step
Epoch 111/200
5969/5969 - 106s - loss: 4.4181e-04 - val_loss: 3.5294e-04 - 106s/epoch - 18ms/step
Epoch 112/200
5969/5969 - 106s - loss: 4.4013e-04 - val_loss: 3.4704e-04 - 106s/epoch - 18ms/step
Epoch 113/200
5969/5969 - 106s - loss: 4.3313e-04 - val_loss: 3.6498e-04 - 106s/epoch - 18ms/step
Epoch 114/200
5969/5969 - 106s - loss: 4.3404e-04 - val_loss: 3.3523e-04 - 106s/epoch - 18ms/step
Epoch 115/200
5969/5969 - 106s - loss: 4.3427e-04 - val_loss: 3.3557e-04 - 106s/epoch - 18ms/step
Epoch 116/200
5969/5969 - 106s - loss: 4.3953e-04 - val_loss: 3.7451e-04 - 106s/epoch - 18ms/step
Epoch 117/200
5969/5969 - 106s - loss: 4.3392e-04 - val_loss: 3.1780e-04 - 106s/epoch - 18ms/step
Epoch 118/200
5969/5969 - 106s - loss: 4.3965e-04 - val_loss: 3.4457e-04 - 106s/epoch - 18ms/step
Epoch 119/200
5969/5969 - 106s - loss: 4.3207e-04 - val_loss: 3.3456e-04 - 106s/epoch - 18ms/step
Epoch 120/200
5969/5969 - 106s - loss: 4.2651e-04 - val_loss: 3.3833e-04 - 106s/epoch - 18ms/step
Epoch 121/200
5969/5969 - 106s - loss: 4.2774e-04 - val_loss: 3.2979e-04 - 106s/epoch - 18ms/step
Epoch 122/200
5969/5969 - 106s - loss: 4.2902e-04 - val_loss: 3.1354e-04 - 106s/epoch - 18ms/step
Epoch 123/200
5969/5969 - 106s - loss: 4.2652e-04 - val_loss: 3.2583e-04 - 106s/epoch - 18ms/step
Epoch 124/200
5969/5969 - 106s - loss: 4.2552e-04 - val_loss: 3.2323e-04 - 106s/epoch - 18ms/step
Epoch 125/200
5969/5969 - 106s - loss: 4.3356e-04 - val_loss: 3.3170e-04 - 106s/epoch - 18ms/step
Epoch 126/200
5969/5969 - 106s - loss: 4.3143e-04 - val_loss: 3.4342e-04 - 106s/epoch - 18ms/step
Epoch 127/200
5969/5969 - 106s - loss: 4.2719e-04 - val_loss: 3.2452e-04 - 106s/epoch - 18ms/step
Epoch 128/200
5969/5969 - 106s - loss: 4.2928e-04 - val_loss: 3.1690e-04 - 106s/epoch - 18ms/step
Epoch 129/200
5969/5969 - 106s - loss: 4.2825e-04 - val_loss: 3.2959e-04 - 106s/epoch - 18ms/step
Epoch 130/200
5969/5969 - 106s - loss: 4.3235e-04 - val_loss: 3.4612e-04 - 106s/epoch - 18ms/step
Epoch 131/200
5969/5969 - 106s - loss: 4.2778e-04 - val_loss: 3.3086e-04 - 106s/epoch - 18ms/step
Epoch 132/200
5969/5969 - 106s - loss: 4.3170e-04 - val_loss: 3.3227e-04 - 106s/epoch - 18ms/step
Epoch 133/200
5969/5969 - 106s - loss: 4.1635e-04 - val_loss: 3.2735e-04 - 106s/epoch - 18ms/step
Epoch 134/200
5969/5969 - 106s - loss: 4.2432e-04 - val_loss: 3.1660e-04 - 106s/epoch - 18ms/step
Epoch 135/200
5969/5969 - 106s - loss: 4.2148e-04 - val_loss: 3.2767e-04 - 106s/epoch - 18ms/step
Epoch 136/200
5969/5969 - 106s - loss: 4.3130e-04 - val_loss: 3.0334e-04 - 106s/epoch - 18ms/step
Epoch 137/200
5969/5969 - 106s - loss: 4.1832e-04 - val_loss: 3.2321e-04 - 106s/epoch - 18ms/step
Epoch 138/200
5969/5969 - 106s - loss: 4.2438e-04 - val_loss: 3.2754e-04 - 106s/epoch - 18ms/step
Epoch 139/200
5969/5969 - 106s - loss: 4.1642e-04 - val_loss: 3.5552e-04 - 106s/epoch - 18ms/step
Epoch 140/200
5969/5969 - 106s - loss: 4.1667e-04 - val_loss: 3.3651e-04 - 106s/epoch - 18ms/step
Epoch 141/200
5969/5969 - 106s - loss: 4.1634e-04 - val_loss: 3.0632e-04 - 106s/epoch - 18ms/step
Epoch 142/200
5969/5969 - 106s - loss: 4.1890e-04 - val_loss: 3.5091e-04 - 106s/epoch - 18ms/step
Epoch 143/200
5969/5969 - 106s - loss: 4.2034e-04 - val_loss: 3.2693e-04 - 106s/epoch - 18ms/step
Epoch 144/200
5969/5969 - 106s - loss: 4.1781e-04 - val_loss: 3.0684e-04 - 106s/epoch - 18ms/step
Epoch 145/200
5969/5969 - 106s - loss: 4.2325e-04 - val_loss: 3.2972e-04 - 106s/epoch - 18ms/step
Epoch 146/200
5969/5969 - 106s - loss: 4.1691e-04 - val_loss: 3.1914e-04 - 106s/epoch - 18ms/step
Epoch 147/200
5969/5969 - 106s - loss: 4.1362e-04 - val_loss: 3.3230e-04 - 106s/epoch - 18ms/step
Epoch 148/200
5969/5969 - 106s - loss: 4.1943e-04 - val_loss: 3.2753e-04 - 106s/epoch - 18ms/step
Epoch 149/200
5969/5969 - 106s - loss: 4.1226e-04 - val_loss: 3.1995e-04 - 106s/epoch - 18ms/step
Epoch 150/200
5969/5969 - 106s - loss: 4.1580e-04 - val_loss: 3.2166e-04 - 106s/epoch - 18ms/step
Epoch 151/200
5969/5969 - 106s - loss: 4.1488e-04 - val_loss: 3.0016e-04 - 106s/epoch - 18ms/step
Epoch 152/200
5969/5969 - 106s - loss: 4.1457e-04 - val_loss: 3.0754e-04 - 106s/epoch - 18ms/step
Epoch 153/200
5969/5969 - 106s - loss: 4.1195e-04 - val_loss: 3.1939e-04 - 106s/epoch - 18ms/step
Epoch 154/200
5969/5969 - 106s - loss: 4.1153e-04 - val_loss: 3.2341e-04 - 106s/epoch - 18ms/step
Epoch 155/200
5969/5969 - 106s - loss: 4.2099e-04 - val_loss: 2.9421e-04 - 106s/epoch - 18ms/step
Epoch 156/200
5969/5969 - 106s - loss: 4.0707e-04 - val_loss: 3.2832e-04 - 106s/epoch - 18ms/step
Epoch 157/200
5969/5969 - 106s - loss: 4.0928e-04 - val_loss: 3.0492e-04 - 106s/epoch - 18ms/step
Epoch 158/200
5969/5969 - 106s - loss: 4.0703e-04 - val_loss: 3.2767e-04 - 106s/epoch - 18ms/step
Epoch 159/200
5969/5969 - 105s - loss: 4.1321e-04 - val_loss: 2.9075e-04 - 105s/epoch - 18ms/step
Epoch 160/200
5969/5969 - 106s - loss: 4.0731e-04 - val_loss: 3.0410e-04 - 106s/epoch - 18ms/step
Epoch 161/200
5969/5969 - 106s - loss: 4.0640e-04 - val_loss: 3.2973e-04 - 106s/epoch - 18ms/step
Epoch 162/200
5969/5969 - 106s - loss: 4.0891e-04 - val_loss: 3.3755e-04 - 106s/epoch - 18ms/step
Epoch 163/200
5969/5969 - 106s - loss: 4.0657e-04 - val_loss: 3.2208e-04 - 106s/epoch - 18ms/step
Epoch 164/200
5969/5969 - 106s - loss: 4.0536e-04 - val_loss: 3.2526e-04 - 106s/epoch - 18ms/step
Epoch 165/200
5969/5969 - 106s - loss: 4.0771e-04 - val_loss: 3.0960e-04 - 106s/epoch - 18ms/step
Epoch 166/200
5969/5969 - 106s - loss: 4.0427e-04 - val_loss: 3.2813e-04 - 106s/epoch - 18ms/step
Epoch 167/200
5969/5969 - 106s - loss: 4.0122e-04 - val_loss: 3.0318e-04 - 106s/epoch - 18ms/step
Epoch 168/200
5969/5969 - 105s - loss: 4.0379e-04 - val_loss: 3.1440e-04 - 105s/epoch - 18ms/step
Epoch 169/200
5969/5969 - 106s - loss: 4.0430e-04 - val_loss: 3.0713e-04 - 106s/epoch - 18ms/step
Epoch 170/200
5969/5969 - 106s - loss: 4.0201e-04 - val_loss: 3.1103e-04 - 106s/epoch - 18ms/step
Epoch 171/200
5969/5969 - 105s - loss: 4.0515e-04 - val_loss: 3.3176e-04 - 105s/epoch - 18ms/step
Epoch 172/200
5969/5969 - 106s - loss: 4.0481e-04 - val_loss: 3.2695e-04 - 106s/epoch - 18ms/step
Epoch 173/200
5969/5969 - 106s - loss: 4.0219e-04 - val_loss: 3.1375e-04 - 106s/epoch - 18ms/step
Epoch 174/200
5969/5969 - 105s - loss: 4.0132e-04 - val_loss: 3.0917e-04 - 105s/epoch - 18ms/step
Epoch 175/200
5969/5969 - 106s - loss: 4.0119e-04 - val_loss: 3.1122e-04 - 106s/epoch - 18ms/step
Epoch 176/200
5969/5969 - 106s - loss: 4.0219e-04 - val_loss: 3.4257e-04 - 106s/epoch - 18ms/step
Epoch 177/200
5969/5969 - 105s - loss: 3.9888e-04 - val_loss: 3.1467e-04 - 105s/epoch - 18ms/step
Epoch 178/200
5969/5969 - 106s - loss: 4.1522e-04 - val_loss: 3.2748e-04 - 106s/epoch - 18ms/step
Epoch 179/200
5969/5969 - 106s - loss: 4.0693e-04 - val_loss: 3.1028e-04 - 106s/epoch - 18ms/step
Epoch 180/200
5969/5969 - 105s - loss: 3.9470e-04 - val_loss: 3.0546e-04 - 105s/epoch - 18ms/step
Epoch 181/200
5969/5969 - 106s - loss: 4.0248e-04 - val_loss: 3.3085e-04 - 106s/epoch - 18ms/step
Epoch 182/200
5969/5969 - 106s - loss: 4.2054e-04 - val_loss: 3.4578e-04 - 106s/epoch - 18ms/step
Epoch 183/200
5969/5969 - 105s - loss: 3.9691e-04 - val_loss: 3.0231e-04 - 105s/epoch - 18ms/step
Epoch 184/200
5969/5969 - 106s - loss: 3.9726e-04 - val_loss: 3.3622e-04 - 106s/epoch - 18ms/step
Epoch 185/200
5969/5969 - 105s - loss: 3.9175e-04 - val_loss: 3.0631e-04 - 105s/epoch - 18ms/step
Epoch 186/200
5969/5969 - 106s - loss: 3.9440e-04 - val_loss: 3.1539e-04 - 106s/epoch - 18ms/step
Epoch 187/200
5969/5969 - 106s - loss: 4.0126e-04 - val_loss: 3.7209e-04 - 106s/epoch - 18ms/step
Epoch 188/200
5969/5969 - 105s - loss: 4.0200e-04 - val_loss: 3.3812e-04 - 105s/epoch - 18ms/step
Epoch 189/200
5969/5969 - 106s - loss: 4.0250e-04 - val_loss: 3.2043e-04 - 106s/epoch - 18ms/step
Epoch 190/200
5969/5969 - 106s - loss: 4.0115e-04 - val_loss: 3.2263e-04 - 106s/epoch - 18ms/step
Epoch 191/200
5969/5969 - 106s - loss: 3.9521e-04 - val_loss: 3.4651e-04 - 106s/epoch - 18ms/step
Epoch 192/200
5969/5969 - 106s - loss: 3.9566e-04 - val_loss: 3.1188e-04 - 106s/epoch - 18ms/step
Epoch 193/200
5969/5969 - 106s - loss: 3.9767e-04 - val_loss: 3.5140e-04 - 106s/epoch - 18ms/step
Epoch 194/200
5969/5969 - 105s - loss: 3.9296e-04 - val_loss: 3.7065e-04 - 105s/epoch - 18ms/step
Epoch 195/200
5969/5969 - 106s - loss: 4.0517e-04 - val_loss: 3.3513e-04 - 106s/epoch - 18ms/step
Epoch 196/200
5969/5969 - 106s - loss: 3.9127e-04 - val_loss: 3.2324e-04 - 106s/epoch - 18ms/step
Epoch 197/200
5969/5969 - 105s - loss: 4.0408e-04 - val_loss: 3.3241e-04 - 105s/epoch - 18ms/step
Epoch 198/200
5969/5969 - 106s - loss: 3.9256e-04 - val_loss: 3.5086e-04 - 106s/epoch - 18ms/step
Epoch 199/200
5969/5969 - 106s - loss: 3.9316e-04 - val_loss: 3.1821e-04 - 106s/epoch - 18ms/step
Epoch 200/200
5969/5969 - 105s - loss: 3.9002e-04 - val_loss: 3.6098e-04 - 105s/epoch - 18ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0003609821724239737
  1/332 [..............................] - ETA: 28s 16/332 [>.............................] - ETA: 1s  32/332 [=>............................] - ETA: 1s 48/332 [===>..........................] - ETA: 0s 64/332 [====>.........................] - ETA: 0s 80/332 [======>.......................] - ETA: 0s 96/332 [=======>......................] - ETA: 0s112/332 [=========>....................] - ETA: 0s128/332 [==========>...................] - ETA: 0s144/332 [============>.................] - ETA: 0s160/332 [=============>................] - ETA: 0s176/332 [==============>...............] - ETA: 0s192/332 [================>.............] - ETA: 0s208/332 [=================>............] - ETA: 0s224/332 [===================>..........] - ETA: 0s240/332 [====================>.........] - ETA: 0s256/332 [======================>.......] - ETA: 0s272/332 [=======================>......] - ETA: 0s288/332 [=========================>....] - ETA: 0s304/332 [==========================>...] - ETA: 0s320/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 3ms/step
correlation 0.004054030482831021
cosine 0.0032778682065738
MAE: 0.009531324
RMSE: 0.018999513
r2: 0.9765825183394445
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 1264)              1598960   
                                                                 
 batch_normalization_12 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 1264)              800112    
                                                                 
 batch_normalization_14 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 1264)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Encoder
Model: "model_13"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_14 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 1264)              1598960   
                                                                 
 batch_normalization_12 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
=================================================================
Total params: 2,403,496
Trainable params: 2,400,968
Non-trainable params: 2,528
_________________________________________________________________
Decoder
Model: "model_14"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_15 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 1264)              800112    
                                                                 
 batch_normalization_14 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 1264)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 2,406,656
Trainable params: 2,402,864
Non-trainable params: 3,792
_________________________________________________________________
['n_b', 'mse', 16, 200, 0.001, 0.5, 632, 0.0003900167066603899, 0.0003609821724239737, 0.004054030482831021, 0.0032778682065738, 0.009531323798000813, 0.018999513238668442, 0.9765825183394445, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_15 (Dense)            (None, 1264)              1598960   
                                                                 
 batch_normalization_15 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 1264)              800112    
                                                                 
 batch_normalization_17 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 1264)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Epoch 1/200
5969/5969 - 105s - loss: 0.0061 - val_loss: 0.0024 - 105s/epoch - 18ms/step
Epoch 2/200
5969/5969 - 105s - loss: 0.0024 - val_loss: 0.0019 - 105s/epoch - 18ms/step
Epoch 3/200
5969/5969 - 104s - loss: 0.0018 - val_loss: 0.0014 - 104s/epoch - 17ms/step
Epoch 4/200
5969/5969 - 105s - loss: 0.0015 - val_loss: 0.0012 - 105s/epoch - 18ms/step
Epoch 5/200
5969/5969 - 105s - loss: 0.0014 - val_loss: 0.0011 - 105s/epoch - 18ms/step
Epoch 6/200
5969/5969 - 102s - loss: 0.0013 - val_loss: 0.0010 - 102s/epoch - 17ms/step
Epoch 7/200
5969/5969 - 102s - loss: 0.0012 - val_loss: 9.7412e-04 - 102s/epoch - 17ms/step
Epoch 8/200
5969/5969 - 102s - loss: 0.0012 - val_loss: 9.0406e-04 - 102s/epoch - 17ms/step
Epoch 9/200
5969/5969 - 102s - loss: 0.0011 - val_loss: 8.6041e-04 - 102s/epoch - 17ms/step
Epoch 10/200
5969/5969 - 102s - loss: 0.0011 - val_loss: 8.8455e-04 - 102s/epoch - 17ms/step
Epoch 11/200
5969/5969 - 103s - loss: 0.0010 - val_loss: 8.2951e-04 - 103s/epoch - 17ms/step
Epoch 12/200
5969/5969 - 102s - loss: 9.9060e-04 - val_loss: 8.1252e-04 - 102s/epoch - 17ms/step
Epoch 13/200
5969/5969 - 102s - loss: 9.7258e-04 - val_loss: 7.6854e-04 - 102s/epoch - 17ms/step
Epoch 14/200
5969/5969 - 102s - loss: 9.6593e-04 - val_loss: 7.6726e-04 - 102s/epoch - 17ms/step
Epoch 15/200
5969/5969 - 102s - loss: 9.3095e-04 - val_loss: 7.6006e-04 - 102s/epoch - 17ms/step
Epoch 16/200
5969/5969 - 102s - loss: 9.1394e-04 - val_loss: 7.7283e-04 - 102s/epoch - 17ms/step
Epoch 17/200
5969/5969 - 102s - loss: 9.0152e-04 - val_loss: 7.3281e-04 - 102s/epoch - 17ms/step
Epoch 18/200
5969/5969 - 102s - loss: 8.9109e-04 - val_loss: 6.8175e-04 - 102s/epoch - 17ms/step
Epoch 19/200
5969/5969 - 102s - loss: 8.6814e-04 - val_loss: 7.2320e-04 - 102s/epoch - 17ms/step
Epoch 20/200
5969/5969 - 102s - loss: 8.5565e-04 - val_loss: 6.6342e-04 - 102s/epoch - 17ms/step
Epoch 21/200
5969/5969 - 102s - loss: 8.4541e-04 - val_loss: 6.8351e-04 - 102s/epoch - 17ms/step
Epoch 22/200
5969/5969 - 102s - loss: 8.3656e-04 - val_loss: 6.7490e-04 - 102s/epoch - 17ms/step
Epoch 23/200
5969/5969 - 102s - loss: 8.3573e-04 - val_loss: 6.5001e-04 - 102s/epoch - 17ms/step
Epoch 24/200
5969/5969 - 102s - loss: 8.1902e-04 - val_loss: 6.7057e-04 - 102s/epoch - 17ms/step
Epoch 25/200
5969/5969 - 102s - loss: 8.1471e-04 - val_loss: 6.2686e-04 - 102s/epoch - 17ms/step
Epoch 26/200
5969/5969 - 102s - loss: 8.2250e-04 - val_loss: 6.4040e-04 - 102s/epoch - 17ms/step
Epoch 27/200
5969/5969 - 102s - loss: 7.8910e-04 - val_loss: 6.0789e-04 - 102s/epoch - 17ms/step
Epoch 28/200
5969/5969 - 102s - loss: 7.8589e-04 - val_loss: 5.8185e-04 - 102s/epoch - 17ms/step
Epoch 29/200
5969/5969 - 102s - loss: 7.7458e-04 - val_loss: 6.4317e-04 - 102s/epoch - 17ms/step
Epoch 30/200
5969/5969 - 102s - loss: 7.6521e-04 - val_loss: 5.7342e-04 - 102s/epoch - 17ms/step
Epoch 31/200
5969/5969 - 102s - loss: 7.7218e-04 - val_loss: 5.6801e-04 - 102s/epoch - 17ms/step
Epoch 32/200
5969/5969 - 102s - loss: 7.5082e-04 - val_loss: 5.9738e-04 - 102s/epoch - 17ms/step
Epoch 33/200
5969/5969 - 102s - loss: 7.4356e-04 - val_loss: 5.5996e-04 - 102s/epoch - 17ms/step
Epoch 34/200
5969/5969 - 102s - loss: 7.4244e-04 - val_loss: 5.6258e-04 - 102s/epoch - 17ms/step
Epoch 35/200
5969/5969 - 102s - loss: 7.4227e-04 - val_loss: 5.6400e-04 - 102s/epoch - 17ms/step
Epoch 36/200
5969/5969 - 102s - loss: 7.4241e-04 - val_loss: 6.1010e-04 - 102s/epoch - 17ms/step
Epoch 37/200
5969/5969 - 102s - loss: 7.2287e-04 - val_loss: 5.7303e-04 - 102s/epoch - 17ms/step
Epoch 38/200
5969/5969 - 102s - loss: 7.2297e-04 - val_loss: 5.5756e-04 - 102s/epoch - 17ms/step
Epoch 39/200
5969/5969 - 102s - loss: 7.1742e-04 - val_loss: 5.4407e-04 - 102s/epoch - 17ms/step
Epoch 40/200
5969/5969 - 102s - loss: 7.0169e-04 - val_loss: 5.4967e-04 - 102s/epoch - 17ms/step
Epoch 41/200
5969/5969 - 102s - loss: 7.0877e-04 - val_loss: 5.2994e-04 - 102s/epoch - 17ms/step
Epoch 42/200
5969/5969 - 102s - loss: 7.0340e-04 - val_loss: 5.2102e-04 - 102s/epoch - 17ms/step
Epoch 43/200
5969/5969 - 102s - loss: 6.9192e-04 - val_loss: 5.3345e-04 - 102s/epoch - 17ms/step
Epoch 44/200
5969/5969 - 102s - loss: 6.9348e-04 - val_loss: 5.4110e-04 - 102s/epoch - 17ms/step
Epoch 45/200
5969/5969 - 102s - loss: 6.9336e-04 - val_loss: 5.1841e-04 - 102s/epoch - 17ms/step
Epoch 46/200
5969/5969 - 102s - loss: 6.8202e-04 - val_loss: 5.1621e-04 - 102s/epoch - 17ms/step
Epoch 47/200
5969/5969 - 102s - loss: 6.8495e-04 - val_loss: 5.1635e-04 - 102s/epoch - 17ms/step
Epoch 48/200
5969/5969 - 102s - loss: 6.9975e-04 - val_loss: 5.2710e-04 - 102s/epoch - 17ms/step
Epoch 49/200
5969/5969 - 102s - loss: 6.8383e-04 - val_loss: 5.2023e-04 - 102s/epoch - 17ms/step
Epoch 50/200
5969/5969 - 102s - loss: 6.8931e-04 - val_loss: 5.2260e-04 - 102s/epoch - 17ms/step
Epoch 51/200
5969/5969 - 102s - loss: 6.8317e-04 - val_loss: 5.1458e-04 - 102s/epoch - 17ms/step
Epoch 52/200
5969/5969 - 102s - loss: 6.6384e-04 - val_loss: 5.2809e-04 - 102s/epoch - 17ms/step
Epoch 53/200
5969/5969 - 102s - loss: 6.6571e-04 - val_loss: 5.0193e-04 - 102s/epoch - 17ms/step
Epoch 54/200
5969/5969 - 102s - loss: 6.6558e-04 - val_loss: 5.2069e-04 - 102s/epoch - 17ms/step
Epoch 55/200
5969/5969 - 102s - loss: 6.6764e-04 - val_loss: 4.8201e-04 - 102s/epoch - 17ms/step
Epoch 56/200
5969/5969 - 102s - loss: 6.6070e-04 - val_loss: 5.0986e-04 - 102s/epoch - 17ms/step
Epoch 57/200
5969/5969 - 102s - loss: 6.6450e-04 - val_loss: 4.8928e-04 - 102s/epoch - 17ms/step
Epoch 58/200
5969/5969 - 102s - loss: 6.4862e-04 - val_loss: 5.0763e-04 - 102s/epoch - 17ms/step
Epoch 59/200
5969/5969 - 102s - loss: 6.5825e-04 - val_loss: 5.0396e-04 - 102s/epoch - 17ms/step
Epoch 60/200
5969/5969 - 102s - loss: 6.4510e-04 - val_loss: 4.9314e-04 - 102s/epoch - 17ms/step
Epoch 61/200
5969/5969 - 102s - loss: 6.5368e-04 - val_loss: 5.3690e-04 - 102s/epoch - 17ms/step
Epoch 62/200
5969/5969 - 102s - loss: 6.5001e-04 - val_loss: 5.2607e-04 - 102s/epoch - 17ms/step
Epoch 63/200
5969/5969 - 102s - loss: 6.3677e-04 - val_loss: 4.9159e-04 - 102s/epoch - 17ms/step
Epoch 64/200
5969/5969 - 102s - loss: 6.3479e-04 - val_loss: 4.8613e-04 - 102s/epoch - 17ms/step
Epoch 65/200
5969/5969 - 102s - loss: 7.1849e-04 - val_loss: 5.1413e-04 - 102s/epoch - 17ms/step
Epoch 66/200
5969/5969 - 102s - loss: 6.6728e-04 - val_loss: 4.8341e-04 - 102s/epoch - 17ms/step
Epoch 67/200
5969/5969 - 102s - loss: 6.5206e-04 - val_loss: 5.1807e-04 - 102s/epoch - 17ms/step
Epoch 68/200
5969/5969 - 102s - loss: 6.3877e-04 - val_loss: 4.9400e-04 - 102s/epoch - 17ms/step
Epoch 69/200
5969/5969 - 102s - loss: 6.3583e-04 - val_loss: 4.7740e-04 - 102s/epoch - 17ms/step
Epoch 70/200
5969/5969 - 102s - loss: 6.3515e-04 - val_loss: 5.9150e-04 - 102s/epoch - 17ms/step
Epoch 71/200
5969/5969 - 102s - loss: 6.3804e-04 - val_loss: 5.1041e-04 - 102s/epoch - 17ms/step
Epoch 72/200
5969/5969 - 102s - loss: 6.2624e-04 - val_loss: 4.9311e-04 - 102s/epoch - 17ms/step
Epoch 73/200
5969/5969 - 102s - loss: 6.4372e-04 - val_loss: 4.8855e-04 - 102s/epoch - 17ms/step
Epoch 74/200
5969/5969 - 102s - loss: 6.2412e-04 - val_loss: 4.9667e-04 - 102s/epoch - 17ms/step
Epoch 75/200
5969/5969 - 102s - loss: 6.4783e-04 - val_loss: 7.0680e-04 - 102s/epoch - 17ms/step
Epoch 76/200
5969/5969 - 102s - loss: 6.1846e-04 - val_loss: 4.8695e-04 - 102s/epoch - 17ms/step
Epoch 77/200
5969/5969 - 102s - loss: 6.2876e-04 - val_loss: 4.5786e-04 - 102s/epoch - 17ms/step
Epoch 78/200
5969/5969 - 102s - loss: 6.2508e-04 - val_loss: 5.0342e-04 - 102s/epoch - 17ms/step
Epoch 79/200
5969/5969 - 102s - loss: 6.2180e-04 - val_loss: 4.7151e-04 - 102s/epoch - 17ms/step
Epoch 80/200
5969/5969 - 102s - loss: 6.1068e-04 - val_loss: 4.5789e-04 - 102s/epoch - 17ms/step
Epoch 81/200
5969/5969 - 102s - loss: 6.0432e-04 - val_loss: 4.9197e-04 - 102s/epoch - 17ms/step
Epoch 82/200
5969/5969 - 102s - loss: 6.2612e-04 - val_loss: 4.7475e-04 - 102s/epoch - 17ms/step
Epoch 83/200
5969/5969 - 102s - loss: 6.1896e-04 - val_loss: 4.9974e-04 - 102s/epoch - 17ms/step
Epoch 84/200
5969/5969 - 102s - loss: 6.1108e-04 - val_loss: 4.8088e-04 - 102s/epoch - 17ms/step
Epoch 85/200
5969/5969 - 102s - loss: 6.0758e-04 - val_loss: 4.5751e-04 - 102s/epoch - 17ms/step
Epoch 86/200
5969/5969 - 101s - loss: 6.3256e-04 - val_loss: 4.8694e-04 - 101s/epoch - 17ms/step
Epoch 87/200
5969/5969 - 102s - loss: 6.0069e-04 - val_loss: 5.0641e-04 - 102s/epoch - 17ms/step
Epoch 88/200
5969/5969 - 102s - loss: 6.3108e-04 - val_loss: 4.4588e-04 - 102s/epoch - 17ms/step
Epoch 89/200
5969/5969 - 101s - loss: 6.0870e-04 - val_loss: 4.5434e-04 - 101s/epoch - 17ms/step
Epoch 90/200
5969/5969 - 102s - loss: 6.0143e-04 - val_loss: 4.4669e-04 - 102s/epoch - 17ms/step
Epoch 91/200
5969/5969 - 102s - loss: 5.9702e-04 - val_loss: 5.6731e-04 - 102s/epoch - 17ms/step
Epoch 92/200
5969/5969 - 101s - loss: 5.9772e-04 - val_loss: 4.8282e-04 - 101s/epoch - 17ms/step
Epoch 93/200
5969/5969 - 102s - loss: 6.1840e-04 - val_loss: 5.1070e-04 - 102s/epoch - 17ms/step
Epoch 94/200
5969/5969 - 102s - loss: 6.0613e-04 - val_loss: 4.4932e-04 - 102s/epoch - 17ms/step
Epoch 95/200
5969/5969 - 101s - loss: 5.9490e-04 - val_loss: 4.6980e-04 - 101s/epoch - 17ms/step
Epoch 96/200
5969/5969 - 102s - loss: 6.1127e-04 - val_loss: 4.9036e-04 - 102s/epoch - 17ms/step
Epoch 97/200
5969/5969 - 102s - loss: 5.9287e-04 - val_loss: 4.5494e-04 - 102s/epoch - 17ms/step
Epoch 98/200
5969/5969 - 101s - loss: 5.9979e-04 - val_loss: 4.5754e-04 - 101s/epoch - 17ms/step
Epoch 99/200
5969/5969 - 102s - loss: 6.0831e-04 - val_loss: 4.6114e-04 - 102s/epoch - 17ms/step
Epoch 100/200
5969/5969 - 102s - loss: 5.9969e-04 - val_loss: 4.3758e-04 - 102s/epoch - 17ms/step
Epoch 101/200
5969/5969 - 101s - loss: 5.8407e-04 - val_loss: 4.5477e-04 - 101s/epoch - 17ms/step
Epoch 102/200
5969/5969 - 102s - loss: 6.1032e-04 - val_loss: 4.7641e-04 - 102s/epoch - 17ms/step
Epoch 103/200
5969/5969 - 102s - loss: 5.9138e-04 - val_loss: 4.4358e-04 - 102s/epoch - 17ms/step
Epoch 104/200
5969/5969 - 101s - loss: 5.9046e-04 - val_loss: 4.7294e-04 - 101s/epoch - 17ms/step
Epoch 105/200
5969/5969 - 102s - loss: 5.9023e-04 - val_loss: 5.2033e-04 - 102s/epoch - 17ms/step
Epoch 106/200
5969/5969 - 102s - loss: 5.8710e-04 - val_loss: 4.6138e-04 - 102s/epoch - 17ms/step
Epoch 107/200
5969/5969 - 101s - loss: 5.8214e-04 - val_loss: 4.7500e-04 - 101s/epoch - 17ms/step
Epoch 108/200
5969/5969 - 102s - loss: 5.7952e-04 - val_loss: 4.5885e-04 - 102s/epoch - 17ms/step
Epoch 109/200
5969/5969 - 102s - loss: 5.8888e-04 - val_loss: 5.5878e-04 - 102s/epoch - 17ms/step
Epoch 110/200
5969/5969 - 101s - loss: 5.7448e-04 - val_loss: 4.6311e-04 - 101s/epoch - 17ms/step
Epoch 111/200
5969/5969 - 102s - loss: 5.7630e-04 - val_loss: 5.1773e-04 - 102s/epoch - 17ms/step
Epoch 112/200
5969/5969 - 102s - loss: 5.9246e-04 - val_loss: 9.3674e-04 - 102s/epoch - 17ms/step
Epoch 113/200
5969/5969 - 101s - loss: 5.8551e-04 - val_loss: 4.5327e-04 - 101s/epoch - 17ms/step
Epoch 114/200
5969/5969 - 102s - loss: 5.7215e-04 - val_loss: 4.6200e-04 - 102s/epoch - 17ms/step
Epoch 115/200
5969/5969 - 102s - loss: 5.7485e-04 - val_loss: 4.4538e-04 - 102s/epoch - 17ms/step
Epoch 116/200
5969/5969 - 101s - loss: 5.9124e-04 - val_loss: 4.5668e-04 - 101s/epoch - 17ms/step
Epoch 117/200
5969/5969 - 102s - loss: 5.7207e-04 - val_loss: 4.2301e-04 - 102s/epoch - 17ms/step
Epoch 118/200
5969/5969 - 102s - loss: 5.8522e-04 - val_loss: 4.6829e-04 - 102s/epoch - 17ms/step
Epoch 119/200
5969/5969 - 101s - loss: 5.7446e-04 - val_loss: 4.7658e-04 - 101s/epoch - 17ms/step
Epoch 120/200
5969/5969 - 102s - loss: 5.8354e-04 - val_loss: 4.7474e-04 - 102s/epoch - 17ms/step
Epoch 121/200
5969/5969 - 102s - loss: 5.7263e-04 - val_loss: 4.4299e-04 - 102s/epoch - 17ms/step
Epoch 122/200
5969/5969 - 101s - loss: 5.6642e-04 - val_loss: 4.2439e-04 - 101s/epoch - 17ms/step
Epoch 123/200
5969/5969 - 102s - loss: 5.6649e-04 - val_loss: 4.7742e-04 - 102s/epoch - 17ms/step
Epoch 124/200
5969/5969 - 102s - loss: 5.6803e-04 - val_loss: 4.6300e-04 - 102s/epoch - 17ms/step
Epoch 125/200
5969/5969 - 101s - loss: 5.6503e-04 - val_loss: 4.3110e-04 - 101s/epoch - 17ms/step
Epoch 126/200
5969/5969 - 102s - loss: 5.7422e-04 - val_loss: 4.8653e-04 - 102s/epoch - 17ms/step
Epoch 127/200
5969/5969 - 102s - loss: 5.6447e-04 - val_loss: 5.8285e-04 - 102s/epoch - 17ms/step
Epoch 128/200
5969/5969 - 102s - loss: 5.5916e-04 - val_loss: 4.4999e-04 - 102s/epoch - 17ms/step
Epoch 129/200
5969/5969 - 101s - loss: 5.7027e-04 - val_loss: 4.5954e-04 - 101s/epoch - 17ms/step
Epoch 130/200
5969/5969 - 102s - loss: 5.6531e-04 - val_loss: 4.7808e-04 - 102s/epoch - 17ms/step
Epoch 131/200
5969/5969 - 102s - loss: 6.1088e-04 - val_loss: 5.0943e-04 - 102s/epoch - 17ms/step
Epoch 132/200
5969/5969 - 101s - loss: 5.6937e-04 - val_loss: 4.6305e-04 - 101s/epoch - 17ms/step
Epoch 133/200
5969/5969 - 102s - loss: 5.7382e-04 - val_loss: 4.5235e-04 - 102s/epoch - 17ms/step
Epoch 134/200
5969/5969 - 102s - loss: 5.7214e-04 - val_loss: 4.3001e-04 - 102s/epoch - 17ms/step
Epoch 135/200
5969/5969 - 101s - loss: 5.5765e-04 - val_loss: 4.3331e-04 - 101s/epoch - 17ms/step
Epoch 136/200
5969/5969 - 102s - loss: 5.6542e-04 - val_loss: 4.4595e-04 - 102s/epoch - 17ms/step
Epoch 137/200
5969/5969 - 102s - loss: 5.6337e-04 - val_loss: 4.2759e-04 - 102s/epoch - 17ms/step
Epoch 138/200
5969/5969 - 101s - loss: 5.6083e-04 - val_loss: 5.0822e-04 - 101s/epoch - 17ms/step
Epoch 139/200
5969/5969 - 102s - loss: 5.6321e-04 - val_loss: 4.6339e-04 - 102s/epoch - 17ms/step
Epoch 140/200
5969/5969 - 102s - loss: 5.5709e-04 - val_loss: 4.3221e-04 - 102s/epoch - 17ms/step
Epoch 141/200
5969/5969 - 101s - loss: 5.6245e-04 - val_loss: 4.2709e-04 - 101s/epoch - 17ms/step
Epoch 142/200
5969/5969 - 102s - loss: 5.5658e-04 - val_loss: 4.4873e-04 - 102s/epoch - 17ms/step
Epoch 143/200
5969/5969 - 102s - loss: 6.0679e-04 - val_loss: 4.8825e-04 - 102s/epoch - 17ms/step
Epoch 144/200
5969/5969 - 101s - loss: 5.5770e-04 - val_loss: 4.2319e-04 - 101s/epoch - 17ms/step
Epoch 145/200
5969/5969 - 102s - loss: 5.6533e-04 - val_loss: 4.5479e-04 - 102s/epoch - 17ms/step
Epoch 146/200
5969/5969 - 102s - loss: 5.5718e-04 - val_loss: 4.3504e-04 - 102s/epoch - 17ms/step
Epoch 147/200
5969/5969 - 102s - loss: 5.4745e-04 - val_loss: 4.6470e-04 - 102s/epoch - 17ms/step
Epoch 148/200
5969/5969 - 102s - loss: 5.6913e-04 - val_loss: 4.5474e-04 - 102s/epoch - 17ms/step
Epoch 149/200
5969/5969 - 102s - loss: 5.5138e-04 - val_loss: 4.1795e-04 - 102s/epoch - 17ms/step
Epoch 150/200
5969/5969 - 101s - loss: 5.7363e-04 - val_loss: 4.6825e-04 - 101s/epoch - 17ms/step
Epoch 151/200
5969/5969 - 102s - loss: 5.5161e-04 - val_loss: 4.4815e-04 - 102s/epoch - 17ms/step
Epoch 152/200
5969/5969 - 102s - loss: 5.4833e-04 - val_loss: 4.2706e-04 - 102s/epoch - 17ms/step
Epoch 153/200
5969/5969 - 101s - loss: 5.5606e-04 - val_loss: 4.3390e-04 - 101s/epoch - 17ms/step
Epoch 154/200
5969/5969 - 102s - loss: 5.5304e-04 - val_loss: 4.3950e-04 - 102s/epoch - 17ms/step
Epoch 155/200
5969/5969 - 102s - loss: 5.5105e-04 - val_loss: 4.1937e-04 - 102s/epoch - 17ms/step
Epoch 156/200
5969/5969 - 101s - loss: 5.5720e-04 - val_loss: 4.5580e-04 - 101s/epoch - 17ms/step
Epoch 157/200
5969/5969 - 102s - loss: 5.4011e-04 - val_loss: 4.1479e-04 - 102s/epoch - 17ms/step
Epoch 158/200
5969/5969 - 102s - loss: 5.5912e-04 - val_loss: 4.4480e-04 - 102s/epoch - 17ms/step
Epoch 159/200
5969/5969 - 101s - loss: 5.5766e-04 - val_loss: 4.3520e-04 - 101s/epoch - 17ms/step
Epoch 160/200
5969/5969 - 102s - loss: 5.4778e-04 - val_loss: 4.5864e-04 - 102s/epoch - 17ms/step
Epoch 161/200
5969/5969 - 102s - loss: 5.4801e-04 - val_loss: 6.3741e-04 - 102s/epoch - 17ms/step
Epoch 162/200
5969/5969 - 101s - loss: 5.4441e-04 - val_loss: 4.5410e-04 - 101s/epoch - 17ms/step
Epoch 163/200
5969/5969 - 102s - loss: 5.5061e-04 - val_loss: 4.4366e-04 - 102s/epoch - 17ms/step
Epoch 164/200
5969/5969 - 102s - loss: 5.4878e-04 - val_loss: 4.7230e-04 - 102s/epoch - 17ms/step
Epoch 165/200
5969/5969 - 101s - loss: 5.5847e-04 - val_loss: 4.6134e-04 - 101s/epoch - 17ms/step
Epoch 166/200
5969/5969 - 102s - loss: 5.3689e-04 - val_loss: 4.3122e-04 - 102s/epoch - 17ms/step
Epoch 167/200
5969/5969 - 102s - loss: 5.4674e-04 - val_loss: 4.4912e-04 - 102s/epoch - 17ms/step
Epoch 168/200
5969/5969 - 101s - loss: 5.4410e-04 - val_loss: 4.2366e-04 - 101s/epoch - 17ms/step
Epoch 169/200
5969/5969 - 102s - loss: 5.4456e-04 - val_loss: 4.2501e-04 - 102s/epoch - 17ms/step
Epoch 170/200
5969/5969 - 102s - loss: 5.6441e-04 - val_loss: 4.1450e-04 - 102s/epoch - 17ms/step
Epoch 171/200
5969/5969 - 101s - loss: 5.4364e-04 - val_loss: 4.4600e-04 - 101s/epoch - 17ms/step
Epoch 172/200
5969/5969 - 102s - loss: 5.3660e-04 - val_loss: 4.8531e-04 - 102s/epoch - 17ms/step
Epoch 173/200
5969/5969 - 102s - loss: 5.3461e-04 - val_loss: 4.4370e-04 - 102s/epoch - 17ms/step
Epoch 174/200
5969/5969 - 102s - loss: 5.3873e-04 - val_loss: 4.4466e-04 - 102s/epoch - 17ms/step
Epoch 175/200
5969/5969 - 101s - loss: 5.3688e-04 - val_loss: 4.2882e-04 - 101s/epoch - 17ms/step
Epoch 176/200
5969/5969 - 102s - loss: 5.7290e-04 - val_loss: 4.4328e-04 - 102s/epoch - 17ms/step
Epoch 177/200
5969/5969 - 102s - loss: 5.4767e-04 - val_loss: 4.6655e-04 - 102s/epoch - 17ms/step
Epoch 178/200
5969/5969 - 101s - loss: 5.3501e-04 - val_loss: 4.2600e-04 - 101s/epoch - 17ms/step
Epoch 179/200
5969/5969 - 102s - loss: 5.3935e-04 - val_loss: 4.6806e-04 - 102s/epoch - 17ms/step
Epoch 180/200
5969/5969 - 102s - loss: 5.4120e-04 - val_loss: 4.7996e-04 - 102s/epoch - 17ms/step
Epoch 181/200
5969/5969 - 102s - loss: 5.4658e-04 - val_loss: 4.5945e-04 - 102s/epoch - 17ms/step
Epoch 182/200
5969/5969 - 102s - loss: 5.5699e-04 - val_loss: 4.8212e-04 - 102s/epoch - 17ms/step
Epoch 183/200
5969/5969 - 102s - loss: 5.4226e-04 - val_loss: 4.5636e-04 - 102s/epoch - 17ms/step
Epoch 184/200
5969/5969 - 102s - loss: 5.3634e-04 - val_loss: 5.0637e-04 - 102s/epoch - 17ms/step
Epoch 185/200
5969/5969 - 102s - loss: 5.4545e-04 - val_loss: 4.5620e-04 - 102s/epoch - 17ms/step
Epoch 186/200
5969/5969 - 102s - loss: 5.2804e-04 - val_loss: 4.5441e-04 - 102s/epoch - 17ms/step
Epoch 187/200
5969/5969 - 102s - loss: 5.4822e-04 - val_loss: 5.1811e-04 - 102s/epoch - 17ms/step
Epoch 188/200
5969/5969 - 102s - loss: 5.4125e-04 - val_loss: 4.6725e-04 - 102s/epoch - 17ms/step
Epoch 189/200
5969/5969 - 102s - loss: 6.9510e-04 - val_loss: 5.4002e-04 - 102s/epoch - 17ms/step
Epoch 190/200
5969/5969 - 102s - loss: 6.2723e-04 - val_loss: 5.1317e-04 - 102s/epoch - 17ms/step
Epoch 191/200
5969/5969 - 102s - loss: 5.9520e-04 - val_loss: 5.0737e-04 - 102s/epoch - 17ms/step
Epoch 192/200
5969/5969 - 102s - loss: 5.8729e-04 - val_loss: 4.8072e-04 - 102s/epoch - 17ms/step
Epoch 193/200
5969/5969 - 102s - loss: 5.8324e-04 - val_loss: 6.3804e-04 - 102s/epoch - 17ms/step
Epoch 194/200
5969/5969 - 102s - loss: 5.7907e-04 - val_loss: 6.5193e-04 - 102s/epoch - 17ms/step
Epoch 195/200
5969/5969 - 102s - loss: 5.7702e-04 - val_loss: 5.2098e-04 - 102s/epoch - 17ms/step
Epoch 196/200
5969/5969 - 101s - loss: 5.8322e-04 - val_loss: 6.4730e-04 - 101s/epoch - 17ms/step
Epoch 197/200
5969/5969 - 102s - loss: 5.9509e-04 - val_loss: 5.8804e-04 - 102s/epoch - 17ms/step
Epoch 198/200
5969/5969 - 102s - loss: 5.7178e-04 - val_loss: 5.5213e-04 - 102s/epoch - 17ms/step
Epoch 199/200
5969/5969 - 102s - loss: 5.7579e-04 - val_loss: 5.8203e-04 - 102s/epoch - 17ms/step
Epoch 200/200
5969/5969 - 102s - loss: 5.7272e-04 - val_loss: 6.4608e-04 - 102s/epoch - 17ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.000646082975436002
  1/332 [..............................] - ETA: 27s 16/332 [>.............................] - ETA: 1s  32/332 [=>............................] - ETA: 1s 48/332 [===>..........................] - ETA: 0s 64/332 [====>.........................] - ETA: 0s 80/332 [======>.......................] - ETA: 0s 96/332 [=======>......................] - ETA: 0s112/332 [=========>....................] - ETA: 0s128/332 [==========>...................] - ETA: 0s144/332 [============>.................] - ETA: 0s160/332 [=============>................] - ETA: 0s176/332 [==============>...............] - ETA: 0s192/332 [================>.............] - ETA: 0s208/332 [=================>............] - ETA: 0s224/332 [===================>..........] - ETA: 0s240/332 [====================>.........] - ETA: 0s256/332 [======================>.......] - ETA: 0s272/332 [=======================>......] - ETA: 0s288/332 [=========================>....] - ETA: 0s304/332 [==========================>...] - ETA: 0s320/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 3ms/step
correlation 0.006952157317805731
cosine 0.005690789064325749
MAE: 0.012129216
RMSE: 0.025418136
r2: 0.9580878347919085
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 1264)              1598960   
                                                                 
 batch_normalization_15 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 1264)              800112    
                                                                 
 batch_normalization_17 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 1264)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Encoder
Model: "model_16"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_17 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 1264)              1598960   
                                                                 
 batch_normalization_15 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
=================================================================
Total params: 2,403,496
Trainable params: 2,400,968
Non-trainable params: 2,528
_________________________________________________________________
Decoder
Model: "model_17"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_18 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 1264)              800112    
                                                                 
 batch_normalization_17 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 1264)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 2,406,656
Trainable params: 2,402,864
Non-trainable params: 3,792
_________________________________________________________________
['n_b', 'mse', 16, 200, 0.002, 0.5, 632, 0.0005727233947254717, 0.000646082975436002, 0.006952157317805731, 0.005690789064325749, 0.012129216454923153, 0.02541813626885414, 0.9580878347919085, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_18 (Dense)            (None, 1264)              1598960   
                                                                 
 batch_normalization_18 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 1264)              800112    
                                                                 
 batch_normalization_20 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 1264)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Epoch 1/300
5969/5969 - 105s - loss: 0.0074 - val_loss: 0.0029 - 105s/epoch - 18ms/step
Epoch 2/300
5969/5969 - 105s - loss: 0.0033 - val_loss: 0.0025 - 105s/epoch - 18ms/step
Epoch 3/300
5969/5969 - 105s - loss: 0.0024 - val_loss: 0.0016 - 105s/epoch - 18ms/step
Epoch 4/300
5969/5969 - 105s - loss: 0.0018 - val_loss: 0.0012 - 105s/epoch - 18ms/step
Epoch 5/300
5969/5969 - 105s - loss: 0.0015 - val_loss: 9.7679e-04 - 105s/epoch - 18ms/step
Epoch 6/300
5969/5969 - 105s - loss: 0.0013 - val_loss: 9.5051e-04 - 105s/epoch - 18ms/step
Epoch 7/300
5969/5969 - 105s - loss: 0.0011 - val_loss: 8.4081e-04 - 105s/epoch - 18ms/step
Epoch 8/300
5969/5969 - 105s - loss: 0.0011 - val_loss: 7.6885e-04 - 105s/epoch - 18ms/step
Epoch 9/300
5969/5969 - 104s - loss: 9.9356e-04 - val_loss: 7.8021e-04 - 104s/epoch - 17ms/step
Epoch 10/300
5969/5969 - 105s - loss: 9.2042e-04 - val_loss: 6.8465e-04 - 105s/epoch - 18ms/step
Epoch 11/300
5969/5969 - 105s - loss: 8.7857e-04 - val_loss: 6.6859e-04 - 105s/epoch - 18ms/step
Epoch 12/300
5969/5969 - 105s - loss: 8.3725e-04 - val_loss: 6.5651e-04 - 105s/epoch - 18ms/step
Epoch 13/300
5969/5969 - 105s - loss: 8.1246e-04 - val_loss: 5.9193e-04 - 105s/epoch - 18ms/step
Epoch 14/300
5969/5969 - 105s - loss: 7.6916e-04 - val_loss: 5.7656e-04 - 105s/epoch - 18ms/step
Epoch 15/300
5969/5969 - 105s - loss: 7.5820e-04 - val_loss: 5.4509e-04 - 105s/epoch - 18ms/step
Epoch 16/300
5969/5969 - 105s - loss: 7.2635e-04 - val_loss: 7.8190e-04 - 105s/epoch - 18ms/step
Epoch 17/300
5969/5969 - 105s - loss: 7.1027e-04 - val_loss: 5.3402e-04 - 105s/epoch - 18ms/step
Epoch 18/300
5969/5969 - 105s - loss: 6.9371e-04 - val_loss: 5.2810e-04 - 105s/epoch - 18ms/step
Epoch 19/300
5969/5969 - 105s - loss: 6.7127e-04 - val_loss: 5.2505e-04 - 105s/epoch - 18ms/step
Epoch 20/300
5969/5969 - 105s - loss: 6.6524e-04 - val_loss: 4.9920e-04 - 105s/epoch - 18ms/step
Epoch 21/300
5969/5969 - 105s - loss: 6.4011e-04 - val_loss: 5.0637e-04 - 105s/epoch - 18ms/step
Epoch 22/300
5969/5969 - 105s - loss: 6.3237e-04 - val_loss: 4.8390e-04 - 105s/epoch - 18ms/step
Epoch 23/300
5969/5969 - 105s - loss: 6.1755e-04 - val_loss: 4.8166e-04 - 105s/epoch - 18ms/step
Epoch 24/300
5969/5969 - 105s - loss: 6.1072e-04 - val_loss: 4.8311e-04 - 105s/epoch - 18ms/step
Epoch 25/300
5969/5969 - 105s - loss: 6.0636e-04 - val_loss: 4.6476e-04 - 105s/epoch - 18ms/step
Epoch 26/300
5969/5969 - 105s - loss: 5.9721e-04 - val_loss: 4.4729e-04 - 105s/epoch - 18ms/step
Epoch 27/300
5969/5969 - 105s - loss: 5.8352e-04 - val_loss: 4.6053e-04 - 105s/epoch - 18ms/step
Epoch 28/300
5969/5969 - 105s - loss: 5.9872e-04 - val_loss: 4.3449e-04 - 105s/epoch - 18ms/step
Epoch 29/300
5969/5969 - 105s - loss: 5.7172e-04 - val_loss: 4.5046e-04 - 105s/epoch - 18ms/step
Epoch 30/300
5969/5969 - 105s - loss: 5.6061e-04 - val_loss: 4.4188e-04 - 105s/epoch - 18ms/step
Epoch 31/300
5969/5969 - 105s - loss: 5.5905e-04 - val_loss: 4.0907e-04 - 105s/epoch - 18ms/step
Epoch 32/300
5969/5969 - 105s - loss: 5.5986e-04 - val_loss: 4.4476e-04 - 105s/epoch - 18ms/step
Epoch 33/300
5969/5969 - 105s - loss: 5.3901e-04 - val_loss: 4.1826e-04 - 105s/epoch - 18ms/step
Epoch 34/300
5969/5969 - 105s - loss: 5.3571e-04 - val_loss: 4.0638e-04 - 105s/epoch - 18ms/step
Epoch 35/300
5969/5969 - 105s - loss: 5.3296e-04 - val_loss: 4.0926e-04 - 105s/epoch - 18ms/step
Epoch 36/300
5969/5969 - 105s - loss: 5.3128e-04 - val_loss: 4.0310e-04 - 105s/epoch - 18ms/step
Epoch 37/300
5969/5969 - 105s - loss: 5.3112e-04 - val_loss: 4.0455e-04 - 105s/epoch - 18ms/step
Epoch 38/300
5969/5969 - 105s - loss: 5.2191e-04 - val_loss: 4.0573e-04 - 105s/epoch - 18ms/step
Epoch 39/300
5969/5969 - 105s - loss: 5.0980e-04 - val_loss: 3.8633e-04 - 105s/epoch - 18ms/step
Epoch 40/300
5969/5969 - 105s - loss: 5.1448e-04 - val_loss: 3.8337e-04 - 105s/epoch - 18ms/step
Epoch 41/300
5969/5969 - 105s - loss: 5.1500e-04 - val_loss: 4.1694e-04 - 105s/epoch - 18ms/step
Epoch 42/300
5969/5969 - 105s - loss: 5.0421e-04 - val_loss: 3.7867e-04 - 105s/epoch - 18ms/step
Epoch 43/300
5969/5969 - 106s - loss: 4.9391e-04 - val_loss: 3.9114e-04 - 106s/epoch - 18ms/step
Epoch 44/300
5969/5969 - 105s - loss: 4.9637e-04 - val_loss: 3.8736e-04 - 105s/epoch - 18ms/step
Epoch 45/300
5969/5969 - 106s - loss: 4.9226e-04 - val_loss: 5.0336e-04 - 106s/epoch - 18ms/step
Epoch 46/300
5969/5969 - 105s - loss: 4.8587e-04 - val_loss: 3.7328e-04 - 105s/epoch - 18ms/step
Epoch 47/300
5969/5969 - 109s - loss: 4.8867e-04 - val_loss: 3.6857e-04 - 109s/epoch - 18ms/step
Epoch 48/300
5969/5969 - 112s - loss: 4.7972e-04 - val_loss: 3.5269e-04 - 112s/epoch - 19ms/step
Epoch 49/300
5969/5969 - 112s - loss: 4.8605e-04 - val_loss: 3.5860e-04 - 112s/epoch - 19ms/step
Epoch 50/300
5969/5969 - 113s - loss: 4.6924e-04 - val_loss: 3.7074e-04 - 113s/epoch - 19ms/step
Epoch 51/300
5969/5969 - 113s - loss: 4.6944e-04 - val_loss: 3.5584e-04 - 113s/epoch - 19ms/step
Epoch 52/300
5969/5969 - 113s - loss: 4.6444e-04 - val_loss: 3.5425e-04 - 113s/epoch - 19ms/step
Epoch 53/300
5969/5969 - 113s - loss: 4.7093e-04 - val_loss: 3.5914e-04 - 113s/epoch - 19ms/step
Epoch 54/300
5969/5969 - 113s - loss: 4.6368e-04 - val_loss: 3.8159e-04 - 113s/epoch - 19ms/step
Epoch 55/300
5969/5969 - 113s - loss: 4.5677e-04 - val_loss: 3.4959e-04 - 113s/epoch - 19ms/step
Epoch 56/300
5969/5969 - 113s - loss: 4.5742e-04 - val_loss: 3.5164e-04 - 113s/epoch - 19ms/step
Epoch 57/300
5969/5969 - 113s - loss: 4.5906e-04 - val_loss: 3.5560e-04 - 113s/epoch - 19ms/step
Epoch 58/300
5969/5969 - 113s - loss: 4.4923e-04 - val_loss: 3.6060e-04 - 113s/epoch - 19ms/step
Epoch 59/300
5969/5969 - 113s - loss: 4.4940e-04 - val_loss: 3.5016e-04 - 113s/epoch - 19ms/step
Epoch 60/300
5969/5969 - 113s - loss: 4.5432e-04 - val_loss: 3.3242e-04 - 113s/epoch - 19ms/step
Epoch 61/300
5969/5969 - 113s - loss: 4.4459e-04 - val_loss: 3.6028e-04 - 113s/epoch - 19ms/step
Epoch 62/300
5969/5969 - 113s - loss: 4.4646e-04 - val_loss: 3.7139e-04 - 113s/epoch - 19ms/step
Epoch 63/300
5969/5969 - 113s - loss: 4.5491e-04 - val_loss: 3.4658e-04 - 113s/epoch - 19ms/step
Epoch 64/300
5969/5969 - 113s - loss: 4.4114e-04 - val_loss: 3.5481e-04 - 113s/epoch - 19ms/step
Epoch 65/300
5969/5969 - 113s - loss: 4.3922e-04 - val_loss: 3.5164e-04 - 113s/epoch - 19ms/step
Epoch 66/300
5969/5969 - 113s - loss: 4.3698e-04 - val_loss: 3.4543e-04 - 113s/epoch - 19ms/step
Epoch 67/300
5969/5969 - 113s - loss: 4.3667e-04 - val_loss: 3.3745e-04 - 113s/epoch - 19ms/step
Epoch 68/300
5969/5969 - 113s - loss: 4.3337e-04 - val_loss: 3.2924e-04 - 113s/epoch - 19ms/step
Epoch 69/300
5969/5969 - 113s - loss: 4.3206e-04 - val_loss: 3.4331e-04 - 113s/epoch - 19ms/step
Epoch 70/300
5969/5969 - 113s - loss: 4.3345e-04 - val_loss: 3.1709e-04 - 113s/epoch - 19ms/step
Epoch 71/300
5969/5969 - 113s - loss: 4.2259e-04 - val_loss: 3.3459e-04 - 113s/epoch - 19ms/step
Epoch 72/300
5969/5969 - 113s - loss: 4.2604e-04 - val_loss: 3.2867e-04 - 113s/epoch - 19ms/step
Epoch 73/300
5969/5969 - 113s - loss: 4.2718e-04 - val_loss: 3.3127e-04 - 113s/epoch - 19ms/step
Epoch 74/300
5969/5969 - 113s - loss: 4.2186e-04 - val_loss: 3.2197e-04 - 113s/epoch - 19ms/step
Epoch 75/300
5969/5969 - 113s - loss: 4.2055e-04 - val_loss: 3.4481e-04 - 113s/epoch - 19ms/step
Epoch 76/300
5969/5969 - 113s - loss: 4.1888e-04 - val_loss: 3.1864e-04 - 113s/epoch - 19ms/step
Epoch 77/300
5969/5969 - 113s - loss: 4.1517e-04 - val_loss: 3.3197e-04 - 113s/epoch - 19ms/step
Epoch 78/300
5969/5969 - 113s - loss: 4.1757e-04 - val_loss: 3.2069e-04 - 113s/epoch - 19ms/step
Epoch 79/300
5969/5969 - 113s - loss: 4.1479e-04 - val_loss: 3.4155e-04 - 113s/epoch - 19ms/step
Epoch 80/300
5969/5969 - 113s - loss: 4.2806e-04 - val_loss: 3.1943e-04 - 113s/epoch - 19ms/step
Epoch 81/300
5969/5969 - 113s - loss: 4.1652e-04 - val_loss: 3.2399e-04 - 113s/epoch - 19ms/step
Epoch 82/300
5969/5969 - 113s - loss: 4.1129e-04 - val_loss: 3.0932e-04 - 113s/epoch - 19ms/step
Epoch 83/300
5969/5969 - 113s - loss: 4.1354e-04 - val_loss: 3.2389e-04 - 113s/epoch - 19ms/step
Epoch 84/300
5969/5969 - 113s - loss: 4.1507e-04 - val_loss: 3.1971e-04 - 113s/epoch - 19ms/step
Epoch 85/300
5969/5969 - 113s - loss: 4.1078e-04 - val_loss: 3.1643e-04 - 113s/epoch - 19ms/step
Epoch 86/300
5969/5969 - 113s - loss: 4.1066e-04 - val_loss: 3.0737e-04 - 113s/epoch - 19ms/step
Epoch 87/300
5969/5969 - 113s - loss: 4.0936e-04 - val_loss: 3.2723e-04 - 113s/epoch - 19ms/step
Epoch 88/300
5969/5969 - 113s - loss: 3.9999e-04 - val_loss: 3.0541e-04 - 113s/epoch - 19ms/step
Epoch 89/300
5969/5969 - 113s - loss: 4.0751e-04 - val_loss: 3.2638e-04 - 113s/epoch - 19ms/step
Epoch 90/300
5969/5969 - 113s - loss: 4.0358e-04 - val_loss: 2.9372e-04 - 113s/epoch - 19ms/step
Epoch 91/300
5969/5969 - 113s - loss: 4.0110e-04 - val_loss: 3.2516e-04 - 113s/epoch - 19ms/step
Epoch 92/300
5969/5969 - 113s - loss: 3.9474e-04 - val_loss: 3.2149e-04 - 113s/epoch - 19ms/step
Epoch 93/300
5969/5969 - 113s - loss: 3.9667e-04 - val_loss: 3.2702e-04 - 113s/epoch - 19ms/step
Epoch 94/300
5969/5969 - 113s - loss: 3.9854e-04 - val_loss: 2.9539e-04 - 113s/epoch - 19ms/step
Epoch 95/300
5969/5969 - 113s - loss: 4.0601e-04 - val_loss: 3.1595e-04 - 113s/epoch - 19ms/step
Epoch 96/300
5969/5969 - 113s - loss: 3.9680e-04 - val_loss: 3.2636e-04 - 113s/epoch - 19ms/step
Epoch 97/300
5969/5969 - 113s - loss: 3.9538e-04 - val_loss: 2.9931e-04 - 113s/epoch - 19ms/step
Epoch 98/300
5969/5969 - 113s - loss: 3.9555e-04 - val_loss: 3.0253e-04 - 113s/epoch - 19ms/step
Epoch 99/300
5969/5969 - 113s - loss: 3.9332e-04 - val_loss: 2.9114e-04 - 113s/epoch - 19ms/step
Epoch 100/300
5969/5969 - 113s - loss: 3.9953e-04 - val_loss: 3.1216e-04 - 113s/epoch - 19ms/step
Epoch 101/300
5969/5969 - 113s - loss: 3.9019e-04 - val_loss: 3.0903e-04 - 113s/epoch - 19ms/step
Epoch 102/300
5969/5969 - 113s - loss: 4.0092e-04 - val_loss: 3.1714e-04 - 113s/epoch - 19ms/step
Epoch 103/300
5969/5969 - 113s - loss: 3.9435e-04 - val_loss: 2.8962e-04 - 113s/epoch - 19ms/step
Epoch 104/300
5969/5969 - 113s - loss: 3.8999e-04 - val_loss: 3.1034e-04 - 113s/epoch - 19ms/step
Epoch 105/300
5969/5969 - 113s - loss: 3.8373e-04 - val_loss: 2.9843e-04 - 113s/epoch - 19ms/step
Epoch 106/300
5969/5969 - 113s - loss: 3.8888e-04 - val_loss: 3.0242e-04 - 113s/epoch - 19ms/step
Epoch 107/300
5969/5969 - 113s - loss: 3.8668e-04 - val_loss: 3.3201e-04 - 113s/epoch - 19ms/step
Epoch 108/300
5969/5969 - 113s - loss: 3.8544e-04 - val_loss: 2.9719e-04 - 113s/epoch - 19ms/step
Epoch 109/300
5969/5969 - 113s - loss: 3.8853e-04 - val_loss: 3.0932e-04 - 113s/epoch - 19ms/step
Epoch 110/300
5969/5969 - 113s - loss: 3.8174e-04 - val_loss: 3.1926e-04 - 113s/epoch - 19ms/step
Epoch 111/300
5969/5969 - 113s - loss: 3.8692e-04 - val_loss: 3.1294e-04 - 113s/epoch - 19ms/step
Epoch 112/300
5969/5969 - 113s - loss: 3.8410e-04 - val_loss: 2.9861e-04 - 113s/epoch - 19ms/step
Epoch 113/300
5969/5969 - 113s - loss: 3.7810e-04 - val_loss: 3.0145e-04 - 113s/epoch - 19ms/step
Epoch 114/300
5969/5969 - 113s - loss: 3.7729e-04 - val_loss: 2.9410e-04 - 113s/epoch - 19ms/step
Epoch 115/300
5969/5969 - 113s - loss: 3.7766e-04 - val_loss: 2.9547e-04 - 113s/epoch - 19ms/step
Epoch 116/300
5969/5969 - 112s - loss: 3.7834e-04 - val_loss: 2.8874e-04 - 112s/epoch - 19ms/step
Epoch 117/300
5969/5969 - 113s - loss: 3.7544e-04 - val_loss: 2.8138e-04 - 113s/epoch - 19ms/step
Epoch 118/300
5969/5969 - 113s - loss: 3.8921e-04 - val_loss: 3.0233e-04 - 113s/epoch - 19ms/step
Epoch 119/300
5969/5969 - 112s - loss: 3.7688e-04 - val_loss: 2.9995e-04 - 112s/epoch - 19ms/step
Epoch 120/300
5969/5969 - 113s - loss: 3.7769e-04 - val_loss: 3.0110e-04 - 113s/epoch - 19ms/step
Epoch 121/300
5969/5969 - 113s - loss: 3.7927e-04 - val_loss: 2.9099e-04 - 113s/epoch - 19ms/step
Epoch 122/300
5969/5969 - 112s - loss: 3.7915e-04 - val_loss: 2.7796e-04 - 112s/epoch - 19ms/step
Epoch 123/300
5969/5969 - 113s - loss: 3.7388e-04 - val_loss: 2.9916e-04 - 113s/epoch - 19ms/step
Epoch 124/300
5969/5969 - 113s - loss: 3.7844e-04 - val_loss: 3.0813e-04 - 113s/epoch - 19ms/step
Epoch 125/300
5969/5969 - 112s - loss: 3.7358e-04 - val_loss: 3.0644e-04 - 112s/epoch - 19ms/step
Epoch 126/300
5969/5969 - 112s - loss: 3.7654e-04 - val_loss: 3.4215e-04 - 112s/epoch - 19ms/step
Epoch 127/300
5969/5969 - 113s - loss: 3.7205e-04 - val_loss: 2.9005e-04 - 113s/epoch - 19ms/step
Epoch 128/300
5969/5969 - 112s - loss: 3.6876e-04 - val_loss: 2.7589e-04 - 112s/epoch - 19ms/step
Epoch 129/300
5969/5969 - 113s - loss: 3.7643e-04 - val_loss: 2.9445e-04 - 113s/epoch - 19ms/step
Epoch 130/300
5969/5969 - 113s - loss: 3.7389e-04 - val_loss: 2.8759e-04 - 113s/epoch - 19ms/step
Epoch 131/300
5969/5969 - 112s - loss: 3.7027e-04 - val_loss: 3.0076e-04 - 112s/epoch - 19ms/step
Epoch 132/300
5969/5969 - 113s - loss: 3.6838e-04 - val_loss: 2.8520e-04 - 113s/epoch - 19ms/step
Epoch 133/300
5969/5969 - 113s - loss: 3.6591e-04 - val_loss: 2.9577e-04 - 113s/epoch - 19ms/step
Epoch 134/300
5969/5969 - 112s - loss: 3.7011e-04 - val_loss: 5.9122e-04 - 112s/epoch - 19ms/step
Epoch 135/300
5969/5969 - 113s - loss: 3.6524e-04 - val_loss: 2.7472e-04 - 113s/epoch - 19ms/step
Epoch 136/300
5969/5969 - 112s - loss: 3.6456e-04 - val_loss: 2.6674e-04 - 112s/epoch - 19ms/step
Epoch 137/300
5969/5969 - 112s - loss: 3.6630e-04 - val_loss: 2.9823e-04 - 112s/epoch - 19ms/step
Epoch 138/300
5969/5969 - 113s - loss: 3.6145e-04 - val_loss: 2.9769e-04 - 113s/epoch - 19ms/step
Epoch 139/300
5969/5969 - 113s - loss: 3.7032e-04 - val_loss: 2.7969e-04 - 113s/epoch - 19ms/step
Epoch 140/300
5969/5969 - 112s - loss: 3.6150e-04 - val_loss: 2.7994e-04 - 112s/epoch - 19ms/step
Epoch 141/300
5969/5969 - 113s - loss: 3.6764e-04 - val_loss: 2.8427e-04 - 113s/epoch - 19ms/step
Epoch 142/300
5969/5969 - 113s - loss: 3.7429e-04 - val_loss: 2.8814e-04 - 113s/epoch - 19ms/step
Epoch 143/300
5969/5969 - 112s - loss: 3.6432e-04 - val_loss: 2.8324e-04 - 112s/epoch - 19ms/step
Epoch 144/300
5969/5969 - 112s - loss: 3.6307e-04 - val_loss: 2.9213e-04 - 112s/epoch - 19ms/step
Epoch 145/300
5969/5969 - 113s - loss: 3.6841e-04 - val_loss: 2.7357e-04 - 113s/epoch - 19ms/step
Epoch 146/300
5969/5969 - 112s - loss: 3.5934e-04 - val_loss: 2.9135e-04 - 112s/epoch - 19ms/step
Epoch 147/300
5969/5969 - 112s - loss: 3.5874e-04 - val_loss: 2.8504e-04 - 112s/epoch - 19ms/step
Epoch 148/300
5969/5969 - 113s - loss: 3.6289e-04 - val_loss: 3.0336e-04 - 113s/epoch - 19ms/step
Epoch 149/300
5969/5969 - 112s - loss: 3.6279e-04 - val_loss: 2.7394e-04 - 112s/epoch - 19ms/step
Epoch 150/300
5969/5969 - 113s - loss: 3.5981e-04 - val_loss: 2.7858e-04 - 113s/epoch - 19ms/step
Epoch 151/300
5969/5969 - 113s - loss: 3.5563e-04 - val_loss: 2.6864e-04 - 113s/epoch - 19ms/step
Epoch 152/300
5969/5969 - 112s - loss: 3.5412e-04 - val_loss: 2.6591e-04 - 112s/epoch - 19ms/step
Epoch 153/300
5969/5969 - 113s - loss: 3.6465e-04 - val_loss: 2.7295e-04 - 113s/epoch - 19ms/step
Epoch 154/300
5969/5969 - 113s - loss: 3.6407e-04 - val_loss: 2.7600e-04 - 113s/epoch - 19ms/step
Epoch 155/300
5969/5969 - 112s - loss: 3.5559e-04 - val_loss: 2.7510e-04 - 112s/epoch - 19ms/step
Epoch 156/300
5969/5969 - 113s - loss: 3.5680e-04 - val_loss: 2.6880e-04 - 113s/epoch - 19ms/step
Epoch 157/300
5969/5969 - 113s - loss: 3.5641e-04 - val_loss: 2.6308e-04 - 113s/epoch - 19ms/step
Epoch 158/300
5969/5969 - 112s - loss: 3.5189e-04 - val_loss: 2.8568e-04 - 112s/epoch - 19ms/step
Epoch 159/300
5969/5969 - 113s - loss: 3.6292e-04 - val_loss: 2.7552e-04 - 113s/epoch - 19ms/step
Epoch 160/300
5969/5969 - 113s - loss: 3.5449e-04 - val_loss: 2.8644e-04 - 113s/epoch - 19ms/step
Epoch 161/300
5969/5969 - 112s - loss: 3.5373e-04 - val_loss: 2.7553e-04 - 112s/epoch - 19ms/step
Epoch 162/300
5969/5969 - 112s - loss: 3.5808e-04 - val_loss: 2.7741e-04 - 112s/epoch - 19ms/step
Epoch 163/300
5969/5969 - 112s - loss: 3.5426e-04 - val_loss: 2.7779e-04 - 112s/epoch - 19ms/step
Epoch 164/300
5969/5969 - 112s - loss: 3.5296e-04 - val_loss: 2.8609e-04 - 112s/epoch - 19ms/step
Epoch 165/300
5969/5969 - 113s - loss: 3.5016e-04 - val_loss: 2.7978e-04 - 113s/epoch - 19ms/step
Epoch 166/300
5969/5969 - 113s - loss: 3.5551e-04 - val_loss: 2.7797e-04 - 113s/epoch - 19ms/step
Epoch 167/300
5969/5969 - 112s - loss: 3.5096e-04 - val_loss: 2.7381e-04 - 112s/epoch - 19ms/step
Epoch 168/300
5969/5969 - 113s - loss: 3.5428e-04 - val_loss: 2.7423e-04 - 113s/epoch - 19ms/step
Epoch 169/300
5969/5969 - 113s - loss: 3.5977e-04 - val_loss: 2.6680e-04 - 113s/epoch - 19ms/step
Epoch 170/300
5969/5969 - 112s - loss: 3.5067e-04 - val_loss: 2.7043e-04 - 112s/epoch - 19ms/step
Epoch 171/300
5969/5969 - 112s - loss: 3.4903e-04 - val_loss: 2.8714e-04 - 112s/epoch - 19ms/step
Epoch 172/300
5969/5969 - 113s - loss: 3.4930e-04 - val_loss: 2.8321e-04 - 113s/epoch - 19ms/step
Epoch 173/300
5969/5969 - 112s - loss: 3.5089e-04 - val_loss: 2.6266e-04 - 112s/epoch - 19ms/step
Epoch 174/300
5969/5969 - 113s - loss: 3.4962e-04 - val_loss: 2.5772e-04 - 113s/epoch - 19ms/step
Epoch 175/300
5969/5969 - 113s - loss: 3.5216e-04 - val_loss: 2.6392e-04 - 113s/epoch - 19ms/step
Epoch 176/300
5969/5969 - 112s - loss: 3.4718e-04 - val_loss: 2.8316e-04 - 112s/epoch - 19ms/step
Epoch 177/300
5969/5969 - 113s - loss: 3.4800e-04 - val_loss: 2.7342e-04 - 113s/epoch - 19ms/step
Epoch 178/300
5969/5969 - 113s - loss: 3.4784e-04 - val_loss: 3.3255e-04 - 113s/epoch - 19ms/step
Epoch 179/300
5969/5969 - 112s - loss: 3.5116e-04 - val_loss: 2.7478e-04 - 112s/epoch - 19ms/step
Epoch 180/300
5969/5969 - 113s - loss: 3.4951e-04 - val_loss: 2.7626e-04 - 113s/epoch - 19ms/step
Epoch 181/300
5969/5969 - 113s - loss: 3.4719e-04 - val_loss: 2.7770e-04 - 113s/epoch - 19ms/step
Epoch 182/300
5969/5969 - 112s - loss: 3.4977e-04 - val_loss: 2.8766e-04 - 112s/epoch - 19ms/step
Epoch 183/300
5969/5969 - 113s - loss: 3.4260e-04 - val_loss: 3.1369e-04 - 113s/epoch - 19ms/step
Epoch 184/300
5969/5969 - 113s - loss: 3.5100e-04 - val_loss: 2.7879e-04 - 113s/epoch - 19ms/step
Epoch 185/300
5969/5969 - 112s - loss: 3.4319e-04 - val_loss: 2.7798e-04 - 112s/epoch - 19ms/step
Epoch 186/300
5969/5969 - 112s - loss: 3.4487e-04 - val_loss: 2.8541e-04 - 112s/epoch - 19ms/step
Epoch 187/300
5969/5969 - 113s - loss: 3.4535e-04 - val_loss: 2.8898e-04 - 113s/epoch - 19ms/step
Epoch 188/300
5969/5969 - 112s - loss: 3.5219e-04 - val_loss: 2.8351e-04 - 112s/epoch - 19ms/step
Epoch 189/300
5969/5969 - 113s - loss: 3.4620e-04 - val_loss: 2.7441e-04 - 113s/epoch - 19ms/step
Epoch 190/300
5969/5969 - 113s - loss: 3.4257e-04 - val_loss: 2.7206e-04 - 113s/epoch - 19ms/step
Epoch 191/300
5969/5969 - 112s - loss: 3.4517e-04 - val_loss: 2.7164e-04 - 112s/epoch - 19ms/step
Epoch 192/300
5969/5969 - 112s - loss: 3.4194e-04 - val_loss: 2.7830e-04 - 112s/epoch - 19ms/step
Epoch 193/300
5969/5969 - 113s - loss: 3.4681e-04 - val_loss: 2.9487e-04 - 113s/epoch - 19ms/step
Epoch 194/300
5969/5969 - 112s - loss: 3.4191e-04 - val_loss: 2.8368e-04 - 112s/epoch - 19ms/step
Epoch 195/300
5969/5969 - 113s - loss: 3.4623e-04 - val_loss: 2.7570e-04 - 113s/epoch - 19ms/step
Epoch 196/300
5969/5969 - 113s - loss: 3.3993e-04 - val_loss: 2.8243e-04 - 113s/epoch - 19ms/step
Epoch 197/300
5969/5969 - 112s - loss: 3.4308e-04 - val_loss: 2.8011e-04 - 112s/epoch - 19ms/step
Epoch 198/300
5969/5969 - 113s - loss: 3.4098e-04 - val_loss: 2.8563e-04 - 113s/epoch - 19ms/step
Epoch 199/300
5969/5969 - 113s - loss: 3.3737e-04 - val_loss: 2.8278e-04 - 113s/epoch - 19ms/step
Epoch 200/300
5969/5969 - 112s - loss: 3.4079e-04 - val_loss: 2.9187e-04 - 112s/epoch - 19ms/step
Epoch 201/300
5969/5969 - 113s - loss: 3.4151e-04 - val_loss: 3.1297e-04 - 113s/epoch - 19ms/step
Epoch 202/300
5969/5969 - 113s - loss: 3.3795e-04 - val_loss: 2.7692e-04 - 113s/epoch - 19ms/step
Epoch 203/300
5969/5969 - 112s - loss: 3.3791e-04 - val_loss: 2.7694e-04 - 112s/epoch - 19ms/step
Epoch 204/300
5969/5969 - 113s - loss: 3.3728e-04 - val_loss: 2.7950e-04 - 113s/epoch - 19ms/step
Epoch 205/300
5969/5969 - 113s - loss: 3.3747e-04 - val_loss: 2.5899e-04 - 113s/epoch - 19ms/step
Epoch 206/300
5969/5969 - 112s - loss: 3.3956e-04 - val_loss: 2.7463e-04 - 112s/epoch - 19ms/step
Epoch 207/300
5969/5969 - 113s - loss: 3.3750e-04 - val_loss: 2.7863e-04 - 113s/epoch - 19ms/step
Epoch 208/300
5969/5969 - 113s - loss: 3.3393e-04 - val_loss: 2.6469e-04 - 113s/epoch - 19ms/step
Epoch 209/300
5969/5969 - 112s - loss: 3.3773e-04 - val_loss: 2.7250e-04 - 112s/epoch - 19ms/step
Epoch 210/300
5969/5969 - 113s - loss: 3.3909e-04 - val_loss: 3.0233e-04 - 113s/epoch - 19ms/step
Epoch 211/300
5969/5969 - 113s - loss: 3.3914e-04 - val_loss: 2.7278e-04 - 113s/epoch - 19ms/step
Epoch 212/300
5969/5969 - 112s - loss: 3.3902e-04 - val_loss: 2.7028e-04 - 112s/epoch - 19ms/step
Epoch 213/300
5969/5969 - 113s - loss: 3.3267e-04 - val_loss: 2.6078e-04 - 113s/epoch - 19ms/step
Epoch 214/300
5969/5969 - 113s - loss: 3.4155e-04 - val_loss: 2.7394e-04 - 113s/epoch - 19ms/step
Epoch 215/300
5969/5969 - 112s - loss: 3.4373e-04 - val_loss: 2.6303e-04 - 112s/epoch - 19ms/step
Epoch 216/300
5969/5969 - 113s - loss: 3.3540e-04 - val_loss: 2.6186e-04 - 113s/epoch - 19ms/step
Epoch 217/300
5969/5969 - 113s - loss: 3.3439e-04 - val_loss: 2.7091e-04 - 113s/epoch - 19ms/step
Epoch 218/300
5969/5969 - 112s - loss: 3.4311e-04 - val_loss: 2.7184e-04 - 112s/epoch - 19ms/step
Epoch 219/300
5969/5969 - 113s - loss: 3.3444e-04 - val_loss: 2.6153e-04 - 113s/epoch - 19ms/step
Epoch 220/300
5969/5969 - 113s - loss: 3.3818e-04 - val_loss: 2.8730e-04 - 113s/epoch - 19ms/step
Epoch 221/300
5969/5969 - 112s - loss: 3.3474e-04 - val_loss: 2.6118e-04 - 112s/epoch - 19ms/step
Epoch 222/300
5969/5969 - 113s - loss: 3.4423e-04 - val_loss: 2.3992e-04 - 113s/epoch - 19ms/step
Epoch 223/300
5969/5969 - 113s - loss: 3.3606e-04 - val_loss: 2.8103e-04 - 113s/epoch - 19ms/step
Epoch 224/300
5969/5969 - 112s - loss: 3.2940e-04 - val_loss: 2.6770e-04 - 112s/epoch - 19ms/step
Epoch 225/300
5969/5969 - 113s - loss: 3.3317e-04 - val_loss: 2.8722e-04 - 113s/epoch - 19ms/step
Epoch 226/300
5969/5969 - 113s - loss: 3.3736e-04 - val_loss: 2.9891e-04 - 113s/epoch - 19ms/step
Epoch 227/300
5969/5969 - 112s - loss: 3.3321e-04 - val_loss: 2.9388e-04 - 112s/epoch - 19ms/step
Epoch 228/300
5969/5969 - 113s - loss: 3.3211e-04 - val_loss: 2.6766e-04 - 113s/epoch - 19ms/step
Epoch 229/300
5969/5969 - 113s - loss: 3.3335e-04 - val_loss: 2.6494e-04 - 113s/epoch - 19ms/step
Epoch 230/300
5969/5969 - 112s - loss: 3.3169e-04 - val_loss: 2.5552e-04 - 112s/epoch - 19ms/step
Epoch 231/300
5969/5969 - 112s - loss: 3.3417e-04 - val_loss: 2.6050e-04 - 112s/epoch - 19ms/step
Epoch 232/300
5969/5969 - 113s - loss: 3.2992e-04 - val_loss: 2.6368e-04 - 113s/epoch - 19ms/step
Epoch 233/300
5969/5969 - 112s - loss: 3.3677e-04 - val_loss: 2.9282e-04 - 112s/epoch - 19ms/step
Epoch 234/300
5969/5969 - 113s - loss: 3.3572e-04 - val_loss: 2.7391e-04 - 113s/epoch - 19ms/step
Epoch 235/300
5969/5969 - 113s - loss: 3.3231e-04 - val_loss: 2.7748e-04 - 113s/epoch - 19ms/step
Epoch 236/300
5969/5969 - 112s - loss: 3.2990e-04 - val_loss: 2.7175e-04 - 112s/epoch - 19ms/step
Epoch 237/300
5969/5969 - 112s - loss: 3.3388e-04 - val_loss: 2.6388e-04 - 112s/epoch - 19ms/step
Epoch 238/300
5969/5969 - 113s - loss: 3.2840e-04 - val_loss: 2.4651e-04 - 113s/epoch - 19ms/step
Epoch 239/300
5969/5969 - 112s - loss: 3.3207e-04 - val_loss: 2.7669e-04 - 112s/epoch - 19ms/step
Epoch 240/300
5969/5969 - 113s - loss: 3.3042e-04 - val_loss: 2.5532e-04 - 113s/epoch - 19ms/step
Epoch 241/300
5969/5969 - 113s - loss: 3.3024e-04 - val_loss: 3.0504e-04 - 113s/epoch - 19ms/step
Epoch 242/300
5969/5969 - 112s - loss: 3.3097e-04 - val_loss: 2.9015e-04 - 112s/epoch - 19ms/step
Epoch 243/300
5969/5969 - 113s - loss: 3.3329e-04 - val_loss: 2.7704e-04 - 113s/epoch - 19ms/step
Epoch 244/300
5969/5969 - 112s - loss: 3.2800e-04 - val_loss: 2.9555e-04 - 112s/epoch - 19ms/step
Epoch 245/300
5969/5969 - 112s - loss: 3.3159e-04 - val_loss: 2.9144e-04 - 112s/epoch - 19ms/step
Epoch 246/300
5969/5969 - 113s - loss: 3.2723e-04 - val_loss: 2.4729e-04 - 113s/epoch - 19ms/step
Epoch 247/300
5969/5969 - 112s - loss: 3.2862e-04 - val_loss: 2.3636e-04 - 112s/epoch - 19ms/step
Epoch 248/300
5969/5969 - 113s - loss: 3.3102e-04 - val_loss: 2.7322e-04 - 113s/epoch - 19ms/step
Epoch 249/300
5969/5969 - 113s - loss: 3.2671e-04 - val_loss: 2.6597e-04 - 113s/epoch - 19ms/step
Epoch 250/300
5969/5969 - 112s - loss: 3.2623e-04 - val_loss: 2.6632e-04 - 112s/epoch - 19ms/step
Epoch 251/300
5969/5969 - 113s - loss: 3.3152e-04 - val_loss: 2.7770e-04 - 113s/epoch - 19ms/step
Epoch 252/300
5969/5969 - 113s - loss: 3.3047e-04 - val_loss: 3.3015e-04 - 113s/epoch - 19ms/step
Epoch 253/300
5969/5969 - 112s - loss: 3.2324e-04 - val_loss: 2.7411e-04 - 112s/epoch - 19ms/step
Epoch 254/300
5969/5969 - 113s - loss: 3.3249e-04 - val_loss: 2.5253e-04 - 113s/epoch - 19ms/step
Epoch 255/300
5969/5969 - 113s - loss: 3.2698e-04 - val_loss: 2.9808e-04 - 113s/epoch - 19ms/step
Epoch 256/300
5969/5969 - 112s - loss: 3.3432e-04 - val_loss: 2.6544e-04 - 112s/epoch - 19ms/step
Epoch 257/300
5969/5969 - 113s - loss: 3.2546e-04 - val_loss: 2.6407e-04 - 113s/epoch - 19ms/step
Epoch 258/300
5969/5969 - 113s - loss: 3.2429e-04 - val_loss: 2.5213e-04 - 113s/epoch - 19ms/step
Epoch 259/300
5969/5969 - 112s - loss: 3.3295e-04 - val_loss: 2.7904e-04 - 112s/epoch - 19ms/step
Epoch 260/300
5969/5969 - 113s - loss: 3.2296e-04 - val_loss: 2.6747e-04 - 113s/epoch - 19ms/step
Epoch 261/300
5969/5969 - 113s - loss: 3.2445e-04 - val_loss: 2.6487e-04 - 113s/epoch - 19ms/step
Epoch 262/300
5969/5969 - 112s - loss: 3.2171e-04 - val_loss: 2.5706e-04 - 112s/epoch - 19ms/step
Epoch 263/300
5969/5969 - 113s - loss: 3.2277e-04 - val_loss: 2.6273e-04 - 113s/epoch - 19ms/step
Epoch 264/300
5969/5969 - 113s - loss: 3.4523e-04 - val_loss: 2.7131e-04 - 113s/epoch - 19ms/step
Epoch 265/300
5969/5969 - 112s - loss: 3.2110e-04 - val_loss: 2.5640e-04 - 112s/epoch - 19ms/step
Epoch 266/300
5969/5969 - 113s - loss: 3.2551e-04 - val_loss: 2.3928e-04 - 113s/epoch - 19ms/step
Epoch 267/300
5969/5969 - 113s - loss: 3.2893e-04 - val_loss: 2.7369e-04 - 113s/epoch - 19ms/step
Epoch 268/300
5969/5969 - 112s - loss: 3.2169e-04 - val_loss: 2.6517e-04 - 112s/epoch - 19ms/step
Epoch 269/300
5969/5969 - 113s - loss: 3.2634e-04 - val_loss: 2.6949e-04 - 113s/epoch - 19ms/step
Epoch 270/300
5969/5969 - 113s - loss: 3.2090e-04 - val_loss: 2.6099e-04 - 113s/epoch - 19ms/step
Epoch 271/300
5969/5969 - 113s - loss: 3.2400e-04 - val_loss: 2.9156e-04 - 113s/epoch - 19ms/step
Epoch 272/300
5969/5969 - 113s - loss: 3.2362e-04 - val_loss: 2.7540e-04 - 113s/epoch - 19ms/step
Epoch 273/300
5969/5969 - 113s - loss: 3.2677e-04 - val_loss: 2.7594e-04 - 113s/epoch - 19ms/step
Epoch 274/300
5969/5969 - 113s - loss: 3.2112e-04 - val_loss: 2.6260e-04 - 113s/epoch - 19ms/step
Epoch 275/300
5969/5969 - 113s - loss: 3.2166e-04 - val_loss: 2.7458e-04 - 113s/epoch - 19ms/step
Epoch 276/300
5969/5969 - 113s - loss: 3.2028e-04 - val_loss: 2.6127e-04 - 113s/epoch - 19ms/step
Epoch 277/300
5969/5969 - 112s - loss: 3.2959e-04 - val_loss: 2.4415e-04 - 112s/epoch - 19ms/step
Epoch 278/300
5969/5969 - 113s - loss: 3.2014e-04 - val_loss: 2.6136e-04 - 113s/epoch - 19ms/step
Epoch 279/300
5969/5969 - 113s - loss: 3.2227e-04 - val_loss: 2.8798e-04 - 113s/epoch - 19ms/step
Epoch 280/300
5969/5969 - 112s - loss: 3.3324e-04 - val_loss: 2.9723e-04 - 112s/epoch - 19ms/step
Epoch 281/300
5969/5969 - 113s - loss: 3.3323e-04 - val_loss: 2.5434e-04 - 113s/epoch - 19ms/step
Epoch 282/300
5969/5969 - 113s - loss: 3.2049e-04 - val_loss: 2.6453e-04 - 113s/epoch - 19ms/step
Epoch 283/300
5969/5969 - 112s - loss: 3.1892e-04 - val_loss: 2.8938e-04 - 112s/epoch - 19ms/step
Epoch 284/300
5969/5969 - 113s - loss: 3.3029e-04 - val_loss: 2.9407e-04 - 113s/epoch - 19ms/step
Epoch 285/300
5969/5969 - 113s - loss: 3.2253e-04 - val_loss: 2.7465e-04 - 113s/epoch - 19ms/step
Epoch 286/300
5969/5969 - 112s - loss: 3.2362e-04 - val_loss: 2.7264e-04 - 112s/epoch - 19ms/step
Epoch 287/300
5969/5969 - 113s - loss: 3.1727e-04 - val_loss: 2.7941e-04 - 113s/epoch - 19ms/step
Epoch 288/300
5969/5969 - 113s - loss: 3.2473e-04 - val_loss: 2.8084e-04 - 113s/epoch - 19ms/step
Epoch 289/300
5969/5969 - 112s - loss: 3.2109e-04 - val_loss: 2.6156e-04 - 112s/epoch - 19ms/step
Epoch 290/300
5969/5969 - 113s - loss: 3.2510e-04 - val_loss: 2.6977e-04 - 113s/epoch - 19ms/step
Epoch 291/300
5969/5969 - 113s - loss: 3.2556e-04 - val_loss: 2.6065e-04 - 113s/epoch - 19ms/step
Epoch 292/300
5969/5969 - 112s - loss: 3.2087e-04 - val_loss: 2.7254e-04 - 112s/epoch - 19ms/step
Epoch 293/300
5969/5969 - 113s - loss: 3.1994e-04 - val_loss: 2.8294e-04 - 113s/epoch - 19ms/step
Epoch 294/300
5969/5969 - 113s - loss: 3.1528e-04 - val_loss: 2.9057e-04 - 113s/epoch - 19ms/step
Epoch 295/300
5969/5969 - 112s - loss: 3.2324e-04 - val_loss: 2.6146e-04 - 112s/epoch - 19ms/step
Epoch 296/300
5969/5969 - 113s - loss: 3.2344e-04 - val_loss: 2.7863e-04 - 113s/epoch - 19ms/step
Epoch 297/300
5969/5969 - 113s - loss: 3.1621e-04 - val_loss: 2.8019e-04 - 113s/epoch - 19ms/step
Epoch 298/300
5969/5969 - 112s - loss: 3.1377e-04 - val_loss: 2.8233e-04 - 112s/epoch - 19ms/step
Epoch 299/300
5969/5969 - 113s - loss: 3.1826e-04 - val_loss: 2.6662e-04 - 113s/epoch - 19ms/step
Epoch 300/300
5969/5969 - 114s - loss: 3.2072e-04 - val_loss: 2.6244e-04 - 114s/epoch - 19ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00026244187029078603
  1/332 [..............................] - ETA: 29s 15/332 [>.............................] - ETA: 1s  29/332 [=>............................] - ETA: 1s 43/332 [==>...........................] - ETA: 1s 57/332 [====>.........................] - ETA: 1s 71/332 [=====>........................] - ETA: 0s 85/332 [======>.......................] - ETA: 0s 99/332 [=======>......................] - ETA: 0s114/332 [=========>....................] - ETA: 0s128/332 [==========>...................] - ETA: 0s142/332 [===========>..................] - ETA: 0s156/332 [=============>................] - ETA: 0s170/332 [==============>...............] - ETA: 0s184/332 [===============>..............] - ETA: 0s198/332 [================>.............] - ETA: 0s212/332 [==================>...........] - ETA: 0s226/332 [===================>..........] - ETA: 0s240/332 [====================>.........] - ETA: 0s254/332 [=====================>........] - ETA: 0s268/332 [=======================>......] - ETA: 0s282/332 [========================>.....] - ETA: 0s296/332 [=========================>....] - ETA: 0s310/332 [===========================>..] - ETA: 0s324/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 4ms/step
correlation 0.0029334850100989256
cosine 0.0023646126861083384
MAE: 0.008295425
RMSE: 0.016200047
r2: 0.9829749358012427
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 1264)              1598960   
                                                                 
 batch_normalization_18 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 1264)              800112    
                                                                 
 batch_normalization_20 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 1264)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Encoder
Model: "model_19"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_20 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 1264)              1598960   
                                                                 
 batch_normalization_18 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
=================================================================
Total params: 2,403,496
Trainable params: 2,400,968
Non-trainable params: 2,528
_________________________________________________________________
Decoder
Model: "model_20"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_21 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 1264)              800112    
                                                                 
 batch_normalization_20 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 1264)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 2,406,656
Trainable params: 2,402,864
Non-trainable params: 3,792
_________________________________________________________________
['n_b', 'mse', 16, 300, 0.0005, 0.5, 632, 0.00032071606256067753, 0.00026244187029078603, 0.0029334850100989256, 0.0023646126861083384, 0.008295425213873386, 0.016200046986341476, 0.9829749358012427, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_21 (Dense)            (None, 1264)              1598960   
                                                                 
 batch_normalization_21 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 1264)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               799480    
                                                                 
 batch_normalization_22 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 632)               0         
                                                                 
 dense_22 (Dense)            (None, 1264)              800112    
                                                                 
 batch_normalization_23 (Bat  (None, 1264)             5056      
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 1264)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              1598960   
                                                                 
=================================================================
Total params: 4,810,152
Trainable params: 4,803,832
Non-trainable params: 6,320
_________________________________________________________________
Epoch 1/300
5969/5969 - 113s - loss: 0.0066 - val_loss: 0.0025 - 113s/epoch - 19ms/step
Epoch 2/300
5969/5969 - 113s - loss: 0.0028 - val_loss: 0.0018 - 113s/epoch - 19ms/step
Epoch 3/300
5969/5969 - 111s - loss: 0.0018 - val_loss: 0.0016 - 111s/epoch - 19ms/step
Epoch 4/300
5969/5969 - 110s - loss: 0.0015 - val_loss: 0.0011 - 110s/epoch - 18ms/step
Epoch 5/300
5969/5969 - 110s - loss: 0.0013 - val_loss: 0.0010 - 110s/epoch - 18ms/step
Epoch 6/300
5969/5969 - 110s - loss: 0.0011 - val_loss: 9.3232e-04 - 110s/epoch - 18ms/step
Epoch 7/300
5969/5969 - 110s - loss: 0.0011 - val_loss: 8.6799e-04 - 110s/epoch - 18ms/step
Epoch 8/300
5969/5969 - 110s - loss: 9.9990e-04 - val_loss: 7.8454e-04 - 110s/epoch - 18ms/step
Epoch 9/300
5969/5969 - 110s - loss: 9.4704e-04 - val_loss: 7.1309e-04 - 110s/epoch - 18ms/step
Epoch 10/300
5969/5969 - 110s - loss: 9.3180e-04 - val_loss: 7.4336e-04 - 110s/epoch - 18ms/step
Epoch 11/300
5969/5969 - 110s - loss: 8.7098e-04 - val_loss: 6.9869e-04 - 110s/epoch - 18ms/step
Epoch 12/300
5969/5969 - 110s - loss: 8.3586e-04 - val_loss: 6.8179e-04 - 110s/epoch - 18ms/step
Epoch 13/300
5969/5969 - 110s - loss: 8.2206e-04 - val_loss: 6.4845e-04 - 110s/epoch - 18ms/step
Epoch 14/300
5969/5969 - 110s - loss: 7.8747e-04 - val_loss: 6.2254e-04 - 110s/epoch - 18ms/step
Epoch 15/300
5969/5969 - 110s - loss: 7.7989e-04 - val_loss: 6.1983e-04 - 110s/epoch - 18ms/step
Epoch 16/300
5969/5969 - 110s - loss: 7.5894e-04 - val_loss: 6.5283e-04 - 110s/epoch - 18ms/step
Epoch 17/300
5969/5969 - 110s - loss: 7.4903e-04 - val_loss: 5.9202e-04 - 110s/epoch - 18ms/step
Epoch 18/300
5969/5969 - 110s - loss: 7.2775e-04 - val_loss: 5.7378e-04 - 110s/epoch - 18ms/step
Epoch 19/300
5969/5969 - 110s - loss: 7.0954e-04 - val_loss: 5.8060e-04 - 110s/epoch - 18ms/step
Epoch 20/300
5969/5969 - 110s - loss: 7.2146e-04 - val_loss: 5.3187e-04 - 110s/epoch - 18ms/step
Epoch 21/300
5969/5969 - 110s - loss: 7.0637e-04 - val_loss: 5.2776e-04 - 110s/epoch - 18ms/step
Epoch 22/300
5969/5969 - 110s - loss: 6.8038e-04 - val_loss: 5.5669e-04 - 110s/epoch - 18ms/step
Epoch 23/300
5969/5969 - 110s - loss: 6.6968e-04 - val_loss: 5.3269e-04 - 110s/epoch - 18ms/step
Epoch 24/300
5969/5969 - 110s - loss: 6.5425e-04 - val_loss: 5.2950e-04 - 110s/epoch - 18ms/step
Epoch 25/300
5969/5969 - 110s - loss: 6.5415e-04 - val_loss: 5.1762e-04 - 110s/epoch - 18ms/step
Epoch 26/300
5969/5969 - 110s - loss: 6.5008e-04 - val_loss: 4.7916e-04 - 110s/epoch - 18ms/step
Epoch 27/300
5969/5969 - 110s - loss: 6.3446e-04 - val_loss: 4.8347e-04 - 110s/epoch - 18ms/step
Epoch 28/300
5969/5969 - 110s - loss: 6.2821e-04 - val_loss: 4.8126e-04 - 110s/epoch - 18ms/step
Epoch 29/300
5969/5969 - 110s - loss: 6.2172e-04 - val_loss: 4.9714e-04 - 110s/epoch - 18ms/step
Epoch 30/300
5969/5969 - 110s - loss: 6.1126e-04 - val_loss: 4.5937e-04 - 110s/epoch - 18ms/step
Epoch 31/300
5969/5969 - 110s - loss: 6.1340e-04 - val_loss: 4.5165e-04 - 110s/epoch - 18ms/step
Epoch 32/300
5969/5969 - 110s - loss: 5.9890e-04 - val_loss: 4.7467e-04 - 110s/epoch - 18ms/step
Epoch 33/300
5969/5969 - 110s - loss: 6.0108e-04 - val_loss: 4.6497e-04 - 110s/epoch - 18ms/step
Epoch 34/300
5969/5969 - 110s - loss: 5.9325e-04 - val_loss: 4.2640e-04 - 110s/epoch - 18ms/step
Epoch 35/300
5969/5969 - 110s - loss: 5.8526e-04 - val_loss: 4.4665e-04 - 110s/epoch - 18ms/step
Epoch 36/300
5969/5969 - 110s - loss: 5.8908e-04 - val_loss: 4.5646e-04 - 110s/epoch - 18ms/step
Epoch 37/300
5969/5969 - 110s - loss: 5.8244e-04 - val_loss: 4.6769e-04 - 110s/epoch - 18ms/step
Epoch 38/300
5969/5969 - 110s - loss: 5.6984e-04 - val_loss: 4.5116e-04 - 110s/epoch - 18ms/step
Epoch 39/300
5969/5969 - 110s - loss: 5.7187e-04 - val_loss: 4.2848e-04 - 110s/epoch - 18ms/step
Epoch 40/300
5969/5969 - 110s - loss: 5.6395e-04 - val_loss: 4.3850e-04 - 110s/epoch - 18ms/step
Epoch 41/300
5969/5969 - 110s - loss: 5.6930e-04 - val_loss: 4.1526e-04 - 110s/epoch - 18ms/step
Epoch 42/300
5969/5969 - 110s - loss: 5.5995e-04 - val_loss: 4.0901e-04 - 110s/epoch - 18ms/step
Epoch 43/300
5969/5969 - 110s - loss: 5.5118e-04 - val_loss: 4.2007e-04 - 110s/epoch - 18ms/step
Epoch 44/300
5969/5969 - 110s - loss: 5.4374e-04 - val_loss: 4.2041e-04 - 110s/epoch - 18ms/step
Epoch 45/300
5969/5969 - 110s - loss: 5.5427e-04 - val_loss: 3.9898e-04 - 110s/epoch - 18ms/step
Epoch 46/300
5969/5969 - 110s - loss: 5.5021e-04 - val_loss: 4.0633e-04 - 110s/epoch - 18ms/step
Epoch 47/300
5969/5969 - 110s - loss: 5.3607e-04 - val_loss: 4.0900e-04 - 110s/epoch - 18ms/step
Epoch 48/300
5969/5969 - 110s - loss: 5.3666e-04 - val_loss: 4.1719e-04 - 110s/epoch - 18ms/step
Epoch 49/300
5969/5969 - 110s - loss: 5.3601e-04 - val_loss: 3.9850e-04 - 110s/epoch - 18ms/step
Epoch 50/300
5969/5969 - 110s - loss: 5.2778e-04 - val_loss: 4.7855e-04 - 110s/epoch - 18ms/step
Epoch 51/300
5969/5969 - 110s - loss: 5.3241e-04 - val_loss: 3.9851e-04 - 110s/epoch - 18ms/step
Epoch 52/300
5969/5969 - 110s - loss: 5.2192e-04 - val_loss: 3.8841e-04 - 110s/epoch - 18ms/step
Epoch 53/300
5969/5969 - 110s - loss: 5.2536e-04 - val_loss: 3.9539e-04 - 110s/epoch - 18ms/step
Epoch 54/300
5969/5969 - 110s - loss: 5.2242e-04 - val_loss: 4.2626e-04 - 110s/epoch - 18ms/step
Epoch 55/300
5969/5969 - 110s - loss: 5.1836e-04 - val_loss: 3.7360e-04 - 110s/epoch - 18ms/step
Epoch 56/300
5969/5969 - 110s - loss: 5.1646e-04 - val_loss: 3.7879e-04 - 110s/epoch - 18ms/step
Epoch 57/300
5969/5969 - 110s - loss: 5.2568e-04 - val_loss: 4.1434e-04 - 110s/epoch - 18ms/step
Epoch 58/300
5969/5969 - 110s - loss: 5.1730e-04 - val_loss: 3.9234e-04 - 110s/epoch - 18ms/step
Epoch 59/300
5969/5969 - 110s - loss: 5.0445e-04 - val_loss: 3.7582e-04 - 110s/epoch - 18ms/step
Epoch 60/300
5969/5969 - 110s - loss: 5.0547e-04 - val_loss: 3.7987e-04 - 110s/epoch - 18ms/step
Epoch 61/300
5969/5969 - 110s - loss: 4.9946e-04 - val_loss: 3.7850e-04 - 110s/epoch - 18ms/step
Epoch 62/300
5969/5969 - 110s - loss: 4.9932e-04 - val_loss: 4.0650e-04 - 110s/epoch - 18ms/step
Epoch 63/300
5969/5969 - 110s - loss: 5.0313e-04 - val_loss: 3.6256e-04 - 110s/epoch - 18ms/step
Epoch 64/300
5969/5969 - 110s - loss: 5.0728e-04 - val_loss: 3.6539e-04 - 110s/epoch - 18ms/step
Epoch 65/300
5969/5969 - 110s - loss: 4.9618e-04 - val_loss: 3.7310e-04 - 110s/epoch - 18ms/step
Epoch 66/300
5969/5969 - 110s - loss: 5.0538e-04 - val_loss: 3.8038e-04 - 110s/epoch - 18ms/step
Epoch 67/300
5969/5969 - 110s - loss: 5.0228e-04 - val_loss: 3.9890e-04 - 110s/epoch - 18ms/step
Epoch 68/300
5969/5969 - 110s - loss: 4.9748e-04 - val_loss: 3.7975e-04 - 110s/epoch - 18ms/step
Epoch 69/300
5969/5969 - 110s - loss: 4.8404e-04 - val_loss: 3.8132e-04 - 110s/epoch - 18ms/step
Epoch 70/300
5969/5969 - 110s - loss: 4.9337e-04 - val_loss: 4.2511e-04 - 110s/epoch - 18ms/step
Epoch 71/300
5969/5969 - 110s - loss: 4.8325e-04 - val_loss: 3.7893e-04 - 110s/epoch - 18ms/step
Epoch 72/300
5969/5969 - 110s - loss: 4.8505e-04 - val_loss: 3.7658e-04 - 110s/epoch - 18ms/step
Epoch 73/300
5969/5969 - 110s - loss: 4.8774e-04 - val_loss: 3.7861e-04 - 110s/epoch - 18ms/step
Epoch 74/300
5969/5969 - 110s - loss: 4.8247e-04 - val_loss: 3.8024e-04 - 110s/epoch - 18ms/step
Epoch 75/300
5969/5969 - 110s - loss: 4.7778e-04 - val_loss: 3.8221e-04 - 110s/epoch - 18ms/step
Epoch 76/300
5969/5969 - 110s - loss: 4.8029e-04 - val_loss: 3.5835e-04 - 110s/epoch - 18ms/step
Epoch 77/300
5969/5969 - 110s - loss: 4.7181e-04 - val_loss: 3.5869e-04 - 110s/epoch - 18ms/step
Epoch 78/300
5969/5969 - 110s - loss: 4.7655e-04 - val_loss: 3.8012e-04 - 110s/epoch - 18ms/step
Epoch 79/300
5969/5969 - 110s - loss: 4.7373e-04 - val_loss: 3.6824e-04 - 110s/epoch - 18ms/step
Epoch 80/300
5969/5969 - 110s - loss: 4.7037e-04 - val_loss: 3.3698e-04 - 110s/epoch - 18ms/step
Epoch 81/300
5969/5969 - 110s - loss: 4.6829e-04 - val_loss: 3.5544e-04 - 110s/epoch - 18ms/step
Epoch 82/300
5969/5969 - 110s - loss: 4.6789e-04 - val_loss: 3.4475e-04 - 110s/epoch - 18ms/step
Epoch 83/300
5969/5969 - 110s - loss: 4.7629e-04 - val_loss: 3.9003e-04 - 110s/epoch - 18ms/step
Epoch 84/300
5969/5969 - 110s - loss: 4.6482e-04 - val_loss: 3.7240e-04 - 110s/epoch - 18ms/step
Epoch 85/300
5969/5969 - 110s - loss: 4.7185e-04 - val_loss: 3.6688e-04 - 110s/epoch - 18ms/step
Epoch 86/300
5969/5969 - 110s - loss: 4.6794e-04 - val_loss: 3.5497e-04 - 110s/epoch - 18ms/step
Epoch 87/300
5969/5969 - 110s - loss: 4.6703e-04 - val_loss: 3.7765e-04 - 110s/epoch - 18ms/step
Epoch 88/300
5969/5969 - 110s - loss: 4.7277e-04 - val_loss: 3.4379e-04 - 110s/epoch - 18ms/step
Epoch 89/300
5969/5969 - 110s - loss: 4.6887e-04 - val_loss: 3.4956e-04 - 110s/epoch - 18ms/step
Epoch 90/300
5969/5969 - 110s - loss: 4.5865e-04 - val_loss: 3.4410e-04 - 110s/epoch - 18ms/step
Epoch 91/300
5969/5969 - 110s - loss: 4.5792e-04 - val_loss: 3.8381e-04 - 110s/epoch - 18ms/step
Epoch 92/300
5969/5969 - 110s - loss: 4.6029e-04 - val_loss: 3.6811e-04 - 110s/epoch - 18ms/step
Epoch 93/300
5969/5969 - 110s - loss: 4.5691e-04 - val_loss: 3.6309e-04 - 110s/epoch - 18ms/step
Epoch 94/300
5969/5969 - 110s - loss: 4.5094e-04 - val_loss: 3.4311e-04 - 110s/epoch - 18ms/step
Epoch 95/300
5969/5969 - 110s - loss: 4.5535e-04 - val_loss: 3.6297e-04 - 110s/epoch - 18ms/step
Epoch 96/300
5969/5969 - 110s - loss: 4.5874e-04 - val_loss: 3.6365e-04 - 110s/epoch - 18ms/step
Epoch 97/300
5969/5969 - 110s - loss: 4.5156e-04 - val_loss: 3.5997e-04 - 110s/epoch - 18ms/step
Epoch 98/300
5969/5969 - 110s - loss: 4.6011e-04 - val_loss: 3.6727e-04 - 110s/epoch - 18ms/step
Epoch 99/300
5969/5969 - 110s - loss: 4.4879e-04 - val_loss: 3.5776e-04 - 110s/epoch - 18ms/step
Epoch 100/300
5969/5969 - 110s - loss: 4.4631e-04 - val_loss: 3.4920e-04 - 110s/epoch - 18ms/step
Epoch 101/300
5969/5969 - 110s - loss: 4.4499e-04 - val_loss: 3.5589e-04 - 110s/epoch - 18ms/step
Epoch 102/300
5969/5969 - 110s - loss: 4.4880e-04 - val_loss: 3.5188e-04 - 110s/epoch - 18ms/step
Epoch 103/300
5969/5969 - 110s - loss: 4.4604e-04 - val_loss: 3.5644e-04 - 110s/epoch - 18ms/step
Epoch 104/300
5969/5969 - 110s - loss: 4.5300e-04 - val_loss: 3.7940e-04 - 110s/epoch - 18ms/step
Epoch 105/300
5969/5969 - 110s - loss: 4.4597e-04 - val_loss: 3.6707e-04 - 110s/epoch - 18ms/step
Epoch 106/300
5969/5969 - 110s - loss: 4.3845e-04 - val_loss: 3.4605e-04 - 110s/epoch - 18ms/step
Epoch 107/300
5969/5969 - 110s - loss: 4.4573e-04 - val_loss: 4.0381e-04 - 110s/epoch - 18ms/step
Epoch 108/300
5969/5969 - 110s - loss: 4.4500e-04 - val_loss: 3.5619e-04 - 110s/epoch - 18ms/step
Epoch 109/300
5969/5969 - 110s - loss: 4.4233e-04 - val_loss: 3.5347e-04 - 110s/epoch - 18ms/step
Epoch 110/300
5969/5969 - 110s - loss: 4.4136e-04 - val_loss: 3.5555e-04 - 110s/epoch - 18ms/step
Epoch 111/300
5969/5969 - 110s - loss: 4.3923e-04 - val_loss: 4.0512e-04 - 110s/epoch - 18ms/step
Epoch 112/300
5969/5969 - 110s - loss: 4.3919e-04 - val_loss: 4.5186e-04 - 110s/epoch - 18ms/step
Epoch 113/300
5969/5969 - 110s - loss: 4.3280e-04 - val_loss: 3.6035e-04 - 110s/epoch - 18ms/step
Epoch 114/300
5969/5969 - 110s - loss: 4.3446e-04 - val_loss: 3.6660e-04 - 110s/epoch - 18ms/step
Epoch 115/300
5969/5969 - 110s - loss: 4.3927e-04 - val_loss: 3.3149e-04 - 110s/epoch - 18ms/step
Epoch 116/300
5969/5969 - 110s - loss: 4.5389e-04 - val_loss: 3.3777e-04 - 110s/epoch - 18ms/step
Epoch 117/300
5969/5969 - 110s - loss: 4.3541e-04 - val_loss: 3.5088e-04 - 110s/epoch - 18ms/step
Epoch 118/300
5969/5969 - 110s - loss: 4.3564e-04 - val_loss: 3.5687e-04 - 110s/epoch - 18ms/step
Epoch 119/300
5969/5969 - 110s - loss: 4.4367e-04 - val_loss: 3.5296e-04 - 110s/epoch - 18ms/step
Epoch 120/300
5969/5969 - 110s - loss: 4.3771e-04 - val_loss: 3.4259e-04 - 110s/epoch - 18ms/step
Epoch 121/300
5969/5969 - 110s - loss: 4.2999e-04 - val_loss: 3.6197e-04 - 110s/epoch - 18ms/step
Epoch 122/300
5969/5969 - 110s - loss: 4.2682e-04 - val_loss: 3.4139e-04 - 110s/epoch - 18ms/step
Epoch 123/300
5969/5969 - 109s - loss: 4.3186e-04 - val_loss: 3.5217e-04 - 109s/epoch - 18ms/step
Epoch 124/300
5969/5969 - 110s - loss: 4.4057e-04 - val_loss: 3.4366e-04 - 110s/epoch - 18ms/step
Epoch 125/300
5969/5969 - 110s - loss: 4.3521e-04 - val_loss: 3.5111e-04 - 110s/epoch - 18ms/step
Epoch 126/300
5969/5969 - 110s - loss: 4.2661e-04 - val_loss: 4.1309e-04 - 110s/epoch - 18ms/step
Epoch 127/300
5969/5969 - 110s - loss: 4.3451e-04 - val_loss: 3.5781e-04 - 110s/epoch - 18ms/step
Epoch 128/300
5969/5969 - 110s - loss: 4.3137e-04 - val_loss: 3.4282e-04 - 110s/epoch - 18ms/step
Epoch 129/300
5969/5969 - 110s - loss: 4.4173e-04 - val_loss: 3.4292e-04 - 110s/epoch - 18ms/step
Epoch 130/300
5969/5969 - 110s - loss: 4.3405e-04 - val_loss: 3.7074e-04 - 110s/epoch - 18ms/step
Epoch 131/300
5969/5969 - 110s - loss: 4.3082e-04 - val_loss: 3.8922e-04 - 110s/epoch - 18ms/step
Epoch 132/300
5969/5969 - 110s - loss: 4.2717e-04 - val_loss: 3.7681e-04 - 110s/epoch - 18ms/step
Epoch 133/300
5969/5969 - 110s - loss: 4.2478e-04 - val_loss: 3.4844e-04 - 110s/epoch - 18ms/step
Epoch 134/300
5969/5969 - 110s - loss: 4.3308e-04 - val_loss: 3.3249e-04 - 110s/epoch - 18ms/step
Epoch 135/300
5969/5969 - 110s - loss: 4.1720e-04 - val_loss: 3.5002e-04 - 110s/epoch - 18ms/step
Epoch 136/300
5969/5969 - 110s - loss: 4.2812e-04 - val_loss: 3.3540e-04 - 110s/epoch - 18ms/step
Epoch 137/300
5969/5969 - 110s - loss: 4.2603e-04 - val_loss: 3.7287e-04 - 110s/epoch - 18ms/step
Epoch 138/300
5969/5969 - 110s - loss: 4.1666e-04 - val_loss: 3.5974e-04 - 110s/epoch - 18ms/step
Epoch 139/300
5969/5969 - 110s - loss: 4.4748e-04 - val_loss: 3.5980e-04 - 110s/epoch - 18ms/step
Epoch 140/300
5969/5969 - 109s - loss: 4.2317e-04 - val_loss: 3.6542e-04 - 109s/epoch - 18ms/step
Epoch 141/300
5969/5969 - 110s - loss: 4.2182e-04 - val_loss: 3.1477e-04 - 110s/epoch - 18ms/step
Epoch 142/300
5969/5969 - 110s - loss: 4.2644e-04 - val_loss: 3.9676e-04 - 110s/epoch - 18ms/step
Epoch 143/300
5969/5969 - 110s - loss: 4.2112e-04 - val_loss: 4.0195e-04 - 110s/epoch - 18ms/step
Epoch 144/300
5969/5969 - 109s - loss: 4.1698e-04 - val_loss: 3.5065e-04 - 109s/epoch - 18ms/step
Epoch 145/300
5969/5969 - 110s - loss: 4.1870e-04 - val_loss: 3.5782e-04 - 110s/epoch - 18ms/step
Epoch 146/300
5969/5969 - 110s - loss: 4.1986e-04 - val_loss: 3.4289e-04 - 110s/epoch - 18ms/step
Epoch 147/300
5969/5969 - 109s - loss: 4.2559e-04 - val_loss: 3.7553e-04 - 109s/epoch - 18ms/step
Epoch 148/300
5969/5969 - 110s - loss: 4.1941e-04 - val_loss: 4.2286e-04 - 110s/epoch - 18ms/step
Epoch 149/300
5969/5969 - 110s - loss: 4.1437e-04 - val_loss: 3.8559e-04 - 110s/epoch - 18ms/step
Epoch 150/300
5969/5969 - 110s - loss: 4.2292e-04 - val_loss: 3.8498e-04 - 110s/epoch - 18ms/step
Epoch 151/300
5969/5969 - 110s - loss: 4.1498e-04 - val_loss: 3.4050e-04 - 110s/epoch - 18ms/step
Epoch 152/300
5969/5969 - 110s - loss: 4.1817e-04 - val_loss: 3.9980e-04 - 110s/epoch - 18ms/step
Epoch 153/300
5969/5969 - 110s - loss: 4.2766e-04 - val_loss: 3.7653e-04 - 110s/epoch - 18ms/step
Epoch 154/300
5969/5969 - 110s - loss: 4.1126e-04 - val_loss: 3.8812e-04 - 110s/epoch - 18ms/step
Epoch 155/300
5969/5969 - 110s - loss: 4.3111e-04 - val_loss: 3.3920e-04 - 110s/epoch - 18ms/step
Epoch 156/300
5969/5969 - 109s - loss: 4.0826e-04 - val_loss: 4.3192e-04 - 109s/epoch - 18ms/step
Epoch 157/300
5969/5969 - 110s - loss: 4.1247e-04 - val_loss: 3.4610e-04 - 110s/epoch - 18ms/step
Epoch 158/300
5969/5969 - 110s - loss: 4.1306e-04 - val_loss: 3.4499e-04 - 110s/epoch - 18ms/step
Epoch 159/300
5969/5969 - 109s - loss: 4.1909e-04 - val_loss: 3.3661e-04 - 109s/epoch - 18ms/step
Epoch 160/300
5969/5969 - 110s - loss: 4.1317e-04 - val_loss: 3.6761e-04 - 110s/epoch - 18ms/step
Epoch 161/300
5969/5969 - 110s - loss: 4.1108e-04 - val_loss: 3.5182e-04 - 110s/epoch - 18ms/step
Epoch 162/300
5969/5969 - 110s - loss: 4.0399e-04 - val_loss: 3.9595e-04 - 110s/epoch - 18ms/step
Epoch 163/300
5969/5969 - 110s - loss: 4.1004e-04 - val_loss: 3.7306e-04 - 110s/epoch - 18ms/step
Epoch 164/300
5969/5969 - 110s - loss: 4.2126e-04 - val_loss: 4.9889e-04 - 110s/epoch - 18ms/step
Epoch 165/300
5969/5969 - 110s - loss: 4.2527e-04 - val_loss: 4.6265e-04 - 110s/epoch - 18ms/step
Epoch 166/300
5969/5969 - 110s - loss: 4.1010e-04 - val_loss: 4.0050e-04 - 110s/epoch - 18ms/step
Epoch 167/300
5969/5969 - 110s - loss: 4.1170e-04 - val_loss: 4.0182e-04 - 110s/epoch - 18ms/step
Epoch 168/300
5969/5969 - 110s - loss: 4.1855e-04 - val_loss: 3.3988e-04 - 110s/epoch - 18ms/step
Epoch 169/300
5969/5969 - 110s - loss: 4.1612e-04 - val_loss: 3.5837e-04 - 110s/epoch - 18ms/step
Epoch 170/300
5969/5969 - 110s - loss: 4.2485e-04 - val_loss: 3.7910e-04 - 110s/epoch - 18ms/step
Epoch 171/300
5969/5969 - 109s - loss: 4.2126e-04 - val_loss: 3.6079e-04 - 109s/epoch - 18ms/step
Epoch 172/300
5969/5969 - 110s - loss: 4.0307e-04 - val_loss: 4.8474e-04 - 110s/epoch - 18ms/step
Epoch 173/300
5969/5969 - 110s - loss: 4.0966e-04 - val_loss: 3.8021e-04 - 110s/epoch - 18ms/step
Epoch 174/300
5969/5969 - 110s - loss: 4.0201e-04 - val_loss: 4.1364e-04 - 110s/epoch - 18ms/step
Epoch 175/300
5969/5969 - 110s - loss: 4.0552e-04 - val_loss: 3.9206e-04 - 110s/epoch - 18ms/step
Epoch 176/300
5969/5969 - 110s - loss: 4.1247e-04 - val_loss: 3.9397e-04 - 110s/epoch - 18ms/step
Epoch 177/300
5969/5969 - 110s - loss: 4.0440e-04 - val_loss: 3.8696e-04 - 110s/epoch - 18ms/step
Epoch 178/300
5969/5969 - 110s - loss: 4.0379e-04 - val_loss: 3.4897e-04 - 110s/epoch - 18ms/step
Epoch 179/300
5969/5969 - 110s - loss: 4.0210e-04 - val_loss: 3.8944e-04 - 110s/epoch - 18ms/step
Epoch 180/300
5969/5969 - 110s - loss: 3.9616e-04 - val_loss: 3.6256e-04 - 110s/epoch - 18ms/step
Epoch 181/300
5969/5969 - 110s - loss: 3.9952e-04 - val_loss: 4.3310e-04 - 110s/epoch - 18ms/step
Epoch 182/300
5969/5969 - 110s - loss: 4.0029e-04 - val_loss: 4.0411e-04 - 110s/epoch - 18ms/step
Epoch 183/300
5969/5969 - 110s - loss: 4.1114e-04 - val_loss: 3.5252e-04 - 110s/epoch - 18ms/step
Epoch 184/300
5969/5969 - 110s - loss: 3.9809e-04 - val_loss: 4.4747e-04 - 110s/epoch - 18ms/step
Epoch 185/300
5969/5969 - 110s - loss: 3.8971e-04 - val_loss: 3.3732e-04 - 110s/epoch - 18ms/step
Epoch 186/300
5969/5969 - 110s - loss: 4.2070e-04 - val_loss: 4.1038e-04 - 110s/epoch - 18ms/step
Epoch 187/300
5969/5969 - 110s - loss: 4.0573e-04 - val_loss: 6.1596e-04 - 110s/epoch - 18ms/step
Epoch 188/300
5969/5969 - 110s - loss: 4.0164e-04 - val_loss: 4.5128e-04 - 110s/epoch - 18ms/step
Epoch 189/300
5969/5969 - 110s - loss: 3.9619e-04 - val_loss: 3.6233e-04 - 110s/epoch - 18ms/step
Epoch 190/300
5969/5969 - 110s - loss: 3.9906e-04 - val_loss: 4.3704e-04 - 110s/epoch - 18ms/step
Epoch 191/300
5969/5969 - 110s - loss: 3.9444e-04 - val_loss: 4.5586e-04 - 110s/epoch - 18ms/step
Epoch 192/300
5969/5969 - 110s - loss: 3.9819e-04 - val_loss: 3.3926e-04 - 110s/epoch - 18ms/step
Epoch 193/300
5969/5969 - 110s - loss: 4.1690e-04 - val_loss: 4.9719e-04 - 110s/epoch - 18ms/step
Epoch 194/300
5969/5969 - 110s - loss: 3.9833e-04 - val_loss: 4.7481e-04 - 110s/epoch - 18ms/step
Epoch 195/300
5969/5969 - 110s - loss: 3.9510e-04 - val_loss: 4.4605e-04 - 110s/epoch - 18ms/step
Epoch 196/300
5969/5969 - 109s - loss: 3.9848e-04 - val_loss: 5.0179e-04 - 109s/epoch - 18ms/step
Epoch 197/300
5969/5969 - 110s - loss: 4.0438e-04 - val_loss: 4.3410e-04 - 110s/epoch - 18ms/step
Epoch 198/300
5969/5969 - 110s - loss: 4.1527e-04 - val_loss: 3.9699e-04 - 110s/epoch - 18ms/step
Epoch 199/300
5969/5969 - 110s - loss: 3.9499e-04 - val_loss: 3.8597e-04 - 110s/epoch - 18ms/step
Epoch 200/300
5969/5969 - 110s - loss: 3.9489e-04 - val_loss: 4.7376e-04 - 110s/epoch - 18ms/step
Epoch 201/300
5969/5969 - 110s - loss: 4.1710e-04 - val_loss: 4.1500e-04 - 110s/epoch - 18ms/step
Epoch 202/300
5969/5969 - 110s - loss: 3.9306e-04 - val_loss: 4.1075e-04 - 110s/epoch - 18ms/step
Epoch 203/300
5969/5969 - 110s - loss: 3.9451e-04 - val_loss: 3.8201e-04 - 110s/epoch - 18ms/step
Epoch 204/300
5969/5969 - 110s - loss: 3.8984e-04 - val_loss: 4.9022e-04 - 110s/epoch - 18ms/step
Epoch 205/300
5969/5969 - 109s - loss: 4.1382e-04 - val_loss: 4.5626e-04 - 109s/epoch - 18ms/step
Epoch 206/300
5969/5969 - 110s - loss: 4.0094e-04 - val_loss: 5.4297e-04 - 110s/epoch - 18ms/step
Epoch 207/300
5969/5969 - 110s - loss: 3.9605e-04 - val_loss: 5.0302e-04 - 110s/epoch - 18ms/step
Epoch 208/300
5969/5969 - 110s - loss: 3.8888e-04 - val_loss: 3.6338e-04 - 110s/epoch - 18ms/step
Epoch 209/300
5969/5969 - 110s - loss: 3.9341e-04 - val_loss: 4.6102e-04 - 110s/epoch - 18ms/step
Epoch 210/300
5969/5969 - 110s - loss: 3.9923e-04 - val_loss: 3.7791e-04 - 110s/epoch - 18ms/step
Epoch 211/300
5969/5969 - 110s - loss: 3.9512e-04 - val_loss: 3.5121e-04 - 110s/epoch - 18ms/step
Epoch 212/300
5969/5969 - 110s - loss: 3.9686e-04 - val_loss: 3.8785e-04 - 110s/epoch - 18ms/step
Epoch 213/300
5969/5969 - 110s - loss: 3.9562e-04 - val_loss: 6.1101e-04 - 110s/epoch - 18ms/step
Epoch 214/300
5969/5969 - 110s - loss: 3.8775e-04 - val_loss: 5.7113e-04 - 110s/epoch - 18ms/step
Epoch 215/300
5969/5969 - 110s - loss: 4.0513e-04 - val_loss: 3.5799e-04 - 110s/epoch - 18ms/step
Epoch 216/300
5969/5969 - 110s - loss: 4.1352e-04 - val_loss: 5.4479e-04 - 110s/epoch - 18ms/step
Epoch 217/300
5969/5969 - 110s - loss: 3.9177e-04 - val_loss: 4.5675e-04 - 110s/epoch - 18ms/step
Epoch 218/300
5969/5969 - 110s - loss: 3.8757e-04 - val_loss: 4.1065e-04 - 110s/epoch - 18ms/step
Epoch 219/300
5969/5969 - 110s - loss: 4.0220e-04 - val_loss: 4.5417e-04 - 110s/epoch - 18ms/step
Epoch 220/300
5969/5969 - 110s - loss: 3.8512e-04 - val_loss: 6.4062e-04 - 110s/epoch - 18ms/step
Epoch 221/300
5969/5969 - 110s - loss: 3.8296e-04 - val_loss: 3.9015e-04 - 110s/epoch - 18ms/step
Epoch 222/300
5969/5969 - 110s - loss: 3.9868e-04 - val_loss: 3.7561e-04 - 110s/epoch - 18ms/step
Epoch 223/300
5969/5969 - 110s - loss: 3.8705e-04 - val_loss: 4.6920e-04 - 110s/epoch - 18ms/step
Epoch 224/300
5969/5969 - 110s - loss: 3.8222e-04 - val_loss: 4.3829e-04 - 110s/epoch - 18ms/step
Epoch 225/300
5969/5969 - 110s - loss: 3.8509e-04 - val_loss: 6.3290e-04 - 110s/epoch - 18ms/step
Epoch 226/300
5969/5969 - 110s - loss: 3.9723e-04 - val_loss: 5.7253e-04 - 110s/epoch - 18ms/step
Epoch 227/300
5969/5969 - 110s - loss: 3.8598e-04 - val_loss: 6.2253e-04 - 110s/epoch - 18ms/step
Epoch 228/300
5969/5969 - 110s - loss: 3.8600e-04 - val_loss: 7.6951e-04 - 110s/epoch - 18ms/step
Epoch 229/300
5969/5969 - 110s - loss: 3.8773e-04 - val_loss: 4.2457e-04 - 110s/epoch - 18ms/step
Epoch 230/300
5969/5969 - 110s - loss: 3.8241e-04 - val_loss: 4.9440e-04 - 110s/epoch - 18ms/step
Epoch 231/300
5969/5969 - 110s - loss: 3.9499e-04 - val_loss: 6.6342e-04 - 110s/epoch - 18ms/step
Epoch 232/300
5969/5969 - 110s - loss: 3.9027e-04 - val_loss: 4.4939e-04 - 110s/epoch - 18ms/step
Epoch 233/300
5969/5969 - 110s - loss: 3.8535e-04 - val_loss: 5.9942e-04 - 110s/epoch - 18ms/step
Epoch 234/300
5969/5969 - 110s - loss: 3.8248e-04 - val_loss: 6.8859e-04 - 110s/epoch - 18ms/step
Epoch 235/300
5969/5969 - 109s - loss: 3.8711e-04 - val_loss: 5.7744e-04 - 109s/epoch - 18ms/step
Epoch 236/300
5969/5969 - 110s - loss: 3.9227e-04 - val_loss: 6.3322e-04 - 110s/epoch - 18ms/step
Epoch 237/300
5969/5969 - 110s - loss: 4.0564e-04 - val_loss: 7.0796e-04 - 110s/epoch - 18ms/step
Epoch 238/300
5969/5969 - 110s - loss: 3.8194e-04 - val_loss: 4.7393e-04 - 110s/epoch - 18ms/step
Epoch 239/300
5969/5969 - 110s - loss: 3.8187e-04 - val_loss: 6.8877e-04 - 110s/epoch - 18ms/step
Epoch 240/300
5969/5969 - 110s - loss: 3.8500e-04 - val_loss: 4.7542e-04 - 110s/epoch - 18ms/step
Epoch 241/300
5969/5969 - 110s - loss: 3.8243e-04 - val_loss: 5.4185e-04 - 110s/epoch - 18ms/step
Epoch 242/300
5969/5969 - 110s - loss: 4.2437e-04 - val_loss: 8.3394e-04 - 110s/epoch - 18ms/step
Epoch 243/300
5969/5969 - 110s - loss: 3.8236e-04 - val_loss: 6.3721e-04 - 110s/epoch - 18ms/step
Epoch 244/300
5969/5969 - 109s - loss: 3.8801e-04 - val_loss: 6.5864e-04 - 109s/epoch - 18ms/step
Epoch 245/300
5969/5969 - 110s - loss: 3.8180e-04 - val_loss: 0.0011 - 110s/epoch - 18ms/step
Epoch 246/300
5969/5969 - 110s - loss: 3.8702e-04 - val_loss: 7.0312e-04 - 110s/epoch - 18ms/step
Epoch 247/300
5969/5969 - 110s - loss: 3.7929e-04 - val_loss: 4.0106e-04 - 110s/epoch - 18ms/step
Epoch 248/300
5969/5969 - 110s - loss: 3.9440e-04 - val_loss: 0.0011 - 110s/epoch - 18ms/step
Epoch 249/300
5969/5969 - 110s - loss: 3.8750e-04 - val_loss: 8.8682e-04 - 110s/epoch - 18ms/step
Epoch 250/300
5969/5969 - 110s - loss: 3.8736e-04 - val_loss: 7.0523e-04 - 110s/epoch - 18ms/step
Epoch 251/300
5969/5969 - 110s - loss: 3.8021e-04 - val_loss: 7.4430e-04 - 110s/epoch - 18ms/step
Epoch 252/300
