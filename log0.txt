start
Mon Dec 26 21:12:06 CET 2022
2022-12-26 21:12:07.810211: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-26 21:12:07.925013: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-12-26 21:12:07.951826: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-12-26 21:12:42,000 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fa7eede68b0> object, created with modnet version 0.1.12
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_default_cr0.5_bs16_ep100_loss_mse_lr0.0005_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_default_cr0.5_bs16_ep100_loss_mse_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_default_cr0.5_bs16_ep100_loss_mse_lr0.002_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_default_cr0.5_bs16_ep200_loss_mse_lr0.0005_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_default_cr0.5_bs16_ep200_loss_mse_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_default_cr0.5_bs16_ep200_loss_mse_lr0.002_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_default_cr0.5_bs16_ep300_loss_mse_lr0.0005_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
File MP_GapFeats_default_cr0.5_bs16_ep300_loss_mse_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
2022-12-26 21:12:53.241850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-26 21:12:53.618141: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2022-12-26 21:12:53.618256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20637 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:0f:00.0, compute capability: 8.6
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense (Dense)               (None, 2528)              3197920   
                                                                 
 batch_normalization (BatchN  (None, 2528)             10112     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_2 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 2528)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/300
2022-12-26 21:12:55.889998: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
5969/5969 - 18s - loss: 0.0078 - val_loss: 0.0036 - 18s/epoch - 3ms/step
Epoch 2/300
5969/5969 - 17s - loss: 0.0025 - val_loss: 0.0019 - 17s/epoch - 3ms/step
Epoch 3/300
5969/5969 - 17s - loss: 0.0019 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 4/300
5969/5969 - 17s - loss: 0.0015 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 5/300
5969/5969 - 17s - loss: 0.0014 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 6/300
5969/5969 - 17s - loss: 0.0013 - val_loss: 0.0010 - 17s/epoch - 3ms/step
Epoch 7/300
5969/5969 - 17s - loss: 0.0012 - val_loss: 9.9128e-04 - 17s/epoch - 3ms/step
Epoch 8/300
5969/5969 - 17s - loss: 0.0011 - val_loss: 8.7857e-04 - 17s/epoch - 3ms/step
Epoch 9/300
5969/5969 - 17s - loss: 0.0011 - val_loss: 8.2599e-04 - 17s/epoch - 3ms/step
Epoch 10/300
5969/5969 - 17s - loss: 0.0011 - val_loss: 8.1319e-04 - 17s/epoch - 3ms/step
Epoch 11/300
5969/5969 - 17s - loss: 0.0010 - val_loss: 7.9085e-04 - 17s/epoch - 3ms/step
Epoch 12/300
5969/5969 - 17s - loss: 9.7246e-04 - val_loss: 7.4971e-04 - 17s/epoch - 3ms/step
Epoch 13/300
5969/5969 - 17s - loss: 9.3465e-04 - val_loss: 7.6450e-04 - 17s/epoch - 3ms/step
Epoch 14/300
5969/5969 - 17s - loss: 9.2197e-04 - val_loss: 7.3925e-04 - 17s/epoch - 3ms/step
Epoch 15/300
5969/5969 - 17s - loss: 9.0316e-04 - val_loss: 7.4257e-04 - 17s/epoch - 3ms/step
Epoch 16/300
5969/5969 - 17s - loss: 8.9111e-04 - val_loss: 7.4569e-04 - 17s/epoch - 3ms/step
Epoch 17/300
5969/5969 - 17s - loss: 8.6817e-04 - val_loss: 7.1277e-04 - 17s/epoch - 3ms/step
Epoch 18/300
5969/5969 - 17s - loss: 8.4797e-04 - val_loss: 6.6837e-04 - 17s/epoch - 3ms/step
Epoch 19/300
5969/5969 - 17s - loss: 8.2803e-04 - val_loss: 6.8302e-04 - 17s/epoch - 3ms/step
Epoch 20/300
5969/5969 - 17s - loss: 8.1579e-04 - val_loss: 6.3538e-04 - 17s/epoch - 3ms/step
Epoch 21/300
5969/5969 - 17s - loss: 8.2379e-04 - val_loss: 7.1444e-04 - 17s/epoch - 3ms/step
Epoch 22/300
5969/5969 - 17s - loss: 8.0917e-04 - val_loss: 6.2505e-04 - 17s/epoch - 3ms/step
Epoch 23/300
5969/5969 - 17s - loss: 8.0221e-04 - val_loss: 6.1924e-04 - 17s/epoch - 3ms/step
Epoch 24/300
5969/5969 - 17s - loss: 7.9041e-04 - val_loss: 6.1802e-04 - 17s/epoch - 3ms/step
Epoch 25/300
5969/5969 - 17s - loss: 7.7289e-04 - val_loss: 6.2757e-04 - 17s/epoch - 3ms/step
Epoch 26/300
5969/5969 - 17s - loss: 7.8022e-04 - val_loss: 5.9594e-04 - 17s/epoch - 3ms/step
Epoch 27/300
5969/5969 - 17s - loss: 7.4903e-04 - val_loss: 5.7243e-04 - 17s/epoch - 3ms/step
Epoch 28/300
5969/5969 - 17s - loss: 7.9090e-04 - val_loss: 5.9546e-04 - 17s/epoch - 3ms/step
Epoch 29/300
5969/5969 - 17s - loss: 7.4765e-04 - val_loss: 5.8185e-04 - 17s/epoch - 3ms/step
Epoch 30/300
5969/5969 - 17s - loss: 7.3143e-04 - val_loss: 5.5409e-04 - 17s/epoch - 3ms/step
Epoch 31/300
5969/5969 - 17s - loss: 7.2893e-04 - val_loss: 5.3324e-04 - 17s/epoch - 3ms/step
Epoch 32/300
5969/5969 - 17s - loss: 7.1880e-04 - val_loss: 5.7259e-04 - 17s/epoch - 3ms/step
Epoch 33/300
5969/5969 - 17s - loss: 7.0497e-04 - val_loss: 5.4542e-04 - 17s/epoch - 3ms/step
Epoch 34/300
5969/5969 - 17s - loss: 7.2485e-04 - val_loss: 5.2923e-04 - 17s/epoch - 3ms/step
Epoch 35/300
5969/5969 - 17s - loss: 7.0939e-04 - val_loss: 5.5412e-04 - 17s/epoch - 3ms/step
Epoch 36/300
5969/5969 - 17s - loss: 7.1107e-04 - val_loss: 6.1526e-04 - 17s/epoch - 3ms/step
Epoch 37/300
5969/5969 - 17s - loss: 7.0748e-04 - val_loss: 5.6902e-04 - 17s/epoch - 3ms/step
Epoch 38/300
5969/5969 - 17s - loss: 7.0644e-04 - val_loss: 5.5510e-04 - 17s/epoch - 3ms/step
Epoch 39/300
5969/5969 - 17s - loss: 6.9329e-04 - val_loss: 5.2664e-04 - 17s/epoch - 3ms/step
Epoch 40/300
5969/5969 - 17s - loss: 6.8300e-04 - val_loss: 5.4693e-04 - 17s/epoch - 3ms/step
Epoch 41/300
5969/5969 - 17s - loss: 6.8226e-04 - val_loss: 5.0345e-04 - 17s/epoch - 3ms/step
Epoch 42/300
5969/5969 - 17s - loss: 6.8824e-04 - val_loss: 5.2100e-04 - 17s/epoch - 3ms/step
Epoch 43/300
5969/5969 - 17s - loss: 6.6266e-04 - val_loss: 5.5427e-04 - 17s/epoch - 3ms/step
Epoch 44/300
5969/5969 - 17s - loss: 6.7984e-04 - val_loss: 5.7498e-04 - 17s/epoch - 3ms/step
Epoch 45/300
5969/5969 - 17s - loss: 6.6696e-04 - val_loss: 5.4355e-04 - 17s/epoch - 3ms/step
Epoch 46/300
5969/5969 - 17s - loss: 6.5361e-04 - val_loss: 5.4758e-04 - 17s/epoch - 3ms/step
Epoch 47/300
5969/5969 - 17s - loss: 6.5375e-04 - val_loss: 5.5262e-04 - 17s/epoch - 3ms/step
Epoch 48/300
5969/5969 - 17s - loss: 6.5126e-04 - val_loss: 5.2900e-04 - 17s/epoch - 3ms/step
Epoch 49/300
5969/5969 - 17s - loss: 6.6465e-04 - val_loss: 5.0430e-04 - 17s/epoch - 3ms/step
Epoch 50/300
5969/5969 - 17s - loss: 6.3978e-04 - val_loss: 5.0109e-04 - 17s/epoch - 3ms/step
Epoch 51/300
5969/5969 - 17s - loss: 6.5025e-04 - val_loss: 5.1836e-04 - 17s/epoch - 3ms/step
Epoch 52/300
5969/5969 - 17s - loss: 6.3671e-04 - val_loss: 4.9789e-04 - 17s/epoch - 3ms/step
Epoch 53/300
5969/5969 - 17s - loss: 6.3458e-04 - val_loss: 4.9758e-04 - 17s/epoch - 3ms/step
Epoch 54/300
5969/5969 - 17s - loss: 6.5614e-04 - val_loss: 7.2206e-04 - 17s/epoch - 3ms/step
Epoch 55/300
5969/5969 - 17s - loss: 6.2585e-04 - val_loss: 4.8193e-04 - 17s/epoch - 3ms/step
Epoch 56/300
5969/5969 - 17s - loss: 6.3510e-04 - val_loss: 4.9154e-04 - 17s/epoch - 3ms/step
Epoch 57/300
5969/5969 - 17s - loss: 6.2834e-04 - val_loss: 4.9755e-04 - 17s/epoch - 3ms/step
Epoch 58/300
5969/5969 - 17s - loss: 6.2450e-04 - val_loss: 5.2772e-04 - 17s/epoch - 3ms/step
Epoch 59/300
5969/5969 - 17s - loss: 6.1576e-04 - val_loss: 4.7306e-04 - 17s/epoch - 3ms/step
Epoch 60/300
5969/5969 - 17s - loss: 6.4634e-04 - val_loss: 5.0366e-04 - 17s/epoch - 3ms/step
Epoch 61/300
5969/5969 - 17s - loss: 6.1169e-04 - val_loss: 5.3906e-04 - 17s/epoch - 3ms/step
Epoch 62/300
5969/5969 - 17s - loss: 6.0891e-04 - val_loss: 5.3195e-04 - 17s/epoch - 3ms/step
Epoch 63/300
5969/5969 - 17s - loss: 6.3190e-04 - val_loss: 5.3578e-04 - 17s/epoch - 3ms/step
Epoch 64/300
5969/5969 - 17s - loss: 6.1086e-04 - val_loss: 4.9424e-04 - 17s/epoch - 3ms/step
Epoch 65/300
5969/5969 - 17s - loss: 6.1574e-04 - val_loss: 5.2392e-04 - 17s/epoch - 3ms/step
Epoch 66/300
5969/5969 - 17s - loss: 6.0861e-04 - val_loss: 4.8437e-04 - 17s/epoch - 3ms/step
Epoch 67/300
5969/5969 - 17s - loss: 6.1458e-04 - val_loss: 5.1151e-04 - 17s/epoch - 3ms/step
Epoch 68/300
5969/5969 - 17s - loss: 5.9879e-04 - val_loss: 5.3861e-04 - 17s/epoch - 3ms/step
Epoch 69/300
5969/5969 - 17s - loss: 6.8255e-04 - val_loss: 5.1054e-04 - 17s/epoch - 3ms/step
Epoch 70/300
5969/5969 - 17s - loss: 6.0781e-04 - val_loss: 4.9659e-04 - 17s/epoch - 3ms/step
Epoch 71/300
5969/5969 - 17s - loss: 6.2523e-04 - val_loss: 7.6057e-04 - 17s/epoch - 3ms/step
Epoch 72/300
5969/5969 - 17s - loss: 6.1028e-04 - val_loss: 5.0069e-04 - 17s/epoch - 3ms/step
Epoch 73/300
5969/5969 - 17s - loss: 6.0677e-04 - val_loss: 5.0345e-04 - 17s/epoch - 3ms/step
Epoch 74/300
5969/5969 - 17s - loss: 5.9977e-04 - val_loss: 4.8285e-04 - 17s/epoch - 3ms/step
Epoch 75/300
5969/5969 - 17s - loss: 6.0098e-04 - val_loss: 5.2392e-04 - 17s/epoch - 3ms/step
Epoch 76/300
5969/5969 - 17s - loss: 5.9264e-04 - val_loss: 4.9748e-04 - 17s/epoch - 3ms/step
Epoch 77/300
5969/5969 - 17s - loss: 6.0034e-04 - val_loss: 4.7994e-04 - 17s/epoch - 3ms/step
Epoch 78/300
5969/5969 - 17s - loss: 5.9828e-04 - val_loss: 4.9405e-04 - 17s/epoch - 3ms/step
Epoch 79/300
5969/5969 - 17s - loss: 6.1000e-04 - val_loss: 7.2046e-04 - 17s/epoch - 3ms/step
Epoch 80/300
5969/5969 - 17s - loss: 5.9494e-04 - val_loss: 4.7152e-04 - 17s/epoch - 3ms/step
Epoch 81/300
5969/5969 - 17s - loss: 5.8859e-04 - val_loss: 5.1795e-04 - 17s/epoch - 3ms/step
Epoch 82/300
5969/5969 - 17s - loss: 5.9306e-04 - val_loss: 5.0644e-04 - 17s/epoch - 3ms/step
Epoch 83/300
5969/5969 - 17s - loss: 5.9946e-04 - val_loss: 5.4089e-04 - 17s/epoch - 3ms/step
Epoch 84/300
5969/5969 - 17s - loss: 5.8403e-04 - val_loss: 5.5118e-04 - 17s/epoch - 3ms/step
Epoch 85/300
5969/5969 - 17s - loss: 5.8727e-04 - val_loss: 5.0018e-04 - 17s/epoch - 3ms/step
Epoch 86/300
5969/5969 - 17s - loss: 5.7998e-04 - val_loss: 5.0823e-04 - 17s/epoch - 3ms/step
Epoch 87/300
5969/5969 - 17s - loss: 5.9042e-04 - val_loss: 5.4759e-04 - 17s/epoch - 3ms/step
Epoch 88/300
5969/5969 - 17s - loss: 5.7895e-04 - val_loss: 4.8536e-04 - 17s/epoch - 3ms/step
Epoch 89/300
5969/5969 - 17s - loss: 6.2328e-04 - val_loss: 6.2663e-04 - 17s/epoch - 3ms/step
Epoch 90/300
5969/5969 - 17s - loss: 5.8135e-04 - val_loss: 4.6143e-04 - 17s/epoch - 3ms/step
Epoch 91/300
5969/5969 - 17s - loss: 5.7857e-04 - val_loss: 4.9035e-04 - 17s/epoch - 3ms/step
Epoch 92/300
5969/5969 - 17s - loss: 5.7240e-04 - val_loss: 6.6005e-04 - 17s/epoch - 3ms/step
Epoch 93/300
5969/5969 - 17s - loss: 5.8688e-04 - val_loss: 5.6786e-04 - 17s/epoch - 3ms/step
Epoch 94/300
5969/5969 - 17s - loss: 5.7187e-04 - val_loss: 4.9478e-04 - 17s/epoch - 3ms/step
Epoch 95/300
5969/5969 - 17s - loss: 5.8283e-04 - val_loss: 5.3382e-04 - 17s/epoch - 3ms/step
Epoch 96/300
5969/5969 - 17s - loss: 5.7478e-04 - val_loss: 6.4170e-04 - 17s/epoch - 3ms/step
Epoch 97/300
5969/5969 - 17s - loss: 5.7352e-04 - val_loss: 6.7068e-04 - 17s/epoch - 3ms/step
Epoch 98/300
5969/5969 - 17s - loss: 5.8410e-04 - val_loss: 6.7569e-04 - 17s/epoch - 3ms/step
Epoch 99/300
5969/5969 - 17s - loss: 5.6602e-04 - val_loss: 6.1445e-04 - 17s/epoch - 3ms/step
Epoch 100/300
5969/5969 - 17s - loss: 5.7253e-04 - val_loss: 5.0507e-04 - 17s/epoch - 3ms/step
Epoch 101/300
5969/5969 - 17s - loss: 5.6804e-04 - val_loss: 5.4830e-04 - 17s/epoch - 3ms/step
Epoch 102/300
5969/5969 - 17s - loss: 5.6324e-04 - val_loss: 6.8707e-04 - 17s/epoch - 3ms/step
Epoch 103/300
5969/5969 - 17s - loss: 5.6139e-04 - val_loss: 6.7548e-04 - 17s/epoch - 3ms/step
Epoch 104/300
5969/5969 - 17s - loss: 5.6193e-04 - val_loss: 6.5183e-04 - 17s/epoch - 3ms/step
Epoch 105/300
5969/5969 - 17s - loss: 5.8237e-04 - val_loss: 7.4950e-04 - 17s/epoch - 3ms/step
Epoch 106/300
5969/5969 - 17s - loss: 5.5853e-04 - val_loss: 6.1781e-04 - 17s/epoch - 3ms/step
Epoch 107/300
5969/5969 - 17s - loss: 5.8060e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 108/300
5969/5969 - 17s - loss: 5.6159e-04 - val_loss: 6.2950e-04 - 17s/epoch - 3ms/step
Epoch 109/300
5969/5969 - 17s - loss: 5.5552e-04 - val_loss: 6.1487e-04 - 17s/epoch - 3ms/step
Epoch 110/300
5969/5969 - 17s - loss: 5.8661e-04 - val_loss: 8.0740e-04 - 17s/epoch - 3ms/step
Epoch 111/300
5969/5969 - 17s - loss: 5.6218e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 112/300
5969/5969 - 17s - loss: 5.6297e-04 - val_loss: 6.7592e-04 - 17s/epoch - 3ms/step
Epoch 113/300
5969/5969 - 17s - loss: 5.5496e-04 - val_loss: 9.0887e-04 - 17s/epoch - 3ms/step
Epoch 114/300
5969/5969 - 17s - loss: 5.4475e-04 - val_loss: 9.4157e-04 - 17s/epoch - 3ms/step
Epoch 115/300
5969/5969 - 17s - loss: 5.5952e-04 - val_loss: 7.3934e-04 - 17s/epoch - 3ms/step
Epoch 116/300
5969/5969 - 17s - loss: 5.8362e-04 - val_loss: 8.5499e-04 - 17s/epoch - 3ms/step
Epoch 117/300
5969/5969 - 17s - loss: 5.6325e-04 - val_loss: 8.4067e-04 - 17s/epoch - 3ms/step
Epoch 118/300
5969/5969 - 17s - loss: 5.6735e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 119/300
5969/5969 - 17s - loss: 5.6383e-04 - val_loss: 8.3976e-04 - 17s/epoch - 3ms/step
Epoch 120/300
5969/5969 - 17s - loss: 5.6556e-04 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 121/300
5969/5969 - 17s - loss: 5.8209e-04 - val_loss: 0.0021 - 17s/epoch - 3ms/step
Epoch 122/300
5969/5969 - 17s - loss: 5.6147e-04 - val_loss: 8.3683e-04 - 17s/epoch - 3ms/step
Epoch 123/300
5969/5969 - 17s - loss: 5.4514e-04 - val_loss: 9.8614e-04 - 17s/epoch - 3ms/step
Epoch 124/300
5969/5969 - 17s - loss: 5.6224e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 125/300
5969/5969 - 17s - loss: 5.5440e-04 - val_loss: 0.0011 - 17s/epoch - 3ms/step
Epoch 126/300
5969/5969 - 17s - loss: 5.5666e-04 - val_loss: 0.0016 - 17s/epoch - 3ms/step
Epoch 127/300
5969/5969 - 17s - loss: 5.5594e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 128/300
5969/5969 - 17s - loss: 5.4480e-04 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 129/300
5969/5969 - 17s - loss: 5.5966e-04 - val_loss: 0.0020 - 17s/epoch - 3ms/step
Epoch 130/300
5969/5969 - 17s - loss: 5.5839e-04 - val_loss: 0.0016 - 17s/epoch - 3ms/step
Epoch 131/300
5969/5969 - 17s - loss: 5.4089e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 132/300
5969/5969 - 17s - loss: 5.6520e-04 - val_loss: 0.0017 - 17s/epoch - 3ms/step
Epoch 133/300
5969/5969 - 17s - loss: 5.5222e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 134/300
5969/5969 - 17s - loss: 5.5273e-04 - val_loss: 0.0016 - 17s/epoch - 3ms/step
Epoch 135/300
5969/5969 - 17s - loss: 5.3752e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 136/300
5969/5969 - 17s - loss: 5.3845e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 137/300
5969/5969 - 17s - loss: 5.4995e-04 - val_loss: 0.0012 - 17s/epoch - 3ms/step
Epoch 138/300
5969/5969 - 17s - loss: 5.3596e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 139/300
5969/5969 - 17s - loss: 5.3859e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 140/300
5969/5969 - 17s - loss: 5.3355e-04 - val_loss: 0.0016 - 17s/epoch - 3ms/step
Epoch 141/300
5969/5969 - 17s - loss: 5.3916e-04 - val_loss: 8.4832e-04 - 17s/epoch - 3ms/step
Epoch 142/300
5969/5969 - 17s - loss: 5.6237e-04 - val_loss: 0.0015 - 17s/epoch - 3ms/step
Epoch 143/300
5969/5969 - 17s - loss: 5.3876e-04 - val_loss: 0.0018 - 17s/epoch - 3ms/step
Epoch 144/300
5969/5969 - 17s - loss: 5.4804e-04 - val_loss: 0.0013 - 17s/epoch - 3ms/step
Epoch 145/300
5969/5969 - 17s - loss: 5.3701e-04 - val_loss: 9.7568e-04 - 17s/epoch - 3ms/step
Epoch 146/300
5969/5969 - 17s - loss: 5.3511e-04 - val_loss: 0.0015 - 17s/epoch - 3ms/step
Epoch 147/300
5969/5969 - 17s - loss: 5.3248e-04 - val_loss: 0.0020 - 17s/epoch - 3ms/step
Epoch 148/300
5969/5969 - 17s - loss: 5.4980e-04 - val_loss: 0.0046 - 17s/epoch - 3ms/step
Epoch 149/300
5969/5969 - 17s - loss: 5.4480e-04 - val_loss: 0.0020 - 17s/epoch - 3ms/step
Epoch 150/300
5969/5969 - 17s - loss: 5.4052e-04 - val_loss: 0.0021 - 17s/epoch - 3ms/step
Epoch 151/300
5969/5969 - 17s - loss: 5.4328e-04 - val_loss: 0.0019 - 17s/epoch - 3ms/step
Epoch 152/300
5969/5969 - 17s - loss: 5.3099e-04 - val_loss: 0.0014 - 17s/epoch - 3ms/step
Epoch 153/300
5969/5969 - 17s - loss: 5.3738e-04 - val_loss: 0.0022 - 17s/epoch - 3ms/step
Epoch 154/300
5969/5969 - 17s - loss: 5.2923e-04 - val_loss: 0.0021 - 17s/epoch - 3ms/step
Epoch 155/300
5969/5969 - 17s - loss: 5.7972e-04 - val_loss: 0.0033 - 17s/epoch - 3ms/step
Epoch 156/300
5969/5969 - 17s - loss: 5.2861e-04 - val_loss: 0.0028 - 17s/epoch - 3ms/step
Epoch 157/300
5969/5969 - 17s - loss: 5.4465e-04 - val_loss: 0.0021 - 17s/epoch - 3ms/step
Epoch 158/300
5969/5969 - 17s - loss: 5.2904e-04 - val_loss: 0.0021 - 17s/epoch - 3ms/step
Epoch 159/300
5969/5969 - 17s - loss: 5.4931e-04 - val_loss: 0.0019 - 17s/epoch - 3ms/step
Epoch 160/300
5969/5969 - 17s - loss: 5.2691e-04 - val_loss: 0.0031 - 17s/epoch - 3ms/step
Epoch 161/300
5969/5969 - 17s - loss: 5.2562e-04 - val_loss: 0.0024 - 17s/epoch - 3ms/step
Epoch 162/300
5969/5969 - 17s - loss: 5.1987e-04 - val_loss: 0.0042 - 17s/epoch - 3ms/step
Epoch 163/300
5969/5969 - 17s - loss: 5.3883e-04 - val_loss: 0.0023 - 17s/epoch - 3ms/step
Epoch 164/300
5969/5969 - 17s - loss: 5.4432e-04 - val_loss: 0.0035 - 17s/epoch - 3ms/step
Epoch 165/300
5969/5969 - 17s - loss: 5.2558e-04 - val_loss: 0.0036 - 17s/epoch - 3ms/step
Epoch 166/300
5969/5969 - 17s - loss: 5.1872e-04 - val_loss: 0.0034 - 17s/epoch - 3ms/step
Epoch 167/300
5969/5969 - 17s - loss: 5.5008e-04 - val_loss: 0.0028 - 17s/epoch - 3ms/step
Epoch 168/300
5969/5969 - 17s - loss: 5.2048e-04 - val_loss: 0.0025 - 17s/epoch - 3ms/step
Epoch 169/300
5969/5969 - 17s - loss: 5.2851e-04 - val_loss: 0.0042 - 17s/epoch - 3ms/step
Epoch 170/300
5969/5969 - 17s - loss: 5.1918e-04 - val_loss: 0.0029 - 17s/epoch - 3ms/step
Epoch 171/300
5969/5969 - 17s - loss: 5.3532e-04 - val_loss: 0.0033 - 17s/epoch - 3ms/step
Epoch 172/300
5969/5969 - 17s - loss: 5.1988e-04 - val_loss: 0.0037 - 17s/epoch - 3ms/step
Epoch 173/300
5969/5969 - 17s - loss: 5.4546e-04 - val_loss: 0.0033 - 17s/epoch - 3ms/step
Epoch 174/300
5969/5969 - 17s - loss: 5.2465e-04 - val_loss: 0.0034 - 17s/epoch - 3ms/step
Epoch 175/300
5969/5969 - 17s - loss: 5.4230e-04 - val_loss: 0.0022 - 17s/epoch - 3ms/step
Epoch 176/300
5969/5969 - 17s - loss: 5.3424e-04 - val_loss: 0.0026 - 17s/epoch - 3ms/step
Epoch 177/300
5969/5969 - 17s - loss: 5.3455e-04 - val_loss: 0.0038 - 17s/epoch - 3ms/step
Epoch 178/300
5969/5969 - 17s - loss: 5.2235e-04 - val_loss: 0.0024 - 17s/epoch - 3ms/step
Epoch 179/300
5969/5969 - 17s - loss: 5.2607e-04 - val_loss: 0.0033 - 17s/epoch - 3ms/step
Epoch 180/300
5969/5969 - 17s - loss: 5.3540e-04 - val_loss: 0.0043 - 17s/epoch - 3ms/step
Epoch 181/300
5969/5969 - 17s - loss: 5.2667e-04 - val_loss: 0.0035 - 17s/epoch - 3ms/step
Epoch 182/300
5969/5969 - 17s - loss: 5.1903e-04 - val_loss: 0.0037 - 17s/epoch - 3ms/step
Epoch 183/300
5969/5969 - 17s - loss: 5.2946e-04 - val_loss: 0.0027 - 17s/epoch - 3ms/step
Epoch 184/300
5969/5969 - 17s - loss: 5.1334e-04 - val_loss: 0.0066 - 17s/epoch - 3ms/step
Epoch 185/300
5969/5969 - 17s - loss: 5.2089e-04 - val_loss: 0.0028 - 17s/epoch - 3ms/step
Epoch 186/300
5969/5969 - 17s - loss: 5.1453e-04 - val_loss: 0.0040 - 17s/epoch - 3ms/step
Epoch 187/300
5969/5969 - 17s - loss: 5.1470e-04 - val_loss: 0.0060 - 17s/epoch - 3ms/step
Epoch 188/300
5969/5969 - 17s - loss: 5.3281e-04 - val_loss: 0.0045 - 17s/epoch - 3ms/step
Epoch 189/300
5969/5969 - 17s - loss: 5.1594e-04 - val_loss: 0.0028 - 17s/epoch - 3ms/step
Epoch 190/300
5969/5969 - 17s - loss: 5.1585e-04 - val_loss: 0.0031 - 17s/epoch - 3ms/step
Epoch 191/300
5969/5969 - 17s - loss: 5.1506e-04 - val_loss: 0.0043 - 17s/epoch - 3ms/step
Epoch 192/300
5969/5969 - 17s - loss: 5.4924e-04 - val_loss: 0.0019 - 17s/epoch - 3ms/step
Epoch 193/300
5969/5969 - 17s - loss: 5.1845e-04 - val_loss: 0.0058 - 17s/epoch - 3ms/step
Epoch 194/300
5969/5969 - 17s - loss: 5.2197e-04 - val_loss: 0.0072 - 17s/epoch - 3ms/step
Epoch 195/300
5969/5969 - 17s - loss: 5.1927e-04 - val_loss: 0.0039 - 17s/epoch - 3ms/step
Epoch 196/300
5969/5969 - 17s - loss: 5.1350e-04 - val_loss: 0.0077 - 17s/epoch - 3ms/step
Epoch 197/300
5969/5969 - 17s - loss: 5.1626e-04 - val_loss: 0.0063 - 17s/epoch - 3ms/step
Epoch 198/300
5969/5969 - 17s - loss: 5.3137e-04 - val_loss: 0.0051 - 17s/epoch - 3ms/step
Epoch 199/300
5969/5969 - 17s - loss: 5.0931e-04 - val_loss: 0.0060 - 17s/epoch - 3ms/step
Epoch 200/300
5969/5969 - 17s - loss: 5.1058e-04 - val_loss: 0.0083 - 17s/epoch - 3ms/step
Epoch 201/300
5969/5969 - 17s - loss: 5.1885e-04 - val_loss: 0.0057 - 17s/epoch - 3ms/step
Epoch 202/300
5969/5969 - 17s - loss: 5.1154e-04 - val_loss: 0.0042 - 17s/epoch - 3ms/step
Epoch 203/300
5969/5969 - 17s - loss: 5.2896e-04 - val_loss: 0.0053 - 17s/epoch - 3ms/step
Epoch 204/300
5969/5969 - 17s - loss: 5.2773e-04 - val_loss: 0.0083 - 17s/epoch - 3ms/step
Epoch 205/300
5969/5969 - 17s - loss: 5.0974e-04 - val_loss: 0.0059 - 17s/epoch - 3ms/step
Epoch 206/300
5969/5969 - 17s - loss: 5.3803e-04 - val_loss: 0.0101 - 17s/epoch - 3ms/step
Epoch 207/300
5969/5969 - 17s - loss: 5.2873e-04 - val_loss: 0.0057 - 17s/epoch - 3ms/step
Epoch 208/300
5969/5969 - 17s - loss: 5.2449e-04 - val_loss: 0.0036 - 17s/epoch - 3ms/step
Epoch 209/300
5969/5969 - 17s - loss: 5.1438e-04 - val_loss: 0.0080 - 17s/epoch - 3ms/step
Epoch 210/300
5969/5969 - 17s - loss: 5.1430e-04 - val_loss: 0.0043 - 17s/epoch - 3ms/step
Epoch 211/300
5969/5969 - 17s - loss: 5.1437e-04 - val_loss: 0.0036 - 17s/epoch - 3ms/step
Epoch 212/300
5969/5969 - 17s - loss: 5.1276e-04 - val_loss: 0.0049 - 17s/epoch - 3ms/step
Epoch 213/300
5969/5969 - 17s - loss: 5.1646e-04 - val_loss: 0.0076 - 17s/epoch - 3ms/step
Epoch 214/300
5969/5969 - 17s - loss: 5.0960e-04 - val_loss: 0.0087 - 17s/epoch - 3ms/step
Epoch 215/300
5969/5969 - 17s - loss: 5.1956e-04 - val_loss: 0.0049 - 17s/epoch - 3ms/step
Epoch 216/300
5969/5969 - 17s - loss: 5.1478e-04 - val_loss: 0.0090 - 17s/epoch - 3ms/step
Epoch 217/300
5969/5969 - 17s - loss: 5.0826e-04 - val_loss: 0.0072 - 17s/epoch - 3ms/step
Epoch 218/300
5969/5969 - 17s - loss: 5.2347e-04 - val_loss: 0.0068 - 17s/epoch - 3ms/step
Epoch 219/300
5969/5969 - 17s - loss: 5.0958e-04 - val_loss: 0.0087 - 17s/epoch - 3ms/step
Epoch 220/300
5969/5969 - 17s - loss: 5.2876e-04 - val_loss: 0.0166 - 17s/epoch - 3ms/step
Epoch 221/300
5969/5969 - 17s - loss: 5.0428e-04 - val_loss: 0.0076 - 17s/epoch - 3ms/step
Epoch 222/300
5969/5969 - 17s - loss: 5.1400e-04 - val_loss: 0.0063 - 17s/epoch - 3ms/step
Epoch 223/300
5969/5969 - 17s - loss: 5.0306e-04 - val_loss: 0.0072 - 17s/epoch - 3ms/step
Epoch 224/300
5969/5969 - 17s - loss: 5.0434e-04 - val_loss: 0.0072 - 17s/epoch - 3ms/step
Epoch 225/300
5969/5969 - 17s - loss: 5.1421e-04 - val_loss: 0.0107 - 17s/epoch - 3ms/step
Epoch 226/300
5969/5969 - 17s - loss: 5.9355e-04 - val_loss: 0.0213 - 17s/epoch - 3ms/step
Epoch 227/300
5969/5969 - 17s - loss: 5.1949e-04 - val_loss: 0.0091 - 17s/epoch - 3ms/step
Epoch 228/300
5969/5969 - 17s - loss: 5.1243e-04 - val_loss: 0.0097 - 17s/epoch - 3ms/step
Epoch 229/300
5969/5969 - 17s - loss: 5.2415e-04 - val_loss: 0.0047 - 17s/epoch - 3ms/step
Epoch 230/300
5969/5969 - 17s - loss: 5.2114e-04 - val_loss: 0.0068 - 17s/epoch - 3ms/step
Epoch 231/300
5969/5969 - 17s - loss: 5.1040e-04 - val_loss: 0.0083 - 17s/epoch - 3ms/step
Epoch 232/300
5969/5969 - 17s - loss: 5.0728e-04 - val_loss: 0.0071 - 17s/epoch - 3ms/step
Epoch 233/300
5969/5969 - 17s - loss: 5.0245e-04 - val_loss: 0.0090 - 17s/epoch - 3ms/step
Epoch 234/300
5969/5969 - 17s - loss: 5.0778e-04 - val_loss: 0.0104 - 17s/epoch - 3ms/step
Epoch 235/300
5969/5969 - 17s - loss: 5.0861e-04 - val_loss: 0.0107 - 17s/epoch - 3ms/step
Epoch 236/300
5969/5969 - 17s - loss: 5.0519e-04 - val_loss: 0.0095 - 17s/epoch - 3ms/step
Epoch 237/300
5969/5969 - 17s - loss: 5.3910e-04 - val_loss: 0.0108 - 17s/epoch - 3ms/step
Epoch 238/300
5969/5969 - 17s - loss: 5.0551e-04 - val_loss: 0.0075 - 17s/epoch - 3ms/step
Epoch 239/300
5969/5969 - 17s - loss: 5.1092e-04 - val_loss: 0.0104 - 17s/epoch - 3ms/step
Epoch 240/300
5969/5969 - 17s - loss: 5.0609e-04 - val_loss: 0.0058 - 17s/epoch - 3ms/step
Epoch 241/300
5969/5969 - 17s - loss: 5.0162e-04 - val_loss: 0.0085 - 17s/epoch - 3ms/step
Epoch 242/300
5969/5969 - 17s - loss: 5.1651e-04 - val_loss: 0.0084 - 17s/epoch - 3ms/step
Epoch 243/300
5969/5969 - 17s - loss: 5.0118e-04 - val_loss: 0.0076 - 17s/epoch - 3ms/step
Epoch 244/300
5969/5969 - 17s - loss: 5.0585e-04 - val_loss: 0.0078 - 17s/epoch - 3ms/step
Epoch 245/300
5969/5969 - 17s - loss: 5.3394e-04 - val_loss: 0.0126 - 17s/epoch - 3ms/step
Epoch 246/300
5969/5969 - 17s - loss: 5.0696e-04 - val_loss: 0.0072 - 17s/epoch - 3ms/step
Epoch 247/300
5969/5969 - 17s - loss: 5.0898e-04 - val_loss: 0.0051 - 17s/epoch - 3ms/step
Epoch 248/300
5969/5969 - 17s - loss: 5.0842e-04 - val_loss: 0.0102 - 17s/epoch - 3ms/step
Epoch 249/300
5969/5969 - 17s - loss: 5.1286e-04 - val_loss: 0.0104 - 17s/epoch - 3ms/step
Epoch 250/300
5969/5969 - 17s - loss: 5.0119e-04 - val_loss: 0.0109 - 17s/epoch - 3ms/step
Epoch 251/300
5969/5969 - 17s - loss: 5.0211e-04 - val_loss: 0.0117 - 17s/epoch - 3ms/step
Epoch 252/300
5969/5969 - 17s - loss: 5.1445e-04 - val_loss: 0.0135 - 17s/epoch - 3ms/step
Epoch 253/300
5969/5969 - 17s - loss: 5.1214e-04 - val_loss: 0.0109 - 17s/epoch - 3ms/step
Epoch 254/300
5969/5969 - 17s - loss: 5.0125e-04 - val_loss: 0.0088 - 17s/epoch - 3ms/step
Epoch 255/300
5969/5969 - 17s - loss: 5.0655e-04 - val_loss: 0.0121 - 17s/epoch - 3ms/step
Epoch 256/300
5969/5969 - 17s - loss: 5.1912e-04 - val_loss: 0.0063 - 17s/epoch - 3ms/step
Epoch 257/300
5969/5969 - 17s - loss: 4.9620e-04 - val_loss: 0.0078 - 17s/epoch - 3ms/step
Epoch 258/300
5969/5969 - 17s - loss: 5.2456e-04 - val_loss: 0.0086 - 17s/epoch - 3ms/step
Epoch 259/300
5969/5969 - 17s - loss: 5.2096e-04 - val_loss: 0.0111 - 17s/epoch - 3ms/step
Epoch 260/300
5969/5969 - 17s - loss: 5.1237e-04 - val_loss: 0.0070 - 17s/epoch - 3ms/step
Epoch 261/300
5969/5969 - 17s - loss: 5.0542e-04 - val_loss: 0.0152 - 17s/epoch - 3ms/step
Epoch 262/300
5969/5969 - 17s - loss: 4.8940e-04 - val_loss: 0.0141 - 17s/epoch - 3ms/step
Epoch 263/300
5969/5969 - 17s - loss: 5.0919e-04 - val_loss: 0.0072 - 17s/epoch - 3ms/step
Epoch 264/300
5969/5969 - 17s - loss: 5.0012e-04 - val_loss: 0.0153 - 17s/epoch - 3ms/step
Epoch 265/300
5969/5969 - 17s - loss: 5.2334e-04 - val_loss: 0.0082 - 17s/epoch - 3ms/step
Epoch 266/300
5969/5969 - 17s - loss: 4.9549e-04 - val_loss: 0.0140 - 17s/epoch - 3ms/step
Epoch 267/300
5969/5969 - 17s - loss: 5.1111e-04 - val_loss: 0.0085 - 17s/epoch - 3ms/step
Epoch 268/300
5969/5969 - 17s - loss: 5.3462e-04 - val_loss: 0.0067 - 17s/epoch - 3ms/step
Epoch 269/300
5969/5969 - 17s - loss: 5.2531e-04 - val_loss: 0.0077 - 17s/epoch - 3ms/step
Epoch 270/300
5969/5969 - 17s - loss: 5.0666e-04 - val_loss: 0.0098 - 17s/epoch - 3ms/step
Epoch 271/300
5969/5969 - 17s - loss: 5.0094e-04 - val_loss: 0.0089 - 17s/epoch - 3ms/step
Epoch 272/300
5969/5969 - 17s - loss: 5.2179e-04 - val_loss: 0.0098 - 17s/epoch - 3ms/step
Epoch 273/300
5969/5969 - 17s - loss: 4.9924e-04 - val_loss: 0.0124 - 17s/epoch - 3ms/step
Epoch 274/300
5969/5969 - 17s - loss: 5.0848e-04 - val_loss: 0.0086 - 17s/epoch - 3ms/step
Epoch 275/300
5969/5969 - 17s - loss: 5.0099e-04 - val_loss: 0.0066 - 17s/epoch - 3ms/step
Epoch 276/300
5969/5969 - 17s - loss: 4.9154e-04 - val_loss: 0.0069 - 17s/epoch - 3ms/step
Epoch 277/300
5969/5969 - 17s - loss: 5.1746e-04 - val_loss: 0.0104 - 17s/epoch - 3ms/step
Epoch 278/300
5969/5969 - 17s - loss: 4.9858e-04 - val_loss: 0.0127 - 17s/epoch - 3ms/step
Epoch 279/300
5969/5969 - 17s - loss: 4.9182e-04 - val_loss: 0.0183 - 17s/epoch - 3ms/step
Epoch 280/300
5969/5969 - 17s - loss: 4.9285e-04 - val_loss: 0.0233 - 17s/epoch - 3ms/step
Epoch 281/300
5969/5969 - 17s - loss: 5.1785e-04 - val_loss: 0.0136 - 17s/epoch - 3ms/step
Epoch 282/300
5969/5969 - 17s - loss: 5.1786e-04 - val_loss: 0.0104 - 17s/epoch - 3ms/step
Epoch 283/300
5969/5969 - 17s - loss: 4.9531e-04 - val_loss: 0.0192 - 17s/epoch - 3ms/step
Epoch 284/300
5969/5969 - 17s - loss: 5.1737e-04 - val_loss: 0.0116 - 17s/epoch - 3ms/step
Epoch 285/300
5969/5969 - 17s - loss: 5.0904e-04 - val_loss: 0.0168 - 17s/epoch - 3ms/step
Epoch 286/300
5969/5969 - 17s - loss: 4.8537e-04 - val_loss: 0.0137 - 17s/epoch - 3ms/step
Epoch 287/300
5969/5969 - 17s - loss: 4.9751e-04 - val_loss: 0.0230 - 17s/epoch - 3ms/step
Epoch 288/300
5969/5969 - 17s - loss: 5.0553e-04 - val_loss: 0.0121 - 17s/epoch - 3ms/step
Epoch 289/300
5969/5969 - 17s - loss: 4.8681e-04 - val_loss: 0.0122 - 17s/epoch - 3ms/step
Epoch 290/300
5969/5969 - 17s - loss: 5.0669e-04 - val_loss: 0.0114 - 17s/epoch - 3ms/step
Epoch 291/300
5969/5969 - 17s - loss: 4.8980e-04 - val_loss: 0.0119 - 17s/epoch - 3ms/step
Epoch 292/300
5969/5969 - 17s - loss: 5.0548e-04 - val_loss: 0.0170 - 17s/epoch - 3ms/step
Epoch 293/300
5969/5969 - 17s - loss: 4.9525e-04 - val_loss: 0.0261 - 17s/epoch - 3ms/step
Epoch 294/300
5969/5969 - 17s - loss: 5.1130e-04 - val_loss: 0.0182 - 17s/epoch - 3ms/step
Epoch 295/300
5969/5969 - 17s - loss: 5.1294e-04 - val_loss: 0.0103 - 17s/epoch - 3ms/step
Epoch 296/300
5969/5969 - 17s - loss: 4.9418e-04 - val_loss: 0.0151 - 17s/epoch - 3ms/step
Epoch 297/300
5969/5969 - 17s - loss: 4.9970e-04 - val_loss: 0.0169 - 17s/epoch - 3ms/step
Epoch 298/300
5969/5969 - 17s - loss: 4.9496e-04 - val_loss: 0.0163 - 17s/epoch - 3ms/step
Epoch 299/300
5969/5969 - 17s - loss: 5.5182e-04 - val_loss: 0.0149 - 17s/epoch - 3ms/step
Epoch 300/300
5969/5969 - 17s - loss: 5.0820e-04 - val_loss: 0.0097 - 17s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.009670952335000038
  1/332 [..............................] - ETA: 35s 50/332 [===>..........................] - ETA: 0s  99/332 [=======>......................] - ETA: 0s148/332 [============>.................] - ETA: 0s198/332 [================>.............] - ETA: 0s248/332 [=====================>........] - ETA: 0s297/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.020372172346968052
cosine 0.019114926600466927
MAE: 0.015857114
RMSE: 0.09833999
r2: 0.37265830873138145
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 2528)              3197920   
                                                                 
 batch_normalization (BatchN  (None, 2528)             10112     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_2 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 2528)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 2528)              3197920   
                                                                 
 batch_normalization (BatchN  (None, 2528)             10112     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_1 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 632)               0         
                                                                 
 dense_1 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_2 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 2528)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 16, 300, 0.002, 0.5, 632, 0.0005082017160020769, 0.009670952335000038, 0.020372172346968052, 0.019114926600466927, 0.015857113525271416, 0.0983399897813797, 0.37265830873138145, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_3 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_3 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_5 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2528)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
2985/2985 - 9s - loss: 0.0081 - val_loss: 0.0033 - 9s/epoch - 3ms/step
Epoch 2/100
2985/2985 - 8s - loss: 0.0030 - val_loss: 0.0023 - 8s/epoch - 3ms/step
Epoch 3/100
2985/2985 - 8s - loss: 0.0026 - val_loss: 0.0014 - 8s/epoch - 3ms/step
Epoch 4/100
2985/2985 - 8s - loss: 0.0020 - val_loss: 0.0012 - 8s/epoch - 3ms/step
Epoch 5/100
2985/2985 - 8s - loss: 0.0015 - val_loss: 0.0010 - 8s/epoch - 3ms/step
Epoch 6/100
2985/2985 - 8s - loss: 0.0012 - val_loss: 0.0015 - 8s/epoch - 3ms/step
Epoch 7/100
2985/2985 - 8s - loss: 0.0011 - val_loss: 7.1917e-04 - 8s/epoch - 3ms/step
Epoch 8/100
2985/2985 - 8s - loss: 8.7563e-04 - val_loss: 6.5966e-04 - 8s/epoch - 3ms/step
Epoch 9/100
2985/2985 - 8s - loss: 7.7359e-04 - val_loss: 5.5779e-04 - 8s/epoch - 3ms/step
Epoch 10/100
2985/2985 - 9s - loss: 6.9492e-04 - val_loss: 5.4896e-04 - 9s/epoch - 3ms/step
Epoch 11/100
2985/2985 - 9s - loss: 6.3490e-04 - val_loss: 5.0411e-04 - 9s/epoch - 3ms/step
Epoch 12/100
2985/2985 - 9s - loss: 5.8093e-04 - val_loss: 8.1897e-04 - 9s/epoch - 3ms/step
Epoch 13/100
2985/2985 - 8s - loss: 5.7272e-04 - val_loss: 4.8429e-04 - 8s/epoch - 3ms/step
Epoch 14/100
2985/2985 - 8s - loss: 5.1013e-04 - val_loss: 4.6821e-04 - 8s/epoch - 3ms/step
Epoch 15/100
2985/2985 - 8s - loss: 4.9460e-04 - val_loss: 4.1800e-04 - 8s/epoch - 3ms/step
Epoch 16/100
2985/2985 - 8s - loss: 4.6262e-04 - val_loss: 4.3673e-04 - 8s/epoch - 3ms/step
Epoch 17/100
2985/2985 - 9s - loss: 4.3989e-04 - val_loss: 3.6665e-04 - 9s/epoch - 3ms/step
Epoch 18/100
2985/2985 - 8s - loss: 4.1827e-04 - val_loss: 4.1717e-04 - 8s/epoch - 3ms/step
Epoch 19/100
2985/2985 - 8s - loss: 4.3195e-04 - val_loss: 3.4185e-04 - 8s/epoch - 3ms/step
Epoch 20/100
2985/2985 - 8s - loss: 4.0403e-04 - val_loss: 3.3272e-04 - 8s/epoch - 3ms/step
Epoch 21/100
2985/2985 - 8s - loss: 3.8365e-04 - val_loss: 3.2728e-04 - 8s/epoch - 3ms/step
Epoch 22/100
2985/2985 - 8s - loss: 3.7067e-04 - val_loss: 3.7391e-04 - 8s/epoch - 3ms/step
Epoch 23/100
2985/2985 - 8s - loss: 3.7275e-04 - val_loss: 3.0710e-04 - 8s/epoch - 3ms/step
Epoch 24/100
2985/2985 - 8s - loss: 3.5313e-04 - val_loss: 3.2053e-04 - 8s/epoch - 3ms/step
Epoch 25/100
2985/2985 - 8s - loss: 3.4388e-04 - val_loss: 2.9680e-04 - 8s/epoch - 3ms/step
Epoch 26/100
2985/2985 - 9s - loss: 3.3773e-04 - val_loss: 2.6727e-04 - 9s/epoch - 3ms/step
Epoch 27/100
2985/2985 - 9s - loss: 3.2450e-04 - val_loss: 2.8969e-04 - 9s/epoch - 3ms/step
Epoch 28/100
2985/2985 - 9s - loss: 3.2114e-04 - val_loss: 2.5921e-04 - 9s/epoch - 3ms/step
Epoch 29/100
2985/2985 - 9s - loss: 3.2032e-04 - val_loss: 2.4733e-04 - 9s/epoch - 3ms/step
Epoch 30/100
2985/2985 - 9s - loss: 3.0800e-04 - val_loss: 2.6936e-04 - 9s/epoch - 3ms/step
Epoch 31/100
2985/2985 - 8s - loss: 3.0111e-04 - val_loss: 2.5540e-04 - 8s/epoch - 3ms/step
Epoch 32/100
2985/2985 - 8s - loss: 2.9575e-04 - val_loss: 2.3397e-04 - 8s/epoch - 3ms/step
Epoch 33/100
2985/2985 - 8s - loss: 3.0710e-04 - val_loss: 2.4102e-04 - 8s/epoch - 3ms/step
Epoch 34/100
2985/2985 - 8s - loss: 2.9949e-04 - val_loss: 2.4024e-04 - 8s/epoch - 3ms/step
Epoch 35/100
2985/2985 - 9s - loss: 2.7918e-04 - val_loss: 2.3553e-04 - 9s/epoch - 3ms/step
Epoch 36/100
2985/2985 - 9s - loss: 2.7525e-04 - val_loss: 2.5431e-04 - 9s/epoch - 3ms/step
Epoch 37/100
2985/2985 - 9s - loss: 2.7384e-04 - val_loss: 2.5419e-04 - 9s/epoch - 3ms/step
Epoch 38/100
2985/2985 - 9s - loss: 2.7369e-04 - val_loss: 2.3846e-04 - 9s/epoch - 3ms/step
Epoch 39/100
2985/2985 - 8s - loss: 2.6719e-04 - val_loss: 2.2080e-04 - 8s/epoch - 3ms/step
Epoch 40/100
2985/2985 - 8s - loss: 2.6404e-04 - val_loss: 2.4529e-04 - 8s/epoch - 3ms/step
Epoch 41/100
2985/2985 - 8s - loss: 2.5841e-04 - val_loss: 2.1373e-04 - 8s/epoch - 3ms/step
Epoch 42/100
2985/2985 - 8s - loss: 2.5925e-04 - val_loss: 2.0982e-04 - 8s/epoch - 3ms/step
Epoch 43/100
2985/2985 - 9s - loss: 2.5494e-04 - val_loss: 2.3155e-04 - 9s/epoch - 3ms/step
Epoch 44/100
2985/2985 - 9s - loss: 2.5204e-04 - val_loss: 3.3146e-04 - 9s/epoch - 3ms/step
Epoch 45/100
2985/2985 - 9s - loss: 2.7549e-04 - val_loss: 2.3646e-04 - 9s/epoch - 3ms/step
Epoch 46/100
2985/2985 - 9s - loss: 2.4649e-04 - val_loss: 2.0870e-04 - 9s/epoch - 3ms/step
Epoch 47/100
2985/2985 - 9s - loss: 2.4455e-04 - val_loss: 2.0894e-04 - 9s/epoch - 3ms/step
Epoch 48/100
2985/2985 - 9s - loss: 2.4098e-04 - val_loss: 1.9267e-04 - 9s/epoch - 3ms/step
Epoch 49/100
2985/2985 - 9s - loss: 2.4108e-04 - val_loss: 3.3672e-04 - 9s/epoch - 3ms/step
Epoch 50/100
2985/2985 - 8s - loss: 2.6953e-04 - val_loss: 2.5435e-04 - 8s/epoch - 3ms/step
Epoch 51/100
2985/2985 - 8s - loss: 2.4381e-04 - val_loss: 1.9505e-04 - 8s/epoch - 3ms/step
Epoch 52/100
2985/2985 - 9s - loss: 2.3240e-04 - val_loss: 1.9727e-04 - 9s/epoch - 3ms/step
Epoch 53/100
2985/2985 - 8s - loss: 2.2765e-04 - val_loss: 2.0841e-04 - 8s/epoch - 3ms/step
Epoch 54/100
2985/2985 - 9s - loss: 2.2693e-04 - val_loss: 2.0621e-04 - 9s/epoch - 3ms/step
Epoch 55/100
2985/2985 - 8s - loss: 2.2205e-04 - val_loss: 1.8167e-04 - 8s/epoch - 3ms/step
Epoch 56/100
2985/2985 - 9s - loss: 2.2479e-04 - val_loss: 1.8438e-04 - 9s/epoch - 3ms/step
Epoch 57/100
2985/2985 - 9s - loss: 2.2110e-04 - val_loss: 1.9503e-04 - 9s/epoch - 3ms/step
Epoch 58/100
2985/2985 - 9s - loss: 2.2223e-04 - val_loss: 2.1880e-04 - 9s/epoch - 3ms/step
Epoch 59/100
2985/2985 - 9s - loss: 2.1524e-04 - val_loss: 1.8667e-04 - 9s/epoch - 3ms/step
Epoch 60/100
2985/2985 - 9s - loss: 2.2716e-04 - val_loss: 1.8175e-04 - 9s/epoch - 3ms/step
Epoch 61/100
2985/2985 - 9s - loss: 2.1616e-04 - val_loss: 1.7878e-04 - 9s/epoch - 3ms/step
Epoch 62/100
2985/2985 - 9s - loss: 2.1360e-04 - val_loss: 2.2663e-04 - 9s/epoch - 3ms/step
Epoch 63/100
2985/2985 - 8s - loss: 2.0917e-04 - val_loss: 1.7884e-04 - 8s/epoch - 3ms/step
Epoch 64/100
2985/2985 - 9s - loss: 2.0769e-04 - val_loss: 1.7773e-04 - 9s/epoch - 3ms/step
Epoch 65/100
2985/2985 - 9s - loss: 2.1936e-04 - val_loss: 2.0687e-04 - 9s/epoch - 3ms/step
Epoch 66/100
2985/2985 - 8s - loss: 2.0628e-04 - val_loss: 1.7778e-04 - 8s/epoch - 3ms/step
Epoch 67/100
2985/2985 - 8s - loss: 2.0737e-04 - val_loss: 1.6866e-04 - 8s/epoch - 3ms/step
Epoch 68/100
2985/2985 - 8s - loss: 2.1227e-04 - val_loss: 1.7390e-04 - 8s/epoch - 3ms/step
Epoch 69/100
2985/2985 - 8s - loss: 2.0415e-04 - val_loss: 1.7286e-04 - 8s/epoch - 3ms/step
Epoch 70/100
2985/2985 - 9s - loss: 2.0350e-04 - val_loss: 2.5084e-04 - 9s/epoch - 3ms/step
Epoch 71/100
2985/2985 - 9s - loss: 2.0906e-04 - val_loss: 1.6811e-04 - 9s/epoch - 3ms/step
Epoch 72/100
2985/2985 - 9s - loss: 2.0742e-04 - val_loss: 2.2039e-04 - 9s/epoch - 3ms/step
Epoch 73/100
2985/2985 - 9s - loss: 2.1076e-04 - val_loss: 1.6684e-04 - 9s/epoch - 3ms/step
Epoch 74/100
2985/2985 - 9s - loss: 1.9719e-04 - val_loss: 1.6608e-04 - 9s/epoch - 3ms/step
Epoch 75/100
2985/2985 - 9s - loss: 2.0415e-04 - val_loss: 1.6250e-04 - 9s/epoch - 3ms/step
Epoch 76/100
2985/2985 - 9s - loss: 1.9741e-04 - val_loss: 1.6749e-04 - 9s/epoch - 3ms/step
Epoch 77/100
2985/2985 - 8s - loss: 2.0718e-04 - val_loss: 2.2945e-04 - 8s/epoch - 3ms/step
Epoch 78/100
2985/2985 - 9s - loss: 1.9671e-04 - val_loss: 1.6919e-04 - 9s/epoch - 3ms/step
Epoch 79/100
2985/2985 - 8s - loss: 1.9318e-04 - val_loss: 1.6813e-04 - 8s/epoch - 3ms/step
Epoch 80/100
2985/2985 - 9s - loss: 1.9288e-04 - val_loss: 1.8342e-04 - 9s/epoch - 3ms/step
Epoch 81/100
2985/2985 - 8s - loss: 2.0057e-04 - val_loss: 1.7437e-04 - 8s/epoch - 3ms/step
Epoch 82/100
2985/2985 - 8s - loss: 1.9123e-04 - val_loss: 1.6325e-04 - 8s/epoch - 3ms/step
Epoch 83/100
2985/2985 - 9s - loss: 1.8925e-04 - val_loss: 1.6128e-04 - 9s/epoch - 3ms/step
Epoch 84/100
2985/2985 - 9s - loss: 1.8808e-04 - val_loss: 1.6132e-04 - 9s/epoch - 3ms/step
Epoch 85/100
2985/2985 - 9s - loss: 1.9216e-04 - val_loss: 1.5886e-04 - 9s/epoch - 3ms/step
Epoch 86/100
2985/2985 - 9s - loss: 1.8744e-04 - val_loss: 1.5785e-04 - 9s/epoch - 3ms/step
Epoch 87/100
2985/2985 - 8s - loss: 1.8410e-04 - val_loss: 1.6595e-04 - 8s/epoch - 3ms/step
Epoch 88/100
2985/2985 - 9s - loss: 1.8647e-04 - val_loss: 2.7382e-04 - 9s/epoch - 3ms/step
Epoch 89/100
2985/2985 - 9s - loss: 2.0858e-04 - val_loss: 1.6216e-04 - 9s/epoch - 3ms/step
Epoch 90/100
2985/2985 - 9s - loss: 1.8339e-04 - val_loss: 1.6128e-04 - 9s/epoch - 3ms/step
Epoch 91/100
2985/2985 - 9s - loss: 1.8372e-04 - val_loss: 1.5534e-04 - 9s/epoch - 3ms/step
Epoch 92/100
2985/2985 - 9s - loss: 1.8200e-04 - val_loss: 1.9315e-04 - 9s/epoch - 3ms/step
Epoch 93/100
2985/2985 - 9s - loss: 1.9406e-04 - val_loss: 1.6998e-04 - 9s/epoch - 3ms/step
Epoch 94/100
2985/2985 - 9s - loss: 1.7986e-04 - val_loss: 1.5497e-04 - 9s/epoch - 3ms/step
Epoch 95/100
2985/2985 - 9s - loss: 1.7908e-04 - val_loss: 1.5330e-04 - 9s/epoch - 3ms/step
Epoch 96/100
2985/2985 - 8s - loss: 1.8089e-04 - val_loss: 1.6995e-04 - 8s/epoch - 3ms/step
Epoch 97/100
2985/2985 - 9s - loss: 1.8571e-04 - val_loss: 1.7269e-04 - 9s/epoch - 3ms/step
Epoch 98/100
2985/2985 - 9s - loss: 1.8170e-04 - val_loss: 1.6206e-04 - 9s/epoch - 3ms/step
Epoch 99/100
2985/2985 - 9s - loss: 1.7658e-04 - val_loss: 1.5214e-04 - 9s/epoch - 3ms/step
Epoch 100/100
2985/2985 - 8s - loss: 1.8176e-04 - val_loss: 1.6289e-04 - 8s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0001628919708309695
  1/332 [..............................] - ETA: 28s 49/332 [===>..........................] - ETA: 0s  97/332 [=======>......................] - ETA: 0s146/332 [============>.................] - ETA: 0s195/332 [================>.............] - ETA: 0s244/332 [=====================>........] - ETA: 0s293/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.001837196734955389
cosine 0.0014603404034051273
MAE: 0.006923228
RMSE: 0.012762908
r2: 0.9894333233621427
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_3 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_5 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2528)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_3 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_4 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 632)               0         
                                                                 
 dense_4 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_5 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2528)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 32, 100, 0.0005, 0.5, 632, 0.00018176183220930398, 0.0001628919708309695, 0.001837196734955389, 0.0014603404034051273, 0.006923228036612272, 0.012762907892465591, 0.9894333233621427, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_6 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_6 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_8 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2528)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
2985/2985 - 9s - loss: 0.0076 - val_loss: 0.0023 - 9s/epoch - 3ms/step
Epoch 2/100
2985/2985 - 8s - loss: 0.0029 - val_loss: 0.0019 - 8s/epoch - 3ms/step
Epoch 3/100
2985/2985 - 9s - loss: 0.0020 - val_loss: 0.0014 - 9s/epoch - 3ms/step
Epoch 4/100
2985/2985 - 9s - loss: 0.0014 - val_loss: 0.0010 - 9s/epoch - 3ms/step
Epoch 5/100
2985/2985 - 9s - loss: 0.0011 - val_loss: 7.9066e-04 - 9s/epoch - 3ms/step
Epoch 6/100
2985/2985 - 9s - loss: 8.8432e-04 - val_loss: 6.9871e-04 - 9s/epoch - 3ms/step
Epoch 7/100
2985/2985 - 9s - loss: 7.7639e-04 - val_loss: 6.0998e-04 - 9s/epoch - 3ms/step
Epoch 8/100
2985/2985 - 9s - loss: 6.8153e-04 - val_loss: 5.4014e-04 - 9s/epoch - 3ms/step
Epoch 9/100
2985/2985 - 9s - loss: 6.2406e-04 - val_loss: 4.9838e-04 - 9s/epoch - 3ms/step
Epoch 10/100
2985/2985 - 9s - loss: 5.8991e-04 - val_loss: 4.6513e-04 - 9s/epoch - 3ms/step
Epoch 11/100
2985/2985 - 9s - loss: 5.4174e-04 - val_loss: 4.4369e-04 - 9s/epoch - 3ms/step
Epoch 12/100
2985/2985 - 9s - loss: 5.4100e-04 - val_loss: 7.7076e-04 - 9s/epoch - 3ms/step
Epoch 13/100
2985/2985 - 9s - loss: 5.3269e-04 - val_loss: 4.1806e-04 - 9s/epoch - 3ms/step
Epoch 14/100
2985/2985 - 9s - loss: 4.7715e-04 - val_loss: 3.9033e-04 - 9s/epoch - 3ms/step
Epoch 15/100
2985/2985 - 9s - loss: 4.6302e-04 - val_loss: 3.9406e-04 - 9s/epoch - 3ms/step
Epoch 16/100
2985/2985 - 9s - loss: 4.4456e-04 - val_loss: 3.6094e-04 - 9s/epoch - 3ms/step
Epoch 17/100
2985/2985 - 9s - loss: 4.1917e-04 - val_loss: 3.4564e-04 - 9s/epoch - 3ms/step
Epoch 18/100
2985/2985 - 9s - loss: 4.1124e-04 - val_loss: 3.8845e-04 - 9s/epoch - 3ms/step
Epoch 19/100
2985/2985 - 9s - loss: 3.9799e-04 - val_loss: 3.6016e-04 - 9s/epoch - 3ms/step
Epoch 20/100
2985/2985 - 9s - loss: 4.4925e-04 - val_loss: 3.1278e-04 - 9s/epoch - 3ms/step
Epoch 21/100
2985/2985 - 9s - loss: 3.8727e-04 - val_loss: 3.0605e-04 - 9s/epoch - 3ms/step
Epoch 22/100
2985/2985 - 9s - loss: 3.8740e-04 - val_loss: 3.7486e-04 - 9s/epoch - 3ms/step
Epoch 23/100
2985/2985 - 9s - loss: 3.8159e-04 - val_loss: 2.8619e-04 - 9s/epoch - 3ms/step
Epoch 24/100
2985/2985 - 9s - loss: 3.7282e-04 - val_loss: 2.9004e-04 - 9s/epoch - 3ms/step
Epoch 25/100
2985/2985 - 9s - loss: 3.5321e-04 - val_loss: 2.8103e-04 - 9s/epoch - 3ms/step
Epoch 26/100
2985/2985 - 9s - loss: 3.4363e-04 - val_loss: 2.5031e-04 - 9s/epoch - 3ms/step
Epoch 27/100
2985/2985 - 9s - loss: 3.4335e-04 - val_loss: 2.6915e-04 - 9s/epoch - 3ms/step
Epoch 28/100
2985/2985 - 9s - loss: 3.3015e-04 - val_loss: 2.5567e-04 - 9s/epoch - 3ms/step
Epoch 29/100
2985/2985 - 9s - loss: 3.2478e-04 - val_loss: 2.5691e-04 - 9s/epoch - 3ms/step
Epoch 30/100
2985/2985 - 9s - loss: 3.3088e-04 - val_loss: 2.5111e-04 - 9s/epoch - 3ms/step
Epoch 31/100
2985/2985 - 9s - loss: 3.2145e-04 - val_loss: 2.5676e-04 - 9s/epoch - 3ms/step
Epoch 32/100
2985/2985 - 9s - loss: 3.1293e-04 - val_loss: 2.4431e-04 - 9s/epoch - 3ms/step
Epoch 33/100
2985/2985 - 9s - loss: 3.0897e-04 - val_loss: 2.5078e-04 - 9s/epoch - 3ms/step
Epoch 34/100
2985/2985 - 9s - loss: 3.0624e-04 - val_loss: 2.4854e-04 - 9s/epoch - 3ms/step
Epoch 35/100
2985/2985 - 9s - loss: 3.0603e-04 - val_loss: 2.4348e-04 - 9s/epoch - 3ms/step
Epoch 36/100
2985/2985 - 9s - loss: 3.0701e-04 - val_loss: 2.3684e-04 - 9s/epoch - 3ms/step
Epoch 37/100
2985/2985 - 9s - loss: 2.9701e-04 - val_loss: 2.3976e-04 - 9s/epoch - 3ms/step
Epoch 38/100
2985/2985 - 9s - loss: 3.0453e-04 - val_loss: 2.2737e-04 - 9s/epoch - 3ms/step
Epoch 39/100
2985/2985 - 9s - loss: 2.9364e-04 - val_loss: 2.2988e-04 - 9s/epoch - 3ms/step
Epoch 40/100
2985/2985 - 9s - loss: 2.8928e-04 - val_loss: 2.2989e-04 - 9s/epoch - 3ms/step
Epoch 41/100
2985/2985 - 9s - loss: 2.8149e-04 - val_loss: 2.0952e-04 - 9s/epoch - 3ms/step
Epoch 42/100
2985/2985 - 9s - loss: 2.7800e-04 - val_loss: 2.1791e-04 - 9s/epoch - 3ms/step
Epoch 43/100
2985/2985 - 9s - loss: 2.7877e-04 - val_loss: 2.2557e-04 - 9s/epoch - 3ms/step
Epoch 44/100
2985/2985 - 9s - loss: 2.7281e-04 - val_loss: 2.3376e-04 - 9s/epoch - 3ms/step
Epoch 45/100
2985/2985 - 9s - loss: 2.7285e-04 - val_loss: 2.0763e-04 - 9s/epoch - 3ms/step
Epoch 46/100
2985/2985 - 9s - loss: 2.9992e-04 - val_loss: 2.3260e-04 - 9s/epoch - 3ms/step
Epoch 47/100
2985/2985 - 9s - loss: 2.7603e-04 - val_loss: 2.1003e-04 - 9s/epoch - 3ms/step
Epoch 48/100
2985/2985 - 9s - loss: 2.7076e-04 - val_loss: 2.0817e-04 - 9s/epoch - 3ms/step
Epoch 49/100
2985/2985 - 9s - loss: 2.6711e-04 - val_loss: 2.4939e-04 - 9s/epoch - 3ms/step
Epoch 50/100
2985/2985 - 9s - loss: 2.7561e-04 - val_loss: 2.1680e-04 - 9s/epoch - 3ms/step
Epoch 51/100
2985/2985 - 9s - loss: 2.6310e-04 - val_loss: 2.0160e-04 - 9s/epoch - 3ms/step
Epoch 52/100
2985/2985 - 9s - loss: 2.5575e-04 - val_loss: 1.9158e-04 - 9s/epoch - 3ms/step
Epoch 53/100
2985/2985 - 9s - loss: 2.6524e-04 - val_loss: 2.0834e-04 - 9s/epoch - 3ms/step
Epoch 54/100
2985/2985 - 9s - loss: 2.8181e-04 - val_loss: 2.0258e-04 - 9s/epoch - 3ms/step
Epoch 55/100
2985/2985 - 9s - loss: 2.5227e-04 - val_loss: 1.8986e-04 - 9s/epoch - 3ms/step
Epoch 56/100
2985/2985 - 9s - loss: 2.5235e-04 - val_loss: 1.8618e-04 - 9s/epoch - 3ms/step
Epoch 57/100
2985/2985 - 9s - loss: 2.5003e-04 - val_loss: 2.0131e-04 - 9s/epoch - 3ms/step
Epoch 58/100
2985/2985 - 9s - loss: 2.5004e-04 - val_loss: 1.9325e-04 - 9s/epoch - 3ms/step
Epoch 59/100
2985/2985 - 9s - loss: 2.5036e-04 - val_loss: 1.9200e-04 - 9s/epoch - 3ms/step
Epoch 60/100
2985/2985 - 9s - loss: 2.6324e-04 - val_loss: 1.8233e-04 - 9s/epoch - 3ms/step
Epoch 61/100
2985/2985 - 9s - loss: 2.4411e-04 - val_loss: 1.8672e-04 - 9s/epoch - 3ms/step
Epoch 62/100
2985/2985 - 9s - loss: 2.4879e-04 - val_loss: 1.8965e-04 - 9s/epoch - 3ms/step
Epoch 63/100
2985/2985 - 9s - loss: 2.4064e-04 - val_loss: 1.9367e-04 - 9s/epoch - 3ms/step
Epoch 64/100
2985/2985 - 9s - loss: 2.6618e-04 - val_loss: 1.7818e-04 - 9s/epoch - 3ms/step
Epoch 65/100
2985/2985 - 9s - loss: 2.4405e-04 - val_loss: 1.8964e-04 - 9s/epoch - 3ms/step
Epoch 66/100
2985/2985 - 9s - loss: 2.3848e-04 - val_loss: 1.9029e-04 - 9s/epoch - 3ms/step
Epoch 67/100
2985/2985 - 9s - loss: 2.3795e-04 - val_loss: 1.8112e-04 - 9s/epoch - 3ms/step
Epoch 68/100
2985/2985 - 9s - loss: 2.3758e-04 - val_loss: 1.8975e-04 - 9s/epoch - 3ms/step
Epoch 69/100
2985/2985 - 9s - loss: 2.3348e-04 - val_loss: 1.7853e-04 - 9s/epoch - 3ms/step
Epoch 70/100
2985/2985 - 9s - loss: 2.3716e-04 - val_loss: 1.9822e-04 - 9s/epoch - 3ms/step
Epoch 71/100
2985/2985 - 9s - loss: 2.3055e-04 - val_loss: 1.7625e-04 - 9s/epoch - 3ms/step
Epoch 72/100
2985/2985 - 9s - loss: 2.3236e-04 - val_loss: 1.7288e-04 - 9s/epoch - 3ms/step
Epoch 73/100
2985/2985 - 9s - loss: 2.3132e-04 - val_loss: 2.9916e-04 - 9s/epoch - 3ms/step
Epoch 74/100
2985/2985 - 9s - loss: 2.7387e-04 - val_loss: 1.7886e-04 - 9s/epoch - 3ms/step
Epoch 75/100
2985/2985 - 9s - loss: 2.4291e-04 - val_loss: 1.7586e-04 - 9s/epoch - 3ms/step
Epoch 76/100
2985/2985 - 9s - loss: 2.3014e-04 - val_loss: 1.8148e-04 - 9s/epoch - 3ms/step
Epoch 77/100
2985/2985 - 9s - loss: 2.2972e-04 - val_loss: 3.0712e-04 - 9s/epoch - 3ms/step
Epoch 78/100
2985/2985 - 9s - loss: 2.5386e-04 - val_loss: 1.7184e-04 - 9s/epoch - 3ms/step
Epoch 79/100
2985/2985 - 9s - loss: 2.2793e-04 - val_loss: 1.7561e-04 - 9s/epoch - 3ms/step
Epoch 80/100
2985/2985 - 9s - loss: 2.2993e-04 - val_loss: 1.8877e-04 - 9s/epoch - 3ms/step
Epoch 81/100
2985/2985 - 9s - loss: 2.3219e-04 - val_loss: 1.9181e-04 - 9s/epoch - 3ms/step
Epoch 82/100
2985/2985 - 9s - loss: 2.2290e-04 - val_loss: 1.7389e-04 - 9s/epoch - 3ms/step
Epoch 83/100
2985/2985 - 9s - loss: 2.2058e-04 - val_loss: 1.7589e-04 - 9s/epoch - 3ms/step
Epoch 84/100
2985/2985 - 9s - loss: 2.2041e-04 - val_loss: 1.6786e-04 - 9s/epoch - 3ms/step
Epoch 85/100
2985/2985 - 9s - loss: 2.1812e-04 - val_loss: 1.7462e-04 - 9s/epoch - 3ms/step
Epoch 86/100
2985/2985 - 9s - loss: 2.2526e-04 - val_loss: 1.6123e-04 - 9s/epoch - 3ms/step
Epoch 87/100
2985/2985 - 9s - loss: 2.1859e-04 - val_loss: 1.7265e-04 - 9s/epoch - 3ms/step
Epoch 88/100
2985/2985 - 9s - loss: 2.1460e-04 - val_loss: 1.6349e-04 - 9s/epoch - 3ms/step
Epoch 89/100
2985/2985 - 9s - loss: 2.1428e-04 - val_loss: 1.7917e-04 - 9s/epoch - 3ms/step
Epoch 90/100
2985/2985 - 9s - loss: 2.1983e-04 - val_loss: 1.9469e-04 - 9s/epoch - 3ms/step
Epoch 91/100
2985/2985 - 9s - loss: 2.1598e-04 - val_loss: 1.6107e-04 - 9s/epoch - 3ms/step
Epoch 92/100
2985/2985 - 9s - loss: 2.1461e-04 - val_loss: 1.7750e-04 - 9s/epoch - 3ms/step
Epoch 93/100
2985/2985 - 9s - loss: 2.1681e-04 - val_loss: 1.6835e-04 - 9s/epoch - 3ms/step
Epoch 94/100
2985/2985 - 9s - loss: 2.1049e-04 - val_loss: 1.4853e-04 - 9s/epoch - 3ms/step
Epoch 95/100
2985/2985 - 9s - loss: 2.1024e-04 - val_loss: 1.6353e-04 - 9s/epoch - 3ms/step
Epoch 96/100
2985/2985 - 9s - loss: 2.0989e-04 - val_loss: 1.6667e-04 - 9s/epoch - 3ms/step
Epoch 97/100
2985/2985 - 9s - loss: 2.0981e-04 - val_loss: 1.6242e-04 - 9s/epoch - 3ms/step
Epoch 98/100
2985/2985 - 9s - loss: 2.0788e-04 - val_loss: 1.5675e-04 - 9s/epoch - 3ms/step
Epoch 99/100
2985/2985 - 9s - loss: 2.0851e-04 - val_loss: 1.5896e-04 - 9s/epoch - 3ms/step
Epoch 100/100
2985/2985 - 9s - loss: 2.0715e-04 - val_loss: 1.6991e-04 - 9s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00016990800213534385
  1/332 [..............................] - ETA: 28s 49/332 [===>..........................] - ETA: 0s  98/332 [=======>......................] - ETA: 0s147/332 [============>.................] - ETA: 0s196/332 [================>.............] - ETA: 0s245/332 [=====================>........] - ETA: 0s294/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0019019960098971163
cosine 0.0015114226623880967
MAE: 0.0070860614
RMSE: 0.013034868
r2: 0.9889784393137098
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_6 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_8 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2528)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_6 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 632)]             0         
                                                                 
 batch_normalization_7 (Batc  (None, 632)              2528      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 632)               0         
                                                                 
 dense_7 (Dense)             (None, 2528)              1600224   
                                                                 
 batch_normalization_8 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2528)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 32, 100, 0.001, 0.5, 632, 0.0002071523922495544, 0.00016990800213534385, 0.0019019960098971163, 0.0015114226623880967, 0.007086061406880617, 0.01303486805409193, 0.9889784393137098, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_9 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_9 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_11 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2528)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
2985/2985 - 9s - loss: 0.0091 - val_loss: 0.0024 - 9s/epoch - 3ms/step
Epoch 2/100
2985/2985 - 9s - loss: 0.0026 - val_loss: 0.0017 - 9s/epoch - 3ms/step
Epoch 3/100
2985/2985 - 9s - loss: 0.0017 - val_loss: 0.0012 - 9s/epoch - 3ms/step
Epoch 4/100
2985/2985 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 3ms/step
Epoch 5/100
2985/2985 - 9s - loss: 0.0010 - val_loss: 8.2204e-04 - 9s/epoch - 3ms/step
Epoch 6/100
2985/2985 - 9s - loss: 8.9454e-04 - val_loss: 7.1388e-04 - 9s/epoch - 3ms/step
Epoch 7/100
2985/2985 - 9s - loss: 7.9876e-04 - val_loss: 6.3500e-04 - 9s/epoch - 3ms/step
Epoch 8/100
2985/2985 - 9s - loss: 7.2413e-04 - val_loss: 5.9654e-04 - 9s/epoch - 3ms/step
Epoch 9/100
2985/2985 - 9s - loss: 6.8656e-04 - val_loss: 5.4644e-04 - 9s/epoch - 3ms/step
Epoch 10/100
2985/2985 - 9s - loss: 6.9885e-04 - val_loss: 5.2966e-04 - 9s/epoch - 3ms/step
Epoch 11/100
2985/2985 - 9s - loss: 6.1739e-04 - val_loss: 4.8679e-04 - 9s/epoch - 3ms/step
Epoch 12/100
2985/2985 - 9s - loss: 6.0031e-04 - val_loss: 5.3674e-04 - 9s/epoch - 3ms/step
Epoch 13/100
2985/2985 - 9s - loss: 5.8733e-04 - val_loss: 4.6366e-04 - 9s/epoch - 3ms/step
Epoch 14/100
2985/2985 - 9s - loss: 5.5053e-04 - val_loss: 4.5213e-04 - 9s/epoch - 3ms/step
Epoch 15/100
2985/2985 - 9s - loss: 5.3738e-04 - val_loss: 4.3183e-04 - 9s/epoch - 3ms/step
Epoch 16/100
2985/2985 - 9s - loss: 5.1885e-04 - val_loss: 4.5400e-04 - 9s/epoch - 3ms/step
Epoch 17/100
2985/2985 - 9s - loss: 5.0297e-04 - val_loss: 4.0988e-04 - 9s/epoch - 3ms/step
Epoch 18/100
2985/2985 - 9s - loss: 4.8864e-04 - val_loss: 4.1681e-04 - 9s/epoch - 3ms/step
Epoch 19/100
2985/2985 - 9s - loss: 4.7922e-04 - val_loss: 4.3041e-04 - 9s/epoch - 3ms/step
Epoch 20/100
2985/2985 - 9s - loss: 4.7581e-04 - val_loss: 3.8290e-04 - 9s/epoch - 3ms/step
Epoch 21/100
2985/2985 - 9s - loss: 4.5983e-04 - val_loss: 3.5790e-04 - 9s/epoch - 3ms/step
Epoch 22/100
2985/2985 - 9s - loss: 4.5342e-04 - val_loss: 4.0943e-04 - 9s/epoch - 3ms/step
Epoch 23/100
2985/2985 - 9s - loss: 4.4697e-04 - val_loss: 3.6618e-04 - 9s/epoch - 3ms/step
Epoch 24/100
2985/2985 - 9s - loss: 4.4476e-04 - val_loss: 3.7610e-04 - 9s/epoch - 3ms/step
Epoch 25/100
2985/2985 - 9s - loss: 4.2674e-04 - val_loss: 3.5129e-04 - 9s/epoch - 3ms/step
Epoch 26/100
2985/2985 - 9s - loss: 4.2476e-04 - val_loss: 3.2712e-04 - 9s/epoch - 3ms/step
Epoch 27/100
2985/2985 - 9s - loss: 4.4636e-04 - val_loss: 3.3436e-04 - 9s/epoch - 3ms/step
Epoch 28/100
2985/2985 - 9s - loss: 4.2876e-04 - val_loss: 3.4320e-04 - 9s/epoch - 3ms/step
Epoch 29/100
2985/2985 - 9s - loss: 4.1876e-04 - val_loss: 3.1893e-04 - 9s/epoch - 3ms/step
Epoch 30/100
2985/2985 - 9s - loss: 4.0199e-04 - val_loss: 3.2208e-04 - 9s/epoch - 3ms/step
Epoch 31/100
2985/2985 - 9s - loss: 3.9971e-04 - val_loss: 3.0694e-04 - 9s/epoch - 3ms/step
Epoch 32/100
2985/2985 - 9s - loss: 4.1192e-04 - val_loss: 3.1416e-04 - 9s/epoch - 3ms/step
Epoch 33/100
2985/2985 - 9s - loss: 3.9114e-04 - val_loss: 3.0316e-04 - 9s/epoch - 3ms/step
Epoch 34/100
2985/2985 - 9s - loss: 3.8537e-04 - val_loss: 3.0166e-04 - 9s/epoch - 3ms/step
Epoch 35/100
2985/2985 - 9s - loss: 3.9030e-04 - val_loss: 2.9360e-04 - 9s/epoch - 3ms/step
Epoch 36/100
2985/2985 - 9s - loss: 3.9532e-04 - val_loss: 3.0116e-04 - 9s/epoch - 3ms/step
Epoch 37/100
2985/2985 - 9s - loss: 3.8892e-04 - val_loss: 2.9254e-04 - 9s/epoch - 3ms/step
Epoch 38/100
2985/2985 - 9s - loss: 3.7045e-04 - val_loss: 3.0072e-04 - 9s/epoch - 3ms/step
Epoch 39/100
2985/2985 - 9s - loss: 4.4137e-04 - val_loss: 2.9309e-04 - 9s/epoch - 3ms/step
Epoch 40/100
2985/2985 - 9s - loss: 3.7898e-04 - val_loss: 3.1365e-04 - 9s/epoch - 3ms/step
Epoch 41/100
2985/2985 - 9s - loss: 3.6703e-04 - val_loss: 2.7399e-04 - 9s/epoch - 3ms/step
Epoch 42/100
2985/2985 - 9s - loss: 3.6281e-04 - val_loss: 2.8168e-04 - 9s/epoch - 3ms/step
Epoch 43/100
2985/2985 - 9s - loss: 3.7165e-04 - val_loss: 3.2436e-04 - 9s/epoch - 3ms/step
Epoch 44/100
2985/2985 - 9s - loss: 3.5909e-04 - val_loss: 3.1680e-04 - 9s/epoch - 3ms/step
Epoch 45/100
2985/2985 - 9s - loss: 3.6274e-04 - val_loss: 2.7884e-04 - 9s/epoch - 3ms/step
Epoch 46/100
2985/2985 - 9s - loss: 3.5858e-04 - val_loss: 2.7763e-04 - 9s/epoch - 3ms/step
Epoch 47/100
2985/2985 - 9s - loss: 3.4729e-04 - val_loss: 2.7384e-04 - 9s/epoch - 3ms/step
Epoch 48/100
2985/2985 - 9s - loss: 3.5288e-04 - val_loss: 2.7376e-04 - 9s/epoch - 3ms/step
Epoch 49/100
2985/2985 - 9s - loss: 3.4779e-04 - val_loss: 2.9888e-04 - 9s/epoch - 3ms/step
Epoch 50/100
2985/2985 - 9s - loss: 3.5375e-04 - val_loss: 3.0695e-04 - 9s/epoch - 3ms/step
Epoch 51/100
2985/2985 - 9s - loss: 3.5503e-04 - val_loss: 2.5808e-04 - 9s/epoch - 3ms/step
Epoch 52/100
2985/2985 - 9s - loss: 3.3844e-04 - val_loss: 2.5558e-04 - 9s/epoch - 3ms/step
Epoch 53/100
2985/2985 - 9s - loss: 3.4090e-04 - val_loss: 2.6653e-04 - 9s/epoch - 3ms/step
Epoch 54/100
2985/2985 - 9s - loss: 3.3231e-04 - val_loss: 2.6594e-04 - 9s/epoch - 3ms/step
Epoch 55/100
2985/2985 - 9s - loss: 3.4780e-04 - val_loss: 2.5250e-04 - 9s/epoch - 3ms/step
Epoch 56/100
2985/2985 - 9s - loss: 3.4478e-04 - val_loss: 2.5371e-04 - 9s/epoch - 3ms/step
Epoch 57/100
2985/2985 - 9s - loss: 3.3307e-04 - val_loss: 2.6091e-04 - 9s/epoch - 3ms/step
Epoch 58/100
2985/2985 - 9s - loss: 3.2863e-04 - val_loss: 2.5198e-04 - 9s/epoch - 3ms/step
Epoch 59/100
2985/2985 - 9s - loss: 3.2471e-04 - val_loss: 2.5418e-04 - 9s/epoch - 3ms/step
Epoch 60/100
2985/2985 - 9s - loss: 3.5169e-04 - val_loss: 2.5176e-04 - 9s/epoch - 3ms/step
Epoch 61/100
2985/2985 - 9s - loss: 3.2867e-04 - val_loss: 2.4997e-04 - 9s/epoch - 3ms/step
Epoch 62/100
2985/2985 - 9s - loss: 3.2173e-04 - val_loss: 2.6854e-04 - 9s/epoch - 3ms/step
Epoch 63/100
2985/2985 - 9s - loss: 3.5104e-04 - val_loss: 2.5678e-04 - 9s/epoch - 3ms/step
Epoch 64/100
2985/2985 - 9s - loss: 3.2155e-04 - val_loss: 2.3788e-04 - 9s/epoch - 3ms/step
Epoch 65/100
2985/2985 - 9s - loss: 3.1923e-04 - val_loss: 2.4806e-04 - 9s/epoch - 3ms/step
Epoch 66/100
2985/2985 - 9s - loss: 3.1632e-04 - val_loss: 2.4733e-04 - 9s/epoch - 3ms/step
Epoch 67/100
2985/2985 - 9s - loss: 3.1843e-04 - val_loss: 2.3980e-04 - 9s/epoch - 3ms/step
Epoch 68/100
2985/2985 - 9s - loss: 3.1527e-04 - val_loss: 3.3824e-04 - 9s/epoch - 3ms/step
Epoch 69/100
2985/2985 - 9s - loss: 3.3763e-04 - val_loss: 2.5774e-04 - 9s/epoch - 3ms/step
Epoch 70/100
2985/2985 - 9s - loss: 3.1586e-04 - val_loss: 2.8833e-04 - 9s/epoch - 3ms/step
Epoch 71/100
2985/2985 - 9s - loss: 3.1445e-04 - val_loss: 2.3627e-04 - 9s/epoch - 3ms/step
Epoch 72/100
2985/2985 - 9s - loss: 3.0933e-04 - val_loss: 2.5342e-04 - 9s/epoch - 3ms/step
Epoch 73/100
2985/2985 - 9s - loss: 3.1277e-04 - val_loss: 2.3055e-04 - 9s/epoch - 3ms/step
Epoch 74/100
2985/2985 - 9s - loss: 4.5675e-04 - val_loss: 2.9603e-04 - 9s/epoch - 3ms/step
Epoch 75/100
2985/2985 - 9s - loss: 3.5644e-04 - val_loss: 2.5966e-04 - 9s/epoch - 3ms/step
Epoch 76/100
2985/2985 - 9s - loss: 3.2715e-04 - val_loss: 2.5022e-04 - 9s/epoch - 3ms/step
Epoch 77/100
2985/2985 - 9s - loss: 3.4564e-04 - val_loss: 8.5147e-04 - 9s/epoch - 3ms/step
Epoch 78/100
2985/2985 - 9s - loss: 4.6354e-04 - val_loss: 2.7196e-04 - 9s/epoch - 3ms/step
Epoch 79/100
2985/2985 - 9s - loss: 3.5713e-04 - val_loss: 2.7657e-04 - 9s/epoch - 3ms/step
Epoch 80/100
2985/2985 - 9s - loss: 3.5681e-04 - val_loss: 2.6644e-04 - 9s/epoch - 3ms/step
Epoch 81/100
2985/2985 - 9s - loss: 3.3489e-04 - val_loss: 3.1017e-04 - 9s/epoch - 3ms/step
Epoch 82/100
2985/2985 - 9s - loss: 3.2970e-04 - val_loss: 2.4764e-04 - 9s/epoch - 3ms/step
Epoch 83/100
2985/2985 - 9s - loss: 3.3535e-04 - val_loss: 2.5817e-04 - 9s/epoch - 3ms/step
Epoch 84/100
2985/2985 - 9s - loss: 3.2402e-04 - val_loss: 2.4659e-04 - 9s/epoch - 3ms/step
Epoch 85/100
2985/2985 - 9s - loss: 3.2225e-04 - val_loss: 2.3024e-04 - 9s/epoch - 3ms/step
Epoch 86/100
2985/2985 - 9s - loss: 3.2014e-04 - val_loss: 2.3603e-04 - 9s/epoch - 3ms/step
Epoch 87/100
2985/2985 - 9s - loss: 3.1719e-04 - val_loss: 2.4981e-04 - 9s/epoch - 3ms/step
Epoch 88/100
2985/2985 - 9s - loss: 3.1509e-04 - val_loss: 2.4377e-04 - 9s/epoch - 3ms/step
Epoch 89/100
2985/2985 - 9s - loss: 3.4098e-04 - val_loss: 2.6332e-04 - 9s/epoch - 3ms/step
Epoch 90/100
2985/2985 - 9s - loss: 3.2548e-04 - val_loss: 2.4457e-04 - 9s/epoch - 3ms/step
Epoch 91/100
2985/2985 - 9s - loss: 3.1007e-04 - val_loss: 2.4094e-04 - 9s/epoch - 3ms/step
Epoch 92/100
2985/2985 - 9s - loss: 3.1418e-04 - val_loss: 3.6888e-04 - 9s/epoch - 3ms/step
Epoch 93/100
2985/2985 - 9s - loss: 3.4392e-04 - val_loss: 2.4919e-04 - 9s/epoch - 3ms/step
Epoch 94/100
2985/2985 - 9s - loss: 3.0928e-04 - val_loss: 2.2910e-04 - 9s/epoch - 3ms/step
Epoch 95/100
2985/2985 - 9s - loss: 3.0919e-04 - val_loss: 2.4166e-04 - 9s/epoch - 3ms/step
Epoch 96/100
2985/2985 - 9s - loss: 3.0897e-04 - val_loss: 2.4973e-04 - 9s/epoch - 3ms/step
Epoch 97/100
2985/2985 - 9s - loss: 3.0530e-04 - val_loss: 2.3786e-04 - 9s/epoch - 3ms/step
Epoch 98/100
2985/2985 - 9s - loss: 3.0245e-04 - val_loss: 2.3238e-04 - 9s/epoch - 3ms/step
Epoch 99/100
2985/2985 - 9s - loss: 2.9998e-04 - val_loss: 2.3211e-04 - 9s/epoch - 3ms/step
Epoch 100/100
2985/2985 - 9s - loss: 2.9804e-04 - val_loss: 2.3570e-04 - 9s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0002356989571126178
  1/332 [..............................] - ETA: 27s 50/332 [===>..........................] - ETA: 0s  99/332 [=======>......................] - ETA: 0s148/332 [============>.................] - ETA: 0s197/332 [================>.............] - ETA: 0s246/332 [=====================>........] - ETA: 0s294/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0026861144502882774
cosine 0.0021260967748460793
MAE: 0.008417151
RMSE: 0.015352482
r2: 0.9847097279647962
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_9 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_11 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2528)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 2528)              3197920   
                                                                 
 batch_normalization_9 (Batc  (None, 2528)             10112     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_10 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 632)               0         
                                                                 
 dense_10 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_11 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2528)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 32, 100, 0.002, 0.5, 632, 0.0002980435674544424, 0.0002356989571126178, 0.0026861144502882774, 0.0021260967748460793, 0.008417150937020779, 0.015352481976151466, 0.9847097279647962, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_12 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_12 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_14 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2528)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
2985/2985 - 9s - loss: 0.0081 - val_loss: 0.0033 - 9s/epoch - 3ms/step
Epoch 2/200
2985/2985 - 9s - loss: 0.0030 - val_loss: 0.0024 - 9s/epoch - 3ms/step
Epoch 3/200
2985/2985 - 9s - loss: 0.0025 - val_loss: 0.0014 - 9s/epoch - 3ms/step
Epoch 4/200
2985/2985 - 9s - loss: 0.0020 - val_loss: 0.0013 - 9s/epoch - 3ms/step
Epoch 5/200
2985/2985 - 9s - loss: 0.0016 - val_loss: 9.8231e-04 - 9s/epoch - 3ms/step
Epoch 6/200
2985/2985 - 9s - loss: 0.0013 - val_loss: 0.0021 - 9s/epoch - 3ms/step
Epoch 7/200
2985/2985 - 9s - loss: 0.0011 - val_loss: 7.4246e-04 - 9s/epoch - 3ms/step
Epoch 8/200
2985/2985 - 9s - loss: 8.9336e-04 - val_loss: 6.8239e-04 - 9s/epoch - 3ms/step
Epoch 9/200
2985/2985 - 9s - loss: 7.9110e-04 - val_loss: 5.5398e-04 - 9s/epoch - 3ms/step
Epoch 10/200
2985/2985 - 9s - loss: 6.9685e-04 - val_loss: 5.4794e-04 - 9s/epoch - 3ms/step
Epoch 11/200
2985/2985 - 9s - loss: 6.5108e-04 - val_loss: 5.1309e-04 - 9s/epoch - 3ms/step
Epoch 12/200
2985/2985 - 9s - loss: 5.8249e-04 - val_loss: 5.3040e-04 - 9s/epoch - 3ms/step
Epoch 13/200
2985/2985 - 9s - loss: 5.5450e-04 - val_loss: 5.2383e-04 - 9s/epoch - 3ms/step
Epoch 14/200
2985/2985 - 9s - loss: 5.0964e-04 - val_loss: 4.1520e-04 - 9s/epoch - 3ms/step
Epoch 15/200
2985/2985 - 9s - loss: 4.8844e-04 - val_loss: 4.0241e-04 - 9s/epoch - 3ms/step
Epoch 16/200
2985/2985 - 9s - loss: 4.6271e-04 - val_loss: 3.9974e-04 - 9s/epoch - 3ms/step
Epoch 17/200
2985/2985 - 9s - loss: 4.3479e-04 - val_loss: 3.6406e-04 - 9s/epoch - 3ms/step
Epoch 18/200
2985/2985 - 9s - loss: 4.1978e-04 - val_loss: 3.9087e-04 - 9s/epoch - 3ms/step
Epoch 19/200
2985/2985 - 9s - loss: 4.0662e-04 - val_loss: 5.0160e-04 - 9s/epoch - 3ms/step
Epoch 20/200
2985/2985 - 9s - loss: 4.0020e-04 - val_loss: 3.3365e-04 - 9s/epoch - 3ms/step
Epoch 21/200
2985/2985 - 9s - loss: 3.8199e-04 - val_loss: 3.2391e-04 - 9s/epoch - 3ms/step
Epoch 22/200
2985/2985 - 9s - loss: 3.6599e-04 - val_loss: 3.7889e-04 - 9s/epoch - 3ms/step
Epoch 23/200
2985/2985 - 9s - loss: 3.5647e-04 - val_loss: 3.0886e-04 - 9s/epoch - 3ms/step
Epoch 24/200
2985/2985 - 9s - loss: 3.5728e-04 - val_loss: 3.1670e-04 - 9s/epoch - 3ms/step
Epoch 25/200
2985/2985 - 9s - loss: 3.3839e-04 - val_loss: 3.0041e-04 - 9s/epoch - 3ms/step
Epoch 26/200
2985/2985 - 9s - loss: 3.2720e-04 - val_loss: 2.7422e-04 - 9s/epoch - 3ms/step
Epoch 27/200
2985/2985 - 9s - loss: 3.3583e-04 - val_loss: 2.8337e-04 - 9s/epoch - 3ms/step
Epoch 28/200
2985/2985 - 9s - loss: 3.2106e-04 - val_loss: 2.5911e-04 - 9s/epoch - 3ms/step
Epoch 29/200
2985/2985 - 9s - loss: 3.1031e-04 - val_loss: 3.1399e-04 - 9s/epoch - 3ms/step
Epoch 30/200
2985/2985 - 9s - loss: 3.2072e-04 - val_loss: 2.6130e-04 - 9s/epoch - 3ms/step
Epoch 31/200
2985/2985 - 9s - loss: 2.9869e-04 - val_loss: 2.6072e-04 - 9s/epoch - 3ms/step
Epoch 32/200
2985/2985 - 9s - loss: 2.9312e-04 - val_loss: 2.3905e-04 - 9s/epoch - 3ms/step
Epoch 33/200
2985/2985 - 9s - loss: 2.8834e-04 - val_loss: 2.4092e-04 - 9s/epoch - 3ms/step
Epoch 34/200
2985/2985 - 9s - loss: 2.8801e-04 - val_loss: 2.4854e-04 - 9s/epoch - 3ms/step
Epoch 35/200
2985/2985 - 9s - loss: 2.7837e-04 - val_loss: 2.2801e-04 - 9s/epoch - 3ms/step
Epoch 36/200
2985/2985 - 9s - loss: 2.7333e-04 - val_loss: 2.3225e-04 - 9s/epoch - 3ms/step
Epoch 37/200
2985/2985 - 9s - loss: 2.7938e-04 - val_loss: 2.3470e-04 - 9s/epoch - 3ms/step
Epoch 38/200
2985/2985 - 9s - loss: 2.6461e-04 - val_loss: 2.1629e-04 - 9s/epoch - 3ms/step
Epoch 39/200
2985/2985 - 9s - loss: 2.9691e-04 - val_loss: 2.1494e-04 - 9s/epoch - 3ms/step
Epoch 40/200
2985/2985 - 9s - loss: 2.6053e-04 - val_loss: 2.3638e-04 - 9s/epoch - 3ms/step
Epoch 41/200
2985/2985 - 9s - loss: 2.6367e-04 - val_loss: 4.3325e-04 - 9s/epoch - 3ms/step
Epoch 42/200
2985/2985 - 9s - loss: 2.5774e-04 - val_loss: 2.1165e-04 - 9s/epoch - 3ms/step
Epoch 43/200
2985/2985 - 9s - loss: 2.5131e-04 - val_loss: 2.2258e-04 - 9s/epoch - 3ms/step
Epoch 44/200
2985/2985 - 9s - loss: 2.4873e-04 - val_loss: 2.9066e-04 - 9s/epoch - 3ms/step
Epoch 45/200
2985/2985 - 9s - loss: 2.6129e-04 - val_loss: 2.0749e-04 - 9s/epoch - 3ms/step
Epoch 46/200
2985/2985 - 9s - loss: 2.4487e-04 - val_loss: 2.0995e-04 - 9s/epoch - 3ms/step
Epoch 47/200
2985/2985 - 9s - loss: 2.7433e-04 - val_loss: 2.0824e-04 - 9s/epoch - 3ms/step
Epoch 48/200
2985/2985 - 9s - loss: 2.5329e-04 - val_loss: 2.1193e-04 - 9s/epoch - 3ms/step
Epoch 49/200
2985/2985 - 9s - loss: 2.4055e-04 - val_loss: 2.7096e-04 - 9s/epoch - 3ms/step
Epoch 50/200
2985/2985 - 9s - loss: 2.5077e-04 - val_loss: 2.3017e-04 - 9s/epoch - 3ms/step
Epoch 51/200
2985/2985 - 9s - loss: 2.3777e-04 - val_loss: 2.1136e-04 - 9s/epoch - 3ms/step
Epoch 52/200
2985/2985 - 9s - loss: 2.4101e-04 - val_loss: 1.8990e-04 - 9s/epoch - 3ms/step
Epoch 53/200
2985/2985 - 9s - loss: 2.3008e-04 - val_loss: 1.9819e-04 - 9s/epoch - 3ms/step
Epoch 54/200
2985/2985 - 9s - loss: 2.2626e-04 - val_loss: 2.0433e-04 - 9s/epoch - 3ms/step
Epoch 55/200
2985/2985 - 9s - loss: 2.2344e-04 - val_loss: 1.8813e-04 - 9s/epoch - 3ms/step
Epoch 56/200
2985/2985 - 9s - loss: 2.2343e-04 - val_loss: 1.9143e-04 - 9s/epoch - 3ms/step
Epoch 57/200
2985/2985 - 9s - loss: 2.2297e-04 - val_loss: 1.8945e-04 - 9s/epoch - 3ms/step
Epoch 58/200
2985/2985 - 9s - loss: 2.2639e-04 - val_loss: 1.9060e-04 - 9s/epoch - 3ms/step
Epoch 59/200
2985/2985 - 9s - loss: 2.1408e-04 - val_loss: 1.7986e-04 - 9s/epoch - 3ms/step
Epoch 60/200
2985/2985 - 9s - loss: 2.2640e-04 - val_loss: 2.0352e-04 - 9s/epoch - 3ms/step
Epoch 61/200
2985/2985 - 9s - loss: 2.2392e-04 - val_loss: 1.6930e-04 - 9s/epoch - 3ms/step
Epoch 62/200
2985/2985 - 9s - loss: 2.2470e-04 - val_loss: 2.0942e-04 - 9s/epoch - 3ms/step
Epoch 63/200
2985/2985 - 9s - loss: 2.1046e-04 - val_loss: 1.8963e-04 - 9s/epoch - 3ms/step
Epoch 64/200
2985/2985 - 9s - loss: 2.1076e-04 - val_loss: 1.8619e-04 - 9s/epoch - 3ms/step
Epoch 65/200
2985/2985 - 9s - loss: 2.1303e-04 - val_loss: 1.9263e-04 - 9s/epoch - 3ms/step
Epoch 66/200
2985/2985 - 9s - loss: 2.0881e-04 - val_loss: 1.8045e-04 - 9s/epoch - 3ms/step
Epoch 67/200
2985/2985 - 9s - loss: 2.0947e-04 - val_loss: 1.7591e-04 - 9s/epoch - 3ms/step
Epoch 68/200
2985/2985 - 9s - loss: 2.1738e-04 - val_loss: 1.7325e-04 - 9s/epoch - 3ms/step
Epoch 69/200
2985/2985 - 9s - loss: 2.0426e-04 - val_loss: 1.7724e-04 - 9s/epoch - 3ms/step
Epoch 70/200
2985/2985 - 9s - loss: 2.0164e-04 - val_loss: 1.8257e-04 - 9s/epoch - 3ms/step
Epoch 71/200
2985/2985 - 9s - loss: 2.0598e-04 - val_loss: 1.7472e-04 - 9s/epoch - 3ms/step
Epoch 72/200
2985/2985 - 9s - loss: 2.0294e-04 - val_loss: 1.7529e-04 - 9s/epoch - 3ms/step
Epoch 73/200
2985/2985 - 9s - loss: 2.1044e-04 - val_loss: 2.1559e-04 - 9s/epoch - 3ms/step
Epoch 74/200
2985/2985 - 9s - loss: 2.1012e-04 - val_loss: 1.8235e-04 - 9s/epoch - 3ms/step
Epoch 75/200
2985/2985 - 9s - loss: 1.9975e-04 - val_loss: 1.8317e-04 - 9s/epoch - 3ms/step
Epoch 76/200
2985/2985 - 9s - loss: 1.9609e-04 - val_loss: 1.8925e-04 - 9s/epoch - 3ms/step
Epoch 77/200
2985/2985 - 9s - loss: 1.9767e-04 - val_loss: 2.9868e-04 - 9s/epoch - 3ms/step
Epoch 78/200
2985/2985 - 9s - loss: 2.1033e-04 - val_loss: 1.6370e-04 - 9s/epoch - 3ms/step
Epoch 79/200
2985/2985 - 9s - loss: 1.9534e-04 - val_loss: 1.8645e-04 - 9s/epoch - 3ms/step
Epoch 80/200
2985/2985 - 9s - loss: 1.9955e-04 - val_loss: 1.9227e-04 - 9s/epoch - 3ms/step
Epoch 81/200
2985/2985 - 9s - loss: 1.9043e-04 - val_loss: 2.2766e-04 - 9s/epoch - 3ms/step
Epoch 82/200
2985/2985 - 9s - loss: 1.9129e-04 - val_loss: 1.6479e-04 - 9s/epoch - 3ms/step
Epoch 83/200
2985/2985 - 9s - loss: 1.9201e-04 - val_loss: 1.8806e-04 - 9s/epoch - 3ms/step
Epoch 84/200
2985/2985 - 9s - loss: 1.8698e-04 - val_loss: 1.6849e-04 - 9s/epoch - 3ms/step
Epoch 85/200
2985/2985 - 9s - loss: 1.9780e-04 - val_loss: 1.5514e-04 - 9s/epoch - 3ms/step
Epoch 86/200
2985/2985 - 9s - loss: 1.8695e-04 - val_loss: 1.6898e-04 - 9s/epoch - 3ms/step
Epoch 87/200
2985/2985 - 9s - loss: 1.8755e-04 - val_loss: 1.6455e-04 - 9s/epoch - 3ms/step
Epoch 88/200
2985/2985 - 9s - loss: 1.8512e-04 - val_loss: 1.5814e-04 - 9s/epoch - 3ms/step
Epoch 89/200
2985/2985 - 9s - loss: 1.8410e-04 - val_loss: 1.7541e-04 - 9s/epoch - 3ms/step
Epoch 90/200
2985/2985 - 9s - loss: 1.8213e-04 - val_loss: 1.7955e-04 - 9s/epoch - 3ms/step
Epoch 91/200
2985/2985 - 9s - loss: 1.8492e-04 - val_loss: 1.5994e-04 - 9s/epoch - 3ms/step
Epoch 92/200
2985/2985 - 9s - loss: 1.8385e-04 - val_loss: 2.8781e-04 - 9s/epoch - 3ms/step
Epoch 93/200
2985/2985 - 9s - loss: 2.1631e-04 - val_loss: 1.6348e-04 - 9s/epoch - 3ms/step
Epoch 94/200
2985/2985 - 9s - loss: 1.8172e-04 - val_loss: 1.5352e-04 - 9s/epoch - 3ms/step
Epoch 95/200
2985/2985 - 9s - loss: 1.9217e-04 - val_loss: 2.3388e-04 - 9s/epoch - 3ms/step
Epoch 96/200
2985/2985 - 9s - loss: 1.8567e-04 - val_loss: 1.7695e-04 - 9s/epoch - 3ms/step
Epoch 97/200
2985/2985 - 9s - loss: 1.8012e-04 - val_loss: 1.7604e-04 - 9s/epoch - 3ms/step
Epoch 98/200
2985/2985 - 9s - loss: 1.8251e-04 - val_loss: 2.4878e-04 - 9s/epoch - 3ms/step
Epoch 99/200
2985/2985 - 9s - loss: 1.9862e-04 - val_loss: 1.5099e-04 - 9s/epoch - 3ms/step
Epoch 100/200
2985/2985 - 9s - loss: 1.8300e-04 - val_loss: 1.5608e-04 - 9s/epoch - 3ms/step
Epoch 101/200
2985/2985 - 9s - loss: 1.7694e-04 - val_loss: 1.7093e-04 - 9s/epoch - 3ms/step
Epoch 102/200
2985/2985 - 9s - loss: 1.8099e-04 - val_loss: 1.6879e-04 - 9s/epoch - 3ms/step
Epoch 103/200
2985/2985 - 9s - loss: 1.8049e-04 - val_loss: 1.9328e-04 - 9s/epoch - 3ms/step
Epoch 104/200
2985/2985 - 9s - loss: 1.7528e-04 - val_loss: 1.9865e-04 - 9s/epoch - 3ms/step
Epoch 105/200
2985/2985 - 9s - loss: 1.8013e-04 - val_loss: 1.7895e-04 - 9s/epoch - 3ms/step
Epoch 106/200
2985/2985 - 9s - loss: 1.7386e-04 - val_loss: 1.5686e-04 - 9s/epoch - 3ms/step
Epoch 107/200
2985/2985 - 9s - loss: 1.7264e-04 - val_loss: 1.5500e-04 - 9s/epoch - 3ms/step
Epoch 108/200
2985/2985 - 9s - loss: 1.7320e-04 - val_loss: 1.7509e-04 - 9s/epoch - 3ms/step
Epoch 109/200
2985/2985 - 9s - loss: 1.7228e-04 - val_loss: 1.5920e-04 - 9s/epoch - 3ms/step
Epoch 110/200
2985/2985 - 9s - loss: 1.7305e-04 - val_loss: 1.5878e-04 - 9s/epoch - 3ms/step
Epoch 111/200
2985/2985 - 9s - loss: 1.6947e-04 - val_loss: 1.5957e-04 - 9s/epoch - 3ms/step
Epoch 112/200
2985/2985 - 9s - loss: 1.7064e-04 - val_loss: 1.5996e-04 - 9s/epoch - 3ms/step
Epoch 113/200
2985/2985 - 9s - loss: 1.6857e-04 - val_loss: 2.0564e-04 - 9s/epoch - 3ms/step
Epoch 114/200
2985/2985 - 9s - loss: 1.8091e-04 - val_loss: 1.4828e-04 - 9s/epoch - 3ms/step
Epoch 115/200
2985/2985 - 9s - loss: 1.6872e-04 - val_loss: 2.2221e-04 - 9s/epoch - 3ms/step
Epoch 116/200
2985/2985 - 9s - loss: 1.7539e-04 - val_loss: 1.5358e-04 - 9s/epoch - 3ms/step
Epoch 117/200
2985/2985 - 9s - loss: 1.6818e-04 - val_loss: 1.4020e-04 - 9s/epoch - 3ms/step
Epoch 118/200
2985/2985 - 9s - loss: 1.6791e-04 - val_loss: 1.5623e-04 - 9s/epoch - 3ms/step
Epoch 119/200
2985/2985 - 9s - loss: 1.6557e-04 - val_loss: 1.5326e-04 - 9s/epoch - 3ms/step
Epoch 120/200
2985/2985 - 9s - loss: 1.6433e-04 - val_loss: 1.5321e-04 - 9s/epoch - 3ms/step
Epoch 121/200
2985/2985 - 9s - loss: 1.6783e-04 - val_loss: 1.7698e-04 - 9s/epoch - 3ms/step
Epoch 122/200
2985/2985 - 9s - loss: 1.6795e-04 - val_loss: 1.4504e-04 - 9s/epoch - 3ms/step
Epoch 123/200
2985/2985 - 9s - loss: 1.6389e-04 - val_loss: 1.7076e-04 - 9s/epoch - 3ms/step
Epoch 124/200
2985/2985 - 9s - loss: 1.6406e-04 - val_loss: 1.6634e-04 - 9s/epoch - 3ms/step
Epoch 125/200
2985/2985 - 9s - loss: 1.6572e-04 - val_loss: 1.3964e-04 - 9s/epoch - 3ms/step
Epoch 126/200
2985/2985 - 9s - loss: 1.6579e-04 - val_loss: 1.5292e-04 - 9s/epoch - 3ms/step
Epoch 127/200
2985/2985 - 9s - loss: 1.6233e-04 - val_loss: 1.4655e-04 - 9s/epoch - 3ms/step
Epoch 128/200
2985/2985 - 9s - loss: 1.6025e-04 - val_loss: 1.4263e-04 - 9s/epoch - 3ms/step
Epoch 129/200
2985/2985 - 9s - loss: 1.6769e-04 - val_loss: 1.6445e-04 - 9s/epoch - 3ms/step
Epoch 130/200
2985/2985 - 9s - loss: 1.6330e-04 - val_loss: 1.4596e-04 - 9s/epoch - 3ms/step
Epoch 131/200
2985/2985 - 9s - loss: 1.6001e-04 - val_loss: 1.4802e-04 - 9s/epoch - 3ms/step
Epoch 132/200
2985/2985 - 9s - loss: 1.5996e-04 - val_loss: 1.4632e-04 - 9s/epoch - 3ms/step
Epoch 133/200
2985/2985 - 9s - loss: 1.5834e-04 - val_loss: 1.4647e-04 - 9s/epoch - 3ms/step
Epoch 134/200
2985/2985 - 9s - loss: 1.5985e-04 - val_loss: 1.6878e-04 - 9s/epoch - 3ms/step
Epoch 135/200
2985/2985 - 9s - loss: 1.5979e-04 - val_loss: 1.3737e-04 - 9s/epoch - 3ms/step
Epoch 136/200
2985/2985 - 9s - loss: 1.6197e-04 - val_loss: 1.3628e-04 - 9s/epoch - 3ms/step
Epoch 137/200
2985/2985 - 9s - loss: 1.6123e-04 - val_loss: 1.4934e-04 - 9s/epoch - 3ms/step
Epoch 138/200
2985/2985 - 9s - loss: 1.5874e-04 - val_loss: 1.6396e-04 - 9s/epoch - 3ms/step
Epoch 139/200
2985/2985 - 9s - loss: 1.6134e-04 - val_loss: 1.5051e-04 - 9s/epoch - 3ms/step
Epoch 140/200
2985/2985 - 9s - loss: 1.5902e-04 - val_loss: 1.4646e-04 - 9s/epoch - 3ms/step
Epoch 141/200
2985/2985 - 9s - loss: 1.5665e-04 - val_loss: 2.3392e-04 - 9s/epoch - 3ms/step
Epoch 142/200
2985/2985 - 9s - loss: 1.7091e-04 - val_loss: 1.5359e-04 - 9s/epoch - 3ms/step
Epoch 143/200
2985/2985 - 9s - loss: 1.5756e-04 - val_loss: 1.3900e-04 - 9s/epoch - 3ms/step
Epoch 144/200
2985/2985 - 9s - loss: 1.5972e-04 - val_loss: 1.4405e-04 - 9s/epoch - 3ms/step
Epoch 145/200
2985/2985 - 9s - loss: 1.5585e-04 - val_loss: 1.3053e-04 - 9s/epoch - 3ms/step
Epoch 146/200
2985/2985 - 9s - loss: 1.5581e-04 - val_loss: 1.4911e-04 - 9s/epoch - 3ms/step
Epoch 147/200
2985/2985 - 9s - loss: 1.5607e-04 - val_loss: 1.5409e-04 - 9s/epoch - 3ms/step
Epoch 148/200
2985/2985 - 9s - loss: 1.6385e-04 - val_loss: 1.6683e-04 - 9s/epoch - 3ms/step
Epoch 149/200
2985/2985 - 9s - loss: 1.5411e-04 - val_loss: 1.3564e-04 - 9s/epoch - 3ms/step
Epoch 150/200
2985/2985 - 9s - loss: 1.5709e-04 - val_loss: 1.3924e-04 - 9s/epoch - 3ms/step
Epoch 151/200
2985/2985 - 9s - loss: 1.5252e-04 - val_loss: 1.4264e-04 - 9s/epoch - 3ms/step
Epoch 152/200
2985/2985 - 9s - loss: 1.6085e-04 - val_loss: 1.3929e-04 - 9s/epoch - 3ms/step
Epoch 153/200
2985/2985 - 9s - loss: 1.5359e-04 - val_loss: 1.4586e-04 - 9s/epoch - 3ms/step
Epoch 154/200
2985/2985 - 9s - loss: 1.5360e-04 - val_loss: 1.3646e-04 - 9s/epoch - 3ms/step
Epoch 155/200
2985/2985 - 9s - loss: 1.5050e-04 - val_loss: 1.7234e-04 - 9s/epoch - 3ms/step
Epoch 156/200
2985/2985 - 9s - loss: 1.5349e-04 - val_loss: 1.4384e-04 - 9s/epoch - 3ms/step
Epoch 157/200
2985/2985 - 9s - loss: 1.5684e-04 - val_loss: 1.4863e-04 - 9s/epoch - 3ms/step
Epoch 158/200
2985/2985 - 9s - loss: 1.5266e-04 - val_loss: 1.4576e-04 - 9s/epoch - 3ms/step
Epoch 159/200
2985/2985 - 9s - loss: 1.5064e-04 - val_loss: 1.3483e-04 - 9s/epoch - 3ms/step
Epoch 160/200
2985/2985 - 9s - loss: 1.5273e-04 - val_loss: 1.3209e-04 - 9s/epoch - 3ms/step
Epoch 161/200
2985/2985 - 9s - loss: 1.5163e-04 - val_loss: 1.5180e-04 - 9s/epoch - 3ms/step
Epoch 162/200
2985/2985 - 9s - loss: 1.5041e-04 - val_loss: 1.7060e-04 - 9s/epoch - 3ms/step
Epoch 163/200
2985/2985 - 9s - loss: 1.4937e-04 - val_loss: 1.5258e-04 - 9s/epoch - 3ms/step
Epoch 164/200
2985/2985 - 9s - loss: 1.5326e-04 - val_loss: 1.5375e-04 - 9s/epoch - 3ms/step
Epoch 165/200
2985/2985 - 9s - loss: 1.4854e-04 - val_loss: 1.4394e-04 - 9s/epoch - 3ms/step
Epoch 166/200
2985/2985 - 9s - loss: 1.4791e-04 - val_loss: 1.4680e-04 - 9s/epoch - 3ms/step
Epoch 167/200
2985/2985 - 9s - loss: 1.5593e-04 - val_loss: 1.3559e-04 - 9s/epoch - 3ms/step
Epoch 168/200
2985/2985 - 9s - loss: 1.5030e-04 - val_loss: 1.4306e-04 - 9s/epoch - 3ms/step
Epoch 169/200
2985/2985 - 9s - loss: 1.5391e-04 - val_loss: 1.4599e-04 - 9s/epoch - 3ms/step
Epoch 170/200
2985/2985 - 9s - loss: 1.5004e-04 - val_loss: 1.4393e-04 - 9s/epoch - 3ms/step
Epoch 171/200
2985/2985 - 9s - loss: 1.4781e-04 - val_loss: 1.3491e-04 - 9s/epoch - 3ms/step
Epoch 172/200
2985/2985 - 9s - loss: 1.4774e-04 - val_loss: 1.3415e-04 - 9s/epoch - 3ms/step
Epoch 173/200
2985/2985 - 9s - loss: 1.4820e-04 - val_loss: 1.3564e-04 - 9s/epoch - 3ms/step
Epoch 174/200
2985/2985 - 9s - loss: 1.5020e-04 - val_loss: 1.2281e-04 - 9s/epoch - 3ms/step
Epoch 175/200
2985/2985 - 9s - loss: 1.4807e-04 - val_loss: 1.3600e-04 - 9s/epoch - 3ms/step
Epoch 176/200
2985/2985 - 9s - loss: 1.4746e-04 - val_loss: 1.3483e-04 - 9s/epoch - 3ms/step
Epoch 177/200
2985/2985 - 9s - loss: 1.4734e-04 - val_loss: 1.2366e-04 - 9s/epoch - 3ms/step
Epoch 178/200
2985/2985 - 9s - loss: 1.4612e-04 - val_loss: 1.2749e-04 - 9s/epoch - 3ms/step
Epoch 179/200
2985/2985 - 9s - loss: 1.4751e-04 - val_loss: 1.3112e-04 - 9s/epoch - 3ms/step
Epoch 180/200
2985/2985 - 9s - loss: 1.4642e-04 - val_loss: 1.3648e-04 - 9s/epoch - 3ms/step
Epoch 181/200
2985/2985 - 9s - loss: 1.4580e-04 - val_loss: 1.3377e-04 - 9s/epoch - 3ms/step
Epoch 182/200
2985/2985 - 9s - loss: 1.4849e-04 - val_loss: 1.5007e-04 - 9s/epoch - 3ms/step
Epoch 183/200
2985/2985 - 9s - loss: 1.4709e-04 - val_loss: 1.4039e-04 - 9s/epoch - 3ms/step
Epoch 184/200
2985/2985 - 9s - loss: 1.4989e-04 - val_loss: 1.3185e-04 - 9s/epoch - 3ms/step
Epoch 185/200
2985/2985 - 9s - loss: 1.4440e-04 - val_loss: 1.6235e-04 - 9s/epoch - 3ms/step
Epoch 186/200
2985/2985 - 9s - loss: 1.4522e-04 - val_loss: 1.2932e-04 - 9s/epoch - 3ms/step
Epoch 187/200
2985/2985 - 9s - loss: 1.4772e-04 - val_loss: 1.4185e-04 - 9s/epoch - 3ms/step
Epoch 188/200
2985/2985 - 9s - loss: 1.5840e-04 - val_loss: 1.4762e-04 - 9s/epoch - 3ms/step
Epoch 189/200
2985/2985 - 9s - loss: 1.5450e-04 - val_loss: 1.3531e-04 - 9s/epoch - 3ms/step
Epoch 190/200
2985/2985 - 9s - loss: 1.4712e-04 - val_loss: 1.3765e-04 - 9s/epoch - 3ms/step
Epoch 191/200
2985/2985 - 9s - loss: 1.4564e-04 - val_loss: 1.9026e-04 - 9s/epoch - 3ms/step
Epoch 192/200
2985/2985 - 9s - loss: 1.4990e-04 - val_loss: 1.4731e-04 - 9s/epoch - 3ms/step
Epoch 193/200
2985/2985 - 9s - loss: 1.4464e-04 - val_loss: 1.4622e-04 - 9s/epoch - 3ms/step
Epoch 194/200
2985/2985 - 9s - loss: 1.5428e-04 - val_loss: 1.6097e-04 - 9s/epoch - 3ms/step
Epoch 195/200
2985/2985 - 9s - loss: 1.4750e-04 - val_loss: 1.3059e-04 - 9s/epoch - 3ms/step
Epoch 196/200
2985/2985 - 9s - loss: 1.4309e-04 - val_loss: 1.3403e-04 - 9s/epoch - 3ms/step
Epoch 197/200
2985/2985 - 9s - loss: 1.4204e-04 - val_loss: 1.3706e-04 - 9s/epoch - 3ms/step
Epoch 198/200
2985/2985 - 9s - loss: 1.4146e-04 - val_loss: 1.4283e-04 - 9s/epoch - 3ms/step
Epoch 199/200
2985/2985 - 9s - loss: 1.4119e-04 - val_loss: 1.4079e-04 - 9s/epoch - 3ms/step
Epoch 200/200
2985/2985 - 9s - loss: 1.4375e-04 - val_loss: 1.5553e-04 - 9s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00015552582044620067
  1/332 [..............................] - ETA: 28s 49/332 [===>..........................] - ETA: 0s  97/332 [=======>......................] - ETA: 0s146/332 [============>.................] - ETA: 0s195/332 [================>.............] - ETA: 0s244/332 [=====================>........] - ETA: 0s293/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.001781284938297135
cosine 0.0014082864291974386
MAE: 0.0064917062
RMSE: 0.012470998
r2: 0.9899116101985281
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_12 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_14 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2528)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_13"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_14 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_12 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_14"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_15 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_13 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 632)               0         
                                                                 
 dense_13 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_14 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2528)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 32, 200, 0.0005, 0.5, 632, 0.00014375407772604376, 0.00015552582044620067, 0.001781284938297135, 0.0014082864291974386, 0.006491706240922213, 0.012470997869968414, 0.9899116101985281, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
2985/2985 - 9s - loss: 0.0079 - val_loss: 0.0024 - 9s/epoch - 3ms/step
Epoch 2/200
2985/2985 - 8s - loss: 0.0028 - val_loss: 0.0019 - 8s/epoch - 3ms/step
Epoch 3/200
2985/2985 - 9s - loss: 0.0023 - val_loss: 0.0014 - 9s/epoch - 3ms/step
Epoch 4/200
2985/2985 - 9s - loss: 0.0015 - val_loss: 0.0011 - 9s/epoch - 3ms/step
Epoch 5/200
2985/2985 - 9s - loss: 0.0011 - val_loss: 8.5200e-04 - 9s/epoch - 3ms/step
Epoch 6/200
2985/2985 - 9s - loss: 9.1993e-04 - val_loss: 7.0441e-04 - 9s/epoch - 3ms/step
Epoch 7/200
2985/2985 - 9s - loss: 8.0377e-04 - val_loss: 6.3977e-04 - 9s/epoch - 3ms/step
Epoch 8/200
2985/2985 - 9s - loss: 7.1072e-04 - val_loss: 5.8072e-04 - 9s/epoch - 3ms/step
Epoch 9/200
2985/2985 - 9s - loss: 6.5856e-04 - val_loss: 5.3948e-04 - 9s/epoch - 3ms/step
Epoch 10/200
2985/2985 - 9s - loss: 6.6752e-04 - val_loss: 4.6896e-04 - 9s/epoch - 3ms/step
Epoch 11/200
2985/2985 - 9s - loss: 6.0187e-04 - val_loss: 4.6793e-04 - 9s/epoch - 3ms/step
Epoch 12/200
2985/2985 - 9s - loss: 5.4948e-04 - val_loss: 5.0909e-04 - 9s/epoch - 3ms/step
Epoch 13/200
2985/2985 - 9s - loss: 5.3032e-04 - val_loss: 4.3606e-04 - 9s/epoch - 3ms/step
Epoch 14/200
2985/2985 - 9s - loss: 5.3410e-04 - val_loss: 3.9769e-04 - 9s/epoch - 3ms/step
Epoch 15/200
2985/2985 - 9s - loss: 4.8728e-04 - val_loss: 3.8864e-04 - 9s/epoch - 3ms/step
Epoch 16/200
2985/2985 - 9s - loss: 4.6377e-04 - val_loss: 3.9658e-04 - 9s/epoch - 3ms/step
Epoch 17/200
2985/2985 - 9s - loss: 4.8276e-04 - val_loss: 3.7534e-04 - 9s/epoch - 3ms/step
Epoch 18/200
2985/2985 - 9s - loss: 4.4545e-04 - val_loss: 4.5971e-04 - 9s/epoch - 3ms/step
Epoch 19/200
2985/2985 - 9s - loss: 4.2225e-04 - val_loss: 3.6315e-04 - 9s/epoch - 3ms/step
Epoch 20/200
2985/2985 - 9s - loss: 4.1924e-04 - val_loss: 3.2155e-04 - 9s/epoch - 3ms/step
Epoch 21/200
2985/2985 - 9s - loss: 4.0319e-04 - val_loss: 3.2004e-04 - 9s/epoch - 3ms/step
Epoch 22/200
2985/2985 - 9s - loss: 3.9553e-04 - val_loss: 3.3306e-04 - 9s/epoch - 3ms/step
Epoch 23/200
2985/2985 - 9s - loss: 3.8077e-04 - val_loss: 2.9954e-04 - 9s/epoch - 3ms/step
Epoch 24/200
2985/2985 - 9s - loss: 3.7978e-04 - val_loss: 3.0195e-04 - 9s/epoch - 3ms/step
Epoch 25/200
2985/2985 - 9s - loss: 3.7073e-04 - val_loss: 2.9821e-04 - 9s/epoch - 3ms/step
Epoch 26/200
2985/2985 - 9s - loss: 3.6114e-04 - val_loss: 2.9312e-04 - 9s/epoch - 3ms/step
Epoch 27/200
2985/2985 - 9s - loss: 3.5537e-04 - val_loss: 2.7783e-04 - 9s/epoch - 3ms/step
Epoch 28/200
2985/2985 - 9s - loss: 3.6587e-04 - val_loss: 2.8325e-04 - 9s/epoch - 3ms/step
Epoch 29/200
2985/2985 - 9s - loss: 3.4133e-04 - val_loss: 2.8042e-04 - 9s/epoch - 3ms/step
Epoch 30/200
2985/2985 - 9s - loss: 3.3773e-04 - val_loss: 2.7982e-04 - 9s/epoch - 3ms/step
Epoch 31/200
2985/2985 - 9s - loss: 3.3145e-04 - val_loss: 2.5528e-04 - 9s/epoch - 3ms/step
Epoch 32/200
2985/2985 - 9s - loss: 3.5913e-04 - val_loss: 2.5154e-04 - 9s/epoch - 3ms/step
Epoch 33/200
2985/2985 - 9s - loss: 3.3172e-04 - val_loss: 2.5103e-04 - 9s/epoch - 3ms/step
Epoch 34/200
2985/2985 - 9s - loss: 3.1983e-04 - val_loss: 2.5738e-04 - 9s/epoch - 3ms/step
Epoch 35/200
2985/2985 - 9s - loss: 3.7547e-04 - val_loss: 2.6367e-04 - 9s/epoch - 3ms/step
Epoch 36/200
2985/2985 - 9s - loss: 3.2363e-04 - val_loss: 2.4398e-04 - 9s/epoch - 3ms/step
Epoch 37/200
2985/2985 - 9s - loss: 3.1344e-04 - val_loss: 2.5140e-04 - 9s/epoch - 3ms/step
Epoch 38/200
2985/2985 - 9s - loss: 3.1068e-04 - val_loss: 2.7159e-04 - 9s/epoch - 3ms/step
Epoch 39/200
2985/2985 - 9s - loss: 3.2116e-04 - val_loss: 2.4482e-04 - 9s/epoch - 3ms/step
Epoch 40/200
2985/2985 - 9s - loss: 2.9964e-04 - val_loss: 2.4506e-04 - 9s/epoch - 3ms/step
Epoch 41/200
2985/2985 - 9s - loss: 2.9897e-04 - val_loss: 2.7181e-04 - 9s/epoch - 3ms/step
Epoch 42/200
2985/2985 - 9s - loss: 2.9873e-04 - val_loss: 3.0598e-04 - 9s/epoch - 3ms/step
Epoch 43/200
2985/2985 - 9s - loss: 3.1407e-04 - val_loss: 2.3522e-04 - 9s/epoch - 3ms/step
Epoch 44/200
2985/2985 - 9s - loss: 2.9120e-04 - val_loss: 2.4838e-04 - 9s/epoch - 3ms/step
Epoch 45/200
2985/2985 - 9s - loss: 2.8906e-04 - val_loss: 2.3320e-04 - 9s/epoch - 3ms/step
Epoch 46/200
2985/2985 - 9s - loss: 2.8406e-04 - val_loss: 2.2534e-04 - 9s/epoch - 3ms/step
Epoch 47/200
2985/2985 - 9s - loss: 2.8227e-04 - val_loss: 2.3380e-04 - 9s/epoch - 3ms/step
Epoch 48/200
2985/2985 - 9s - loss: 2.8023e-04 - val_loss: 2.1467e-04 - 9s/epoch - 3ms/step
Epoch 49/200
2985/2985 - 9s - loss: 2.8874e-04 - val_loss: 2.4902e-04 - 9s/epoch - 3ms/step
Epoch 50/200
2985/2985 - 9s - loss: 2.9566e-04 - val_loss: 2.1317e-04 - 9s/epoch - 3ms/step
Epoch 51/200
2985/2985 - 9s - loss: 2.7974e-04 - val_loss: 2.0859e-04 - 9s/epoch - 3ms/step
Epoch 52/200
2985/2985 - 9s - loss: 2.7597e-04 - val_loss: 2.1863e-04 - 9s/epoch - 3ms/step
Epoch 53/200
2985/2985 - 9s - loss: 2.7166e-04 - val_loss: 2.1236e-04 - 9s/epoch - 3ms/step
Epoch 54/200
2985/2985 - 9s - loss: 2.9798e-04 - val_loss: 2.1735e-04 - 9s/epoch - 3ms/step
Epoch 55/200
2985/2985 - 9s - loss: 2.7063e-04 - val_loss: 2.2033e-04 - 9s/epoch - 3ms/step
Epoch 56/200
2985/2985 - 9s - loss: 2.6754e-04 - val_loss: 2.0006e-04 - 9s/epoch - 3ms/step
Epoch 57/200
2985/2985 - 9s - loss: 2.6598e-04 - val_loss: 2.0896e-04 - 9s/epoch - 3ms/step
Epoch 58/200
2985/2985 - 9s - loss: 2.6151e-04 - val_loss: 2.0881e-04 - 9s/epoch - 3ms/step
Epoch 59/200
2985/2985 - 9s - loss: 2.6793e-04 - val_loss: 2.0925e-04 - 9s/epoch - 3ms/step
Epoch 60/200
2985/2985 - 9s - loss: 2.7488e-04 - val_loss: 1.9911e-04 - 9s/epoch - 3ms/step
Epoch 61/200
2985/2985 - 9s - loss: 2.6068e-04 - val_loss: 2.0003e-04 - 9s/epoch - 3ms/step
Epoch 62/200
2985/2985 - 9s - loss: 2.5840e-04 - val_loss: 2.1598e-04 - 9s/epoch - 3ms/step
Epoch 63/200
2985/2985 - 9s - loss: 2.5666e-04 - val_loss: 4.7758e-04 - 9s/epoch - 3ms/step
Epoch 64/200
2985/2985 - 9s - loss: 2.5548e-04 - val_loss: 1.9554e-04 - 9s/epoch - 3ms/step
Epoch 65/200
2985/2985 - 9s - loss: 2.5395e-04 - val_loss: 2.8703e-04 - 9s/epoch - 3ms/step
Epoch 66/200
2985/2985 - 9s - loss: 2.5919e-04 - val_loss: 1.9465e-04 - 9s/epoch - 3ms/step
Epoch 67/200
2985/2985 - 9s - loss: 2.5365e-04 - val_loss: 1.8889e-04 - 9s/epoch - 3ms/step
Epoch 68/200
2985/2985 - 9s - loss: 2.5107e-04 - val_loss: 2.2206e-04 - 9s/epoch - 3ms/step
Epoch 69/200
2985/2985 - 9s - loss: 2.6022e-04 - val_loss: 1.9657e-04 - 9s/epoch - 3ms/step
Epoch 70/200
2985/2985 - 9s - loss: 2.4503e-04 - val_loss: 2.0376e-04 - 9s/epoch - 3ms/step
Epoch 71/200
2985/2985 - 9s - loss: 2.4407e-04 - val_loss: 1.9542e-04 - 9s/epoch - 3ms/step
Epoch 72/200
2985/2985 - 9s - loss: 2.4545e-04 - val_loss: 1.8165e-04 - 9s/epoch - 3ms/step
Epoch 73/200
2985/2985 - 9s - loss: 2.4109e-04 - val_loss: 1.8478e-04 - 9s/epoch - 3ms/step
Epoch 74/200
2985/2985 - 9s - loss: 2.5050e-04 - val_loss: 1.9144e-04 - 9s/epoch - 3ms/step
Epoch 75/200
2985/2985 - 9s - loss: 2.4444e-04 - val_loss: 1.9406e-04 - 9s/epoch - 3ms/step
Epoch 76/200
2985/2985 - 9s - loss: 2.3928e-04 - val_loss: 1.8974e-04 - 9s/epoch - 3ms/step
Epoch 77/200
2985/2985 - 9s - loss: 2.4164e-04 - val_loss: 2.0226e-04 - 9s/epoch - 3ms/step
Epoch 78/200
2985/2985 - 9s - loss: 2.3784e-04 - val_loss: 1.8464e-04 - 9s/epoch - 3ms/step
Epoch 79/200
2985/2985 - 9s - loss: 2.5220e-04 - val_loss: 1.9438e-04 - 9s/epoch - 3ms/step
Epoch 80/200
2985/2985 - 9s - loss: 2.3624e-04 - val_loss: 1.9776e-04 - 9s/epoch - 3ms/step
Epoch 81/200
2985/2985 - 9s - loss: 2.3387e-04 - val_loss: 2.0487e-04 - 9s/epoch - 3ms/step
Epoch 82/200
2985/2985 - 9s - loss: 2.3703e-04 - val_loss: 1.8095e-04 - 9s/epoch - 3ms/step
Epoch 83/200
2985/2985 - 9s - loss: 2.5202e-04 - val_loss: 1.9652e-04 - 9s/epoch - 3ms/step
Epoch 84/200
2985/2985 - 9s - loss: 2.3502e-04 - val_loss: 1.7752e-04 - 9s/epoch - 3ms/step
Epoch 85/200
2985/2985 - 9s - loss: 2.3140e-04 - val_loss: 1.8674e-04 - 9s/epoch - 3ms/step
Epoch 86/200
2985/2985 - 9s - loss: 2.3015e-04 - val_loss: 1.7077e-04 - 9s/epoch - 3ms/step
Epoch 87/200
2985/2985 - 9s - loss: 2.3161e-04 - val_loss: 1.9653e-04 - 9s/epoch - 3ms/step
Epoch 88/200
2985/2985 - 9s - loss: 2.2847e-04 - val_loss: 1.7611e-04 - 9s/epoch - 3ms/step
Epoch 89/200
2985/2985 - 9s - loss: 2.2647e-04 - val_loss: 1.9652e-04 - 9s/epoch - 3ms/step
Epoch 90/200
2985/2985 - 9s - loss: 2.3537e-04 - val_loss: 1.8915e-04 - 9s/epoch - 3ms/step
Epoch 91/200
2985/2985 - 9s - loss: 2.2983e-04 - val_loss: 1.8700e-04 - 9s/epoch - 3ms/step
Epoch 92/200
2985/2985 - 9s - loss: 2.3239e-04 - val_loss: 2.6539e-04 - 9s/epoch - 3ms/step
Epoch 93/200
2985/2985 - 9s - loss: 2.3411e-04 - val_loss: 1.8208e-04 - 9s/epoch - 3ms/step
Epoch 94/200
2985/2985 - 9s - loss: 2.2291e-04 - val_loss: 1.6863e-04 - 9s/epoch - 3ms/step
Epoch 95/200
2985/2985 - 9s - loss: 2.2180e-04 - val_loss: 1.7825e-04 - 9s/epoch - 3ms/step
Epoch 96/200
2985/2985 - 9s - loss: 2.2300e-04 - val_loss: 1.7744e-04 - 9s/epoch - 3ms/step
Epoch 97/200
2985/2985 - 9s - loss: 2.1930e-04 - val_loss: 1.7301e-04 - 9s/epoch - 3ms/step
Epoch 98/200
2985/2985 - 9s - loss: 2.2028e-04 - val_loss: 1.7019e-04 - 9s/epoch - 3ms/step
Epoch 99/200
2985/2985 - 9s - loss: 2.1960e-04 - val_loss: 1.8054e-04 - 9s/epoch - 3ms/step
Epoch 100/200
2985/2985 - 9s - loss: 2.1742e-04 - val_loss: 1.8154e-04 - 9s/epoch - 3ms/step
Epoch 101/200
2985/2985 - 9s - loss: 2.1938e-04 - val_loss: 1.7344e-04 - 9s/epoch - 3ms/step
Epoch 102/200
2985/2985 - 9s - loss: 2.1851e-04 - val_loss: 1.8975e-04 - 9s/epoch - 3ms/step
Epoch 103/200
2985/2985 - 9s - loss: 2.2555e-04 - val_loss: 2.0081e-04 - 9s/epoch - 3ms/step
Epoch 104/200
2985/2985 - 9s - loss: 2.1803e-04 - val_loss: 1.9889e-04 - 9s/epoch - 3ms/step
Epoch 105/200
2985/2985 - 9s - loss: 2.3240e-04 - val_loss: 1.7488e-04 - 9s/epoch - 3ms/step
Epoch 106/200
2985/2985 - 9s - loss: 2.1710e-04 - val_loss: 1.7742e-04 - 9s/epoch - 3ms/step
Epoch 107/200
2985/2985 - 9s - loss: 2.1358e-04 - val_loss: 1.7563e-04 - 9s/epoch - 3ms/step
Epoch 108/200
2985/2985 - 9s - loss: 2.1654e-04 - val_loss: 1.8176e-04 - 9s/epoch - 3ms/step
Epoch 109/200
2985/2985 - 9s - loss: 2.1583e-04 - val_loss: 1.6871e-04 - 9s/epoch - 3ms/step
Epoch 110/200
2985/2985 - 9s - loss: 2.1384e-04 - val_loss: 1.8670e-04 - 9s/epoch - 3ms/step
Epoch 111/200
2985/2985 - 9s - loss: 2.1194e-04 - val_loss: 1.7335e-04 - 9s/epoch - 3ms/step
Epoch 112/200
2985/2985 - 9s - loss: 2.2026e-04 - val_loss: 1.8438e-04 - 9s/epoch - 3ms/step
Epoch 113/200
2985/2985 - 9s - loss: 2.1168e-04 - val_loss: 2.3477e-04 - 9s/epoch - 3ms/step
Epoch 114/200
2985/2985 - 9s - loss: 2.3856e-04 - val_loss: 1.7992e-04 - 9s/epoch - 3ms/step
Epoch 115/200
2985/2985 - 9s - loss: 2.1200e-04 - val_loss: 1.7051e-04 - 9s/epoch - 3ms/step
Epoch 116/200
2985/2985 - 9s - loss: 2.1151e-04 - val_loss: 1.6544e-04 - 9s/epoch - 3ms/step
Epoch 117/200
2985/2985 - 9s - loss: 2.0778e-04 - val_loss: 1.5990e-04 - 9s/epoch - 3ms/step
Epoch 118/200
2985/2985 - 9s - loss: 2.1083e-04 - val_loss: 1.6119e-04 - 9s/epoch - 3ms/step
Epoch 119/200
2985/2985 - 9s - loss: 2.0705e-04 - val_loss: 1.6247e-04 - 9s/epoch - 3ms/step
Epoch 120/200
2985/2985 - 9s - loss: 2.1785e-04 - val_loss: 1.7845e-04 - 9s/epoch - 3ms/step
Epoch 121/200
2985/2985 - 9s - loss: 2.1105e-04 - val_loss: 1.6857e-04 - 9s/epoch - 3ms/step
Epoch 122/200
2985/2985 - 9s - loss: 2.0780e-04 - val_loss: 1.5713e-04 - 9s/epoch - 3ms/step
Epoch 123/200
2985/2985 - 9s - loss: 2.1200e-04 - val_loss: 1.6843e-04 - 9s/epoch - 3ms/step
Epoch 124/200
2985/2985 - 9s - loss: 2.0554e-04 - val_loss: 1.6923e-04 - 9s/epoch - 3ms/step
Epoch 125/200
2985/2985 - 9s - loss: 2.0521e-04 - val_loss: 1.6143e-04 - 9s/epoch - 3ms/step
Epoch 126/200
2985/2985 - 9s - loss: 2.1204e-04 - val_loss: 1.6743e-04 - 9s/epoch - 3ms/step
Epoch 127/200
2985/2985 - 9s - loss: 2.0386e-04 - val_loss: 1.6136e-04 - 9s/epoch - 3ms/step
Epoch 128/200
2985/2985 - 9s - loss: 2.0330e-04 - val_loss: 1.5737e-04 - 9s/epoch - 3ms/step
Epoch 129/200
2985/2985 - 9s - loss: 2.0484e-04 - val_loss: 1.5786e-04 - 9s/epoch - 3ms/step
Epoch 130/200
2985/2985 - 9s - loss: 2.0441e-04 - val_loss: 1.6450e-04 - 9s/epoch - 3ms/step
Epoch 131/200
2985/2985 - 9s - loss: 2.0983e-04 - val_loss: 1.5766e-04 - 9s/epoch - 3ms/step
Epoch 132/200
2985/2985 - 9s - loss: 2.0575e-04 - val_loss: 1.6102e-04 - 9s/epoch - 3ms/step
Epoch 133/200
2985/2985 - 9s - loss: 2.0486e-04 - val_loss: 1.6008e-04 - 9s/epoch - 3ms/step
Epoch 134/200
2985/2985 - 9s - loss: 2.0072e-04 - val_loss: 3.6428e-04 - 9s/epoch - 3ms/step
Epoch 135/200
2985/2985 - 9s - loss: 2.0690e-04 - val_loss: 1.6131e-04 - 9s/epoch - 3ms/step
Epoch 136/200
2985/2985 - 9s - loss: 2.0649e-04 - val_loss: 1.5402e-04 - 9s/epoch - 3ms/step
Epoch 137/200
2985/2985 - 9s - loss: 2.0301e-04 - val_loss: 1.6904e-04 - 9s/epoch - 3ms/step
Epoch 138/200
2985/2985 - 9s - loss: 2.0048e-04 - val_loss: 1.6505e-04 - 9s/epoch - 3ms/step
Epoch 139/200
2985/2985 - 9s - loss: 2.0690e-04 - val_loss: 1.9474e-04 - 9s/epoch - 3ms/step
Epoch 140/200
2985/2985 - 9s - loss: 2.1477e-04 - val_loss: 1.9062e-04 - 9s/epoch - 3ms/step
Epoch 141/200
2985/2985 - 9s - loss: 2.1186e-04 - val_loss: 2.9871e-04 - 9s/epoch - 3ms/step
Epoch 142/200
2985/2985 - 9s - loss: 2.3440e-04 - val_loss: 1.7213e-04 - 9s/epoch - 3ms/step
Epoch 143/200
2985/2985 - 9s - loss: 2.0541e-04 - val_loss: 1.7846e-04 - 9s/epoch - 3ms/step
Epoch 144/200
2985/2985 - 9s - loss: 2.0441e-04 - val_loss: 1.5527e-04 - 9s/epoch - 3ms/step
Epoch 145/200
2985/2985 - 9s - loss: 1.9995e-04 - val_loss: 1.5564e-04 - 9s/epoch - 3ms/step
Epoch 146/200
2985/2985 - 9s - loss: 1.9788e-04 - val_loss: 1.6037e-04 - 9s/epoch - 3ms/step
Epoch 147/200
2985/2985 - 9s - loss: 2.0311e-04 - val_loss: 2.1555e-04 - 9s/epoch - 3ms/step
Epoch 148/200
2985/2985 - 9s - loss: 2.0629e-04 - val_loss: 1.8216e-04 - 9s/epoch - 3ms/step
Epoch 149/200
2985/2985 - 9s - loss: 1.9710e-04 - val_loss: 1.5600e-04 - 9s/epoch - 3ms/step
Epoch 150/200
2985/2985 - 9s - loss: 1.9854e-04 - val_loss: 1.5344e-04 - 9s/epoch - 3ms/step
Epoch 151/200
2985/2985 - 9s - loss: 1.9623e-04 - val_loss: 1.6430e-04 - 9s/epoch - 3ms/step
Epoch 152/200
2985/2985 - 9s - loss: 1.9530e-04 - val_loss: 1.6319e-04 - 9s/epoch - 3ms/step
Epoch 153/200
2985/2985 - 9s - loss: 1.9922e-04 - val_loss: 1.5873e-04 - 9s/epoch - 3ms/step
Epoch 154/200
2985/2985 - 9s - loss: 1.9935e-04 - val_loss: 1.6610e-04 - 9s/epoch - 3ms/step
Epoch 155/200
2985/2985 - 9s - loss: 1.9548e-04 - val_loss: 1.5417e-04 - 9s/epoch - 3ms/step
Epoch 156/200
2985/2985 - 9s - loss: 1.9296e-04 - val_loss: 1.5237e-04 - 9s/epoch - 3ms/step
Epoch 157/200
2985/2985 - 9s - loss: 1.9409e-04 - val_loss: 1.8565e-04 - 9s/epoch - 3ms/step
Epoch 158/200
2985/2985 - 9s - loss: 1.9570e-04 - val_loss: 1.6406e-04 - 9s/epoch - 3ms/step
Epoch 159/200
2985/2985 - 9s - loss: 1.9504e-04 - val_loss: 2.2986e-04 - 9s/epoch - 3ms/step
Epoch 160/200
2985/2985 - 9s - loss: 1.9946e-04 - val_loss: 1.5193e-04 - 9s/epoch - 3ms/step
Epoch 161/200
2985/2985 - 9s - loss: 1.9182e-04 - val_loss: 1.6794e-04 - 9s/epoch - 3ms/step
Epoch 162/200
2985/2985 - 9s - loss: 1.9214e-04 - val_loss: 1.6094e-04 - 9s/epoch - 3ms/step
Epoch 163/200
2985/2985 - 9s - loss: 1.9639e-04 - val_loss: 1.5746e-04 - 9s/epoch - 3ms/step
Epoch 164/200
2985/2985 - 9s - loss: 1.9580e-04 - val_loss: 1.6784e-04 - 9s/epoch - 3ms/step
Epoch 165/200
2985/2985 - 9s - loss: 1.9660e-04 - val_loss: 1.6547e-04 - 9s/epoch - 3ms/step
Epoch 166/200
2985/2985 - 9s - loss: 1.9162e-04 - val_loss: 1.6665e-04 - 9s/epoch - 3ms/step
Epoch 167/200
2985/2985 - 9s - loss: 2.0139e-04 - val_loss: 1.5768e-04 - 9s/epoch - 3ms/step
Epoch 168/200
2985/2985 - 9s - loss: 1.9078e-04 - val_loss: 1.5549e-04 - 9s/epoch - 3ms/step
Epoch 169/200
2985/2985 - 9s - loss: 1.9058e-04 - val_loss: 1.6854e-04 - 9s/epoch - 3ms/step
Epoch 170/200
2985/2985 - 9s - loss: 1.9224e-04 - val_loss: 1.5784e-04 - 9s/epoch - 3ms/step
Epoch 171/200
2985/2985 - 9s - loss: 1.8917e-04 - val_loss: 1.5167e-04 - 9s/epoch - 3ms/step
Epoch 172/200
2985/2985 - 9s - loss: 1.8764e-04 - val_loss: 1.5704e-04 - 9s/epoch - 3ms/step
Epoch 173/200
2985/2985 - 9s - loss: 1.9312e-04 - val_loss: 1.5006e-04 - 9s/epoch - 3ms/step
Epoch 174/200
2985/2985 - 9s - loss: 1.9346e-04 - val_loss: 1.6691e-04 - 9s/epoch - 3ms/step
Epoch 175/200
2985/2985 - 9s - loss: 1.9291e-04 - val_loss: 1.6072e-04 - 9s/epoch - 3ms/step
Epoch 176/200
2985/2985 - 9s - loss: 1.8827e-04 - val_loss: 1.5037e-04 - 9s/epoch - 3ms/step
Epoch 177/200
2985/2985 - 9s - loss: 1.8922e-04 - val_loss: 1.4283e-04 - 9s/epoch - 3ms/step
Epoch 178/200
2985/2985 - 9s - loss: 1.8671e-04 - val_loss: 1.4973e-04 - 9s/epoch - 3ms/step
Epoch 179/200
2985/2985 - 9s - loss: 1.8947e-04 - val_loss: 1.5375e-04 - 9s/epoch - 3ms/step
Epoch 180/200
2985/2985 - 9s - loss: 2.0578e-04 - val_loss: 1.5321e-04 - 9s/epoch - 3ms/step
Epoch 181/200
2985/2985 - 9s - loss: 1.8892e-04 - val_loss: 1.5714e-04 - 9s/epoch - 3ms/step
Epoch 182/200
2985/2985 - 9s - loss: 1.8888e-04 - val_loss: 1.5848e-04 - 9s/epoch - 3ms/step
Epoch 183/200
2985/2985 - 9s - loss: 1.8400e-04 - val_loss: 1.5173e-04 - 9s/epoch - 3ms/step
Epoch 184/200
2985/2985 - 9s - loss: 1.9686e-04 - val_loss: 1.5419e-04 - 9s/epoch - 3ms/step
Epoch 185/200
2985/2985 - 9s - loss: 1.8968e-04 - val_loss: 1.5923e-04 - 9s/epoch - 3ms/step
Epoch 186/200
2985/2985 - 9s - loss: 1.8656e-04 - val_loss: 1.5391e-04 - 9s/epoch - 3ms/step
Epoch 187/200
2985/2985 - 9s - loss: 1.8630e-04 - val_loss: 1.7857e-04 - 9s/epoch - 3ms/step
Epoch 188/200
2985/2985 - 9s - loss: 1.8463e-04 - val_loss: 1.5798e-04 - 9s/epoch - 3ms/step
Epoch 189/200
2985/2985 - 9s - loss: 1.9448e-04 - val_loss: 1.6125e-04 - 9s/epoch - 3ms/step
Epoch 190/200
2985/2985 - 9s - loss: 1.8600e-04 - val_loss: 1.6972e-04 - 9s/epoch - 3ms/step
Epoch 191/200
2985/2985 - 9s - loss: 1.8913e-04 - val_loss: 1.5446e-04 - 9s/epoch - 3ms/step
Epoch 192/200
2985/2985 - 9s - loss: 1.8523e-04 - val_loss: 1.6848e-04 - 9s/epoch - 3ms/step
Epoch 193/200
2985/2985 - 9s - loss: 1.8355e-04 - val_loss: 1.5658e-04 - 9s/epoch - 3ms/step
Epoch 194/200
2985/2985 - 9s - loss: 1.8564e-04 - val_loss: 2.0302e-04 - 9s/epoch - 3ms/step
Epoch 195/200
2985/2985 - 9s - loss: 1.8740e-04 - val_loss: 1.4741e-04 - 9s/epoch - 3ms/step
Epoch 196/200
2985/2985 - 9s - loss: 1.8767e-04 - val_loss: 1.7096e-04 - 9s/epoch - 3ms/step
Epoch 197/200
2985/2985 - 9s - loss: 1.8639e-04 - val_loss: 1.5308e-04 - 9s/epoch - 3ms/step
Epoch 198/200
2985/2985 - 9s - loss: 1.8224e-04 - val_loss: 1.8875e-04 - 9s/epoch - 3ms/step
Epoch 199/200
2985/2985 - 9s - loss: 1.8615e-04 - val_loss: 1.5410e-04 - 9s/epoch - 3ms/step
Epoch 200/200
2985/2985 - 9s - loss: 1.8253e-04 - val_loss: 1.4841e-04 - 9s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00014840910444036126
  1/332 [..............................] - ETA: 31s 48/332 [===>..........................] - ETA: 0s  96/332 [=======>......................] - ETA: 0s144/332 [============>.................] - ETA: 0s193/332 [================>.............] - ETA: 0s242/332 [====================>.........] - ETA: 0s291/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0016860583122388579
cosine 0.0013337583136118181
MAE: 0.0066547743
RMSE: 0.012182322
r2: 0.9903727465439762
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_16"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_17 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_15 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_17"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_18 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_16 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 632)               0         
                                                                 
 dense_16 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_17 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2528)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 32, 200, 0.001, 0.5, 632, 0.00018252863083034754, 0.00014840910444036126, 0.0016860583122388579, 0.0013337583136118181, 0.006654774304479361, 0.012182322330772877, 0.9903727465439762, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_18 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_18 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_20 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2528)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
2985/2985 - 9s - loss: 0.0093 - val_loss: 0.0025 - 9s/epoch - 3ms/step
Epoch 2/200
2985/2985 - 9s - loss: 0.0026 - val_loss: 0.0018 - 9s/epoch - 3ms/step
Epoch 3/200
2985/2985 - 9s - loss: 0.0017 - val_loss: 0.0014 - 9s/epoch - 3ms/step
Epoch 4/200
2985/2985 - 9s - loss: 0.0013 - val_loss: 0.0017 - 9s/epoch - 3ms/step
Epoch 5/200
2985/2985 - 9s - loss: 0.0014 - val_loss: 9.1942e-04 - 9s/epoch - 3ms/step
Epoch 6/200
2985/2985 - 9s - loss: 9.7734e-04 - val_loss: 7.9705e-04 - 9s/epoch - 3ms/step
Epoch 7/200
2985/2985 - 9s - loss: 9.3427e-04 - val_loss: 7.1201e-04 - 9s/epoch - 3ms/step
Epoch 8/200
2985/2985 - 9s - loss: 8.0541e-04 - val_loss: 6.5464e-04 - 9s/epoch - 3ms/step
Epoch 9/200
2985/2985 - 9s - loss: 7.5898e-04 - val_loss: 6.0181e-04 - 9s/epoch - 3ms/step
Epoch 10/200
2985/2985 - 9s - loss: 7.1781e-04 - val_loss: 5.8410e-04 - 9s/epoch - 3ms/step
Epoch 11/200
2985/2985 - 9s - loss: 6.7012e-04 - val_loss: 5.3325e-04 - 9s/epoch - 3ms/step
Epoch 12/200
2985/2985 - 9s - loss: 6.6045e-04 - val_loss: 5.5603e-04 - 9s/epoch - 3ms/step
Epoch 13/200
2985/2985 - 9s - loss: 6.7751e-04 - val_loss: 4.9809e-04 - 9s/epoch - 3ms/step
Epoch 14/200
2985/2985 - 9s - loss: 6.1064e-04 - val_loss: 4.8698e-04 - 9s/epoch - 3ms/step
Epoch 15/200
2985/2985 - 9s - loss: 6.0113e-04 - val_loss: 4.5199e-04 - 9s/epoch - 3ms/step
Epoch 16/200
2985/2985 - 9s - loss: 5.6895e-04 - val_loss: 4.7912e-04 - 9s/epoch - 3ms/step
Epoch 17/200
2985/2985 - 9s - loss: 5.5650e-04 - val_loss: 4.4836e-04 - 9s/epoch - 3ms/step
Epoch 18/200
2985/2985 - 9s - loss: 5.5067e-04 - val_loss: 4.6209e-04 - 9s/epoch - 3ms/step
Epoch 19/200
2985/2985 - 9s - loss: 5.3402e-04 - val_loss: 5.8300e-04 - 9s/epoch - 3ms/step
Epoch 20/200
2985/2985 - 9s - loss: 5.3268e-04 - val_loss: 4.1103e-04 - 9s/epoch - 3ms/step
Epoch 21/200
2985/2985 - 9s - loss: 5.1808e-04 - val_loss: 3.9227e-04 - 9s/epoch - 3ms/step
Epoch 22/200
2985/2985 - 9s - loss: 4.9902e-04 - val_loss: 4.9452e-04 - 9s/epoch - 3ms/step
Epoch 23/200
2985/2985 - 9s - loss: 5.2663e-04 - val_loss: 3.9313e-04 - 9s/epoch - 3ms/step
Epoch 24/200
2985/2985 - 9s - loss: 4.9470e-04 - val_loss: 4.0374e-04 - 9s/epoch - 3ms/step
Epoch 25/200
2985/2985 - 9s - loss: 4.9433e-04 - val_loss: 3.7888e-04 - 9s/epoch - 3ms/step
Epoch 26/200
2985/2985 - 9s - loss: 4.7555e-04 - val_loss: 3.6952e-04 - 9s/epoch - 3ms/step
Epoch 27/200
2985/2985 - 9s - loss: 4.6437e-04 - val_loss: 3.7583e-04 - 9s/epoch - 3ms/step
Epoch 28/200
2985/2985 - 9s - loss: 4.8091e-04 - val_loss: 3.6498e-04 - 9s/epoch - 3ms/step
Epoch 29/200
2985/2985 - 9s - loss: 4.5517e-04 - val_loss: 4.8943e-04 - 9s/epoch - 3ms/step
Epoch 30/200
2985/2985 - 9s - loss: 4.5013e-04 - val_loss: 3.6460e-04 - 9s/epoch - 3ms/step
Epoch 31/200
2985/2985 - 9s - loss: 4.3955e-04 - val_loss: 3.5573e-04 - 9s/epoch - 3ms/step
Epoch 32/200
2985/2985 - 9s - loss: 4.3709e-04 - val_loss: 3.4755e-04 - 9s/epoch - 3ms/step
Epoch 33/200
2985/2985 - 9s - loss: 4.5187e-04 - val_loss: 3.5299e-04 - 9s/epoch - 3ms/step
Epoch 34/200
2985/2985 - 9s - loss: 4.2931e-04 - val_loss: 3.2483e-04 - 9s/epoch - 3ms/step
Epoch 35/200
2985/2985 - 9s - loss: 4.4273e-04 - val_loss: 3.2423e-04 - 9s/epoch - 3ms/step
Epoch 36/200
2985/2985 - 9s - loss: 4.2319e-04 - val_loss: 3.1806e-04 - 9s/epoch - 3ms/step
Epoch 37/200
2985/2985 - 9s - loss: 4.2111e-04 - val_loss: 3.2255e-04 - 9s/epoch - 3ms/step
Epoch 38/200
2985/2985 - 9s - loss: 4.1138e-04 - val_loss: 3.1219e-04 - 9s/epoch - 3ms/step
Epoch 39/200
2985/2985 - 9s - loss: 4.4281e-04 - val_loss: 3.1235e-04 - 9s/epoch - 3ms/step
Epoch 40/200
2985/2985 - 9s - loss: 4.0976e-04 - val_loss: 4.0423e-04 - 9s/epoch - 3ms/step
Epoch 41/200
2985/2985 - 9s - loss: 4.0607e-04 - val_loss: 3.0021e-04 - 9s/epoch - 3ms/step
Epoch 42/200
2985/2985 - 9s - loss: 3.9987e-04 - val_loss: 3.0761e-04 - 9s/epoch - 3ms/step
Epoch 43/200
2985/2985 - 9s - loss: 3.9378e-04 - val_loss: 3.0807e-04 - 9s/epoch - 3ms/step
Epoch 44/200
2985/2985 - 9s - loss: 3.9033e-04 - val_loss: 3.4769e-04 - 9s/epoch - 3ms/step
Epoch 45/200
2985/2985 - 9s - loss: 4.0376e-04 - val_loss: 5.2767e-04 - 9s/epoch - 3ms/step
Epoch 46/200
2985/2985 - 9s - loss: 3.9782e-04 - val_loss: 2.9026e-04 - 9s/epoch - 3ms/step
Epoch 47/200
2985/2985 - 9s - loss: 3.8652e-04 - val_loss: 2.8488e-04 - 9s/epoch - 3ms/step
Epoch 48/200
2985/2985 - 9s - loss: 4.0516e-04 - val_loss: 2.9338e-04 - 9s/epoch - 3ms/step
Epoch 49/200
2985/2985 - 9s - loss: 3.9664e-04 - val_loss: 7.8649e-04 - 9s/epoch - 3ms/step
Epoch 50/200
2985/2985 - 9s - loss: 5.2780e-04 - val_loss: 0.0019 - 9s/epoch - 3ms/step
Epoch 51/200
2985/2985 - 9s - loss: 7.2582e-04 - val_loss: 4.0326e-04 - 9s/epoch - 3ms/step
Epoch 52/200
2985/2985 - 9s - loss: 4.8505e-04 - val_loss: 3.5805e-04 - 9s/epoch - 3ms/step
Epoch 53/200
2985/2985 - 9s - loss: 4.5714e-04 - val_loss: 3.4571e-04 - 9s/epoch - 3ms/step
Epoch 54/200
2985/2985 - 9s - loss: 4.3522e-04 - val_loss: 3.5339e-04 - 9s/epoch - 3ms/step
Epoch 55/200
2985/2985 - 9s - loss: 4.2450e-04 - val_loss: 3.1680e-04 - 9s/epoch - 3ms/step
Epoch 56/200
2985/2985 - 9s - loss: 4.3964e-04 - val_loss: 3.1445e-04 - 9s/epoch - 3ms/step
Epoch 57/200
2985/2985 - 9s - loss: 4.1539e-04 - val_loss: 3.5225e-04 - 9s/epoch - 3ms/step
Epoch 58/200
2985/2985 - 9s - loss: 4.1159e-04 - val_loss: 3.0132e-04 - 9s/epoch - 3ms/step
Epoch 59/200
2985/2985 - 9s - loss: 4.0417e-04 - val_loss: 3.2187e-04 - 9s/epoch - 3ms/step
Epoch 60/200
2985/2985 - 9s - loss: 4.0815e-04 - val_loss: 3.0400e-04 - 9s/epoch - 3ms/step
Epoch 61/200
2985/2985 - 9s - loss: 4.0370e-04 - val_loss: 3.0653e-04 - 9s/epoch - 3ms/step
Epoch 62/200
2985/2985 - 9s - loss: 3.9622e-04 - val_loss: 3.0955e-04 - 9s/epoch - 3ms/step
Epoch 63/200
2985/2985 - 9s - loss: 3.9182e-04 - val_loss: 3.1957e-04 - 9s/epoch - 3ms/step
Epoch 64/200
2985/2985 - 9s - loss: 3.8790e-04 - val_loss: 2.9983e-04 - 9s/epoch - 3ms/step
Epoch 65/200
2985/2985 - 9s - loss: 4.2810e-04 - val_loss: 3.0743e-04 - 9s/epoch - 3ms/step
Epoch 66/200
2985/2985 - 9s - loss: 4.8693e-04 - val_loss: 3.0339e-04 - 9s/epoch - 3ms/step
Epoch 67/200
2985/2985 - 9s - loss: 4.0532e-04 - val_loss: 2.9911e-04 - 9s/epoch - 3ms/step
Epoch 68/200
2985/2985 - 9s - loss: 3.9433e-04 - val_loss: 3.1308e-04 - 9s/epoch - 3ms/step
Epoch 69/200
2985/2985 - 9s - loss: 3.9848e-04 - val_loss: 3.0790e-04 - 9s/epoch - 3ms/step
Epoch 70/200
2985/2985 - 9s - loss: 3.8170e-04 - val_loss: 4.4349e-04 - 9s/epoch - 3ms/step
Epoch 71/200
2985/2985 - 9s - loss: 3.9427e-04 - val_loss: 2.9662e-04 - 9s/epoch - 3ms/step
Epoch 72/200
2985/2985 - 9s - loss: 3.7849e-04 - val_loss: 2.9924e-04 - 9s/epoch - 3ms/step
Epoch 73/200
2985/2985 - 9s - loss: 3.7563e-04 - val_loss: 2.9017e-04 - 9s/epoch - 3ms/step
Epoch 74/200
2985/2985 - 9s - loss: 4.1479e-04 - val_loss: 2.9153e-04 - 9s/epoch - 3ms/step
Epoch 75/200
2985/2985 - 9s - loss: 3.7163e-04 - val_loss: 2.8631e-04 - 9s/epoch - 3ms/step
Epoch 76/200
2985/2985 - 9s - loss: 3.6891e-04 - val_loss: 2.8165e-04 - 9s/epoch - 3ms/step
Epoch 77/200
2985/2985 - 9s - loss: 3.7707e-04 - val_loss: 4.8859e-04 - 9s/epoch - 3ms/step
Epoch 78/200
2985/2985 - 9s - loss: 4.1623e-04 - val_loss: 2.8968e-04 - 9s/epoch - 3ms/step
Epoch 79/200
2985/2985 - 9s - loss: 3.7329e-04 - val_loss: 3.0060e-04 - 9s/epoch - 3ms/step
Epoch 80/200
2985/2985 - 9s - loss: 3.6802e-04 - val_loss: 2.9232e-04 - 9s/epoch - 3ms/step
Epoch 81/200
2985/2985 - 9s - loss: 3.6246e-04 - val_loss: 3.2457e-04 - 9s/epoch - 3ms/step
Epoch 82/200
2985/2985 - 9s - loss: 3.6034e-04 - val_loss: 2.7089e-04 - 9s/epoch - 3ms/step
Epoch 83/200
2985/2985 - 9s - loss: 3.6625e-04 - val_loss: 3.1322e-04 - 9s/epoch - 3ms/step
Epoch 84/200
2985/2985 - 9s - loss: 3.6373e-04 - val_loss: 2.8747e-04 - 9s/epoch - 3ms/step
Epoch 85/200
2985/2985 - 9s - loss: 3.5812e-04 - val_loss: 2.5970e-04 - 9s/epoch - 3ms/step
Epoch 86/200
2985/2985 - 9s - loss: 3.5355e-04 - val_loss: 2.6862e-04 - 9s/epoch - 3ms/step
Epoch 87/200
2985/2985 - 9s - loss: 3.6591e-04 - val_loss: 2.9218e-04 - 9s/epoch - 3ms/step
Epoch 88/200
2985/2985 - 9s - loss: 3.5587e-04 - val_loss: 2.8075e-04 - 9s/epoch - 3ms/step
Epoch 89/200
2985/2985 - 9s - loss: 3.5328e-04 - val_loss: 2.8630e-04 - 9s/epoch - 3ms/step
Epoch 90/200
2985/2985 - 9s - loss: 3.5768e-04 - val_loss: 2.6825e-04 - 9s/epoch - 3ms/step
Epoch 91/200
2985/2985 - 9s - loss: 3.4953e-04 - val_loss: 2.6742e-04 - 9s/epoch - 3ms/step
Epoch 92/200
2985/2985 - 9s - loss: 3.5503e-04 - val_loss: 3.8819e-04 - 9s/epoch - 3ms/step
Epoch 93/200
2985/2985 - 9s - loss: 3.7992e-04 - val_loss: 2.8957e-04 - 9s/epoch - 3ms/step
Epoch 94/200
2985/2985 - 9s - loss: 3.4589e-04 - val_loss: 2.6366e-04 - 9s/epoch - 3ms/step
Epoch 95/200
2985/2985 - 9s - loss: 3.4644e-04 - val_loss: 2.6394e-04 - 9s/epoch - 3ms/step
Epoch 96/200
2985/2985 - 9s - loss: 3.4867e-04 - val_loss: 2.9344e-04 - 9s/epoch - 3ms/step
Epoch 97/200
2985/2985 - 9s - loss: 3.5604e-04 - val_loss: 2.6982e-04 - 9s/epoch - 3ms/step
Epoch 98/200
2985/2985 - 9s - loss: 3.4293e-04 - val_loss: 2.6449e-04 - 9s/epoch - 3ms/step
Epoch 99/200
2985/2985 - 9s - loss: 3.4894e-04 - val_loss: 2.6736e-04 - 9s/epoch - 3ms/step
Epoch 100/200
2985/2985 - 9s - loss: 3.4254e-04 - val_loss: 2.6885e-04 - 9s/epoch - 3ms/step
Epoch 101/200
2985/2985 - 9s - loss: 3.4107e-04 - val_loss: 2.4822e-04 - 9s/epoch - 3ms/step
Epoch 102/200
2985/2985 - 9s - loss: 3.3913e-04 - val_loss: 2.6766e-04 - 9s/epoch - 3ms/step
Epoch 103/200
2985/2985 - 9s - loss: 3.5011e-04 - val_loss: 2.7956e-04 - 9s/epoch - 3ms/step
Epoch 104/200
2985/2985 - 9s - loss: 3.3966e-04 - val_loss: 2.7868e-04 - 9s/epoch - 3ms/step
Epoch 105/200
2985/2985 - 9s - loss: 3.4129e-04 - val_loss: 2.5854e-04 - 9s/epoch - 3ms/step
Epoch 106/200
2985/2985 - 9s - loss: 3.4015e-04 - val_loss: 4.5767e-04 - 9s/epoch - 3ms/step
Epoch 107/200
2985/2985 - 9s - loss: 3.4033e-04 - val_loss: 2.5958e-04 - 9s/epoch - 3ms/step
Epoch 108/200
2985/2985 - 9s - loss: 3.3307e-04 - val_loss: 2.5502e-04 - 9s/epoch - 3ms/step
Epoch 109/200
2985/2985 - 9s - loss: 3.3385e-04 - val_loss: 2.5099e-04 - 9s/epoch - 3ms/step
Epoch 110/200
2985/2985 - 9s - loss: 3.3818e-04 - val_loss: 3.3231e-04 - 9s/epoch - 3ms/step
Epoch 111/200
2985/2985 - 9s - loss: 3.5752e-04 - val_loss: 2.5416e-04 - 9s/epoch - 3ms/step
Epoch 112/200
2985/2985 - 9s - loss: 3.3092e-04 - val_loss: 2.9067e-04 - 9s/epoch - 3ms/step
Epoch 113/200
2985/2985 - 9s - loss: 3.2907e-04 - val_loss: 3.2967e-04 - 9s/epoch - 3ms/step
Epoch 114/200
2985/2985 - 9s - loss: 3.4411e-04 - val_loss: 2.7933e-04 - 9s/epoch - 3ms/step
Epoch 115/200
2985/2985 - 9s - loss: 3.4164e-04 - val_loss: 2.4412e-04 - 9s/epoch - 3ms/step
Epoch 116/200
2985/2985 - 9s - loss: 3.2933e-04 - val_loss: 2.5545e-04 - 9s/epoch - 3ms/step
Epoch 117/200
2985/2985 - 9s - loss: 3.2113e-04 - val_loss: 2.4796e-04 - 9s/epoch - 3ms/step
Epoch 118/200
2985/2985 - 9s - loss: 3.5432e-04 - val_loss: 2.6744e-04 - 9s/epoch - 3ms/step
Epoch 119/200
2985/2985 - 9s - loss: 3.2649e-04 - val_loss: 2.6250e-04 - 9s/epoch - 3ms/step
Epoch 120/200
2985/2985 - 9s - loss: 3.2428e-04 - val_loss: 2.6879e-04 - 9s/epoch - 3ms/step
Epoch 121/200
2985/2985 - 9s - loss: 3.3472e-04 - val_loss: 2.5627e-04 - 9s/epoch - 3ms/step
Epoch 122/200
2985/2985 - 9s - loss: 3.2240e-04 - val_loss: 2.4851e-04 - 9s/epoch - 3ms/step
Epoch 123/200
2985/2985 - 9s - loss: 3.2775e-04 - val_loss: 2.6048e-04 - 9s/epoch - 3ms/step
Epoch 124/200
2985/2985 - 9s - loss: 3.2323e-04 - val_loss: 2.6672e-04 - 9s/epoch - 3ms/step
Epoch 125/200
2985/2985 - 9s - loss: 3.1784e-04 - val_loss: 2.5633e-04 - 9s/epoch - 3ms/step
Epoch 126/200
2985/2985 - 9s - loss: 3.4127e-04 - val_loss: 2.5858e-04 - 9s/epoch - 3ms/step
Epoch 127/200
2985/2985 - 9s - loss: 3.2299e-04 - val_loss: 2.6323e-04 - 9s/epoch - 3ms/step
Epoch 128/200
2985/2985 - 9s - loss: 3.2046e-04 - val_loss: 2.4726e-04 - 9s/epoch - 3ms/step
Epoch 129/200
2985/2985 - 9s - loss: 3.2783e-04 - val_loss: 2.4376e-04 - 9s/epoch - 3ms/step
Epoch 130/200
2985/2985 - 9s - loss: 3.4368e-04 - val_loss: 2.4794e-04 - 9s/epoch - 3ms/step
Epoch 131/200
2985/2985 - 9s - loss: 3.2674e-04 - val_loss: 3.0788e-04 - 9s/epoch - 3ms/step
Epoch 132/200
2985/2985 - 9s - loss: 3.2618e-04 - val_loss: 2.5220e-04 - 9s/epoch - 3ms/step
Epoch 133/200
2985/2985 - 9s - loss: 3.1547e-04 - val_loss: 2.3965e-04 - 9s/epoch - 3ms/step
Epoch 134/200
2985/2985 - 9s - loss: 3.1391e-04 - val_loss: 2.4670e-04 - 9s/epoch - 3ms/step
Epoch 135/200
2985/2985 - 9s - loss: 3.2176e-04 - val_loss: 2.4365e-04 - 9s/epoch - 3ms/step
Epoch 136/200
2985/2985 - 9s - loss: 3.3585e-04 - val_loss: 2.3687e-04 - 9s/epoch - 3ms/step
Epoch 137/200
2985/2985 - 9s - loss: 3.1931e-04 - val_loss: 3.0660e-04 - 9s/epoch - 3ms/step
Epoch 138/200
2985/2985 - 9s - loss: 3.1881e-04 - val_loss: 2.5651e-04 - 9s/epoch - 3ms/step
Epoch 139/200
2985/2985 - 9s - loss: 3.4411e-04 - val_loss: 2.9450e-04 - 9s/epoch - 3ms/step
Epoch 140/200
2985/2985 - 9s - loss: 3.2693e-04 - val_loss: 3.9090e-04 - 9s/epoch - 3ms/step
Epoch 141/200
2985/2985 - 9s - loss: 3.3738e-04 - val_loss: 4.0541e-04 - 9s/epoch - 3ms/step
Epoch 142/200
2985/2985 - 9s - loss: 3.4421e-04 - val_loss: 2.4612e-04 - 9s/epoch - 3ms/step
Epoch 143/200
2985/2985 - 9s - loss: 3.1690e-04 - val_loss: 2.5413e-04 - 9s/epoch - 3ms/step
Epoch 144/200
2985/2985 - 9s - loss: 3.1900e-04 - val_loss: 2.4799e-04 - 9s/epoch - 3ms/step
Epoch 145/200
2985/2985 - 9s - loss: 3.1654e-04 - val_loss: 2.3678e-04 - 9s/epoch - 3ms/step
Epoch 146/200
2985/2985 - 9s - loss: 3.3234e-04 - val_loss: 2.4448e-04 - 9s/epoch - 3ms/step
Epoch 147/200
2985/2985 - 9s - loss: 3.1199e-04 - val_loss: 2.4798e-04 - 9s/epoch - 3ms/step
Epoch 148/200
2985/2985 - 9s - loss: 3.1305e-04 - val_loss: 2.7653e-04 - 9s/epoch - 3ms/step
Epoch 149/200
2985/2985 - 9s - loss: 3.1273e-04 - val_loss: 2.3635e-04 - 9s/epoch - 3ms/step
Epoch 150/200
2985/2985 - 9s - loss: 3.1158e-04 - val_loss: 2.4420e-04 - 9s/epoch - 3ms/step
Epoch 151/200
2985/2985 - 9s - loss: 3.0957e-04 - val_loss: 2.3432e-04 - 9s/epoch - 3ms/step
Epoch 152/200
2985/2985 - 9s - loss: 3.0841e-04 - val_loss: 2.3460e-04 - 9s/epoch - 3ms/step
Epoch 153/200
2985/2985 - 9s - loss: 3.1215e-04 - val_loss: 2.4116e-04 - 9s/epoch - 3ms/step
Epoch 154/200
2985/2985 - 9s - loss: 3.0297e-04 - val_loss: 2.4851e-04 - 9s/epoch - 3ms/step
Epoch 155/200
2985/2985 - 9s - loss: 3.3038e-04 - val_loss: 2.3823e-04 - 9s/epoch - 3ms/step
Epoch 156/200
2985/2985 - 9s - loss: 3.1894e-04 - val_loss: 2.4016e-04 - 9s/epoch - 3ms/step
Epoch 157/200
2985/2985 - 9s - loss: 3.0393e-04 - val_loss: 2.5164e-04 - 9s/epoch - 3ms/step
Epoch 158/200
2985/2985 - 9s - loss: 3.0549e-04 - val_loss: 2.4048e-04 - 9s/epoch - 3ms/step
Epoch 159/200
2985/2985 - 9s - loss: 3.0847e-04 - val_loss: 2.2961e-04 - 9s/epoch - 3ms/step
Epoch 160/200
2985/2985 - 9s - loss: 3.0533e-04 - val_loss: 4.1137e-04 - 9s/epoch - 3ms/step
Epoch 161/200
2985/2985 - 9s - loss: 3.5899e-04 - val_loss: 2.7966e-04 - 9s/epoch - 3ms/step
Epoch 162/200
2985/2985 - 9s - loss: 3.0651e-04 - val_loss: 2.4621e-04 - 9s/epoch - 3ms/step
Epoch 163/200
2985/2985 - 9s - loss: 3.0381e-04 - val_loss: 2.4784e-04 - 9s/epoch - 3ms/step
Epoch 164/200
2985/2985 - 9s - loss: 3.2049e-04 - val_loss: 3.7245e-04 - 9s/epoch - 3ms/step
Epoch 165/200
2985/2985 - 9s - loss: 3.0275e-04 - val_loss: 2.4620e-04 - 9s/epoch - 3ms/step
Epoch 166/200
2985/2985 - 9s - loss: 3.0826e-04 - val_loss: 2.3746e-04 - 9s/epoch - 3ms/step
Epoch 167/200
2985/2985 - 9s - loss: 3.0257e-04 - val_loss: 2.4265e-04 - 9s/epoch - 3ms/step
Epoch 168/200
2985/2985 - 9s - loss: 3.0060e-04 - val_loss: 2.3166e-04 - 9s/epoch - 3ms/step
Epoch 169/200
2985/2985 - 9s - loss: 2.9776e-04 - val_loss: 2.5445e-04 - 9s/epoch - 3ms/step
Epoch 170/200
2985/2985 - 9s - loss: 3.4189e-04 - val_loss: 3.4827e-04 - 9s/epoch - 3ms/step
Epoch 171/200
2985/2985 - 9s - loss: 3.5067e-04 - val_loss: 2.4383e-04 - 9s/epoch - 3ms/step
Epoch 172/200
2985/2985 - 9s - loss: 3.0306e-04 - val_loss: 2.3897e-04 - 9s/epoch - 3ms/step
Epoch 173/200
2985/2985 - 9s - loss: 3.0857e-04 - val_loss: 2.3295e-04 - 9s/epoch - 3ms/step
Epoch 174/200
2985/2985 - 9s - loss: 3.0857e-04 - val_loss: 2.4586e-04 - 9s/epoch - 3ms/step
Epoch 175/200
2985/2985 - 9s - loss: 2.9748e-04 - val_loss: 2.4936e-04 - 9s/epoch - 3ms/step
Epoch 176/200
2985/2985 - 9s - loss: 3.0306e-04 - val_loss: 2.4158e-04 - 9s/epoch - 3ms/step
Epoch 177/200
2985/2985 - 9s - loss: 3.0661e-04 - val_loss: 2.2413e-04 - 9s/epoch - 3ms/step
Epoch 178/200
2985/2985 - 9s - loss: 2.9590e-04 - val_loss: 2.2835e-04 - 9s/epoch - 3ms/step
Epoch 179/200
2985/2985 - 9s - loss: 2.9736e-04 - val_loss: 2.3098e-04 - 9s/epoch - 3ms/step
Epoch 180/200
2985/2985 - 9s - loss: 2.9481e-04 - val_loss: 3.2775e-04 - 9s/epoch - 3ms/step
Epoch 181/200
2985/2985 - 9s - loss: 3.0517e-04 - val_loss: 2.5172e-04 - 9s/epoch - 3ms/step
Epoch 182/200
2985/2985 - 9s - loss: 2.9887e-04 - val_loss: 2.3039e-04 - 9s/epoch - 3ms/step
Epoch 183/200
2985/2985 - 9s - loss: 2.9664e-04 - val_loss: 2.3172e-04 - 9s/epoch - 3ms/step
Epoch 184/200
2985/2985 - 9s - loss: 3.1603e-04 - val_loss: 2.3470e-04 - 9s/epoch - 3ms/step
Epoch 185/200
2985/2985 - 9s - loss: 2.9829e-04 - val_loss: 2.9052e-04 - 9s/epoch - 3ms/step
Epoch 186/200
2985/2985 - 9s - loss: 2.9575e-04 - val_loss: 2.1742e-04 - 9s/epoch - 3ms/step
Epoch 187/200
2985/2985 - 9s - loss: 2.9674e-04 - val_loss: 2.3871e-04 - 9s/epoch - 3ms/step
Epoch 188/200
2985/2985 - 9s - loss: 2.9609e-04 - val_loss: 2.4535e-04 - 9s/epoch - 3ms/step
Epoch 189/200
2985/2985 - 9s - loss: 2.9814e-04 - val_loss: 2.2001e-04 - 9s/epoch - 3ms/step
Epoch 190/200
2985/2985 - 9s - loss: 2.8984e-04 - val_loss: 2.4225e-04 - 9s/epoch - 3ms/step
Epoch 191/200
2985/2985 - 9s - loss: 3.3786e-04 - val_loss: 3.0538e-04 - 9s/epoch - 3ms/step
Epoch 192/200
2985/2985 - 9s - loss: 3.1562e-04 - val_loss: 2.7237e-04 - 9s/epoch - 3ms/step
Epoch 193/200
2985/2985 - 9s - loss: 2.9684e-04 - val_loss: 2.4596e-04 - 9s/epoch - 3ms/step
Epoch 194/200
2985/2985 - 9s - loss: 2.8932e-04 - val_loss: 2.3837e-04 - 9s/epoch - 3ms/step
Epoch 195/200
2985/2985 - 9s - loss: 2.9178e-04 - val_loss: 2.3609e-04 - 9s/epoch - 3ms/step
Epoch 196/200
2985/2985 - 9s - loss: 3.0957e-04 - val_loss: 2.7038e-04 - 9s/epoch - 3ms/step
Epoch 197/200
2985/2985 - 9s - loss: 2.9590e-04 - val_loss: 2.3181e-04 - 9s/epoch - 3ms/step
Epoch 198/200
2985/2985 - 9s - loss: 2.9309e-04 - val_loss: 2.9266e-04 - 9s/epoch - 3ms/step
Epoch 199/200
2985/2985 - 9s - loss: 2.9579e-04 - val_loss: 2.5577e-04 - 9s/epoch - 3ms/step
Epoch 200/200
2985/2985 - 9s - loss: 2.9054e-04 - val_loss: 2.3468e-04 - 9s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00023468132712878287
  1/332 [..............................] - ETA: 30s 49/332 [===>..........................] - ETA: 0s  97/332 [=======>......................] - ETA: 0s146/332 [============>.................] - ETA: 0s195/332 [================>.............] - ETA: 0s244/332 [=====================>........] - ETA: 0s293/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0026652862328742522
cosine 0.0021308303002901363
MAE: 0.008426009
RMSE: 0.0153193055
r2: 0.9847764529322112
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_18 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_20 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2528)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_19"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_20 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_18 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_20"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_21 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_19 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 632)               0         
                                                                 
 dense_19 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_20 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2528)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 32, 200, 0.002, 0.5, 632, 0.00029054080368950963, 0.00023468132712878287, 0.0026652862328742522, 0.0021308303002901363, 0.008426008746027946, 0.01531930547207594, 0.9847764529322112, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_21 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_21 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_22 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 632)               0         
                                                                 
 dense_22 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_23 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2528)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/300
2985/2985 - 9s - loss: 0.0080 - val_loss: 0.0033 - 9s/epoch - 3ms/step
Epoch 2/300
2985/2985 - 9s - loss: 0.0030 - val_loss: 0.0019 - 9s/epoch - 3ms/step
Epoch 3/300
2985/2985 - 9s - loss: 0.0026 - val_loss: 0.0013 - 9s/epoch - 3ms/step
Epoch 4/300
2985/2985 - 9s - loss: 0.0019 - val_loss: 0.0014 - 9s/epoch - 3ms/step
Epoch 5/300
2985/2985 - 9s - loss: 0.0016 - val_loss: 0.0010 - 9s/epoch - 3ms/step
Epoch 6/300
2985/2985 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 3ms/step
Epoch 7/300
2985/2985 - 9s - loss: 0.0011 - val_loss: 7.1720e-04 - 9s/epoch - 3ms/step
Epoch 8/300
2985/2985 - 9s - loss: 8.7127e-04 - val_loss: 7.4513e-04 - 9s/epoch - 3ms/step
Epoch 9/300
2985/2985 - 9s - loss: 7.7468e-04 - val_loss: 5.8089e-04 - 9s/epoch - 3ms/step
Epoch 10/300
2985/2985 - 9s - loss: 6.8173e-04 - val_loss: 5.4442e-04 - 9s/epoch - 3ms/step
Epoch 11/300
2985/2985 - 9s - loss: 6.3336e-04 - val_loss: 5.1320e-04 - 9s/epoch - 3ms/step
Epoch 12/300
2985/2985 - 9s - loss: 5.7478e-04 - val_loss: 6.4388e-04 - 9s/epoch - 3ms/step
Epoch 13/300
2985/2985 - 9s - loss: 5.5838e-04 - val_loss: 4.4905e-04 - 9s/epoch - 3ms/step
Epoch 14/300
2985/2985 - 9s - loss: 5.0767e-04 - val_loss: 4.4617e-04 - 9s/epoch - 3ms/step
Epoch 15/300
2985/2985 - 9s - loss: 4.8981e-04 - val_loss: 3.7703e-04 - 9s/epoch - 3ms/step
Epoch 16/300
2985/2985 - 9s - loss: 4.5640e-04 - val_loss: 3.8758e-04 - 9s/epoch - 3ms/step
Epoch 17/300
2985/2985 - 9s - loss: 4.3723e-04 - val_loss: 3.6956e-04 - 9s/epoch - 3ms/step
Epoch 18/300
2985/2985 - 9s - loss: 4.1839e-04 - val_loss: 3.8571e-04 - 9s/epoch - 3ms/step
Epoch 19/300
2985/2985 - 9s - loss: 4.0009e-04 - val_loss: 3.4950e-04 - 9s/epoch - 3ms/step
Epoch 20/300
2985/2985 - 9s - loss: 3.9507e-04 - val_loss: 3.3109e-04 - 9s/epoch - 3ms/step
Epoch 21/300
2985/2985 - 9s - loss: 3.8380e-04 - val_loss: 3.1924e-04 - 9s/epoch - 3ms/step
Epoch 22/300
2985/2985 - 9s - loss: 3.6463e-04 - val_loss: 3.7933e-04 - 9s/epoch - 3ms/step
Epoch 23/300
2985/2985 - 9s - loss: 3.7192e-04 - val_loss: 4.2241e-04 - 9s/epoch - 3ms/step
Epoch 24/300
2985/2985 - 9s - loss: 3.6025e-04 - val_loss: 3.2880e-04 - 9s/epoch - 3ms/step
Epoch 25/300
2985/2985 - 9s - loss: 3.5640e-04 - val_loss: 2.7832e-04 - 9s/epoch - 3ms/step
Epoch 26/300
2985/2985 - 9s - loss: 3.3716e-04 - val_loss: 2.5915e-04 - 9s/epoch - 3ms/step
Epoch 27/300
2985/2985 - 9s - loss: 3.2546e-04 - val_loss: 2.7534e-04 - 9s/epoch - 3ms/step
Epoch 28/300
2985/2985 - 9s - loss: 3.2569e-04 - val_loss: 2.7501e-04 - 9s/epoch - 3ms/step
Epoch 29/300
2985/2985 - 9s - loss: 3.1142e-04 - val_loss: 2.7651e-04 - 9s/epoch - 3ms/step
Epoch 30/300
2985/2985 - 9s - loss: 3.1047e-04 - val_loss: 2.6599e-04 - 9s/epoch - 3ms/step
Epoch 31/300
2985/2985 - 9s - loss: 2.9783e-04 - val_loss: 2.8860e-04 - 9s/epoch - 3ms/step
Epoch 32/300
2985/2985 - 9s - loss: 2.9762e-04 - val_loss: 2.4661e-04 - 9s/epoch - 3ms/step
Epoch 33/300
2985/2985 - 9s - loss: 2.9010e-04 - val_loss: 2.4536e-04 - 9s/epoch - 3ms/step
Epoch 34/300
2985/2985 - 9s - loss: 2.8270e-04 - val_loss: 2.6457e-04 - 9s/epoch - 3ms/step
Epoch 35/300
2985/2985 - 9s - loss: 2.8414e-04 - val_loss: 2.4164e-04 - 9s/epoch - 3ms/step
Epoch 36/300
2985/2985 - 9s - loss: 2.7527e-04 - val_loss: 2.6389e-04 - 9s/epoch - 3ms/step
Epoch 37/300
2985/2985 - 9s - loss: 2.8100e-04 - val_loss: 2.3860e-04 - 9s/epoch - 3ms/step
Epoch 38/300
2985/2985 - 9s - loss: 2.8289e-04 - val_loss: 2.1660e-04 - 9s/epoch - 3ms/step
Epoch 39/300
2985/2985 - 9s - loss: 2.7889e-04 - val_loss: 2.1017e-04 - 9s/epoch - 3ms/step
Epoch 40/300
2985/2985 - 9s - loss: 2.6133e-04 - val_loss: 2.4822e-04 - 9s/epoch - 3ms/step
Epoch 41/300
2985/2985 - 9s - loss: 2.5841e-04 - val_loss: 2.4166e-04 - 9s/epoch - 3ms/step
Epoch 42/300
2985/2985 - 9s - loss: 2.5108e-04 - val_loss: 2.0831e-04 - 9s/epoch - 3ms/step
Epoch 43/300
2985/2985 - 9s - loss: 2.5056e-04 - val_loss: 2.2226e-04 - 9s/epoch - 3ms/step
Epoch 44/300
2985/2985 - 9s - loss: 2.4668e-04 - val_loss: 2.3405e-04 - 9s/epoch - 3ms/step
Epoch 45/300
2985/2985 - 9s - loss: 2.6144e-04 - val_loss: 2.0956e-04 - 9s/epoch - 3ms/step
Epoch 46/300
2985/2985 - 9s - loss: 2.4852e-04 - val_loss: 2.0380e-04 - 9s/epoch - 3ms/step
Epoch 47/300
2985/2985 - 9s - loss: 2.3974e-04 - val_loss: 2.0074e-04 - 9s/epoch - 3ms/step
Epoch 48/300
2985/2985 - 9s - loss: 2.6330e-04 - val_loss: 2.1027e-04 - 9s/epoch - 3ms/step
Epoch 49/300
2985/2985 - 9s - loss: 2.3549e-04 - val_loss: 2.3407e-04 - 9s/epoch - 3ms/step
Epoch 50/300
2985/2985 - 9s - loss: 2.3678e-04 - val_loss: 6.2778e-04 - 9s/epoch - 3ms/step
Epoch 51/300
2985/2985 - 9s - loss: 2.6570e-04 - val_loss: 1.9369e-04 - 9s/epoch - 3ms/step
Epoch 52/300
2985/2985 - 9s - loss: 2.5996e-04 - val_loss: 1.9046e-04 - 9s/epoch - 3ms/step
Epoch 53/300
2985/2985 - 9s - loss: 2.3131e-04 - val_loss: 2.0159e-04 - 9s/epoch - 3ms/step
Epoch 54/300
2985/2985 - 9s - loss: 2.2578e-04 - val_loss: 1.9613e-04 - 9s/epoch - 3ms/step
Epoch 55/300
2985/2985 - 9s - loss: 2.2177e-04 - val_loss: 1.9368e-04 - 9s/epoch - 3ms/step
Epoch 56/300
2985/2985 - 9s - loss: 2.2169e-04 - val_loss: 1.9674e-04 - 9s/epoch - 3ms/step
Epoch 57/300
2985/2985 - 9s - loss: 2.2227e-04 - val_loss: 2.0169e-04 - 9s/epoch - 3ms/step
Epoch 58/300
2985/2985 - 9s - loss: 2.1765e-04 - val_loss: 2.0031e-04 - 9s/epoch - 3ms/step
Epoch 59/300
2985/2985 - 9s - loss: 2.1722e-04 - val_loss: 1.8724e-04 - 9s/epoch - 3ms/step
Epoch 60/300
2985/2985 - 9s - loss: 2.3221e-04 - val_loss: 1.6968e-04 - 9s/epoch - 3ms/step
Epoch 61/300
2985/2985 - 9s - loss: 2.1503e-04 - val_loss: 1.7077e-04 - 9s/epoch - 3ms/step
Epoch 62/300
2985/2985 - 9s - loss: 2.1387e-04 - val_loss: 2.1544e-04 - 9s/epoch - 3ms/step
Epoch 63/300
2985/2985 - 9s - loss: 2.0742e-04 - val_loss: 1.8034e-04 - 9s/epoch - 3ms/step
Epoch 64/300
2985/2985 - 9s - loss: 2.1552e-04 - val_loss: 1.7226e-04 - 9s/epoch - 3ms/step
Epoch 65/300
2985/2985 - 9s - loss: 2.1179e-04 - val_loss: 2.0561e-04 - 9s/epoch - 3ms/step
Epoch 66/300
2985/2985 - 9s - loss: 2.0522e-04 - val_loss: 1.7981e-04 - 9s/epoch - 3ms/step
Epoch 67/300
2985/2985 - 9s - loss: 2.0366e-04 - val_loss: 1.7485e-04 - 9s/epoch - 3ms/step
Epoch 68/300
2985/2985 - 9s - loss: 2.0429e-04 - val_loss: 1.7989e-04 - 9s/epoch - 3ms/step
Epoch 69/300
2985/2985 - 9s - loss: 2.0572e-04 - val_loss: 1.7687e-04 - 9s/epoch - 3ms/step
Epoch 70/300
2985/2985 - 9s - loss: 1.9926e-04 - val_loss: 1.8347e-04 - 9s/epoch - 3ms/step
Epoch 71/300
2985/2985 - 9s - loss: 2.0021e-04 - val_loss: 1.7771e-04 - 9s/epoch - 3ms/step
Epoch 72/300
2985/2985 - 9s - loss: 1.9796e-04 - val_loss: 2.0604e-04 - 9s/epoch - 3ms/step
Epoch 73/300
2985/2985 - 9s - loss: 2.0919e-04 - val_loss: 1.8206e-04 - 9s/epoch - 3ms/step
Epoch 74/300
2985/2985 - 9s - loss: 1.9647e-04 - val_loss: 1.7178e-04 - 9s/epoch - 3ms/step
Epoch 75/300
2985/2985 - 9s - loss: 2.0453e-04 - val_loss: 1.7171e-04 - 9s/epoch - 3ms/step
Epoch 76/300
2985/2985 - 9s - loss: 1.9270e-04 - val_loss: 1.7566e-04 - 9s/epoch - 3ms/step
Epoch 77/300
2985/2985 - 9s - loss: 2.0404e-04 - val_loss: 4.4400e-04 - 9s/epoch - 3ms/step
Epoch 78/300
2985/2985 - 9s - loss: 2.3413e-04 - val_loss: 1.6937e-04 - 9s/epoch - 3ms/step
Epoch 79/300
2985/2985 - 9s - loss: 1.9718e-04 - val_loss: 2.1917e-04 - 9s/epoch - 3ms/step
Epoch 80/300
2985/2985 - 9s - loss: 1.9924e-04 - val_loss: 1.7155e-04 - 9s/epoch - 3ms/step
Epoch 81/300
2985/2985 - 9s - loss: 1.9039e-04 - val_loss: 1.9554e-04 - 9s/epoch - 3ms/step
Epoch 82/300
2985/2985 - 9s - loss: 1.9285e-04 - val_loss: 1.7287e-04 - 9s/epoch - 3ms/step
Epoch 83/300
2985/2985 - 9s - loss: 1.8808e-04 - val_loss: 1.7548e-04 - 9s/epoch - 3ms/step
Epoch 84/300
2985/2985 - 9s - loss: 1.8770e-04 - val_loss: 1.6326e-04 - 9s/epoch - 3ms/step
Epoch 85/300
2985/2985 - 9s - loss: 1.8733e-04 - val_loss: 1.6232e-04 - 9s/epoch - 3ms/step
Epoch 86/300
2985/2985 - 9s - loss: 1.8357e-04 - val_loss: 1.6183e-04 - 9s/epoch - 3ms/step
Epoch 87/300
2985/2985 - 9s - loss: 1.8746e-04 - val_loss: 1.8916e-04 - 9s/epoch - 3ms/step
Epoch 88/300
2985/2985 - 9s - loss: 1.8298e-04 - val_loss: 1.5903e-04 - 9s/epoch - 3ms/step
Epoch 89/300
2985/2985 - 9s - loss: 1.9230e-04 - val_loss: 1.6307e-04 - 9s/epoch - 3ms/step
Epoch 90/300
2985/2985 - 9s - loss: 1.8089e-04 - val_loss: 1.5618e-04 - 9s/epoch - 3ms/step
Epoch 91/300
2985/2985 - 9s - loss: 1.7910e-04 - val_loss: 1.7696e-04 - 9s/epoch - 3ms/step
Epoch 92/300
2985/2985 - 9s - loss: 1.8319e-04 - val_loss: 1.6465e-04 - 9s/epoch - 3ms/step
Epoch 93/300
2985/2985 - 9s - loss: 1.7892e-04 - val_loss: 1.6551e-04 - 9s/epoch - 3ms/step
Epoch 94/300
2985/2985 - 9s - loss: 1.7576e-04 - val_loss: 1.5511e-04 - 9s/epoch - 3ms/step
Epoch 95/300
2985/2985 - 9s - loss: 1.8527e-04 - val_loss: 2.4723e-04 - 9s/epoch - 3ms/step
Epoch 96/300
2985/2985 - 9s - loss: 1.8392e-04 - val_loss: 1.6930e-04 - 9s/epoch - 3ms/step
Epoch 97/300
2985/2985 - 9s - loss: 1.7658e-04 - val_loss: 1.7041e-04 - 9s/epoch - 3ms/step
Epoch 98/300
2985/2985 - 9s - loss: 1.7584e-04 - val_loss: 1.6595e-04 - 9s/epoch - 3ms/step
Epoch 99/300
2985/2985 - 9s - loss: 1.7690e-04 - val_loss: 1.5337e-04 - 9s/epoch - 3ms/step
Epoch 100/300
2985/2985 - 9s - loss: 1.8378e-04 - val_loss: 1.5125e-04 - 9s/epoch - 3ms/step
Epoch 101/300
2985/2985 - 9s - loss: 1.7592e-04 - val_loss: 1.6316e-04 - 9s/epoch - 3ms/step
Epoch 102/300
2985/2985 - 9s - loss: 1.7509e-04 - val_loss: 1.6477e-04 - 9s/epoch - 3ms/step
Epoch 103/300
2985/2985 - 9s - loss: 1.7277e-04 - val_loss: 1.5864e-04 - 9s/epoch - 3ms/step
Epoch 104/300
2985/2985 - 9s - loss: 1.7832e-04 - val_loss: 2.0922e-04 - 9s/epoch - 3ms/step
Epoch 105/300
2985/2985 - 9s - loss: 1.7688e-04 - val_loss: 1.6426e-04 - 9s/epoch - 3ms/step
Epoch 106/300
2985/2985 - 9s - loss: 1.7629e-04 - val_loss: 1.6869e-04 - 9s/epoch - 3ms/step
Epoch 107/300
2985/2985 - 9s - loss: 1.6952e-04 - val_loss: 1.6291e-04 - 9s/epoch - 3ms/step
Epoch 108/300
2985/2985 - 9s - loss: 1.6951e-04 - val_loss: 1.7406e-04 - 9s/epoch - 3ms/step
Epoch 109/300
2985/2985 - 9s - loss: 1.6756e-04 - val_loss: 1.6416e-04 - 9s/epoch - 3ms/step
Epoch 110/300
2985/2985 - 9s - loss: 1.6761e-04 - val_loss: 1.7769e-04 - 9s/epoch - 3ms/step
Epoch 111/300
2985/2985 - 9s - loss: 1.6990e-04 - val_loss: 1.5418e-04 - 9s/epoch - 3ms/step
Epoch 112/300
2985/2985 - 9s - loss: 1.6694e-04 - val_loss: 1.5404e-04 - 9s/epoch - 3ms/step
Epoch 113/300
2985/2985 - 9s - loss: 1.6582e-04 - val_loss: 2.0558e-04 - 9s/epoch - 3ms/step
Epoch 114/300
2985/2985 - 9s - loss: 1.7733e-04 - val_loss: 1.4344e-04 - 9s/epoch - 3ms/step
Epoch 115/300
2985/2985 - 9s - loss: 1.6632e-04 - val_loss: 1.6662e-04 - 9s/epoch - 3ms/step
Epoch 116/300
2985/2985 - 9s - loss: 1.7275e-04 - val_loss: 1.5395e-04 - 9s/epoch - 3ms/step
Epoch 117/300
2985/2985 - 9s - loss: 1.6386e-04 - val_loss: 1.4334e-04 - 9s/epoch - 3ms/step
Epoch 118/300
2985/2985 - 9s - loss: 1.6582e-04 - val_loss: 1.5540e-04 - 9s/epoch - 3ms/step
Epoch 119/300
2985/2985 - 9s - loss: 1.6385e-04 - val_loss: 1.4768e-04 - 9s/epoch - 3ms/step
Epoch 120/300
2985/2985 - 9s - loss: 1.6222e-04 - val_loss: 1.6026e-04 - 9s/epoch - 3ms/step
Epoch 121/300
2985/2985 - 9s - loss: 1.6580e-04 - val_loss: 1.6726e-04 - 9s/epoch - 3ms/step
Epoch 122/300
2985/2985 - 9s - loss: 1.6594e-04 - val_loss: 1.4134e-04 - 9s/epoch - 3ms/step
Epoch 123/300
2985/2985 - 9s - loss: 1.6109e-04 - val_loss: 1.5882e-04 - 9s/epoch - 3ms/step
Epoch 124/300
2985/2985 - 9s - loss: 1.6381e-04 - val_loss: 1.9356e-04 - 9s/epoch - 3ms/step
Epoch 125/300
2985/2985 - 9s - loss: 1.6383e-04 - val_loss: 1.4203e-04 - 9s/epoch - 3ms/step
Epoch 126/300
2985/2985 - 9s - loss: 1.6268e-04 - val_loss: 1.5090e-04 - 9s/epoch - 3ms/step
Epoch 127/300
2985/2985 - 9s - loss: 1.6070e-04 - val_loss: 1.5149e-04 - 9s/epoch - 3ms/step
Epoch 128/300
2985/2985 - 9s - loss: 1.5965e-04 - val_loss: 1.5313e-04 - 9s/epoch - 3ms/step
Epoch 129/300
2985/2985 - 9s - loss: 1.6635e-04 - val_loss: 1.4491e-04 - 9s/epoch - 3ms/step
Epoch 130/300
2985/2985 - 9s - loss: 1.5953e-04 - val_loss: 1.4146e-04 - 9s/epoch - 3ms/step
Epoch 131/300
2985/2985 - 9s - loss: 1.5802e-04 - val_loss: 1.6128e-04 - 9s/epoch - 3ms/step
Epoch 132/300
2985/2985 - 9s - loss: 1.5996e-04 - val_loss: 1.4340e-04 - 9s/epoch - 3ms/step
Epoch 133/300
2985/2985 - 9s - loss: 1.5979e-04 - val_loss: 1.5153e-04 - 9s/epoch - 3ms/step
Epoch 134/300
2985/2985 - 9s - loss: 1.5805e-04 - val_loss: 1.5479e-04 - 9s/epoch - 3ms/step
Epoch 135/300
2985/2985 - 9s - loss: 1.5804e-04 - val_loss: 1.4549e-04 - 9s/epoch - 3ms/step
Epoch 136/300
2985/2985 - 9s - loss: 1.6420e-04 - val_loss: 1.3868e-04 - 9s/epoch - 3ms/step
Epoch 137/300
2985/2985 - 9s - loss: 1.5791e-04 - val_loss: 1.4406e-04 - 9s/epoch - 3ms/step
Epoch 138/300
2985/2985 - 9s - loss: 1.6040e-04 - val_loss: 1.4741e-04 - 9s/epoch - 3ms/step
Epoch 139/300
2985/2985 - 9s - loss: 1.5585e-04 - val_loss: 1.4158e-04 - 9s/epoch - 3ms/step
Epoch 140/300
2985/2985 - 9s - loss: 1.5536e-04 - val_loss: 1.4920e-04 - 9s/epoch - 3ms/step
Epoch 141/300
2985/2985 - 9s - loss: 1.5545e-04 - val_loss: 1.5038e-04 - 9s/epoch - 3ms/step
Epoch 142/300
2985/2985 - 9s - loss: 1.5578e-04 - val_loss: 1.5749e-04 - 9s/epoch - 3ms/step
Epoch 143/300
2985/2985 - 9s - loss: 1.5719e-04 - val_loss: 1.3881e-04 - 9s/epoch - 3ms/step
Epoch 144/300
2985/2985 - 9s - loss: 1.6643e-04 - val_loss: 1.4026e-04 - 9s/epoch - 3ms/step
Epoch 145/300
2985/2985 - 9s - loss: 1.5859e-04 - val_loss: 1.3404e-04 - 9s/epoch - 3ms/step
Epoch 146/300
2985/2985 - 9s - loss: 1.5367e-04 - val_loss: 1.5753e-04 - 9s/epoch - 3ms/step
Epoch 147/300
2985/2985 - 9s - loss: 1.5923e-04 - val_loss: 1.7565e-04 - 9s/epoch - 3ms/step
Epoch 148/300
2985/2985 - 9s - loss: 1.5546e-04 - val_loss: 1.6400e-04 - 9s/epoch - 3ms/step
Epoch 149/300
2985/2985 - 9s - loss: 1.5109e-04 - val_loss: 1.4061e-04 - 9s/epoch - 3ms/step
Epoch 150/300
2985/2985 - 9s - loss: 1.5802e-04 - val_loss: 1.3091e-04 - 9s/epoch - 3ms/step
Epoch 151/300
2985/2985 - 9s - loss: 1.6095e-04 - val_loss: 1.4649e-04 - 9s/epoch - 3ms/step
Epoch 152/300
2985/2985 - 9s - loss: 1.5350e-04 - val_loss: 1.3060e-04 - 9s/epoch - 3ms/step
Epoch 153/300
2985/2985 - 9s - loss: 1.5207e-04 - val_loss: 1.5652e-04 - 9s/epoch - 3ms/step
Epoch 154/300
2985/2985 - 9s - loss: 1.5223e-04 - val_loss: 1.4771e-04 - 9s/epoch - 3ms/step
Epoch 155/300
2985/2985 - 9s - loss: 1.4997e-04 - val_loss: 1.5927e-04 - 9s/epoch - 3ms/step
Epoch 156/300
2985/2985 - 9s - loss: 1.5141e-04 - val_loss: 1.4463e-04 - 9s/epoch - 3ms/step
Epoch 157/300
2985/2985 - 9s - loss: 1.4806e-04 - val_loss: 1.5592e-04 - 9s/epoch - 3ms/step
Epoch 158/300
2985/2985 - 9s - loss: 1.5013e-04 - val_loss: 1.4042e-04 - 9s/epoch - 3ms/step
Epoch 159/300
2985/2985 - 9s - loss: 1.5162e-04 - val_loss: 1.2893e-04 - 9s/epoch - 3ms/step
Epoch 160/300
2985/2985 - 9s - loss: 1.5439e-04 - val_loss: 1.4247e-04 - 9s/epoch - 3ms/step
Epoch 161/300
2985/2985 - 9s - loss: 1.5139e-04 - val_loss: 1.4914e-04 - 9s/epoch - 3ms/step
Epoch 162/300
2985/2985 - 9s - loss: 1.5210e-04 - val_loss: 1.4809e-04 - 9s/epoch - 3ms/step
Epoch 163/300
2985/2985 - 9s - loss: 1.4865e-04 - val_loss: 1.3065e-04 - 9s/epoch - 3ms/step
Epoch 164/300
2985/2985 - 9s - loss: 1.5101e-04 - val_loss: 1.6709e-04 - 9s/epoch - 3ms/step
Epoch 165/300
2985/2985 - 9s - loss: 1.4792e-04 - val_loss: 1.4042e-04 - 9s/epoch - 3ms/step
Epoch 166/300
2985/2985 - 9s - loss: 1.4668e-04 - val_loss: 1.5945e-04 - 9s/epoch - 3ms/step
Epoch 167/300
2985/2985 - 9s - loss: 1.5112e-04 - val_loss: 1.4502e-04 - 9s/epoch - 3ms/step
Epoch 168/300
2985/2985 - 9s - loss: 1.4889e-04 - val_loss: 1.4058e-04 - 9s/epoch - 3ms/step
Epoch 169/300
2985/2985 - 9s - loss: 1.4668e-04 - val_loss: 1.5765e-04 - 9s/epoch - 3ms/step
Epoch 170/300
2985/2985 - 9s - loss: 1.4977e-04 - val_loss: 1.2709e-04 - 9s/epoch - 3ms/step
Epoch 171/300
2985/2985 - 9s - loss: 1.4793e-04 - val_loss: 1.4394e-04 - 9s/epoch - 3ms/step
Epoch 172/300
2985/2985 - 9s - loss: 1.4964e-04 - val_loss: 1.5355e-04 - 9s/epoch - 3ms/step
Epoch 173/300
2985/2985 - 9s - loss: 1.4665e-04 - val_loss: 1.4795e-04 - 9s/epoch - 3ms/step
Epoch 174/300
2985/2985 - 9s - loss: 1.4668e-04 - val_loss: 1.2675e-04 - 9s/epoch - 3ms/step
Epoch 175/300
2985/2985 - 9s - loss: 1.5020e-04 - val_loss: 1.3753e-04 - 9s/epoch - 3ms/step
Epoch 176/300
2985/2985 - 9s - loss: 1.4681e-04 - val_loss: 1.3913e-04 - 9s/epoch - 3ms/step
Epoch 177/300
2985/2985 - 9s - loss: 1.4510e-04 - val_loss: 1.2886e-04 - 9s/epoch - 3ms/step
Epoch 178/300
2985/2985 - 9s - loss: 1.4683e-04 - val_loss: 1.2456e-04 - 9s/epoch - 3ms/step
Epoch 179/300
2985/2985 - 9s - loss: 1.4413e-04 - val_loss: 1.3632e-04 - 9s/epoch - 3ms/step
Epoch 180/300
2985/2985 - 9s - loss: 1.4383e-04 - val_loss: 1.4354e-04 - 9s/epoch - 3ms/step
Epoch 181/300
2985/2985 - 9s - loss: 1.4474e-04 - val_loss: 1.3032e-04 - 9s/epoch - 3ms/step
Epoch 182/300
2985/2985 - 9s - loss: 1.4338e-04 - val_loss: 1.4431e-04 - 9s/epoch - 3ms/step
Epoch 183/300
2985/2985 - 9s - loss: 1.4297e-04 - val_loss: 1.3493e-04 - 9s/epoch - 3ms/step
Epoch 184/300
2985/2985 - 9s - loss: 1.4988e-04 - val_loss: 1.3014e-04 - 9s/epoch - 3ms/step
Epoch 185/300
2985/2985 - 9s - loss: 1.4838e-04 - val_loss: 1.4803e-04 - 9s/epoch - 3ms/step
Epoch 186/300
2985/2985 - 9s - loss: 1.4415e-04 - val_loss: 1.2837e-04 - 9s/epoch - 3ms/step
Epoch 187/300
2985/2985 - 9s - loss: 1.5215e-04 - val_loss: 1.4375e-04 - 9s/epoch - 3ms/step
Epoch 188/300
2985/2985 - 9s - loss: 1.4296e-04 - val_loss: 1.3350e-04 - 9s/epoch - 3ms/step
Epoch 189/300
2985/2985 - 9s - loss: 1.5323e-04 - val_loss: 1.4324e-04 - 9s/epoch - 3ms/step
Epoch 190/300
2985/2985 - 9s - loss: 1.5578e-04 - val_loss: 1.2369e-04 - 9s/epoch - 3ms/step
Epoch 191/300
2985/2985 - 9s - loss: 1.4448e-04 - val_loss: 1.5161e-04 - 9s/epoch - 3ms/step
Epoch 192/300
2985/2985 - 9s - loss: 1.4549e-04 - val_loss: 1.6419e-04 - 9s/epoch - 3ms/step
Epoch 193/300
2985/2985 - 9s - loss: 1.4458e-04 - val_loss: 2.1826e-04 - 9s/epoch - 3ms/step
Epoch 194/300
2985/2985 - 9s - loss: 1.6453e-04 - val_loss: 1.5727e-04 - 9s/epoch - 3ms/step
Epoch 195/300
2985/2985 - 9s - loss: 1.4742e-04 - val_loss: 1.2518e-04 - 9s/epoch - 3ms/step
Epoch 196/300
2985/2985 - 9s - loss: 1.4237e-04 - val_loss: 1.2950e-04 - 9s/epoch - 3ms/step
Epoch 197/300
2985/2985 - 9s - loss: 1.4201e-04 - val_loss: 1.3520e-04 - 9s/epoch - 3ms/step
Epoch 198/300
2985/2985 - 9s - loss: 1.4097e-04 - val_loss: 1.6416e-04 - 9s/epoch - 3ms/step
Epoch 199/300
2985/2985 - 9s - loss: 1.4075e-04 - val_loss: 1.3690e-04 - 9s/epoch - 3ms/step
Epoch 200/300
2985/2985 - 9s - loss: 1.4212e-04 - val_loss: 1.2902e-04 - 9s/epoch - 3ms/step
Epoch 201/300
2985/2985 - 9s - loss: 1.3999e-04 - val_loss: 1.3290e-04 - 9s/epoch - 3ms/step
Epoch 202/300
2985/2985 - 9s - loss: 1.4075e-04 - val_loss: 1.4409e-04 - 9s/epoch - 3ms/step
Epoch 203/300
2985/2985 - 9s - loss: 1.4354e-04 - val_loss: 3.0281e-04 - 9s/epoch - 3ms/step
Epoch 204/300
2985/2985 - 9s - loss: 1.5692e-04 - val_loss: 1.2007e-04 - 9s/epoch - 3ms/step
Epoch 205/300
2985/2985 - 9s - loss: 1.3980e-04 - val_loss: 1.1884e-04 - 9s/epoch - 3ms/step
Epoch 206/300
2985/2985 - 9s - loss: 1.3875e-04 - val_loss: 1.5768e-04 - 9s/epoch - 3ms/step
Epoch 207/300
2985/2985 - 9s - loss: 1.4115e-04 - val_loss: 1.8334e-04 - 9s/epoch - 3ms/step
Epoch 208/300
2985/2985 - 9s - loss: 1.4433e-04 - val_loss: 1.4842e-04 - 9s/epoch - 3ms/step
Epoch 209/300
2985/2985 - 9s - loss: 1.4145e-04 - val_loss: 1.3986e-04 - 9s/epoch - 3ms/step
Epoch 210/300
2985/2985 - 9s - loss: 1.4069e-04 - val_loss: 1.2183e-04 - 9s/epoch - 3ms/step
Epoch 211/300
2985/2985 - 9s - loss: 1.4105e-04 - val_loss: 2.2348e-04 - 9s/epoch - 3ms/step
Epoch 212/300
2985/2985 - 9s - loss: 1.5466e-04 - val_loss: 1.2591e-04 - 9s/epoch - 3ms/step
Epoch 213/300
2985/2985 - 9s - loss: 1.4114e-04 - val_loss: 1.4883e-04 - 9s/epoch - 3ms/step
Epoch 214/300
2985/2985 - 9s - loss: 1.4158e-04 - val_loss: 1.3546e-04 - 9s/epoch - 3ms/step
Epoch 215/300
2985/2985 - 9s - loss: 1.4032e-04 - val_loss: 1.3440e-04 - 9s/epoch - 3ms/step
Epoch 216/300
2985/2985 - 9s - loss: 1.3798e-04 - val_loss: 1.3162e-04 - 9s/epoch - 3ms/step
Epoch 217/300
2985/2985 - 9s - loss: 1.3780e-04 - val_loss: 1.2738e-04 - 9s/epoch - 3ms/step
Epoch 218/300
2985/2985 - 9s - loss: 1.3797e-04 - val_loss: 1.6311e-04 - 9s/epoch - 3ms/step
Epoch 219/300
2985/2985 - 9s - loss: 1.3807e-04 - val_loss: 1.2004e-04 - 9s/epoch - 3ms/step
Epoch 220/300
2985/2985 - 9s - loss: 1.3875e-04 - val_loss: 1.2679e-04 - 9s/epoch - 3ms/step
Epoch 221/300
2985/2985 - 9s - loss: 1.3764e-04 - val_loss: 1.2695e-04 - 9s/epoch - 3ms/step
Epoch 222/300
2985/2985 - 9s - loss: 1.3881e-04 - val_loss: 1.1901e-04 - 9s/epoch - 3ms/step
Epoch 223/300
2985/2985 - 9s - loss: 1.3929e-04 - val_loss: 1.8958e-04 - 9s/epoch - 3ms/step
Epoch 224/300
2985/2985 - 9s - loss: 1.3540e-04 - val_loss: 1.3300e-04 - 9s/epoch - 3ms/step
Epoch 225/300
2985/2985 - 9s - loss: 1.3627e-04 - val_loss: 1.5665e-04 - 9s/epoch - 3ms/step
Epoch 226/300
2985/2985 - 9s - loss: 1.4734e-04 - val_loss: 1.5324e-04 - 9s/epoch - 3ms/step
Epoch 227/300
2985/2985 - 9s - loss: 1.4512e-04 - val_loss: 1.2810e-04 - 9s/epoch - 3ms/step
Epoch 228/300
2985/2985 - 9s - loss: 1.3537e-04 - val_loss: 1.4977e-04 - 9s/epoch - 3ms/step
Epoch 229/300
2985/2985 - 9s - loss: 1.3608e-04 - val_loss: 1.8016e-04 - 9s/epoch - 3ms/step
Epoch 230/300
2985/2985 - 9s - loss: 1.3579e-04 - val_loss: 1.3716e-04 - 9s/epoch - 3ms/step
Epoch 231/300
2985/2985 - 9s - loss: 1.3720e-04 - val_loss: 1.3155e-04 - 9s/epoch - 3ms/step
Epoch 232/300
2985/2985 - 9s - loss: 1.3617e-04 - val_loss: 1.2342e-04 - 9s/epoch - 3ms/step
Epoch 233/300
2985/2985 - 9s - loss: 1.3506e-04 - val_loss: 1.3894e-04 - 9s/epoch - 3ms/step
Epoch 234/300
2985/2985 - 9s - loss: 1.3627e-04 - val_loss: 1.4141e-04 - 9s/epoch - 3ms/step
Epoch 235/300
2985/2985 - 9s - loss: 1.3481e-04 - val_loss: 1.1153e-04 - 9s/epoch - 3ms/step
Epoch 236/300
2985/2985 - 9s - loss: 1.3433e-04 - val_loss: 1.3530e-04 - 9s/epoch - 3ms/step
Epoch 237/300
2985/2985 - 9s - loss: 1.3482e-04 - val_loss: 1.5144e-04 - 9s/epoch - 3ms/step
Epoch 238/300
2985/2985 - 9s - loss: 1.3545e-04 - val_loss: 1.3339e-04 - 9s/epoch - 3ms/step
Epoch 239/300
2985/2985 - 9s - loss: 1.3556e-04 - val_loss: 1.6285e-04 - 9s/epoch - 3ms/step
Epoch 240/300
2985/2985 - 9s - loss: 1.4647e-04 - val_loss: 1.2224e-04 - 9s/epoch - 3ms/step
Epoch 241/300
2985/2985 - 9s - loss: 1.3358e-04 - val_loss: 1.1852e-04 - 9s/epoch - 3ms/step
Epoch 242/300
2985/2985 - 9s - loss: 1.3350e-04 - val_loss: 1.2070e-04 - 9s/epoch - 3ms/step
Epoch 243/300
2985/2985 - 9s - loss: 1.3254e-04 - val_loss: 1.2079e-04 - 9s/epoch - 3ms/step
Epoch 244/300
2985/2985 - 9s - loss: 1.3302e-04 - val_loss: 1.2659e-04 - 9s/epoch - 3ms/step
Epoch 245/300
2985/2985 - 9s - loss: 1.3385e-04 - val_loss: 1.7188e-04 - 9s/epoch - 3ms/step
Epoch 246/300
2985/2985 - 9s - loss: 1.5888e-04 - val_loss: 1.4461e-04 - 9s/epoch - 3ms/step
Epoch 247/300
2985/2985 - 9s - loss: 1.4387e-04 - val_loss: 1.1502e-04 - 9s/epoch - 3ms/step
Epoch 248/300
2985/2985 - 9s - loss: 1.3649e-04 - val_loss: 1.3145e-04 - 9s/epoch - 3ms/step
Epoch 249/300
2985/2985 - 9s - loss: 1.3686e-04 - val_loss: 1.4343e-04 - 9s/epoch - 3ms/step
Epoch 250/300
2985/2985 - 9s - loss: 1.3650e-04 - val_loss: 1.1971e-04 - 9s/epoch - 3ms/step
Epoch 251/300
2985/2985 - 9s - loss: 1.3309e-04 - val_loss: 1.2375e-04 - 9s/epoch - 3ms/step
Epoch 252/300
2985/2985 - 9s - loss: 1.4127e-04 - val_loss: 1.4866e-04 - 9s/epoch - 3ms/step
Epoch 253/300
2985/2985 - 9s - loss: 1.4496e-04 - val_loss: 1.4177e-04 - 9s/epoch - 3ms/step
Epoch 254/300
2985/2985 - 9s - loss: 1.3600e-04 - val_loss: 1.1759e-04 - 9s/epoch - 3ms/step
Epoch 255/300
2985/2985 - 9s - loss: 1.3301e-04 - val_loss: 1.2134e-04 - 9s/epoch - 3ms/step
Epoch 256/300
2985/2985 - 9s - loss: 1.3430e-04 - val_loss: 1.2494e-04 - 9s/epoch - 3ms/step
Epoch 257/300
2985/2985 - 9s - loss: 1.3324e-04 - val_loss: 1.6291e-04 - 9s/epoch - 3ms/step
Epoch 258/300
2985/2985 - 9s - loss: 1.3946e-04 - val_loss: 1.2250e-04 - 9s/epoch - 3ms/step
Epoch 259/300
2985/2985 - 9s - loss: 1.3808e-04 - val_loss: 1.2817e-04 - 9s/epoch - 3ms/step
Epoch 260/300
2985/2985 - 9s - loss: 1.3111e-04 - val_loss: 1.3443e-04 - 9s/epoch - 3ms/step
Epoch 261/300
2985/2985 - 9s - loss: 1.3706e-04 - val_loss: 1.6370e-04 - 9s/epoch - 3ms/step
Epoch 262/300
2985/2985 - 9s - loss: 1.4832e-04 - val_loss: 1.2122e-04 - 9s/epoch - 3ms/step
Epoch 263/300
2985/2985 - 9s - loss: 1.3225e-04 - val_loss: 1.2229e-04 - 9s/epoch - 3ms/step
Epoch 264/300
2985/2985 - 9s - loss: 1.3383e-04 - val_loss: 1.4428e-04 - 9s/epoch - 3ms/step
Epoch 265/300
2985/2985 - 9s - loss: 1.3204e-04 - val_loss: 1.5024e-04 - 9s/epoch - 3ms/step
Epoch 266/300
2985/2985 - 9s - loss: 1.3083e-04 - val_loss: 1.2137e-04 - 9s/epoch - 3ms/step
Epoch 267/300
2985/2985 - 9s - loss: 1.3044e-04 - val_loss: 1.2254e-04 - 9s/epoch - 3ms/step
Epoch 268/300
2985/2985 - 9s - loss: 1.2942e-04 - val_loss: 1.2648e-04 - 9s/epoch - 3ms/step
Epoch 269/300
2985/2985 - 9s - loss: 1.3247e-04 - val_loss: 1.6627e-04 - 9s/epoch - 3ms/step
Epoch 270/300
2985/2985 - 9s - loss: 1.3191e-04 - val_loss: 1.4249e-04 - 9s/epoch - 3ms/step
Epoch 271/300
2985/2985 - 9s - loss: 1.3217e-04 - val_loss: 1.2801e-04 - 9s/epoch - 3ms/step
Epoch 272/300
2985/2985 - 9s - loss: 1.3152e-04 - val_loss: 1.2944e-04 - 9s/epoch - 3ms/step
Epoch 273/300
2985/2985 - 9s - loss: 1.2905e-04 - val_loss: 2.2251e-04 - 9s/epoch - 3ms/step
Epoch 274/300
2985/2985 - 9s - loss: 1.3735e-04 - val_loss: 1.2617e-04 - 9s/epoch - 3ms/step
Epoch 275/300
2985/2985 - 9s - loss: 1.3069e-04 - val_loss: 1.2818e-04 - 9s/epoch - 3ms/step
Epoch 276/300
2985/2985 - 9s - loss: 1.2839e-04 - val_loss: 1.3275e-04 - 9s/epoch - 3ms/step
Epoch 277/300
2985/2985 - 9s - loss: 1.3099e-04 - val_loss: 1.2819e-04 - 9s/epoch - 3ms/step
Epoch 278/300
2985/2985 - 9s - loss: 1.2913e-04 - val_loss: 1.1776e-04 - 9s/epoch - 3ms/step
Epoch 279/300
2985/2985 - 9s - loss: 1.2858e-04 - val_loss: 1.2414e-04 - 9s/epoch - 3ms/step
Epoch 280/300
2985/2985 - 9s - loss: 1.2915e-04 - val_loss: 1.1889e-04 - 9s/epoch - 3ms/step
Epoch 281/300
2985/2985 - 9s - loss: 1.2959e-04 - val_loss: 1.2869e-04 - 9s/epoch - 3ms/step
Epoch 282/300
2985/2985 - 9s - loss: 1.2764e-04 - val_loss: 1.2795e-04 - 9s/epoch - 3ms/step
Epoch 283/300
2985/2985 - 9s - loss: 1.2984e-04 - val_loss: 1.3198e-04 - 9s/epoch - 3ms/step
Epoch 284/300
2985/2985 - 9s - loss: 1.2840e-04 - val_loss: 1.3380e-04 - 9s/epoch - 3ms/step
Epoch 285/300
2985/2985 - 9s - loss: 1.2861e-04 - val_loss: 1.3476e-04 - 9s/epoch - 3ms/step
Epoch 286/300
2985/2985 - 9s - loss: 1.2742e-04 - val_loss: 1.3716e-04 - 9s/epoch - 3ms/step
Epoch 287/300
2985/2985 - 9s - loss: 1.4095e-04 - val_loss: 1.3353e-04 - 9s/epoch - 3ms/step
Epoch 288/300
2985/2985 - 9s - loss: 1.3067e-04 - val_loss: 1.1307e-04 - 9s/epoch - 3ms/step
Epoch 289/300
2985/2985 - 9s - loss: 1.3200e-04 - val_loss: 2.7462e-04 - 9s/epoch - 3ms/step
Epoch 290/300
2985/2985 - 9s - loss: 1.5003e-04 - val_loss: 1.7750e-04 - 9s/epoch - 3ms/step
Epoch 291/300
2985/2985 - 9s - loss: 1.4316e-04 - val_loss: 1.2821e-04 - 9s/epoch - 3ms/step
Epoch 292/300
2985/2985 - 9s - loss: 1.3153e-04 - val_loss: 1.4335e-04 - 9s/epoch - 3ms/step
Epoch 293/300
2985/2985 - 9s - loss: 1.2860e-04 - val_loss: 1.2386e-04 - 9s/epoch - 3ms/step
Epoch 294/300
2985/2985 - 9s - loss: 1.3089e-04 - val_loss: 1.2210e-04 - 9s/epoch - 3ms/step
Epoch 295/300
2985/2985 - 9s - loss: 1.3030e-04 - val_loss: 1.1505e-04 - 9s/epoch - 3ms/step
Epoch 296/300
2985/2985 - 9s - loss: 1.3157e-04 - val_loss: 1.1483e-04 - 9s/epoch - 3ms/step
Epoch 297/300
2985/2985 - 9s - loss: 1.2636e-04 - val_loss: 1.4600e-04 - 9s/epoch - 3ms/step
Epoch 298/300
2985/2985 - 9s - loss: 1.2642e-04 - val_loss: 1.2731e-04 - 9s/epoch - 3ms/step
Epoch 299/300
2985/2985 - 9s - loss: 1.2920e-04 - val_loss: 1.3537e-04 - 9s/epoch - 3ms/step
Epoch 300/300
2985/2985 - 9s - loss: 1.3433e-04 - val_loss: 1.2181e-04 - 9s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00012180966587038711
  1/332 [..............................] - ETA: 31s 48/332 [===>..........................] - ETA: 0s  96/332 [=======>......................] - ETA: 0s145/332 [============>.................] - ETA: 0s194/332 [================>.............] - ETA: 0s243/332 [====================>.........] - ETA: 0s291/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.001359284586875878
cosine 0.0010827181532928462
MAE: 0.0059853327
RMSE: 0.011036734
r2: 0.9920984169162189
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       multiple                  0         
                                                                 
 dense_21 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_21 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_22 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 632)               0         
                                                                 
 dense_22 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_23 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2528)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_22"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_23 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_22 (InputLayer)       multiple                  0         
                                                                 
 dense_21 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_21 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_23"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_24 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_22 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 632)               0         
                                                                 
 dense_22 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_23 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2528)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 32, 300, 0.0005, 0.5, 632, 0.00013433120329864323, 0.00012180966587038711, 0.001359284586875878, 0.0010827181532928462, 0.005985332652926445, 0.011036734096705914, 0.9920984169162189, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_24"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_25 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_24 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_24 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_25 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 632)               0         
                                                                 
 dense_25 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_26 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2528)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/300
2985/2985 - 9s - loss: 0.0076 - val_loss: 0.0024 - 9s/epoch - 3ms/step
Epoch 2/300
2985/2985 - 9s - loss: 0.0033 - val_loss: 0.0019 - 9s/epoch - 3ms/step
Epoch 3/300
2985/2985 - 9s - loss: 0.0020 - val_loss: 0.0013 - 9s/epoch - 3ms/step
Epoch 4/300
2985/2985 - 9s - loss: 0.0014 - val_loss: 0.0012 - 9s/epoch - 3ms/step
Epoch 5/300
2985/2985 - 9s - loss: 0.0011 - val_loss: 7.9582e-04 - 9s/epoch - 3ms/step
Epoch 6/300
2985/2985 - 9s - loss: 9.2425e-04 - val_loss: 7.6050e-04 - 9s/epoch - 3ms/step
Epoch 7/300
2985/2985 - 9s - loss: 7.9356e-04 - val_loss: 8.1204e-04 - 9s/epoch - 3ms/step
Epoch 8/300
2985/2985 - 9s - loss: 7.0996e-04 - val_loss: 5.6195e-04 - 9s/epoch - 3ms/step
Epoch 9/300
2985/2985 - 9s - loss: 6.5507e-04 - val_loss: 5.0261e-04 - 9s/epoch - 3ms/step
Epoch 10/300
2985/2985 - 9s - loss: 5.9962e-04 - val_loss: 4.8687e-04 - 9s/epoch - 3ms/step
Epoch 11/300
2985/2985 - 9s - loss: 5.6739e-04 - val_loss: 4.6040e-04 - 9s/epoch - 3ms/step
Epoch 12/300
2985/2985 - 9s - loss: 5.7662e-04 - val_loss: 6.7685e-04 - 9s/epoch - 3ms/step
Epoch 13/300
2985/2985 - 9s - loss: 5.5314e-04 - val_loss: 4.3955e-04 - 9s/epoch - 3ms/step
Epoch 14/300
2985/2985 - 9s - loss: 4.9970e-04 - val_loss: 3.9516e-04 - 9s/epoch - 3ms/step
Epoch 15/300
2985/2985 - 9s - loss: 4.7577e-04 - val_loss: 3.8040e-04 - 9s/epoch - 3ms/step
Epoch 16/300
2985/2985 - 9s - loss: 4.5226e-04 - val_loss: 3.7948e-04 - 9s/epoch - 3ms/step
Epoch 17/300
2985/2985 - 9s - loss: 4.4807e-04 - val_loss: 3.6013e-04 - 9s/epoch - 3ms/step
Epoch 18/300
2985/2985 - 9s - loss: 4.2915e-04 - val_loss: 3.4977e-04 - 9s/epoch - 3ms/step
Epoch 19/300
2985/2985 - 9s - loss: 4.1566e-04 - val_loss: 3.5038e-04 - 9s/epoch - 3ms/step
Epoch 20/300
2985/2985 - 9s - loss: 4.0097e-04 - val_loss: 3.1921e-04 - 9s/epoch - 3ms/step
Epoch 21/300
2985/2985 - 9s - loss: 3.9224e-04 - val_loss: 3.1215e-04 - 9s/epoch - 3ms/step
Epoch 22/300
2985/2985 - 9s - loss: 3.8335e-04 - val_loss: 3.2086e-04 - 9s/epoch - 3ms/step
Epoch 23/300
2985/2985 - 9s - loss: 3.7576e-04 - val_loss: 3.1598e-04 - 9s/epoch - 3ms/step
Epoch 24/300
2985/2985 - 9s - loss: 3.7265e-04 - val_loss: 3.1155e-04 - 9s/epoch - 3ms/step
Epoch 25/300
2985/2985 - 9s - loss: 3.6079e-04 - val_loss: 2.9921e-04 - 9s/epoch - 3ms/step
Epoch 26/300
2985/2985 - 9s - loss: 3.5126e-04 - val_loss: 2.7302e-04 - 9s/epoch - 3ms/step
Epoch 27/300
2985/2985 - 9s - loss: 3.4246e-04 - val_loss: 2.6592e-04 - 9s/epoch - 3ms/step
Epoch 28/300
2985/2985 - 9s - loss: 3.5561e-04 - val_loss: 2.7028e-04 - 9s/epoch - 3ms/step
Epoch 29/300
2985/2985 - 9s - loss: 3.3680e-04 - val_loss: 2.5853e-04 - 9s/epoch - 3ms/step
Epoch 30/300
2985/2985 - 9s - loss: 3.3347e-04 - val_loss: 2.5666e-04 - 9s/epoch - 3ms/step
Epoch 31/300
2985/2985 - 9s - loss: 3.2556e-04 - val_loss: 2.5414e-04 - 9s/epoch - 3ms/step
Epoch 32/300
2985/2985 - 9s - loss: 3.8826e-04 - val_loss: 2.6535e-04 - 9s/epoch - 3ms/step
Epoch 33/300
2985/2985 - 9s - loss: 3.2858e-04 - val_loss: 2.5041e-04 - 9s/epoch - 3ms/step
Epoch 34/300
2985/2985 - 9s - loss: 3.1997e-04 - val_loss: 2.4472e-04 - 9s/epoch - 3ms/step
Epoch 35/300
2985/2985 - 9s - loss: 3.1168e-04 - val_loss: 2.4219e-04 - 9s/epoch - 3ms/step
Epoch 36/300
2985/2985 - 9s - loss: 3.0706e-04 - val_loss: 2.2758e-04 - 9s/epoch - 3ms/step
Epoch 37/300
2985/2985 - 9s - loss: 3.0587e-04 - val_loss: 2.4046e-04 - 9s/epoch - 3ms/step
Epoch 38/300
2985/2985 - 9s - loss: 3.0077e-04 - val_loss: 2.2709e-04 - 9s/epoch - 3ms/step
Epoch 39/300
2985/2985 - 9s - loss: 2.9418e-04 - val_loss: 2.2902e-04 - 9s/epoch - 3ms/step
Epoch 40/300
2985/2985 - 9s - loss: 3.0058e-04 - val_loss: 2.3312e-04 - 9s/epoch - 3ms/step
Epoch 41/300
2985/2985 - 9s - loss: 2.9295e-04 - val_loss: 2.2665e-04 - 9s/epoch - 3ms/step
Epoch 42/300
2985/2985 - 9s - loss: 2.8935e-04 - val_loss: 2.2167e-04 - 9s/epoch - 3ms/step
Epoch 43/300
2985/2985 - 9s - loss: 2.8421e-04 - val_loss: 2.2660e-04 - 9s/epoch - 3ms/step
Epoch 44/300
2985/2985 - 9s - loss: 2.7957e-04 - val_loss: 2.2385e-04 - 9s/epoch - 3ms/step
Epoch 45/300
2985/2985 - 9s - loss: 2.8008e-04 - val_loss: 2.1412e-04 - 9s/epoch - 3ms/step
Epoch 46/300
2985/2985 - 9s - loss: 2.7265e-04 - val_loss: 2.1421e-04 - 9s/epoch - 3ms/step
Epoch 47/300
2985/2985 - 9s - loss: 2.8139e-04 - val_loss: 2.0439e-04 - 9s/epoch - 3ms/step
Epoch 48/300
2985/2985 - 9s - loss: 2.8482e-04 - val_loss: 2.1445e-04 - 9s/epoch - 3ms/step
Epoch 49/300
2985/2985 - 9s - loss: 2.7163e-04 - val_loss: 2.8261e-04 - 9s/epoch - 3ms/step
Epoch 50/300
2985/2985 - 9s - loss: 2.8860e-04 - val_loss: 2.1866e-04 - 9s/epoch - 3ms/step
Epoch 51/300
2985/2985 - 9s - loss: 2.6805e-04 - val_loss: 2.0250e-04 - 9s/epoch - 3ms/step
Epoch 52/300
2985/2985 - 9s - loss: 2.6297e-04 - val_loss: 2.1423e-04 - 9s/epoch - 3ms/step
Epoch 53/300
2985/2985 - 9s - loss: 2.6631e-04 - val_loss: 2.0251e-04 - 9s/epoch - 3ms/step
Epoch 54/300
2985/2985 - 9s - loss: 2.6041e-04 - val_loss: 2.0765e-04 - 9s/epoch - 3ms/step
Epoch 55/300
2985/2985 - 9s - loss: 2.5626e-04 - val_loss: 1.9087e-04 - 9s/epoch - 3ms/step
Epoch 56/300
2985/2985 - 9s - loss: 2.5875e-04 - val_loss: 1.9940e-04 - 9s/epoch - 3ms/step
Epoch 57/300
2985/2985 - 9s - loss: 2.6220e-04 - val_loss: 2.0538e-04 - 9s/epoch - 3ms/step
Epoch 58/300
2985/2985 - 9s - loss: 2.5594e-04 - val_loss: 1.9708e-04 - 9s/epoch - 3ms/step
Epoch 59/300
2985/2985 - 9s - loss: 2.4962e-04 - val_loss: 1.9218e-04 - 9s/epoch - 3ms/step
Epoch 60/300
2985/2985 - 9s - loss: 2.4708e-04 - val_loss: 1.9409e-04 - 9s/epoch - 3ms/step
Epoch 61/300
2985/2985 - 9s - loss: 2.5300e-04 - val_loss: 1.9041e-04 - 9s/epoch - 3ms/step
Epoch 62/300
2985/2985 - 9s - loss: 2.4646e-04 - val_loss: 2.0312e-04 - 9s/epoch - 3ms/step
Epoch 63/300
2985/2985 - 9s - loss: 2.4421e-04 - val_loss: 1.9891e-04 - 9s/epoch - 3ms/step
Epoch 64/300
2985/2985 - 9s - loss: 2.4162e-04 - val_loss: 1.8292e-04 - 9s/epoch - 3ms/step
Epoch 65/300
2985/2985 - 9s - loss: 2.4137e-04 - val_loss: 1.9196e-04 - 9s/epoch - 3ms/step
Epoch 66/300
2985/2985 - 9s - loss: 2.5477e-04 - val_loss: 1.8998e-04 - 9s/epoch - 3ms/step
Epoch 67/300
2985/2985 - 9s - loss: 2.4036e-04 - val_loss: 1.7813e-04 - 9s/epoch - 3ms/step
Epoch 68/300
2985/2985 - 9s - loss: 2.4125e-04 - val_loss: 2.0042e-04 - 9s/epoch - 3ms/step
Epoch 69/300
2985/2985 - 9s - loss: 2.4004e-04 - val_loss: 1.8510e-04 - 9s/epoch - 3ms/step
Epoch 70/300
2985/2985 - 9s - loss: 2.3942e-04 - val_loss: 1.9438e-04 - 9s/epoch - 3ms/step
Epoch 71/300
2985/2985 - 9s - loss: 2.3582e-04 - val_loss: 1.8344e-04 - 9s/epoch - 3ms/step
Epoch 72/300
2985/2985 - 9s - loss: 2.3573e-04 - val_loss: 1.9501e-04 - 9s/epoch - 3ms/step
Epoch 73/300
2985/2985 - 9s - loss: 2.4370e-04 - val_loss: 2.2853e-04 - 9s/epoch - 3ms/step
Epoch 74/300
2985/2985 - 9s - loss: 2.5315e-04 - val_loss: 1.8207e-04 - 9s/epoch - 3ms/step
Epoch 75/300
2985/2985 - 9s - loss: 2.3567e-04 - val_loss: 1.7960e-04 - 9s/epoch - 3ms/step
Epoch 76/300
2985/2985 - 9s - loss: 2.3055e-04 - val_loss: 1.7132e-04 - 9s/epoch - 3ms/step
Epoch 77/300
2985/2985 - 9s - loss: 2.3101e-04 - val_loss: 1.7907e-04 - 9s/epoch - 3ms/step
Epoch 78/300
2985/2985 - 9s - loss: 2.2893e-04 - val_loss: 1.7395e-04 - 9s/epoch - 3ms/step
Epoch 79/300
2985/2985 - 9s - loss: 2.2828e-04 - val_loss: 1.7726e-04 - 9s/epoch - 3ms/step
Epoch 80/300
2985/2985 - 9s - loss: 2.2722e-04 - val_loss: 1.9410e-04 - 9s/epoch - 3ms/step
Epoch 81/300
2985/2985 - 9s - loss: 2.3313e-04 - val_loss: 1.9245e-04 - 9s/epoch - 3ms/step
Epoch 82/300
2985/2985 - 9s - loss: 2.2631e-04 - val_loss: 1.7854e-04 - 9s/epoch - 3ms/step
Epoch 83/300
2985/2985 - 9s - loss: 2.2345e-04 - val_loss: 1.8079e-04 - 9s/epoch - 3ms/step
Epoch 84/300
2985/2985 - 9s - loss: 2.2142e-04 - val_loss: 1.7021e-04 - 9s/epoch - 3ms/step
Epoch 85/300
2985/2985 - 9s - loss: 2.2332e-04 - val_loss: 1.8125e-04 - 9s/epoch - 3ms/step
Epoch 86/300
2985/2985 - 9s - loss: 2.1913e-04 - val_loss: 1.6548e-04 - 9s/epoch - 3ms/step
Epoch 87/300
2985/2985 - 9s - loss: 2.2306e-04 - val_loss: 1.8475e-04 - 9s/epoch - 3ms/step
Epoch 88/300
2985/2985 - 9s - loss: 2.1879e-04 - val_loss: 1.7738e-04 - 9s/epoch - 3ms/step
Epoch 89/300
2985/2985 - 9s - loss: 2.1869e-04 - val_loss: 1.7365e-04 - 9s/epoch - 3ms/step
Epoch 90/300
2985/2985 - 9s - loss: 2.2627e-04 - val_loss: 1.7072e-04 - 9s/epoch - 3ms/step
Epoch 91/300
2985/2985 - 9s - loss: 2.1542e-04 - val_loss: 1.6070e-04 - 9s/epoch - 3ms/step
Epoch 92/300
2985/2985 - 9s - loss: 2.1810e-04 - val_loss: 2.1759e-04 - 9s/epoch - 3ms/step
Epoch 93/300
2985/2985 - 9s - loss: 2.1599e-04 - val_loss: 1.8622e-04 - 9s/epoch - 3ms/step
Epoch 94/300
2985/2985 - 9s - loss: 2.1241e-04 - val_loss: 1.6685e-04 - 9s/epoch - 3ms/step
Epoch 95/300
2985/2985 - 9s - loss: 2.3044e-04 - val_loss: 1.7753e-04 - 9s/epoch - 3ms/step
Epoch 96/300
2985/2985 - 9s - loss: 2.1510e-04 - val_loss: 1.7370e-04 - 9s/epoch - 3ms/step
Epoch 97/300
2985/2985 - 9s - loss: 2.1191e-04 - val_loss: 1.6481e-04 - 9s/epoch - 3ms/step
Epoch 98/300
2985/2985 - 9s - loss: 2.1130e-04 - val_loss: 1.6991e-04 - 9s/epoch - 3ms/step
Epoch 99/300
2985/2985 - 9s - loss: 2.0899e-04 - val_loss: 1.6529e-04 - 9s/epoch - 3ms/step
Epoch 100/300
2985/2985 - 9s - loss: 2.1514e-04 - val_loss: 1.7308e-04 - 9s/epoch - 3ms/step
Epoch 101/300
2985/2985 - 9s - loss: 2.0955e-04 - val_loss: 1.6072e-04 - 9s/epoch - 3ms/step
Epoch 102/300
2985/2985 - 9s - loss: 2.1379e-04 - val_loss: 2.4233e-04 - 9s/epoch - 3ms/step
Epoch 103/300
2985/2985 - 9s - loss: 2.3390e-04 - val_loss: 1.7151e-04 - 9s/epoch - 3ms/step
Epoch 104/300
2985/2985 - 9s - loss: 2.1105e-04 - val_loss: 1.6391e-04 - 9s/epoch - 3ms/step
Epoch 105/300
2985/2985 - 9s - loss: 2.0726e-04 - val_loss: 1.7266e-04 - 9s/epoch - 3ms/step
Epoch 106/300
2985/2985 - 9s - loss: 2.0700e-04 - val_loss: 1.6551e-04 - 9s/epoch - 3ms/step
Epoch 107/300
2985/2985 - 9s - loss: 2.1974e-04 - val_loss: 1.6245e-04 - 9s/epoch - 3ms/step
Epoch 108/300
2985/2985 - 9s - loss: 2.0788e-04 - val_loss: 1.7314e-04 - 9s/epoch - 3ms/step
Epoch 109/300
2985/2985 - 9s - loss: 2.0457e-04 - val_loss: 1.6507e-04 - 9s/epoch - 3ms/step
Epoch 110/300
2985/2985 - 9s - loss: 2.1271e-04 - val_loss: 1.7836e-04 - 9s/epoch - 3ms/step
Epoch 111/300
2985/2985 - 9s - loss: 2.1138e-04 - val_loss: 1.6976e-04 - 9s/epoch - 3ms/step
Epoch 112/300
2985/2985 - 9s - loss: 2.0831e-04 - val_loss: 1.7181e-04 - 9s/epoch - 3ms/step
Epoch 113/300
2985/2985 - 9s - loss: 2.0081e-04 - val_loss: 1.5331e-04 - 9s/epoch - 3ms/step
Epoch 114/300
2985/2985 - 9s - loss: 2.0136e-04 - val_loss: 1.6712e-04 - 9s/epoch - 3ms/step
Epoch 115/300
2985/2985 - 9s - loss: 2.0412e-04 - val_loss: 1.6055e-04 - 9s/epoch - 3ms/step
Epoch 116/300
2985/2985 - 9s - loss: 2.0113e-04 - val_loss: 1.5998e-04 - 9s/epoch - 3ms/step
Epoch 117/300
2985/2985 - 9s - loss: 2.0020e-04 - val_loss: 1.5975e-04 - 9s/epoch - 3ms/step
Epoch 118/300
2985/2985 - 9s - loss: 2.0002e-04 - val_loss: 1.6314e-04 - 9s/epoch - 3ms/step
Epoch 119/300
2985/2985 - 9s - loss: 1.9712e-04 - val_loss: 1.5583e-04 - 9s/epoch - 3ms/step
Epoch 120/300
2985/2985 - 9s - loss: 1.9826e-04 - val_loss: 1.6213e-04 - 9s/epoch - 3ms/step
Epoch 121/300
2985/2985 - 9s - loss: 1.9747e-04 - val_loss: 1.6436e-04 - 9s/epoch - 3ms/step
Epoch 122/300
2985/2985 - 9s - loss: 2.1112e-04 - val_loss: 1.4431e-04 - 9s/epoch - 3ms/step
Epoch 123/300
2985/2985 - 9s - loss: 2.4029e-04 - val_loss: 1.7215e-04 - 9s/epoch - 3ms/step
Epoch 124/300
2985/2985 - 9s - loss: 2.0669e-04 - val_loss: 1.5971e-04 - 9s/epoch - 3ms/step
Epoch 125/300
2985/2985 - 9s - loss: 2.0271e-04 - val_loss: 1.5722e-04 - 9s/epoch - 3ms/step
Epoch 126/300
2985/2985 - 9s - loss: 1.9904e-04 - val_loss: 1.5812e-04 - 9s/epoch - 3ms/step
Epoch 127/300
2985/2985 - 9s - loss: 1.9776e-04 - val_loss: 1.5160e-04 - 9s/epoch - 3ms/step
Epoch 128/300
2985/2985 - 9s - loss: 1.9515e-04 - val_loss: 1.5536e-04 - 9s/epoch - 3ms/step
Epoch 129/300
2985/2985 - 9s - loss: 1.9818e-04 - val_loss: 1.4475e-04 - 9s/epoch - 3ms/step
Epoch 130/300
2985/2985 - 9s - loss: 1.9835e-04 - val_loss: 1.5637e-04 - 9s/epoch - 3ms/step
Epoch 131/300
2985/2985 - 9s - loss: 1.9742e-04 - val_loss: 1.5226e-04 - 9s/epoch - 3ms/step
Epoch 132/300
2985/2985 - 9s - loss: 1.9485e-04 - val_loss: 1.5629e-04 - 9s/epoch - 3ms/step
Epoch 133/300
2985/2985 - 9s - loss: 1.9070e-04 - val_loss: 1.4687e-04 - 9s/epoch - 3ms/step
Epoch 134/300
2985/2985 - 9s - loss: 2.0390e-04 - val_loss: 1.5521e-04 - 9s/epoch - 3ms/step
Epoch 135/300
2985/2985 - 9s - loss: 1.9861e-04 - val_loss: 1.5050e-04 - 9s/epoch - 3ms/step
Epoch 136/300
2985/2985 - 9s - loss: 2.0599e-04 - val_loss: 1.4363e-04 - 9s/epoch - 3ms/step
Epoch 137/300
2985/2985 - 9s - loss: 1.9727e-04 - val_loss: 2.5508e-04 - 9s/epoch - 3ms/step
Epoch 138/300
2985/2985 - 9s - loss: 2.0619e-04 - val_loss: 1.6680e-04 - 9s/epoch - 3ms/step
Epoch 139/300
2985/2985 - 9s - loss: 2.0166e-04 - val_loss: 1.6054e-04 - 9s/epoch - 3ms/step
Epoch 140/300
2985/2985 - 9s - loss: 1.9942e-04 - val_loss: 1.6564e-04 - 9s/epoch - 3ms/step
Epoch 141/300
2985/2985 - 9s - loss: 1.9392e-04 - val_loss: 1.9124e-04 - 9s/epoch - 3ms/step
Epoch 142/300
2985/2985 - 9s - loss: 1.9967e-04 - val_loss: 1.5963e-04 - 9s/epoch - 3ms/step
Epoch 143/300
2985/2985 - 9s - loss: 1.9332e-04 - val_loss: 1.6031e-04 - 9s/epoch - 3ms/step
Epoch 144/300
2985/2985 - 9s - loss: 1.9246e-04 - val_loss: 1.4656e-04 - 9s/epoch - 3ms/step
Epoch 145/300
2985/2985 - 9s - loss: 1.9044e-04 - val_loss: 1.4901e-04 - 9s/epoch - 3ms/step
Epoch 146/300
2985/2985 - 9s - loss: 1.8983e-04 - val_loss: 1.5824e-04 - 9s/epoch - 3ms/step
Epoch 147/300
2985/2985 - 9s - loss: 1.8956e-04 - val_loss: 1.6098e-04 - 9s/epoch - 3ms/step
Epoch 148/300
2985/2985 - 9s - loss: 1.9418e-04 - val_loss: 1.6085e-04 - 9s/epoch - 3ms/step
Epoch 149/300
2985/2985 - 9s - loss: 1.8961e-04 - val_loss: 1.4660e-04 - 9s/epoch - 3ms/step
Epoch 150/300
2985/2985 - 9s - loss: 1.8821e-04 - val_loss: 1.6735e-04 - 9s/epoch - 3ms/step
Epoch 151/300
2985/2985 - 9s - loss: 1.9193e-04 - val_loss: 1.5367e-04 - 9s/epoch - 3ms/step
Epoch 152/300
2985/2985 - 9s - loss: 1.9610e-04 - val_loss: 1.9916e-04 - 9s/epoch - 3ms/step
Epoch 153/300
2985/2985 - 9s - loss: 1.9739e-04 - val_loss: 1.5693e-04 - 9s/epoch - 3ms/step
Epoch 154/300
2985/2985 - 9s - loss: 2.1136e-04 - val_loss: 1.5248e-04 - 9s/epoch - 3ms/step
Epoch 155/300
2985/2985 - 9s - loss: 1.9122e-04 - val_loss: 1.9272e-04 - 9s/epoch - 3ms/step
Epoch 156/300
2985/2985 - 9s - loss: 1.9119e-04 - val_loss: 1.5562e-04 - 9s/epoch - 3ms/step
Epoch 157/300
2985/2985 - 9s - loss: 1.8473e-04 - val_loss: 1.8110e-04 - 9s/epoch - 3ms/step
Epoch 158/300
2985/2985 - 9s - loss: 1.8931e-04 - val_loss: 1.5086e-04 - 9s/epoch - 3ms/step
Epoch 159/300
2985/2985 - 9s - loss: 1.8511e-04 - val_loss: 1.4215e-04 - 9s/epoch - 3ms/step
Epoch 160/300
2985/2985 - 9s - loss: 1.8704e-04 - val_loss: 1.4500e-04 - 9s/epoch - 3ms/step
Epoch 161/300
2985/2985 - 9s - loss: 1.8412e-04 - val_loss: 1.7295e-04 - 9s/epoch - 3ms/step
Epoch 162/300
2985/2985 - 9s - loss: 1.8713e-04 - val_loss: 1.5220e-04 - 9s/epoch - 3ms/step
Epoch 163/300
2985/2985 - 9s - loss: 1.8172e-04 - val_loss: 1.4798e-04 - 9s/epoch - 3ms/step
Epoch 164/300
2985/2985 - 9s - loss: 1.8495e-04 - val_loss: 1.4786e-04 - 9s/epoch - 3ms/step
Epoch 165/300
2985/2985 - 9s - loss: 1.8582e-04 - val_loss: 1.4249e-04 - 9s/epoch - 3ms/step
Epoch 166/300
2985/2985 - 9s - loss: 1.8161e-04 - val_loss: 1.5052e-04 - 9s/epoch - 3ms/step
Epoch 167/300
2985/2985 - 9s - loss: 1.8259e-04 - val_loss: 1.4927e-04 - 9s/epoch - 3ms/step
Epoch 168/300
2985/2985 - 9s - loss: 1.8199e-04 - val_loss: 1.6464e-04 - 9s/epoch - 3ms/step
Epoch 169/300
2985/2985 - 9s - loss: 1.8519e-04 - val_loss: 1.5381e-04 - 9s/epoch - 3ms/step
Epoch 170/300
2985/2985 - 9s - loss: 1.8782e-04 - val_loss: 2.8153e-04 - 9s/epoch - 3ms/step
Epoch 171/300
2985/2985 - 9s - loss: 2.2722e-04 - val_loss: 1.5337e-04 - 9s/epoch - 3ms/step
Epoch 172/300
2985/2985 - 9s - loss: 1.8630e-04 - val_loss: 1.4261e-04 - 9s/epoch - 3ms/step
Epoch 173/300
2985/2985 - 9s - loss: 1.8490e-04 - val_loss: 1.5183e-04 - 9s/epoch - 3ms/step
Epoch 174/300
2985/2985 - 9s - loss: 1.8217e-04 - val_loss: 1.3930e-04 - 9s/epoch - 3ms/step
Epoch 175/300
2985/2985 - 9s - loss: 1.8063e-04 - val_loss: 1.5365e-04 - 9s/epoch - 3ms/step
Epoch 176/300
2985/2985 - 9s - loss: 1.7997e-04 - val_loss: 1.4262e-04 - 9s/epoch - 3ms/step
Epoch 177/300
2985/2985 - 9s - loss: 1.8125e-04 - val_loss: 1.3450e-04 - 9s/epoch - 3ms/step
Epoch 178/300
2985/2985 - 9s - loss: 1.8040e-04 - val_loss: 1.4029e-04 - 9s/epoch - 3ms/step
Epoch 179/300
2985/2985 - 9s - loss: 1.7743e-04 - val_loss: 1.4691e-04 - 9s/epoch - 3ms/step
Epoch 180/300
2985/2985 - 9s - loss: 1.7818e-04 - val_loss: 1.4660e-04 - 9s/epoch - 3ms/step
Epoch 181/300
2985/2985 - 9s - loss: 1.7736e-04 - val_loss: 1.5466e-04 - 9s/epoch - 3ms/step
Epoch 182/300
2985/2985 - 9s - loss: 1.7827e-04 - val_loss: 1.5141e-04 - 9s/epoch - 3ms/step
Epoch 183/300
2985/2985 - 9s - loss: 1.8175e-04 - val_loss: 1.5058e-04 - 9s/epoch - 3ms/step
Epoch 184/300
2985/2985 - 9s - loss: 1.8579e-04 - val_loss: 1.4913e-04 - 9s/epoch - 3ms/step
Epoch 185/300
2985/2985 - 9s - loss: 1.7816e-04 - val_loss: 1.5355e-04 - 9s/epoch - 3ms/step
Epoch 186/300
2985/2985 - 9s - loss: 1.7846e-04 - val_loss: 1.4673e-04 - 9s/epoch - 3ms/step
Epoch 187/300
2985/2985 - 9s - loss: 1.7714e-04 - val_loss: 1.4306e-04 - 9s/epoch - 3ms/step
Epoch 188/300
2985/2985 - 9s - loss: 1.7513e-04 - val_loss: 1.4643e-04 - 9s/epoch - 3ms/step
Epoch 189/300
2985/2985 - 9s - loss: 1.8023e-04 - val_loss: 1.5593e-04 - 9s/epoch - 3ms/step
Epoch 190/300
2985/2985 - 9s - loss: 1.7467e-04 - val_loss: 1.4099e-04 - 9s/epoch - 3ms/step
Epoch 191/300
2985/2985 - 9s - loss: 1.8681e-04 - val_loss: 2.9414e-04 - 9s/epoch - 3ms/step
Epoch 192/300
2985/2985 - 9s - loss: 2.0812e-04 - val_loss: 1.5374e-04 - 9s/epoch - 3ms/step
Epoch 193/300
2985/2985 - 9s - loss: 1.8312e-04 - val_loss: 1.4679e-04 - 9s/epoch - 3ms/step
Epoch 194/300
2985/2985 - 9s - loss: 1.7527e-04 - val_loss: 1.4515e-04 - 9s/epoch - 3ms/step
Epoch 195/300
2985/2985 - 9s - loss: 1.7603e-04 - val_loss: 1.4363e-04 - 9s/epoch - 3ms/step
Epoch 196/300
2985/2985 - 9s - loss: 1.8173e-04 - val_loss: 1.4123e-04 - 9s/epoch - 3ms/step
Epoch 197/300
2985/2985 - 9s - loss: 1.7770e-04 - val_loss: 1.5162e-04 - 9s/epoch - 3ms/step
Epoch 198/300
2985/2985 - 9s - loss: 1.8234e-04 - val_loss: 1.5073e-04 - 9s/epoch - 3ms/step
Epoch 199/300
2985/2985 - 9s - loss: 1.7361e-04 - val_loss: 1.4074e-04 - 9s/epoch - 3ms/step
Epoch 200/300
2985/2985 - 9s - loss: 1.7577e-04 - val_loss: 1.4486e-04 - 9s/epoch - 3ms/step
Epoch 201/300
2985/2985 - 9s - loss: 1.7219e-04 - val_loss: 1.5270e-04 - 9s/epoch - 3ms/step
Epoch 202/300
2985/2985 - 9s - loss: 1.8209e-04 - val_loss: 1.5423e-04 - 9s/epoch - 3ms/step
Epoch 203/300
2985/2985 - 9s - loss: 1.7769e-04 - val_loss: 2.7420e-04 - 9s/epoch - 3ms/step
Epoch 204/300
2985/2985 - 9s - loss: 1.9206e-04 - val_loss: 1.3929e-04 - 9s/epoch - 3ms/step
Epoch 205/300
2985/2985 - 9s - loss: 1.7507e-04 - val_loss: 1.4551e-04 - 9s/epoch - 3ms/step
Epoch 206/300
2985/2985 - 9s - loss: 1.7422e-04 - val_loss: 1.3893e-04 - 9s/epoch - 3ms/step
Epoch 207/300
2985/2985 - 9s - loss: 1.7410e-04 - val_loss: 1.4778e-04 - 9s/epoch - 3ms/step
Epoch 208/300
2985/2985 - 9s - loss: 1.7513e-04 - val_loss: 1.6772e-04 - 9s/epoch - 3ms/step
Epoch 209/300
2985/2985 - 9s - loss: 1.7624e-04 - val_loss: 1.3799e-04 - 9s/epoch - 3ms/step
Epoch 210/300
2985/2985 - 9s - loss: 1.7300e-04 - val_loss: 1.3876e-04 - 9s/epoch - 3ms/step
Epoch 211/300
2985/2985 - 9s - loss: 1.7311e-04 - val_loss: 1.4125e-04 - 9s/epoch - 3ms/step
Epoch 212/300
2985/2985 - 9s - loss: 1.7051e-04 - val_loss: 1.3487e-04 - 9s/epoch - 3ms/step
Epoch 213/300
2985/2985 - 9s - loss: 1.7200e-04 - val_loss: 1.3495e-04 - 9s/epoch - 3ms/step
Epoch 214/300
2985/2985 - 9s - loss: 1.7280e-04 - val_loss: 1.3579e-04 - 9s/epoch - 3ms/step
Epoch 215/300
2985/2985 - 9s - loss: 1.7793e-04 - val_loss: 1.5079e-04 - 9s/epoch - 3ms/step
Epoch 216/300
2985/2985 - 9s - loss: 1.7184e-04 - val_loss: 1.3195e-04 - 9s/epoch - 3ms/step
Epoch 217/300
2985/2985 - 9s - loss: 1.6893e-04 - val_loss: 1.3189e-04 - 9s/epoch - 3ms/step
Epoch 218/300
2985/2985 - 9s - loss: 1.6784e-04 - val_loss: 1.3231e-04 - 9s/epoch - 3ms/step
Epoch 219/300
2985/2985 - 9s - loss: 1.7859e-04 - val_loss: 1.3575e-04 - 9s/epoch - 3ms/step
Epoch 220/300
2985/2985 - 9s - loss: 1.7820e-04 - val_loss: 1.4904e-04 - 9s/epoch - 3ms/step
Epoch 221/300
2985/2985 - 9s - loss: 1.7240e-04 - val_loss: 1.4402e-04 - 9s/epoch - 3ms/step
Epoch 222/300
2985/2985 - 9s - loss: 1.7014e-04 - val_loss: 1.4334e-04 - 9s/epoch - 3ms/step
Epoch 223/300
2985/2985 - 9s - loss: 1.7184e-04 - val_loss: 1.3605e-04 - 9s/epoch - 3ms/step
Epoch 224/300
2985/2985 - 9s - loss: 1.6742e-04 - val_loss: 1.4084e-04 - 9s/epoch - 3ms/step
Epoch 225/300
2985/2985 - 9s - loss: 1.7280e-04 - val_loss: 2.4508e-04 - 9s/epoch - 3ms/step
Epoch 226/300
2985/2985 - 9s - loss: 1.9578e-04 - val_loss: 1.5477e-04 - 9s/epoch - 3ms/step
Epoch 227/300
2985/2985 - 9s - loss: 1.7844e-04 - val_loss: 1.3422e-04 - 9s/epoch - 3ms/step
Epoch 228/300
2985/2985 - 9s - loss: 1.6841e-04 - val_loss: 1.4122e-04 - 9s/epoch - 3ms/step
Epoch 229/300
2985/2985 - 9s - loss: 1.6884e-04 - val_loss: 1.4463e-04 - 9s/epoch - 3ms/step
Epoch 230/300
2985/2985 - 9s - loss: 1.7055e-04 - val_loss: 1.3143e-04 - 9s/epoch - 3ms/step
Epoch 231/300
2985/2985 - 9s - loss: 1.6772e-04 - val_loss: 1.3698e-04 - 9s/epoch - 3ms/step
Epoch 232/300
2985/2985 - 9s - loss: 1.7673e-04 - val_loss: 1.6676e-04 - 9s/epoch - 3ms/step
Epoch 233/300
2985/2985 - 9s - loss: 1.6998e-04 - val_loss: 1.6038e-04 - 9s/epoch - 3ms/step
Epoch 234/300
2985/2985 - 9s - loss: 1.8934e-04 - val_loss: 1.4022e-04 - 9s/epoch - 3ms/step
Epoch 235/300
2985/2985 - 9s - loss: 1.7090e-04 - val_loss: 1.3083e-04 - 9s/epoch - 3ms/step
Epoch 236/300
2985/2985 - 9s - loss: 1.6698e-04 - val_loss: 1.4183e-04 - 9s/epoch - 3ms/step
Epoch 237/300
2985/2985 - 9s - loss: 1.6799e-04 - val_loss: 1.4556e-04 - 9s/epoch - 3ms/step
Epoch 238/300
2985/2985 - 9s - loss: 1.6672e-04 - val_loss: 1.4420e-04 - 9s/epoch - 3ms/step
Epoch 239/300
2985/2985 - 9s - loss: 1.6598e-04 - val_loss: 1.3134e-04 - 9s/epoch - 3ms/step
Epoch 240/300
2985/2985 - 9s - loss: 1.9105e-04 - val_loss: 1.4776e-04 - 9s/epoch - 3ms/step
Epoch 241/300
2985/2985 - 9s - loss: 1.7478e-04 - val_loss: 1.3988e-04 - 9s/epoch - 3ms/step
Epoch 242/300
2985/2985 - 9s - loss: 1.7083e-04 - val_loss: 1.3624e-04 - 9s/epoch - 3ms/step
Epoch 243/300
2985/2985 - 9s - loss: 1.6540e-04 - val_loss: 1.3331e-04 - 9s/epoch - 3ms/step
Epoch 244/300
2985/2985 - 9s - loss: 1.6672e-04 - val_loss: 1.6806e-04 - 9s/epoch - 3ms/step
Epoch 245/300
2985/2985 - 9s - loss: 1.6984e-04 - val_loss: 1.3662e-04 - 9s/epoch - 3ms/step
Epoch 246/300
2985/2985 - 9s - loss: 1.7108e-04 - val_loss: 2.1839e-04 - 9s/epoch - 3ms/step
Epoch 247/300
2985/2985 - 9s - loss: 1.9942e-04 - val_loss: 1.4092e-04 - 9s/epoch - 3ms/step
Epoch 248/300
2985/2985 - 9s - loss: 1.7184e-04 - val_loss: 1.4527e-04 - 9s/epoch - 3ms/step
Epoch 249/300
2985/2985 - 9s - loss: 1.7566e-04 - val_loss: 1.5019e-04 - 9s/epoch - 3ms/step
Epoch 250/300
2985/2985 - 9s - loss: 1.6627e-04 - val_loss: 1.4379e-04 - 9s/epoch - 3ms/step
Epoch 251/300
2985/2985 - 9s - loss: 1.6567e-04 - val_loss: 1.4181e-04 - 9s/epoch - 3ms/step
Epoch 252/300
2985/2985 - 9s - loss: 1.7400e-04 - val_loss: 1.5106e-04 - 9s/epoch - 3ms/step
Epoch 253/300
2985/2985 - 9s - loss: 1.6607e-04 - val_loss: 1.3589e-04 - 9s/epoch - 3ms/step
Epoch 254/300
2985/2985 - 9s - loss: 1.6701e-04 - val_loss: 1.5942e-04 - 9s/epoch - 3ms/step
Epoch 255/300
2985/2985 - 9s - loss: 1.7039e-04 - val_loss: 1.2994e-04 - 9s/epoch - 3ms/step
Epoch 256/300
2985/2985 - 9s - loss: 1.6281e-04 - val_loss: 1.5037e-04 - 9s/epoch - 3ms/step
Epoch 257/300
2985/2985 - 9s - loss: 1.6599e-04 - val_loss: 1.4851e-04 - 9s/epoch - 3ms/step
Epoch 258/300
2985/2985 - 9s - loss: 1.6629e-04 - val_loss: 1.3153e-04 - 9s/epoch - 3ms/step
Epoch 259/300
2985/2985 - 9s - loss: 1.8440e-04 - val_loss: 1.4220e-04 - 9s/epoch - 3ms/step
Epoch 260/300
2985/2985 - 9s - loss: 1.6600e-04 - val_loss: 1.3483e-04 - 9s/epoch - 3ms/step
Epoch 261/300
2985/2985 - 9s - loss: 1.6350e-04 - val_loss: 1.3658e-04 - 9s/epoch - 3ms/step
Epoch 262/300
2985/2985 - 9s - loss: 1.6619e-04 - val_loss: 1.3485e-04 - 9s/epoch - 3ms/step
Epoch 263/300
2985/2985 - 9s - loss: 1.6305e-04 - val_loss: 1.4312e-04 - 9s/epoch - 3ms/step
Epoch 264/300
2985/2985 - 9s - loss: 1.7105e-04 - val_loss: 1.4437e-04 - 9s/epoch - 3ms/step
Epoch 265/300
2985/2985 - 9s - loss: 1.6627e-04 - val_loss: 1.4356e-04 - 9s/epoch - 3ms/step
Epoch 266/300
2985/2985 - 9s - loss: 1.6793e-04 - val_loss: 1.2610e-04 - 9s/epoch - 3ms/step
Epoch 267/300
2985/2985 - 9s - loss: 1.6186e-04 - val_loss: 1.3891e-04 - 9s/epoch - 3ms/step
Epoch 268/300
2985/2985 - 9s - loss: 1.6513e-04 - val_loss: 1.3117e-04 - 9s/epoch - 3ms/step
Epoch 269/300
2985/2985 - 9s - loss: 1.7007e-04 - val_loss: 2.6521e-04 - 9s/epoch - 3ms/step
Epoch 270/300
2985/2985 - 9s - loss: 1.6538e-04 - val_loss: 1.2788e-04 - 9s/epoch - 3ms/step
Epoch 271/300
2985/2985 - 9s - loss: 1.6107e-04 - val_loss: 1.4184e-04 - 9s/epoch - 3ms/step
Epoch 272/300
2985/2985 - 9s - loss: 1.6425e-04 - val_loss: 1.4062e-04 - 9s/epoch - 3ms/step
Epoch 273/300
2985/2985 - 9s - loss: 1.6069e-04 - val_loss: 1.5945e-04 - 9s/epoch - 3ms/step
Epoch 274/300
2985/2985 - 9s - loss: 1.7449e-04 - val_loss: 1.8878e-04 - 9s/epoch - 3ms/step
Epoch 275/300
2985/2985 - 9s - loss: 1.7982e-04 - val_loss: 1.4027e-04 - 9s/epoch - 3ms/step
Epoch 276/300
2985/2985 - 9s - loss: 1.6234e-04 - val_loss: 1.3466e-04 - 9s/epoch - 3ms/step
Epoch 277/300
2985/2985 - 9s - loss: 1.6145e-04 - val_loss: 1.3859e-04 - 9s/epoch - 3ms/step
Epoch 278/300
2985/2985 - 9s - loss: 1.5969e-04 - val_loss: 1.2805e-04 - 9s/epoch - 3ms/step
Epoch 279/300
2985/2985 - 9s - loss: 1.6043e-04 - val_loss: 1.2770e-04 - 9s/epoch - 3ms/step
Epoch 280/300
2985/2985 - 9s - loss: 1.6134e-04 - val_loss: 1.4268e-04 - 9s/epoch - 3ms/step
Epoch 281/300
2985/2985 - 9s - loss: 1.6548e-04 - val_loss: 1.4690e-04 - 9s/epoch - 3ms/step
Epoch 282/300
2985/2985 - 9s - loss: 1.6060e-04 - val_loss: 1.3941e-04 - 9s/epoch - 3ms/step
Epoch 283/300
2985/2985 - 9s - loss: 1.6353e-04 - val_loss: 1.3850e-04 - 9s/epoch - 3ms/step
Epoch 284/300
2985/2985 - 9s - loss: 1.6249e-04 - val_loss: 1.3191e-04 - 9s/epoch - 3ms/step
Epoch 285/300
2985/2985 - 9s - loss: 1.8192e-04 - val_loss: 1.4271e-04 - 9s/epoch - 3ms/step
Epoch 286/300
2985/2985 - 9s - loss: 1.6001e-04 - val_loss: 1.4034e-04 - 9s/epoch - 3ms/step
Epoch 287/300
2985/2985 - 9s - loss: 1.5827e-04 - val_loss: 1.5054e-04 - 9s/epoch - 3ms/step
Epoch 288/300
2985/2985 - 9s - loss: 1.6374e-04 - val_loss: 1.4275e-04 - 9s/epoch - 3ms/step
Epoch 289/300
2985/2985 - 9s - loss: 1.5961e-04 - val_loss: 1.3264e-04 - 9s/epoch - 3ms/step
Epoch 290/300
2985/2985 - 9s - loss: 1.6284e-04 - val_loss: 1.5582e-04 - 9s/epoch - 3ms/step
Epoch 291/300
2985/2985 - 9s - loss: 1.6381e-04 - val_loss: 1.4641e-04 - 9s/epoch - 3ms/step
Epoch 292/300
2985/2985 - 9s - loss: 1.5973e-04 - val_loss: 1.3341e-04 - 9s/epoch - 3ms/step
Epoch 293/300
2985/2985 - 9s - loss: 1.6140e-04 - val_loss: 1.3802e-04 - 9s/epoch - 3ms/step
Epoch 294/300
2985/2985 - 9s - loss: 1.8132e-04 - val_loss: 1.3259e-04 - 9s/epoch - 3ms/step
Epoch 295/300
2985/2985 - 9s - loss: 1.6156e-04 - val_loss: 1.2520e-04 - 9s/epoch - 3ms/step
Epoch 296/300
2985/2985 - 9s - loss: 1.5929e-04 - val_loss: 1.3475e-04 - 9s/epoch - 3ms/step
Epoch 297/300
2985/2985 - 9s - loss: 1.5704e-04 - val_loss: 1.4623e-04 - 9s/epoch - 3ms/step
Epoch 298/300
2985/2985 - 9s - loss: 1.5657e-04 - val_loss: 1.4754e-04 - 9s/epoch - 3ms/step
Epoch 299/300
2985/2985 - 9s - loss: 1.7171e-04 - val_loss: 1.4422e-04 - 9s/epoch - 3ms/step
Epoch 300/300
2985/2985 - 9s - loss: 1.6516e-04 - val_loss: 1.3229e-04 - 9s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00013228839088696986
  1/332 [..............................] - ETA: 31s 48/332 [===>..........................] - ETA: 0s  95/332 [=======>......................] - ETA: 0s144/332 [============>.................] - ETA: 0s193/332 [================>.............] - ETA: 0s242/332 [====================>.........] - ETA: 0s290/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0015003983530960485
cosine 0.0011858196302130678
MAE: 0.0063303285
RMSE: 0.011501664
r2: 0.9914196555729797
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_24"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_25 (InputLayer)       multiple                  0         
                                                                 
 dense_24 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_24 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_25 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 632)               0         
                                                                 
 dense_25 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_26 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2528)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_25"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_26 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_25 (InputLayer)       multiple                  0         
                                                                 
 dense_24 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_24 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_26"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_27 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_25 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 632)               0         
                                                                 
 dense_25 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_26 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2528)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 32, 300, 0.001, 0.5, 632, 0.00016515975585207343, 0.00013228839088696986, 0.0015003983530960485, 0.0011858196302130678, 0.006330328527837992, 0.01150166429579258, 0.9914196555729797, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_27"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_27 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_27 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_27 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_28 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_28 (ReLU)             (None, 632)               0         
                                                                 
 dense_28 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_29 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_29 (ReLU)             (None, 2528)              0         
                                                                 
 dense_29 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/300
2985/2985 - 9s - loss: 0.0092 - val_loss: 0.0027 - 9s/epoch - 3ms/step
Epoch 2/300
2985/2985 - 9s - loss: 0.0026 - val_loss: 0.0018 - 9s/epoch - 3ms/step
Epoch 3/300
2985/2985 - 9s - loss: 0.0017 - val_loss: 0.0013 - 9s/epoch - 3ms/step
Epoch 4/300
2985/2985 - 9s - loss: 0.0013 - val_loss: 9.9318e-04 - 9s/epoch - 3ms/step
Epoch 5/300
2985/2985 - 9s - loss: 0.0011 - val_loss: 8.1069e-04 - 9s/epoch - 3ms/step
Epoch 6/300
2985/2985 - 9s - loss: 9.2106e-04 - val_loss: 7.2074e-04 - 9s/epoch - 3ms/step
Epoch 7/300
2985/2985 - 9s - loss: 7.9886e-04 - val_loss: 6.2591e-04 - 9s/epoch - 3ms/step
Epoch 8/300
2985/2985 - 9s - loss: 7.3647e-04 - val_loss: 5.8550e-04 - 9s/epoch - 3ms/step
Epoch 9/300
2985/2985 - 9s - loss: 7.8389e-04 - val_loss: 7.7845e-04 - 9s/epoch - 3ms/step
Epoch 10/300
2985/2985 - 9s - loss: 7.1461e-04 - val_loss: 5.5695e-04 - 9s/epoch - 3ms/step
Epoch 11/300
2985/2985 - 9s - loss: 6.4180e-04 - val_loss: 5.0059e-04 - 9s/epoch - 3ms/step
Epoch 12/300
2985/2985 - 9s - loss: 6.2295e-04 - val_loss: 7.1540e-04 - 9s/epoch - 3ms/step
Epoch 13/300
2985/2985 - 9s - loss: 6.4111e-04 - val_loss: 4.8492e-04 - 9s/epoch - 3ms/step
Epoch 14/300
2985/2985 - 9s - loss: 5.7446e-04 - val_loss: 4.5164e-04 - 9s/epoch - 3ms/step
Epoch 15/300
2985/2985 - 9s - loss: 5.8195e-04 - val_loss: 4.5090e-04 - 9s/epoch - 3ms/step
Epoch 16/300
2985/2985 - 9s - loss: 5.4901e-04 - val_loss: 4.6399e-04 - 9s/epoch - 3ms/step
Epoch 17/300
2985/2985 - 9s - loss: 5.2120e-04 - val_loss: 4.4748e-04 - 9s/epoch - 3ms/step
Epoch 18/300
2985/2985 - 9s - loss: 5.2860e-04 - val_loss: 4.9300e-04 - 9s/epoch - 3ms/step
Epoch 19/300
2985/2985 - 9s - loss: 5.0737e-04 - val_loss: 4.7816e-04 - 9s/epoch - 3ms/step
Epoch 20/300
2985/2985 - 9s - loss: 5.2542e-04 - val_loss: 4.0791e-04 - 9s/epoch - 3ms/step
Epoch 21/300
2985/2985 - 9s - loss: 4.7936e-04 - val_loss: 3.8176e-04 - 9s/epoch - 3ms/step
Epoch 22/300
2985/2985 - 9s - loss: 4.7213e-04 - val_loss: 3.8224e-04 - 9s/epoch - 3ms/step
Epoch 23/300
2985/2985 - 9s - loss: 4.8568e-04 - val_loss: 3.7614e-04 - 9s/epoch - 3ms/step
Epoch 24/300
2985/2985 - 9s - loss: 4.6501e-04 - val_loss: 3.7065e-04 - 9s/epoch - 3ms/step
Epoch 25/300
2985/2985 - 9s - loss: 4.4701e-04 - val_loss: 3.8649e-04 - 9s/epoch - 3ms/step
Epoch 26/300
2985/2985 - 9s - loss: 5.2054e-04 - val_loss: 4.0602e-04 - 9s/epoch - 3ms/step
Epoch 27/300
2985/2985 - 9s - loss: 4.6482e-04 - val_loss: 3.5903e-04 - 9s/epoch - 3ms/step
Epoch 28/300
2985/2985 - 9s - loss: 4.4251e-04 - val_loss: 3.5256e-04 - 9s/epoch - 3ms/step
Epoch 29/300
2985/2985 - 9s - loss: 4.5188e-04 - val_loss: 3.7152e-04 - 9s/epoch - 3ms/step
Epoch 30/300
2985/2985 - 9s - loss: 4.3789e-04 - val_loss: 3.4685e-04 - 9s/epoch - 3ms/step
Epoch 31/300
2985/2985 - 9s - loss: 4.3638e-04 - val_loss: 3.2613e-04 - 9s/epoch - 3ms/step
Epoch 32/300
2985/2985 - 9s - loss: 4.1925e-04 - val_loss: 3.2594e-04 - 9s/epoch - 3ms/step
Epoch 33/300
2985/2985 - 9s - loss: 4.1866e-04 - val_loss: 3.1588e-04 - 9s/epoch - 3ms/step
Epoch 34/300
2985/2985 - 9s - loss: 4.1455e-04 - val_loss: 3.1364e-04 - 9s/epoch - 3ms/step
Epoch 35/300
2985/2985 - 9s - loss: 3.9884e-04 - val_loss: 3.0992e-04 - 9s/epoch - 3ms/step
Epoch 36/300
2985/2985 - 9s - loss: 4.0047e-04 - val_loss: 3.2255e-04 - 9s/epoch - 3ms/step
Epoch 37/300
2985/2985 - 9s - loss: 3.9942e-04 - val_loss: 3.1886e-04 - 9s/epoch - 3ms/step
Epoch 38/300
2985/2985 - 9s - loss: 4.0622e-04 - val_loss: 3.0326e-04 - 9s/epoch - 3ms/step
Epoch 39/300
2985/2985 - 9s - loss: 3.9152e-04 - val_loss: 2.9798e-04 - 9s/epoch - 3ms/step
Epoch 40/300
2985/2985 - 9s - loss: 3.8900e-04 - val_loss: 3.6017e-04 - 9s/epoch - 3ms/step
Epoch 41/300
2985/2985 - 9s - loss: 3.8791e-04 - val_loss: 2.9346e-04 - 9s/epoch - 3ms/step
Epoch 42/300
2985/2985 - 9s - loss: 3.7754e-04 - val_loss: 3.0523e-04 - 9s/epoch - 3ms/step
Epoch 43/300
2985/2985 - 9s - loss: 3.7491e-04 - val_loss: 3.0151e-04 - 9s/epoch - 3ms/step
Epoch 44/300
2985/2985 - 9s - loss: 3.6848e-04 - val_loss: 3.7196e-04 - 9s/epoch - 3ms/step
Epoch 45/300
2985/2985 - 9s - loss: 3.7523e-04 - val_loss: 2.8559e-04 - 9s/epoch - 3ms/step
Epoch 46/300
2985/2985 - 9s - loss: 3.6853e-04 - val_loss: 2.7657e-04 - 9s/epoch - 3ms/step
Epoch 47/300
2985/2985 - 9s - loss: 3.6096e-04 - val_loss: 2.8462e-04 - 9s/epoch - 3ms/step
Epoch 48/300
2985/2985 - 9s - loss: 3.6148e-04 - val_loss: 2.9226e-04 - 9s/epoch - 3ms/step
Epoch 49/300
2985/2985 - 9s - loss: 3.7592e-04 - val_loss: 7.5880e-04 - 9s/epoch - 3ms/step
Epoch 50/300
2985/2985 - 9s - loss: 5.1501e-04 - val_loss: 3.2186e-04 - 9s/epoch - 3ms/step
Epoch 51/300
2985/2985 - 9s - loss: 3.8920e-04 - val_loss: 2.7906e-04 - 9s/epoch - 3ms/step
Epoch 52/300
2985/2985 - 9s - loss: 3.7048e-04 - val_loss: 2.7253e-04 - 9s/epoch - 3ms/step
Epoch 53/300
2985/2985 - 9s - loss: 3.6397e-04 - val_loss: 2.8291e-04 - 9s/epoch - 3ms/step
Epoch 54/300
2985/2985 - 9s - loss: 3.9109e-04 - val_loss: 3.0292e-04 - 9s/epoch - 3ms/step
Epoch 55/300
2985/2985 - 9s - loss: 3.5883e-04 - val_loss: 2.7550e-04 - 9s/epoch - 3ms/step
Epoch 56/300
2985/2985 - 9s - loss: 3.6409e-04 - val_loss: 2.6951e-04 - 9s/epoch - 3ms/step
Epoch 57/300
2985/2985 - 9s - loss: 3.5033e-04 - val_loss: 2.7670e-04 - 9s/epoch - 3ms/step
Epoch 58/300
2985/2985 - 9s - loss: 3.5868e-04 - val_loss: 2.7745e-04 - 9s/epoch - 3ms/step
Epoch 59/300
2985/2985 - 9s - loss: 3.4365e-04 - val_loss: 2.6879e-04 - 9s/epoch - 3ms/step
Epoch 60/300
2985/2985 - 9s - loss: 3.5737e-04 - val_loss: 2.6507e-04 - 9s/epoch - 3ms/step
Epoch 61/300
2985/2985 - 9s - loss: 3.4604e-04 - val_loss: 2.8025e-04 - 9s/epoch - 3ms/step
Epoch 62/300
2985/2985 - 9s - loss: 3.4348e-04 - val_loss: 2.8152e-04 - 9s/epoch - 3ms/step
Epoch 63/300
2985/2985 - 9s - loss: 3.3917e-04 - val_loss: 2.7529e-04 - 9s/epoch - 3ms/step
Epoch 64/300
2985/2985 - 9s - loss: 3.3783e-04 - val_loss: 2.5477e-04 - 9s/epoch - 3ms/step
Epoch 65/300
2985/2985 - 9s - loss: 3.3287e-04 - val_loss: 2.7952e-04 - 9s/epoch - 3ms/step
Epoch 66/300
2985/2985 - 9s - loss: 3.5314e-04 - val_loss: 2.5488e-04 - 9s/epoch - 3ms/step
Epoch 67/300
2985/2985 - 9s - loss: 3.3514e-04 - val_loss: 2.5556e-04 - 9s/epoch - 3ms/step
Epoch 68/300
2985/2985 - 9s - loss: 3.6357e-04 - val_loss: 2.6122e-04 - 9s/epoch - 3ms/step
Epoch 69/300
2985/2985 - 9s - loss: 3.3665e-04 - val_loss: 2.5918e-04 - 9s/epoch - 3ms/step
Epoch 70/300
2985/2985 - 9s - loss: 3.3335e-04 - val_loss: 3.7246e-04 - 9s/epoch - 3ms/step
Epoch 71/300
2985/2985 - 9s - loss: 3.7290e-04 - val_loss: 3.1231e-04 - 9s/epoch - 3ms/step
Epoch 72/300
2985/2985 - 9s - loss: 3.5598e-04 - val_loss: 2.6618e-04 - 9s/epoch - 3ms/step
Epoch 73/300
2985/2985 - 9s - loss: 3.4268e-04 - val_loss: 2.8916e-04 - 9s/epoch - 3ms/step
Epoch 74/300
2985/2985 - 9s - loss: 3.4045e-04 - val_loss: 2.5926e-04 - 9s/epoch - 3ms/step
Epoch 75/300
2985/2985 - 9s - loss: 3.3492e-04 - val_loss: 2.5332e-04 - 9s/epoch - 3ms/step
Epoch 76/300
2985/2985 - 9s - loss: 3.2307e-04 - val_loss: 2.4504e-04 - 9s/epoch - 3ms/step
Epoch 77/300
2985/2985 - 9s - loss: 3.4839e-04 - val_loss: 7.1462e-04 - 9s/epoch - 3ms/step
Epoch 78/300
2985/2985 - 9s - loss: 4.5769e-04 - val_loss: 2.7483e-04 - 9s/epoch - 3ms/step
Epoch 79/300
2985/2985 - 9s - loss: 3.5226e-04 - val_loss: 2.8668e-04 - 9s/epoch - 3ms/step
Epoch 80/300
2985/2985 - 9s - loss: 3.4297e-04 - val_loss: 2.6314e-04 - 9s/epoch - 3ms/step
Epoch 81/300
2985/2985 - 9s - loss: 3.2934e-04 - val_loss: 3.3723e-04 - 9s/epoch - 3ms/step
Epoch 82/300
2985/2985 - 9s - loss: 3.2982e-04 - val_loss: 2.5137e-04 - 9s/epoch - 3ms/step
Epoch 83/300
2985/2985 - 9s - loss: 3.3977e-04 - val_loss: 2.5218e-04 - 9s/epoch - 3ms/step
Epoch 84/300
2985/2985 - 9s - loss: 3.2611e-04 - val_loss: 2.6219e-04 - 9s/epoch - 3ms/step
Epoch 85/300
2985/2985 - 9s - loss: 3.2533e-04 - val_loss: 2.2510e-04 - 9s/epoch - 3ms/step
Epoch 86/300
2985/2985 - 9s - loss: 3.2376e-04 - val_loss: 2.3774e-04 - 9s/epoch - 3ms/step
Epoch 87/300
2985/2985 - 9s - loss: 3.1876e-04 - val_loss: 2.6493e-04 - 9s/epoch - 3ms/step
Epoch 88/300
2985/2985 - 9s - loss: 3.2866e-04 - val_loss: 2.4289e-04 - 9s/epoch - 3ms/step
Epoch 89/300
2985/2985 - 9s - loss: 3.1799e-04 - val_loss: 2.5154e-04 - 9s/epoch - 3ms/step
Epoch 90/300
2985/2985 - 9s - loss: 3.1233e-04 - val_loss: 2.3904e-04 - 9s/epoch - 3ms/step
Epoch 91/300
2985/2985 - 9s - loss: 3.0867e-04 - val_loss: 2.3354e-04 - 9s/epoch - 3ms/step
Epoch 92/300
2985/2985 - 9s - loss: 3.1156e-04 - val_loss: 2.4738e-04 - 9s/epoch - 3ms/step
Epoch 93/300
2985/2985 - 9s - loss: 3.0807e-04 - val_loss: 2.7123e-04 - 9s/epoch - 3ms/step
Epoch 94/300
2985/2985 - 9s - loss: 3.0680e-04 - val_loss: 2.2386e-04 - 9s/epoch - 3ms/step
Epoch 95/300
2985/2985 - 9s - loss: 3.2189e-04 - val_loss: 3.0135e-04 - 9s/epoch - 3ms/step
Epoch 96/300
2985/2985 - 9s - loss: 3.1430e-04 - val_loss: 2.6860e-04 - 9s/epoch - 3ms/step
Epoch 97/300
2985/2985 - 9s - loss: 3.1837e-04 - val_loss: 2.4564e-04 - 9s/epoch - 3ms/step
Epoch 98/300
2985/2985 - 9s - loss: 3.0906e-04 - val_loss: 2.4067e-04 - 9s/epoch - 3ms/step
Epoch 99/300
2985/2985 - 9s - loss: 3.0310e-04 - val_loss: 2.3859e-04 - 9s/epoch - 3ms/step
Epoch 100/300
2985/2985 - 9s - loss: 3.2842e-04 - val_loss: 2.3566e-04 - 9s/epoch - 3ms/step
Epoch 101/300
2985/2985 - 9s - loss: 3.0468e-04 - val_loss: 2.2199e-04 - 9s/epoch - 3ms/step
Epoch 102/300
2985/2985 - 9s - loss: 3.0294e-04 - val_loss: 2.3392e-04 - 9s/epoch - 3ms/step
Epoch 103/300
2985/2985 - 9s - loss: 3.0392e-04 - val_loss: 2.8206e-04 - 9s/epoch - 3ms/step
Epoch 104/300
2985/2985 - 9s - loss: 3.0113e-04 - val_loss: 2.6838e-04 - 9s/epoch - 3ms/step
Epoch 105/300
2985/2985 - 9s - loss: 3.0683e-04 - val_loss: 2.3692e-04 - 9s/epoch - 3ms/step
Epoch 106/300
2985/2985 - 9s - loss: 3.0096e-04 - val_loss: 2.3457e-04 - 9s/epoch - 3ms/step
Epoch 107/300
2985/2985 - 9s - loss: 2.9711e-04 - val_loss: 2.4092e-04 - 9s/epoch - 3ms/step
Epoch 108/300
2985/2985 - 9s - loss: 3.0370e-04 - val_loss: 2.2955e-04 - 9s/epoch - 3ms/step
Epoch 109/300
2985/2985 - 9s - loss: 2.9554e-04 - val_loss: 2.1965e-04 - 9s/epoch - 3ms/step
Epoch 110/300
2985/2985 - 9s - loss: 3.5493e-04 - val_loss: 3.2731e-04 - 9s/epoch - 3ms/step
Epoch 111/300
2985/2985 - 9s - loss: 3.0190e-04 - val_loss: 2.2195e-04 - 9s/epoch - 3ms/step
Epoch 112/300
2985/2985 - 9s - loss: 2.9684e-04 - val_loss: 2.3445e-04 - 9s/epoch - 3ms/step
Epoch 113/300
2985/2985 - 9s - loss: 2.9154e-04 - val_loss: 2.2777e-04 - 9s/epoch - 3ms/step
Epoch 114/300
2985/2985 - 9s - loss: 3.0032e-04 - val_loss: 2.3236e-04 - 9s/epoch - 3ms/step
Epoch 115/300
2985/2985 - 9s - loss: 2.9407e-04 - val_loss: 2.9357e-04 - 9s/epoch - 3ms/step
Epoch 116/300
2985/2985 - 9s - loss: 3.0172e-04 - val_loss: 3.7651e-04 - 9s/epoch - 3ms/step
Epoch 117/300
2985/2985 - 9s - loss: 2.9365e-04 - val_loss: 2.1667e-04 - 9s/epoch - 3ms/step
Epoch 118/300
2985/2985 - 9s - loss: 2.9041e-04 - val_loss: 2.2403e-04 - 9s/epoch - 3ms/step
Epoch 119/300
2985/2985 - 9s - loss: 2.9849e-04 - val_loss: 2.2673e-04 - 9s/epoch - 3ms/step
Epoch 120/300
2985/2985 - 9s - loss: 2.8795e-04 - val_loss: 2.2458e-04 - 9s/epoch - 3ms/step
Epoch 121/300
2985/2985 - 9s - loss: 2.8842e-04 - val_loss: 2.2160e-04 - 9s/epoch - 3ms/step
Epoch 122/300
2985/2985 - 9s - loss: 2.9302e-04 - val_loss: 2.1567e-04 - 9s/epoch - 3ms/step
Epoch 123/300
2985/2985 - 9s - loss: 2.8639e-04 - val_loss: 2.1951e-04 - 9s/epoch - 3ms/step
Epoch 124/300
2985/2985 - 9s - loss: 2.9654e-04 - val_loss: 2.4059e-04 - 9s/epoch - 3ms/step
Epoch 125/300
2985/2985 - 9s - loss: 2.8526e-04 - val_loss: 2.3037e-04 - 9s/epoch - 3ms/step
Epoch 126/300
2985/2985 - 9s - loss: 2.8411e-04 - val_loss: 2.1670e-04 - 9s/epoch - 3ms/step
Epoch 127/300
2985/2985 - 9s - loss: 2.8454e-04 - val_loss: 2.2562e-04 - 9s/epoch - 3ms/step
Epoch 128/300
2985/2985 - 9s - loss: 2.9420e-04 - val_loss: 2.9018e-04 - 9s/epoch - 3ms/step
Epoch 129/300
2985/2985 - 9s - loss: 2.9047e-04 - val_loss: 2.0687e-04 - 9s/epoch - 3ms/step
Epoch 130/300
2985/2985 - 9s - loss: 2.8428e-04 - val_loss: 2.1692e-04 - 9s/epoch - 3ms/step
Epoch 131/300
2985/2985 - 9s - loss: 2.8236e-04 - val_loss: 2.2700e-04 - 9s/epoch - 3ms/step
Epoch 132/300
2985/2985 - 9s - loss: 2.8645e-04 - val_loss: 2.1515e-04 - 9s/epoch - 3ms/step
Epoch 133/300
2985/2985 - 9s - loss: 2.8191e-04 - val_loss: 2.1656e-04 - 9s/epoch - 3ms/step
Epoch 134/300
2985/2985 - 9s - loss: 2.7965e-04 - val_loss: 2.1252e-04 - 9s/epoch - 3ms/step
Epoch 135/300
2985/2985 - 9s - loss: 2.8529e-04 - val_loss: 2.0937e-04 - 9s/epoch - 3ms/step
Epoch 136/300
2985/2985 - 9s - loss: 3.2985e-04 - val_loss: 2.1204e-04 - 9s/epoch - 3ms/step
Epoch 137/300
2985/2985 - 9s - loss: 2.8505e-04 - val_loss: 2.3821e-04 - 9s/epoch - 3ms/step
Epoch 138/300
2985/2985 - 9s - loss: 2.8359e-04 - val_loss: 2.4113e-04 - 9s/epoch - 3ms/step
Epoch 139/300
2985/2985 - 9s - loss: 2.8071e-04 - val_loss: 2.1492e-04 - 9s/epoch - 3ms/step
Epoch 140/300
2985/2985 - 9s - loss: 2.8770e-04 - val_loss: 2.2186e-04 - 9s/epoch - 3ms/step
Epoch 141/300
2985/2985 - 9s - loss: 2.7687e-04 - val_loss: 2.7742e-04 - 9s/epoch - 3ms/step
Epoch 142/300
2985/2985 - 9s - loss: 2.8585e-04 - val_loss: 2.0630e-04 - 9s/epoch - 3ms/step
Epoch 143/300
2985/2985 - 9s - loss: 2.7778e-04 - val_loss: 2.3318e-04 - 9s/epoch - 3ms/step
Epoch 144/300
2985/2985 - 9s - loss: 3.1554e-04 - val_loss: 2.1037e-04 - 9s/epoch - 3ms/step
Epoch 145/300
2985/2985 - 9s - loss: 2.7733e-04 - val_loss: 2.0979e-04 - 9s/epoch - 3ms/step
Epoch 146/300
2985/2985 - 9s - loss: 2.7493e-04 - val_loss: 2.1268e-04 - 9s/epoch - 3ms/step
Epoch 147/300
2985/2985 - 9s - loss: 2.7572e-04 - val_loss: 2.1536e-04 - 9s/epoch - 3ms/step
Epoch 148/300
2985/2985 - 9s - loss: 2.7373e-04 - val_loss: 2.3364e-04 - 9s/epoch - 3ms/step
Epoch 149/300
2985/2985 - 9s - loss: 2.8365e-04 - val_loss: 2.0585e-04 - 9s/epoch - 3ms/step
Epoch 150/300
2985/2985 - 9s - loss: 2.7849e-04 - val_loss: 2.1358e-04 - 9s/epoch - 3ms/step
Epoch 151/300
2985/2985 - 9s - loss: 2.7497e-04 - val_loss: 2.0438e-04 - 9s/epoch - 3ms/step
Epoch 152/300
2985/2985 - 9s - loss: 2.7498e-04 - val_loss: 2.0375e-04 - 9s/epoch - 3ms/step
Epoch 153/300
2985/2985 - 9s - loss: 2.7339e-04 - val_loss: 2.1121e-04 - 9s/epoch - 3ms/step
Epoch 154/300
2985/2985 - 9s - loss: 2.8388e-04 - val_loss: 2.1770e-04 - 9s/epoch - 3ms/step
Epoch 155/300
2985/2985 - 9s - loss: 2.7331e-04 - val_loss: 2.1189e-04 - 9s/epoch - 3ms/step
Epoch 156/300
2985/2985 - 9s - loss: 2.6871e-04 - val_loss: 2.0958e-04 - 9s/epoch - 3ms/step
Epoch 157/300
2985/2985 - 9s - loss: 3.0003e-04 - val_loss: 2.2366e-04 - 9s/epoch - 3ms/step
Epoch 158/300
2985/2985 - 9s - loss: 2.7906e-04 - val_loss: 2.0180e-04 - 9s/epoch - 3ms/step
Epoch 159/300
2985/2985 - 9s - loss: 2.7418e-04 - val_loss: 1.9345e-04 - 9s/epoch - 3ms/step
Epoch 160/300
2985/2985 - 9s - loss: 2.7760e-04 - val_loss: 2.1585e-04 - 9s/epoch - 3ms/step
Epoch 161/300
2985/2985 - 9s - loss: 2.7425e-04 - val_loss: 2.2278e-04 - 9s/epoch - 3ms/step
Epoch 162/300
2985/2985 - 9s - loss: 2.7169e-04 - val_loss: 2.1579e-04 - 9s/epoch - 3ms/step
Epoch 163/300
2985/2985 - 9s - loss: 2.6672e-04 - val_loss: 2.2145e-04 - 9s/epoch - 3ms/step
Epoch 164/300
2985/2985 - 9s - loss: 2.7440e-04 - val_loss: 2.2294e-04 - 9s/epoch - 3ms/step
Epoch 165/300
2985/2985 - 9s - loss: 2.6643e-04 - val_loss: 2.0702e-04 - 9s/epoch - 3ms/step
Epoch 166/300
2985/2985 - 9s - loss: 2.6650e-04 - val_loss: 2.0874e-04 - 9s/epoch - 3ms/step
Epoch 167/300
2985/2985 - 9s - loss: 2.7333e-04 - val_loss: 2.0782e-04 - 9s/epoch - 3ms/step
Epoch 168/300
2985/2985 - 9s - loss: 3.0318e-04 - val_loss: 2.0308e-04 - 9s/epoch - 3ms/step
Epoch 169/300
2985/2985 - 9s - loss: 2.7364e-04 - val_loss: 2.2219e-04 - 9s/epoch - 3ms/step
Epoch 170/300
2985/2985 - 9s - loss: 2.6815e-04 - val_loss: 2.0488e-04 - 9s/epoch - 3ms/step
Epoch 171/300
2985/2985 - 9s - loss: 2.6693e-04 - val_loss: 2.0995e-04 - 9s/epoch - 3ms/step
Epoch 172/300
2985/2985 - 9s - loss: 2.6303e-04 - val_loss: 2.1596e-04 - 9s/epoch - 3ms/step
Epoch 173/300
2985/2985 - 9s - loss: 2.7430e-04 - val_loss: 2.1766e-04 - 9s/epoch - 3ms/step
Epoch 174/300
2985/2985 - 9s - loss: 2.6490e-04 - val_loss: 2.1527e-04 - 9s/epoch - 3ms/step
Epoch 175/300
2985/2985 - 9s - loss: 2.6888e-04 - val_loss: 2.1297e-04 - 9s/epoch - 3ms/step
Epoch 176/300
2985/2985 - 9s - loss: 2.6517e-04 - val_loss: 2.1184e-04 - 9s/epoch - 3ms/step
Epoch 177/300
2985/2985 - 9s - loss: 2.6571e-04 - val_loss: 1.9441e-04 - 9s/epoch - 3ms/step
Epoch 178/300
2985/2985 - 9s - loss: 2.6054e-04 - val_loss: 1.8849e-04 - 9s/epoch - 3ms/step
Epoch 179/300
2985/2985 - 9s - loss: 2.8079e-04 - val_loss: 2.1433e-04 - 9s/epoch - 3ms/step
Epoch 180/300
2985/2985 - 9s - loss: 2.6810e-04 - val_loss: 1.9947e-04 - 9s/epoch - 3ms/step
Epoch 181/300
2985/2985 - 9s - loss: 2.6154e-04 - val_loss: 2.0732e-04 - 9s/epoch - 3ms/step
Epoch 182/300
2985/2985 - 9s - loss: 2.6187e-04 - val_loss: 2.0668e-04 - 9s/epoch - 3ms/step
Epoch 183/300
2985/2985 - 9s - loss: 2.8018e-04 - val_loss: 2.0233e-04 - 9s/epoch - 3ms/step
Epoch 184/300
2985/2985 - 9s - loss: 2.6628e-04 - val_loss: 2.0782e-04 - 9s/epoch - 3ms/step
Epoch 185/300
2985/2985 - 9s - loss: 2.6091e-04 - val_loss: 2.2115e-04 - 9s/epoch - 3ms/step
Epoch 186/300
2985/2985 - 9s - loss: 2.5755e-04 - val_loss: 1.9330e-04 - 9s/epoch - 3ms/step
Epoch 187/300
2985/2985 - 9s - loss: 2.6103e-04 - val_loss: 2.0820e-04 - 9s/epoch - 3ms/step
Epoch 188/300
2985/2985 - 9s - loss: 2.6022e-04 - val_loss: 2.2222e-04 - 9s/epoch - 3ms/step
Epoch 189/300
2985/2985 - 9s - loss: 2.7822e-04 - val_loss: 1.9256e-04 - 9s/epoch - 3ms/step
Epoch 190/300
2985/2985 - 9s - loss: 2.6781e-04 - val_loss: 2.0711e-04 - 9s/epoch - 3ms/step
Epoch 191/300
2985/2985 - 9s - loss: 2.8201e-04 - val_loss: 3.3896e-04 - 9s/epoch - 3ms/step
Epoch 192/300
2985/2985 - 9s - loss: 2.7271e-04 - val_loss: 2.0727e-04 - 9s/epoch - 3ms/step
Epoch 193/300
2985/2985 - 9s - loss: 2.6873e-04 - val_loss: 2.0172e-04 - 9s/epoch - 3ms/step
Epoch 194/300
2985/2985 - 9s - loss: 2.6122e-04 - val_loss: 2.4850e-04 - 9s/epoch - 3ms/step
Epoch 195/300
2985/2985 - 9s - loss: 2.6442e-04 - val_loss: 1.9844e-04 - 9s/epoch - 3ms/step
Epoch 196/300
2985/2985 - 9s - loss: 2.6343e-04 - val_loss: 2.2300e-04 - 9s/epoch - 3ms/step
Epoch 197/300
2985/2985 - 9s - loss: 2.5771e-04 - val_loss: 2.3042e-04 - 9s/epoch - 3ms/step
Epoch 198/300
2985/2985 - 9s - loss: 2.6094e-04 - val_loss: 2.6807e-04 - 9s/epoch - 3ms/step
Epoch 199/300
2985/2985 - 9s - loss: 2.7215e-04 - val_loss: 2.0507e-04 - 9s/epoch - 3ms/step
Epoch 200/300
2985/2985 - 9s - loss: 2.5782e-04 - val_loss: 2.1701e-04 - 9s/epoch - 3ms/step
Epoch 201/300
2985/2985 - 9s - loss: 2.5656e-04 - val_loss: 2.0330e-04 - 9s/epoch - 3ms/step
Epoch 202/300
2985/2985 - 9s - loss: 2.6117e-04 - val_loss: 2.0710e-04 - 9s/epoch - 3ms/step
Epoch 203/300
2985/2985 - 9s - loss: 2.5914e-04 - val_loss: 2.2652e-04 - 9s/epoch - 3ms/step
Epoch 204/300
2985/2985 - 9s - loss: 2.5723e-04 - val_loss: 1.9924e-04 - 9s/epoch - 3ms/step
Epoch 205/300
2985/2985 - 9s - loss: 2.5653e-04 - val_loss: 2.1396e-04 - 9s/epoch - 3ms/step
Epoch 206/300
2985/2985 - 9s - loss: 2.5454e-04 - val_loss: 1.9470e-04 - 9s/epoch - 3ms/step
Epoch 207/300
2985/2985 - 9s - loss: 2.5242e-04 - val_loss: 2.1851e-04 - 9s/epoch - 3ms/step
Epoch 208/300
2985/2985 - 9s - loss: 2.5550e-04 - val_loss: 2.2055e-04 - 9s/epoch - 3ms/step
Epoch 209/300
2985/2985 - 9s - loss: 2.5289e-04 - val_loss: 2.0321e-04 - 9s/epoch - 3ms/step
Epoch 210/300
2985/2985 - 9s - loss: 2.5254e-04 - val_loss: 1.9588e-04 - 9s/epoch - 3ms/step
Epoch 211/300
2985/2985 - 9s - loss: 2.5596e-04 - val_loss: 1.9998e-04 - 9s/epoch - 3ms/step
Epoch 212/300
2985/2985 - 9s - loss: 2.6701e-04 - val_loss: 1.9324e-04 - 9s/epoch - 3ms/step
Epoch 213/300
2985/2985 - 9s - loss: 2.5754e-04 - val_loss: 1.9367e-04 - 9s/epoch - 3ms/step
Epoch 214/300
2985/2985 - 9s - loss: 2.5885e-04 - val_loss: 2.2039e-04 - 9s/epoch - 3ms/step
Epoch 215/300
2985/2985 - 9s - loss: 2.6176e-04 - val_loss: 2.3354e-04 - 9s/epoch - 3ms/step
Epoch 216/300
2985/2985 - 9s - loss: 2.6224e-04 - val_loss: 2.1292e-04 - 9s/epoch - 3ms/step
Epoch 217/300
2985/2985 - 9s - loss: 2.5448e-04 - val_loss: 1.9364e-04 - 9s/epoch - 3ms/step
Epoch 218/300
2985/2985 - 9s - loss: 2.5262e-04 - val_loss: 2.3281e-04 - 9s/epoch - 3ms/step
Epoch 219/300
2985/2985 - 9s - loss: 2.5485e-04 - val_loss: 1.9676e-04 - 9s/epoch - 3ms/step
Epoch 220/300
2985/2985 - 9s - loss: 2.5022e-04 - val_loss: 1.9584e-04 - 9s/epoch - 3ms/step
Epoch 221/300
2985/2985 - 9s - loss: 3.0923e-04 - val_loss: 2.5892e-04 - 9s/epoch - 3ms/step
Epoch 222/300
2985/2985 - 9s - loss: 2.6183e-04 - val_loss: 2.2570e-04 - 9s/epoch - 3ms/step
Epoch 223/300
2985/2985 - 9s - loss: 2.6507e-04 - val_loss: 3.1123e-04 - 9s/epoch - 3ms/step
Epoch 224/300
2985/2985 - 9s - loss: 2.5296e-04 - val_loss: 2.0550e-04 - 9s/epoch - 3ms/step
Epoch 225/300
2985/2985 - 9s - loss: 2.5066e-04 - val_loss: 2.2199e-04 - 9s/epoch - 3ms/step
Epoch 226/300
2985/2985 - 9s - loss: 2.6986e-04 - val_loss: 4.5428e-04 - 9s/epoch - 3ms/step
Epoch 227/300
2985/2985 - 9s - loss: 3.3603e-04 - val_loss: 2.1240e-04 - 9s/epoch - 3ms/step
Epoch 228/300
2985/2985 - 9s - loss: 2.6257e-04 - val_loss: 2.0922e-04 - 9s/epoch - 3ms/step
Epoch 229/300
2985/2985 - 9s - loss: 2.6219e-04 - val_loss: 2.0266e-04 - 9s/epoch - 3ms/step
Epoch 230/300
2985/2985 - 9s - loss: 2.5509e-04 - val_loss: 1.9208e-04 - 9s/epoch - 3ms/step
Epoch 231/300
2985/2985 - 9s - loss: 2.5348e-04 - val_loss: 2.0211e-04 - 9s/epoch - 3ms/step
Epoch 232/300
2985/2985 - 9s - loss: 2.5106e-04 - val_loss: 2.0043e-04 - 9s/epoch - 3ms/step
Epoch 233/300
2985/2985 - 9s - loss: 2.5401e-04 - val_loss: 2.1820e-04 - 9s/epoch - 3ms/step
Epoch 234/300
2985/2985 - 9s - loss: 2.6037e-04 - val_loss: 2.1446e-04 - 9s/epoch - 3ms/step
Epoch 235/300
2985/2985 - 9s - loss: 2.5687e-04 - val_loss: 2.0017e-04 - 9s/epoch - 3ms/step
Epoch 236/300
2985/2985 - 9s - loss: 2.4879e-04 - val_loss: 2.1866e-04 - 9s/epoch - 3ms/step
Epoch 237/300
2985/2985 - 9s - loss: 2.6411e-04 - val_loss: 2.1015e-04 - 9s/epoch - 3ms/step
Epoch 238/300
2985/2985 - 9s - loss: 2.5238e-04 - val_loss: 2.0155e-04 - 9s/epoch - 3ms/step
Epoch 239/300
2985/2985 - 9s - loss: 2.5616e-04 - val_loss: 1.9382e-04 - 9s/epoch - 3ms/step
Epoch 240/300
2985/2985 - 9s - loss: 2.4988e-04 - val_loss: 1.9610e-04 - 9s/epoch - 3ms/step
Epoch 241/300
2985/2985 - 9s - loss: 2.4956e-04 - val_loss: 1.8912e-04 - 9s/epoch - 3ms/step
Epoch 242/300
2985/2985 - 9s - loss: 2.5379e-04 - val_loss: 1.9095e-04 - 9s/epoch - 3ms/step
Epoch 243/300
2985/2985 - 9s - loss: 2.4731e-04 - val_loss: 1.9229e-04 - 9s/epoch - 3ms/step
Epoch 244/300
2985/2985 - 9s - loss: 2.5471e-04 - val_loss: 4.9744e-04 - 9s/epoch - 3ms/step
Epoch 245/300
2985/2985 - 9s - loss: 2.9391e-04 - val_loss: 2.0347e-04 - 9s/epoch - 3ms/step
Epoch 246/300
2985/2985 - 9s - loss: 2.5912e-04 - val_loss: 3.0423e-04 - 9s/epoch - 3ms/step
Epoch 247/300
2985/2985 - 9s - loss: 2.6473e-04 - val_loss: 2.2145e-04 - 9s/epoch - 3ms/step
Epoch 248/300
2985/2985 - 9s - loss: 2.5969e-04 - val_loss: 2.5514e-04 - 9s/epoch - 3ms/step
Epoch 249/300
2985/2985 - 9s - loss: 2.5744e-04 - val_loss: 2.0605e-04 - 9s/epoch - 3ms/step
Epoch 250/300
2985/2985 - 9s - loss: 2.4969e-04 - val_loss: 2.0291e-04 - 9s/epoch - 3ms/step
Epoch 251/300
2985/2985 - 9s - loss: 2.4902e-04 - val_loss: 1.8569e-04 - 9s/epoch - 3ms/step
Epoch 252/300
2985/2985 - 9s - loss: 2.7316e-04 - val_loss: 2.1549e-04 - 9s/epoch - 3ms/step
Epoch 253/300
2985/2985 - 9s - loss: 2.5379e-04 - val_loss: 2.0863e-04 - 9s/epoch - 3ms/step
Epoch 254/300
2985/2985 - 9s - loss: 2.5215e-04 - val_loss: 4.1779e-04 - 9s/epoch - 3ms/step
Epoch 255/300
2985/2985 - 9s - loss: 2.7852e-04 - val_loss: 2.0740e-04 - 9s/epoch - 3ms/step
Epoch 256/300
2985/2985 - 9s - loss: 2.5006e-04 - val_loss: 1.8808e-04 - 9s/epoch - 3ms/step
Epoch 257/300
2985/2985 - 9s - loss: 2.5058e-04 - val_loss: 2.1264e-04 - 9s/epoch - 3ms/step
Epoch 258/300
2985/2985 - 9s - loss: 2.4905e-04 - val_loss: 1.9323e-04 - 9s/epoch - 3ms/step
Epoch 259/300
2985/2985 - 9s - loss: 2.6821e-04 - val_loss: 3.6779e-04 - 9s/epoch - 3ms/step
Epoch 260/300
2985/2985 - 9s - loss: 2.8597e-04 - val_loss: 1.9305e-04 - 9s/epoch - 3ms/step
Epoch 261/300
2985/2985 - 9s - loss: 2.4900e-04 - val_loss: 1.9908e-04 - 9s/epoch - 3ms/step
Epoch 262/300
2985/2985 - 9s - loss: 2.5097e-04 - val_loss: 1.8446e-04 - 9s/epoch - 3ms/step
Epoch 263/300
2985/2985 - 9s - loss: 2.4815e-04 - val_loss: 2.4343e-04 - 9s/epoch - 3ms/step
Epoch 264/300
2985/2985 - 9s - loss: 2.4532e-04 - val_loss: 2.0459e-04 - 9s/epoch - 3ms/step
Epoch 265/300
2985/2985 - 9s - loss: 2.4547e-04 - val_loss: 1.9057e-04 - 9s/epoch - 3ms/step
Epoch 266/300
2985/2985 - 9s - loss: 2.7209e-04 - val_loss: 1.9799e-04 - 9s/epoch - 3ms/step
Epoch 267/300
2985/2985 - 9s - loss: 2.5344e-04 - val_loss: 1.9437e-04 - 9s/epoch - 3ms/step
Epoch 268/300
2985/2985 - 9s - loss: 2.4748e-04 - val_loss: 1.9817e-04 - 9s/epoch - 3ms/step
Epoch 269/300
2985/2985 - 9s - loss: 2.4917e-04 - val_loss: 2.3147e-04 - 9s/epoch - 3ms/step
Epoch 270/300
2985/2985 - 9s - loss: 2.4978e-04 - val_loss: 1.9821e-04 - 9s/epoch - 3ms/step
Epoch 271/300
2985/2985 - 9s - loss: 2.4760e-04 - val_loss: 2.0186e-04 - 9s/epoch - 3ms/step
Epoch 272/300
2985/2985 - 9s - loss: 2.4510e-04 - val_loss: 2.0547e-04 - 9s/epoch - 3ms/step
Epoch 273/300
2985/2985 - 9s - loss: 2.4613e-04 - val_loss: 3.7836e-04 - 9s/epoch - 3ms/step
Epoch 274/300
2985/2985 - 9s - loss: 2.7659e-04 - val_loss: 2.0536e-04 - 9s/epoch - 3ms/step
Epoch 275/300
2985/2985 - 9s - loss: 2.5824e-04 - val_loss: 1.9148e-04 - 9s/epoch - 3ms/step
Epoch 276/300
2985/2985 - 9s - loss: 2.4606e-04 - val_loss: 1.8586e-04 - 9s/epoch - 3ms/step
Epoch 277/300
2985/2985 - 9s - loss: 2.4394e-04 - val_loss: 2.0505e-04 - 9s/epoch - 3ms/step
Epoch 278/300
2985/2985 - 9s - loss: 2.4281e-04 - val_loss: 1.8881e-04 - 9s/epoch - 3ms/step
Epoch 279/300
2985/2985 - 9s - loss: 2.4112e-04 - val_loss: 1.9169e-04 - 9s/epoch - 3ms/step
Epoch 280/300
2985/2985 - 9s - loss: 2.4184e-04 - val_loss: 2.0965e-04 - 9s/epoch - 3ms/step
Epoch 281/300
2985/2985 - 9s - loss: 2.4429e-04 - val_loss: 2.1347e-04 - 9s/epoch - 3ms/step
Epoch 282/300
2985/2985 - 9s - loss: 2.7445e-04 - val_loss: 2.1693e-04 - 9s/epoch - 3ms/step
Epoch 283/300
2985/2985 - 9s - loss: 2.4940e-04 - val_loss: 1.9983e-04 - 9s/epoch - 3ms/step
Epoch 284/300
2985/2985 - 9s - loss: 2.4618e-04 - val_loss: 1.9476e-04 - 9s/epoch - 3ms/step
Epoch 285/300
2985/2985 - 9s - loss: 2.4630e-04 - val_loss: 2.0004e-04 - 9s/epoch - 3ms/step
Epoch 286/300
2985/2985 - 9s - loss: 2.4187e-04 - val_loss: 2.0234e-04 - 9s/epoch - 3ms/step
Epoch 287/300
2985/2985 - 9s - loss: 2.4294e-04 - val_loss: 2.0424e-04 - 9s/epoch - 3ms/step
Epoch 288/300
2985/2985 - 9s - loss: 2.4285e-04 - val_loss: 3.6565e-04 - 9s/epoch - 3ms/step
Epoch 289/300
2985/2985 - 9s - loss: 2.5249e-04 - val_loss: 3.8930e-04 - 9s/epoch - 3ms/step
Epoch 290/300
2985/2985 - 9s - loss: 3.1275e-04 - val_loss: 3.8785e-04 - 9s/epoch - 3ms/step
Epoch 291/300
2985/2985 - 9s - loss: 2.7207e-04 - val_loss: 2.1563e-04 - 9s/epoch - 3ms/step
Epoch 292/300
2985/2985 - 9s - loss: 2.5170e-04 - val_loss: 2.0193e-04 - 9s/epoch - 3ms/step
Epoch 293/300
2985/2985 - 9s - loss: 2.4794e-04 - val_loss: 2.1111e-04 - 9s/epoch - 3ms/step
Epoch 294/300
2985/2985 - 9s - loss: 2.5134e-04 - val_loss: 2.1033e-04 - 9s/epoch - 3ms/step
Epoch 295/300
2985/2985 - 9s - loss: 2.4446e-04 - val_loss: 1.8662e-04 - 9s/epoch - 3ms/step
Epoch 296/300
2985/2985 - 9s - loss: 2.4839e-04 - val_loss: 2.0540e-04 - 9s/epoch - 3ms/step
Epoch 297/300
2985/2985 - 9s - loss: 2.4163e-04 - val_loss: 2.2087e-04 - 9s/epoch - 3ms/step
Epoch 298/300
2985/2985 - 9s - loss: 2.4635e-04 - val_loss: 1.9687e-04 - 9s/epoch - 3ms/step
Epoch 299/300
2985/2985 - 9s - loss: 2.4373e-04 - val_loss: 2.9684e-04 - 9s/epoch - 3ms/step
Epoch 300/300
2985/2985 - 9s - loss: 2.6798e-04 - val_loss: 2.0668e-04 - 9s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0002066752058453858
  1/332 [..............................] - ETA: 34s 49/332 [===>..........................] - ETA: 0s  97/332 [=======>......................] - ETA: 0s145/332 [============>.................] - ETA: 0s193/332 [================>.............] - ETA: 0s242/332 [====================>.........] - ETA: 0s291/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.00236897668185164
cosine 0.0018808428651649368
MAE: 0.008244418
RMSE: 0.014376196
r2: 0.9865925853199173
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_27"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       multiple                  0         
                                                                 
 dense_27 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_27 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_27 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_28 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_28 (ReLU)             (None, 632)               0         
                                                                 
 dense_28 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_29 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_29 (ReLU)             (None, 2528)              0         
                                                                 
 dense_29 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_28"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_29 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_28 (InputLayer)       multiple                  0         
                                                                 
 dense_27 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_27 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_27 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_29"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_30 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_28 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_28 (ReLU)             (None, 632)               0         
                                                                 
 dense_28 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_29 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_29 (ReLU)             (None, 2528)              0         
                                                                 
 dense_29 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 32, 300, 0.002, 0.5, 632, 0.0002679811732377857, 0.0002066752058453858, 0.00236897668185164, 0.0018808428651649368, 0.008244417607784271, 0.014376196078956127, 0.9865925853199173, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_30"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_31 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_30 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_30 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_30 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_31 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_31 (ReLU)             (None, 632)               0         
                                                                 
 dense_31 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_32 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_32 (ReLU)             (None, 2528)              0         
                                                                 
 dense_32 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
1493/1493 - 5s - loss: 0.0094 - val_loss: 0.0039 - 5s/epoch - 4ms/step
Epoch 2/100
1493/1493 - 4s - loss: 0.0029 - val_loss: 0.0028 - 4s/epoch - 3ms/step
Epoch 3/100
1493/1493 - 4s - loss: 0.0021 - val_loss: 0.0020 - 4s/epoch - 3ms/step
Epoch 4/100
1493/1493 - 4s - loss: 0.0017 - val_loss: 0.0026 - 4s/epoch - 3ms/step
Epoch 5/100
1493/1493 - 4s - loss: 0.0016 - val_loss: 0.0014 - 4s/epoch - 3ms/step
Epoch 6/100
1493/1493 - 4s - loss: 0.0014 - val_loss: 0.0015 - 4s/epoch - 3ms/step
Epoch 7/100
1493/1493 - 4s - loss: 0.0013 - val_loss: 9.9225e-04 - 4s/epoch - 3ms/step
Epoch 8/100
1493/1493 - 4s - loss: 0.0012 - val_loss: 0.0012 - 4s/epoch - 3ms/step
Epoch 9/100
1493/1493 - 4s - loss: 0.0011 - val_loss: 0.0016 - 4s/epoch - 3ms/step
Epoch 10/100
1493/1493 - 4s - loss: 0.0011 - val_loss: 8.5011e-04 - 4s/epoch - 3ms/step
Epoch 11/100
1493/1493 - 4s - loss: 9.1795e-04 - val_loss: 7.5524e-04 - 4s/epoch - 3ms/step
Epoch 12/100
1493/1493 - 4s - loss: 7.7435e-04 - val_loss: 0.0011 - 4s/epoch - 3ms/step
Epoch 13/100
1493/1493 - 4s - loss: 7.5438e-04 - val_loss: 0.0028 - 4s/epoch - 3ms/step
Epoch 14/100
1493/1493 - 4s - loss: 8.7472e-04 - val_loss: 5.7524e-04 - 4s/epoch - 3ms/step
Epoch 15/100
1493/1493 - 4s - loss: 6.6182e-04 - val_loss: 8.1891e-04 - 4s/epoch - 3ms/step
Epoch 16/100
1493/1493 - 4s - loss: 6.1862e-04 - val_loss: 5.2870e-04 - 4s/epoch - 3ms/step
Epoch 17/100
1493/1493 - 4s - loss: 5.5249e-04 - val_loss: 6.8839e-04 - 4s/epoch - 3ms/step
Epoch 18/100
1493/1493 - 4s - loss: 5.3701e-04 - val_loss: 0.0014 - 4s/epoch - 3ms/step
Epoch 19/100
1493/1493 - 4s - loss: 5.6411e-04 - val_loss: 0.0011 - 4s/epoch - 3ms/step
Epoch 20/100
1493/1493 - 4s - loss: 4.9745e-04 - val_loss: 4.3559e-04 - 4s/epoch - 3ms/step
Epoch 21/100
1493/1493 - 4s - loss: 4.4744e-04 - val_loss: 5.7107e-04 - 4s/epoch - 3ms/step
Epoch 22/100
1493/1493 - 4s - loss: 4.2875e-04 - val_loss: 5.6213e-04 - 4s/epoch - 3ms/step
Epoch 23/100
1493/1493 - 4s - loss: 4.3326e-04 - val_loss: 7.5851e-04 - 4s/epoch - 3ms/step
Epoch 24/100
1493/1493 - 4s - loss: 4.1957e-04 - val_loss: 3.7735e-04 - 4s/epoch - 3ms/step
Epoch 25/100
1493/1493 - 4s - loss: 3.8404e-04 - val_loss: 3.6382e-04 - 4s/epoch - 3ms/step
Epoch 26/100
1493/1493 - 4s - loss: 3.6068e-04 - val_loss: 4.7298e-04 - 4s/epoch - 3ms/step
Epoch 27/100
1493/1493 - 4s - loss: 3.4461e-04 - val_loss: 3.5198e-04 - 4s/epoch - 3ms/step
Epoch 28/100
1493/1493 - 4s - loss: 3.2614e-04 - val_loss: 6.1041e-04 - 4s/epoch - 3ms/step
Epoch 29/100
1493/1493 - 4s - loss: 3.2205e-04 - val_loss: 4.6157e-04 - 4s/epoch - 3ms/step
Epoch 30/100
1493/1493 - 4s - loss: 3.2694e-04 - val_loss: 3.5766e-04 - 4s/epoch - 3ms/step
Epoch 31/100
1493/1493 - 4s - loss: 3.0130e-04 - val_loss: 5.4224e-04 - 4s/epoch - 3ms/step
Epoch 32/100
1493/1493 - 4s - loss: 3.7799e-04 - val_loss: 3.6716e-04 - 4s/epoch - 3ms/step
Epoch 33/100
1493/1493 - 4s - loss: 3.0826e-04 - val_loss: 2.7370e-04 - 4s/epoch - 3ms/step
Epoch 34/100
1493/1493 - 4s - loss: 2.8548e-04 - val_loss: 2.6517e-04 - 4s/epoch - 3ms/step
Epoch 35/100
1493/1493 - 4s - loss: 2.7093e-04 - val_loss: 6.2025e-04 - 4s/epoch - 3ms/step
Epoch 36/100
1493/1493 - 4s - loss: 2.9623e-04 - val_loss: 3.4593e-04 - 4s/epoch - 3ms/step
Epoch 37/100
1493/1493 - 4s - loss: 2.6511e-04 - val_loss: 3.3948e-04 - 4s/epoch - 3ms/step
Epoch 38/100
1493/1493 - 4s - loss: 2.7421e-04 - val_loss: 2.7091e-04 - 4s/epoch - 3ms/step
Epoch 39/100
1493/1493 - 4s - loss: 2.5138e-04 - val_loss: 2.3186e-04 - 4s/epoch - 3ms/step
Epoch 40/100
1493/1493 - 4s - loss: 2.4131e-04 - val_loss: 3.2904e-04 - 4s/epoch - 3ms/step
Epoch 41/100
1493/1493 - 4s - loss: 2.4168e-04 - val_loss: 2.2713e-04 - 4s/epoch - 3ms/step
Epoch 42/100
1493/1493 - 4s - loss: 2.3647e-04 - val_loss: 2.2856e-04 - 4s/epoch - 3ms/step
Epoch 43/100
1493/1493 - 4s - loss: 2.3915e-04 - val_loss: 3.9514e-04 - 4s/epoch - 3ms/step
Epoch 44/100
1493/1493 - 4s - loss: 2.4034e-04 - val_loss: 5.1675e-04 - 4s/epoch - 3ms/step
Epoch 45/100
1493/1493 - 4s - loss: 2.5128e-04 - val_loss: 1.9398e-04 - 4s/epoch - 3ms/step
Epoch 46/100
1493/1493 - 4s - loss: 2.2092e-04 - val_loss: 2.1147e-04 - 4s/epoch - 3ms/step
Epoch 47/100
1493/1493 - 4s - loss: 2.1803e-04 - val_loss: 2.3484e-04 - 4s/epoch - 3ms/step
Epoch 48/100
1493/1493 - 4s - loss: 2.1871e-04 - val_loss: 2.9077e-04 - 4s/epoch - 3ms/step
Epoch 49/100
1493/1493 - 4s - loss: 2.1389e-04 - val_loss: 6.1833e-04 - 4s/epoch - 3ms/step
Epoch 50/100
1493/1493 - 4s - loss: 2.6576e-04 - val_loss: 3.2720e-04 - 4s/epoch - 3ms/step
Epoch 51/100
1493/1493 - 4s - loss: 2.1760e-04 - val_loss: 2.2158e-04 - 4s/epoch - 3ms/step
Epoch 52/100
1493/1493 - 4s - loss: 2.0293e-04 - val_loss: 2.0761e-04 - 4s/epoch - 3ms/step
Epoch 53/100
1493/1493 - 4s - loss: 2.0520e-04 - val_loss: 1.9195e-04 - 4s/epoch - 3ms/step
Epoch 54/100
1493/1493 - 4s - loss: 1.9795e-04 - val_loss: 2.3564e-04 - 4s/epoch - 3ms/step
Epoch 55/100
1493/1493 - 4s - loss: 1.9266e-04 - val_loss: 2.0466e-04 - 4s/epoch - 3ms/step
Epoch 56/100
1493/1493 - 4s - loss: 1.9187e-04 - val_loss: 2.5622e-04 - 4s/epoch - 3ms/step
Epoch 57/100
1493/1493 - 4s - loss: 1.8704e-04 - val_loss: 2.2067e-04 - 4s/epoch - 3ms/step
Epoch 58/100
1493/1493 - 4s - loss: 1.8667e-04 - val_loss: 1.7179e-04 - 4s/epoch - 3ms/step
Epoch 59/100
1493/1493 - 5s - loss: 1.8298e-04 - val_loss: 1.8230e-04 - 5s/epoch - 3ms/step
Epoch 60/100
1493/1493 - 4s - loss: 1.8613e-04 - val_loss: 6.6555e-04 - 4s/epoch - 3ms/step
Epoch 61/100
1493/1493 - 4s - loss: 3.3364e-04 - val_loss: 1.6722e-04 - 4s/epoch - 3ms/step
Epoch 62/100
1493/1493 - 4s - loss: 1.9306e-04 - val_loss: 2.1512e-04 - 4s/epoch - 3ms/step
Epoch 63/100
1493/1493 - 4s - loss: 1.8376e-04 - val_loss: 2.3484e-04 - 4s/epoch - 3ms/step
Epoch 64/100
1493/1493 - 4s - loss: 1.8287e-04 - val_loss: 0.0014 - 4s/epoch - 3ms/step
Epoch 65/100
1493/1493 - 4s - loss: 2.9611e-04 - val_loss: 2.5895e-04 - 4s/epoch - 3ms/step
Epoch 66/100
1493/1493 - 4s - loss: 1.9273e-04 - val_loss: 2.3063e-04 - 4s/epoch - 3ms/step
Epoch 67/100
1493/1493 - 4s - loss: 1.8019e-04 - val_loss: 1.7614e-04 - 4s/epoch - 3ms/step
Epoch 68/100
1493/1493 - 4s - loss: 1.7531e-04 - val_loss: 5.1967e-04 - 4s/epoch - 3ms/step
Epoch 69/100
1493/1493 - 4s - loss: 2.1071e-04 - val_loss: 2.1554e-04 - 4s/epoch - 3ms/step
Epoch 70/100
1493/1493 - 4s - loss: 1.7511e-04 - val_loss: 4.5015e-04 - 4s/epoch - 3ms/step
Epoch 71/100
1493/1493 - 4s - loss: 2.3197e-04 - val_loss: 1.6221e-04 - 4s/epoch - 3ms/step
Epoch 72/100
1493/1493 - 4s - loss: 1.7547e-04 - val_loss: 1.8749e-04 - 4s/epoch - 3ms/step
Epoch 73/100
1493/1493 - 4s - loss: 1.7286e-04 - val_loss: 1.8445e-04 - 4s/epoch - 3ms/step
Epoch 74/100
1493/1493 - 5s - loss: 1.7005e-04 - val_loss: 1.7582e-04 - 5s/epoch - 3ms/step
Epoch 75/100
1493/1493 - 5s - loss: 1.6272e-04 - val_loss: 1.5428e-04 - 5s/epoch - 3ms/step
Epoch 76/100
1493/1493 - 4s - loss: 1.6080e-04 - val_loss: 1.6047e-04 - 4s/epoch - 3ms/step
Epoch 77/100
1493/1493 - 4s - loss: 1.6263e-04 - val_loss: 3.3799e-04 - 4s/epoch - 3ms/step
Epoch 78/100
1493/1493 - 4s - loss: 1.6511e-04 - val_loss: 4.6921e-04 - 4s/epoch - 3ms/step
Epoch 79/100
1493/1493 - 4s - loss: 1.8894e-04 - val_loss: 4.2777e-04 - 4s/epoch - 3ms/step
Epoch 80/100
1493/1493 - 4s - loss: 2.1067e-04 - val_loss: 1.7847e-04 - 4s/epoch - 3ms/step
Epoch 81/100
1493/1493 - 4s - loss: 1.6344e-04 - val_loss: 2.6506e-04 - 4s/epoch - 3ms/step
Epoch 82/100
1493/1493 - 4s - loss: 1.5547e-04 - val_loss: 1.5406e-04 - 4s/epoch - 3ms/step
Epoch 83/100
1493/1493 - 4s - loss: 1.5198e-04 - val_loss: 1.6139e-04 - 4s/epoch - 3ms/step
Epoch 84/100
1493/1493 - 4s - loss: 1.5061e-04 - val_loss: 1.8278e-04 - 4s/epoch - 3ms/step
Epoch 85/100
1493/1493 - 4s - loss: 1.5318e-04 - val_loss: 1.7542e-04 - 4s/epoch - 3ms/step
Epoch 86/100
1493/1493 - 4s - loss: 1.4950e-04 - val_loss: 1.4905e-04 - 4s/epoch - 3ms/step
Epoch 87/100
1493/1493 - 4s - loss: 1.4461e-04 - val_loss: 1.4381e-04 - 4s/epoch - 3ms/step
Epoch 88/100
1493/1493 - 4s - loss: 1.4407e-04 - val_loss: 2.3086e-04 - 4s/epoch - 3ms/step
Epoch 89/100
1493/1493 - 4s - loss: 1.6437e-04 - val_loss: 1.5017e-04 - 4s/epoch - 3ms/step
Epoch 90/100
1493/1493 - 4s - loss: 1.4299e-04 - val_loss: 1.5440e-04 - 4s/epoch - 3ms/step
Epoch 91/100
1493/1493 - 4s - loss: 1.4271e-04 - val_loss: 1.7575e-04 - 4s/epoch - 3ms/step
Epoch 92/100
1493/1493 - 5s - loss: 1.4140e-04 - val_loss: 3.7618e-04 - 5s/epoch - 3ms/step
Epoch 93/100
1493/1493 - 4s - loss: 1.8053e-04 - val_loss: 1.7931e-04 - 4s/epoch - 3ms/step
Epoch 94/100
1493/1493 - 4s - loss: 1.4875e-04 - val_loss: 1.2934e-04 - 4s/epoch - 3ms/step
Epoch 95/100
1493/1493 - 4s - loss: 1.3992e-04 - val_loss: 1.6883e-04 - 4s/epoch - 3ms/step
Epoch 96/100
1493/1493 - 4s - loss: 1.4105e-04 - val_loss: 1.6930e-04 - 4s/epoch - 3ms/step
Epoch 97/100
1493/1493 - 4s - loss: 1.3758e-04 - val_loss: 1.5294e-04 - 4s/epoch - 3ms/step
Epoch 98/100
1493/1493 - 4s - loss: 1.4078e-04 - val_loss: 6.6266e-04 - 4s/epoch - 3ms/step
Epoch 99/100
1493/1493 - 4s - loss: 2.2818e-04 - val_loss: 1.2661e-04 - 4s/epoch - 3ms/step
Epoch 100/100
1493/1493 - 4s - loss: 1.5877e-04 - val_loss: 1.2566e-04 - 4s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0001256649848073721
  1/332 [..............................] - ETA: 33s 48/332 [===>..........................] - ETA: 0s  96/332 [=======>......................] - ETA: 0s144/332 [============>.................] - ETA: 0s192/332 [================>.............] - ETA: 0s240/332 [====================>.........] - ETA: 0s288/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0014129523124687634
cosine 0.0011271716478497317
MAE: 0.00641287
RMSE: 0.011210035
r2: 0.9918482897526631
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_30"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_31 (InputLayer)       multiple                  0         
                                                                 
 dense_30 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_30 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_30 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_31 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_31 (ReLU)             (None, 632)               0         
                                                                 
 dense_31 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_32 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_32 (ReLU)             (None, 2528)              0         
                                                                 
 dense_32 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_31"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_32 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_31 (InputLayer)       multiple                  0         
                                                                 
 dense_30 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_30 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_30 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_31 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_31 (ReLU)             (None, 632)               0         
                                                                 
 dense_31 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_32 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_32 (ReLU)             (None, 2528)              0         
                                                                 
 dense_32 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 64, 100, 0.0005, 0.5, 632, 0.00015876736142672598, 0.0001256649848073721, 0.0014129523124687634, 0.0011271716478497317, 0.006412869784981012, 0.011210034601390362, 0.9918482897526631, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_33"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_34 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_33 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_33 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_33 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_34 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_34 (ReLU)             (None, 632)               0         
                                                                 
 dense_34 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_35 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_35 (ReLU)             (None, 2528)              0         
                                                                 
 dense_35 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
1493/1493 - 5s - loss: 0.0089 - val_loss: 0.0035 - 5s/epoch - 4ms/step
Epoch 2/100
1493/1493 - 4s - loss: 0.0026 - val_loss: 0.0030 - 4s/epoch - 3ms/step
Epoch 3/100
1493/1493 - 4s - loss: 0.0021 - val_loss: 0.0017 - 4s/epoch - 3ms/step
Epoch 4/100
1493/1493 - 4s - loss: 0.0018 - val_loss: 0.0031 - 4s/epoch - 3ms/step
Epoch 5/100
1493/1493 - 4s - loss: 0.0017 - val_loss: 0.0012 - 4s/epoch - 3ms/step
Epoch 6/100
1493/1493 - 4s - loss: 0.0012 - val_loss: 0.0010 - 4s/epoch - 3ms/step
Epoch 7/100
1493/1493 - 4s - loss: 0.0011 - val_loss: 9.5557e-04 - 4s/epoch - 3ms/step
Epoch 8/100
1493/1493 - 4s - loss: 8.8964e-04 - val_loss: 0.0010 - 4s/epoch - 3ms/step
Epoch 9/100
1493/1493 - 5s - loss: 7.7198e-04 - val_loss: 7.6972e-04 - 5s/epoch - 3ms/step
Epoch 10/100
1493/1493 - 4s - loss: 6.7368e-04 - val_loss: 6.6053e-04 - 4s/epoch - 3ms/step
Epoch 11/100
1493/1493 - 4s - loss: 6.0599e-04 - val_loss: 5.7402e-04 - 4s/epoch - 3ms/step
Epoch 12/100
1493/1493 - 4s - loss: 5.3584e-04 - val_loss: 0.0021 - 4s/epoch - 3ms/step
Epoch 13/100
1493/1493 - 4s - loss: 8.0048e-04 - val_loss: 0.0026 - 4s/epoch - 3ms/step
Epoch 14/100
1493/1493 - 4s - loss: 7.3922e-04 - val_loss: 5.6956e-04 - 4s/epoch - 3ms/step
Epoch 15/100
1493/1493 - 4s - loss: 5.2285e-04 - val_loss: 0.0028 - 4s/epoch - 3ms/step
Epoch 16/100
1493/1493 - 4s - loss: 8.2868e-04 - val_loss: 4.4408e-04 - 4s/epoch - 3ms/step
Epoch 17/100
1493/1493 - 4s - loss: 4.8705e-04 - val_loss: 4.7016e-04 - 4s/epoch - 3ms/step
Epoch 18/100
1493/1493 - 4s - loss: 4.4474e-04 - val_loss: 0.0018 - 4s/epoch - 3ms/step
Epoch 19/100
1493/1493 - 4s - loss: 5.9928e-04 - val_loss: 5.6565e-04 - 4s/epoch - 3ms/step
Epoch 20/100
1493/1493 - 4s - loss: 4.2753e-04 - val_loss: 3.9173e-04 - 4s/epoch - 3ms/step
Epoch 21/100
1493/1493 - 4s - loss: 3.9051e-04 - val_loss: 5.1261e-04 - 4s/epoch - 3ms/step
Epoch 22/100
1493/1493 - 4s - loss: 3.8237e-04 - val_loss: 9.2302e-04 - 4s/epoch - 3ms/step
Epoch 23/100
1493/1493 - 4s - loss: 4.3183e-04 - val_loss: 3.8708e-04 - 4s/epoch - 3ms/step
Epoch 24/100
1493/1493 - 4s - loss: 3.5816e-04 - val_loss: 9.2582e-04 - 4s/epoch - 3ms/step
Epoch 25/100
1493/1493 - 5s - loss: 3.7406e-04 - val_loss: 3.3090e-04 - 5s/epoch - 3ms/step
Epoch 26/100
1493/1493 - 4s - loss: 3.2370e-04 - val_loss: 6.1649e-04 - 4s/epoch - 3ms/step
Epoch 27/100
1493/1493 - 4s - loss: 3.1096e-04 - val_loss: 3.0245e-04 - 4s/epoch - 3ms/step
Epoch 28/100
1493/1493 - 4s - loss: 2.9583e-04 - val_loss: 3.4047e-04 - 4s/epoch - 3ms/step
Epoch 29/100
1493/1493 - 4s - loss: 2.8850e-04 - val_loss: 4.4572e-04 - 4s/epoch - 3ms/step
Epoch 30/100
1493/1493 - 4s - loss: 3.3216e-04 - val_loss: 2.8209e-04 - 4s/epoch - 3ms/step
Epoch 31/100
1493/1493 - 4s - loss: 2.9188e-04 - val_loss: 2.7806e-04 - 4s/epoch - 3ms/step
Epoch 32/100
1493/1493 - 4s - loss: 2.7243e-04 - val_loss: 2.5628e-04 - 4s/epoch - 3ms/step
Epoch 33/100
1493/1493 - 4s - loss: 2.6377e-04 - val_loss: 2.5040e-04 - 4s/epoch - 3ms/step
Epoch 34/100
1493/1493 - 4s - loss: 2.5506e-04 - val_loss: 2.3072e-04 - 4s/epoch - 3ms/step
Epoch 35/100
1493/1493 - 4s - loss: 2.4809e-04 - val_loss: 4.1800e-04 - 4s/epoch - 3ms/step
Epoch 36/100
1493/1493 - 4s - loss: 2.9224e-04 - val_loss: 2.4025e-04 - 4s/epoch - 3ms/step
Epoch 37/100
1493/1493 - 4s - loss: 2.4826e-04 - val_loss: 3.1097e-04 - 4s/epoch - 3ms/step
Epoch 38/100
1493/1493 - 4s - loss: 2.7127e-04 - val_loss: 2.2023e-04 - 4s/epoch - 3ms/step
Epoch 39/100
1493/1493 - 4s - loss: 2.3742e-04 - val_loss: 2.2043e-04 - 4s/epoch - 3ms/step
Epoch 40/100
1493/1493 - 5s - loss: 2.3036e-04 - val_loss: 2.4991e-04 - 5s/epoch - 3ms/step
Epoch 41/100
1493/1493 - 4s - loss: 2.3817e-04 - val_loss: 2.0212e-04 - 4s/epoch - 3ms/step
Epoch 42/100
1493/1493 - 4s - loss: 2.2348e-04 - val_loss: 2.1324e-04 - 4s/epoch - 3ms/step
Epoch 43/100
1493/1493 - 4s - loss: 2.2844e-04 - val_loss: 9.9008e-04 - 4s/epoch - 3ms/step
Epoch 44/100
1493/1493 - 4s - loss: 4.1952e-04 - val_loss: 4.1196e-04 - 4s/epoch - 3ms/step
Epoch 45/100
1493/1493 - 4s - loss: 2.7032e-04 - val_loss: 1.9411e-04 - 4s/epoch - 3ms/step
Epoch 46/100
1493/1493 - 4s - loss: 2.2988e-04 - val_loss: 2.0769e-04 - 4s/epoch - 3ms/step
Epoch 47/100
1493/1493 - 4s - loss: 2.2129e-04 - val_loss: 2.2819e-04 - 4s/epoch - 3ms/step
Epoch 48/100
1493/1493 - 4s - loss: 2.1986e-04 - val_loss: 2.2342e-04 - 4s/epoch - 3ms/step
Epoch 49/100
1493/1493 - 4s - loss: 2.1508e-04 - val_loss: 3.9895e-04 - 4s/epoch - 3ms/step
Epoch 50/100
1493/1493 - 4s - loss: 2.5927e-04 - val_loss: 0.0011 - 4s/epoch - 3ms/step
Epoch 51/100
1493/1493 - 4s - loss: 3.1732e-04 - val_loss: 1.8927e-04 - 4s/epoch - 3ms/step
Epoch 52/100
1493/1493 - 4s - loss: 2.1906e-04 - val_loss: 1.9169e-04 - 4s/epoch - 3ms/step
Epoch 53/100
1493/1493 - 4s - loss: 2.1732e-04 - val_loss: 1.8291e-04 - 4s/epoch - 3ms/step
Epoch 54/100
1493/1493 - 4s - loss: 2.0647e-04 - val_loss: 1.9740e-04 - 4s/epoch - 3ms/step
Epoch 55/100
1493/1493 - 4s - loss: 2.0223e-04 - val_loss: 1.8999e-04 - 4s/epoch - 3ms/step
Epoch 56/100
1493/1493 - 4s - loss: 1.9998e-04 - val_loss: 2.1356e-04 - 4s/epoch - 3ms/step
Epoch 57/100
1493/1493 - 4s - loss: 1.9704e-04 - val_loss: 1.8440e-04 - 4s/epoch - 3ms/step
Epoch 58/100
1493/1493 - 5s - loss: 1.9416e-04 - val_loss: 1.8086e-04 - 5s/epoch - 3ms/step
Epoch 59/100
1493/1493 - 4s - loss: 1.9177e-04 - val_loss: 1.8869e-04 - 4s/epoch - 3ms/step
Epoch 60/100
1493/1493 - 4s - loss: 1.9046e-04 - val_loss: 3.1417e-04 - 4s/epoch - 3ms/step
Epoch 61/100
1493/1493 - 4s - loss: 2.3631e-04 - val_loss: 1.7081e-04 - 4s/epoch - 3ms/step
Epoch 62/100
1493/1493 - 4s - loss: 2.0195e-04 - val_loss: 1.9828e-04 - 4s/epoch - 3ms/step
Epoch 63/100
1493/1493 - 4s - loss: 1.8732e-04 - val_loss: 2.2829e-04 - 4s/epoch - 3ms/step
Epoch 64/100
1493/1493 - 4s - loss: 1.8584e-04 - val_loss: 2.0561e-04 - 4s/epoch - 3ms/step
Epoch 65/100
1493/1493 - 4s - loss: 1.8363e-04 - val_loss: 1.9749e-04 - 4s/epoch - 3ms/step
Epoch 66/100
1493/1493 - 4s - loss: 1.8565e-04 - val_loss: 3.2118e-04 - 4s/epoch - 3ms/step
Epoch 67/100
1493/1493 - 4s - loss: 1.9481e-04 - val_loss: 1.6585e-04 - 4s/epoch - 3ms/step
Epoch 68/100
1493/1493 - 4s - loss: 1.7836e-04 - val_loss: 2.4092e-04 - 4s/epoch - 3ms/step
Epoch 69/100
1493/1493 - 4s - loss: 2.1983e-04 - val_loss: 1.6862e-04 - 4s/epoch - 3ms/step
Epoch 70/100
1493/1493 - 4s - loss: 1.8054e-04 - val_loss: 3.2083e-04 - 4s/epoch - 3ms/step
Epoch 71/100
1493/1493 - 4s - loss: 2.0422e-04 - val_loss: 1.6855e-04 - 4s/epoch - 3ms/step
Epoch 72/100
1493/1493 - 4s - loss: 1.7860e-04 - val_loss: 1.7191e-04 - 4s/epoch - 3ms/step
Epoch 73/100
1493/1493 - 4s - loss: 1.7958e-04 - val_loss: 3.1588e-04 - 4s/epoch - 3ms/step
Epoch 74/100
1493/1493 - 4s - loss: 2.1361e-04 - val_loss: 1.6929e-04 - 4s/epoch - 3ms/step
Epoch 75/100
1493/1493 - 4s - loss: 1.7582e-04 - val_loss: 1.6311e-04 - 4s/epoch - 3ms/step
Epoch 76/100
1493/1493 - 4s - loss: 1.7453e-04 - val_loss: 1.5752e-04 - 4s/epoch - 3ms/step
Epoch 77/100
1493/1493 - 4s - loss: 1.6968e-04 - val_loss: 3.1084e-04 - 4s/epoch - 3ms/step
Epoch 78/100
1493/1493 - 4s - loss: 1.9764e-04 - val_loss: 1.6832e-04 - 4s/epoch - 3ms/step
Epoch 79/100
1493/1493 - 5s - loss: 1.7206e-04 - val_loss: 1.8012e-04 - 5s/epoch - 3ms/step
Epoch 80/100
1493/1493 - 4s - loss: 1.6878e-04 - val_loss: 1.9216e-04 - 4s/epoch - 3ms/step
Epoch 81/100
1493/1493 - 4s - loss: 1.6826e-04 - val_loss: 2.5711e-04 - 4s/epoch - 3ms/step
Epoch 82/100
1493/1493 - 4s - loss: 1.6448e-04 - val_loss: 1.6849e-04 - 4s/epoch - 3ms/step
Epoch 83/100
1493/1493 - 4s - loss: 1.6310e-04 - val_loss: 1.6380e-04 - 4s/epoch - 3ms/step
Epoch 84/100
1493/1493 - 4s - loss: 1.6282e-04 - val_loss: 1.8572e-04 - 4s/epoch - 3ms/step
Epoch 85/100
1493/1493 - 4s - loss: 1.6781e-04 - val_loss: 4.1945e-04 - 4s/epoch - 3ms/step
Epoch 86/100
1493/1493 - 4s - loss: 2.2118e-04 - val_loss: 1.5154e-04 - 4s/epoch - 3ms/step
Epoch 87/100
1493/1493 - 4s - loss: 1.6590e-04 - val_loss: 1.5861e-04 - 4s/epoch - 3ms/step
Epoch 88/100
1493/1493 - 4s - loss: 1.6206e-04 - val_loss: 2.4330e-04 - 4s/epoch - 3ms/step
Epoch 89/100
1493/1493 - 4s - loss: 1.6979e-04 - val_loss: 1.6002e-04 - 4s/epoch - 3ms/step
Epoch 90/100
1493/1493 - 4s - loss: 1.5947e-04 - val_loss: 1.8917e-04 - 4s/epoch - 3ms/step
Epoch 91/100
1493/1493 - 4s - loss: 1.5849e-04 - val_loss: 1.6414e-04 - 4s/epoch - 3ms/step
Epoch 92/100
1493/1493 - 4s - loss: 1.5885e-04 - val_loss: 3.3637e-04 - 4s/epoch - 3ms/step
Epoch 93/100
1493/1493 - 4s - loss: 2.0333e-04 - val_loss: 1.6018e-04 - 4s/epoch - 3ms/step
Epoch 94/100
1493/1493 - 4s - loss: 1.6213e-04 - val_loss: 1.3748e-04 - 4s/epoch - 3ms/step
Epoch 95/100
1493/1493 - 4s - loss: 1.5696e-04 - val_loss: 1.6501e-04 - 4s/epoch - 3ms/step
Epoch 96/100
1493/1493 - 4s - loss: 1.6082e-04 - val_loss: 3.4684e-04 - 4s/epoch - 3ms/step
Epoch 97/100
1493/1493 - 4s - loss: 2.0018e-04 - val_loss: 2.1116e-04 - 4s/epoch - 3ms/step
Epoch 98/100
1493/1493 - 4s - loss: 1.6266e-04 - val_loss: 4.6848e-04 - 4s/epoch - 3ms/step
Epoch 99/100
1493/1493 - 4s - loss: 1.9922e-04 - val_loss: 1.3698e-04 - 4s/epoch - 3ms/step
Epoch 100/100
1493/1493 - 4s - loss: 1.6138e-04 - val_loss: 1.4067e-04 - 4s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00014066616131458431
  1/332 [..............................] - ETA: 31s 49/332 [===>..........................] - ETA: 0s  97/332 [=======>......................] - ETA: 0s145/332 [============>.................] - ETA: 0s193/332 [================>.............] - ETA: 0s241/332 [====================>.........] - ETA: 0s289/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0016088487121953723
cosine 0.001273248112970058
MAE: 0.0066616167
RMSE: 0.011860274
r2: 0.9908749393871613
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_33"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_34 (InputLayer)       multiple                  0         
                                                                 
 dense_33 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_33 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_33 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_34 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_34 (ReLU)             (None, 632)               0         
                                                                 
 dense_34 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_35 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_35 (ReLU)             (None, 2528)              0         
                                                                 
 dense_35 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_34"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_35 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_34 (InputLayer)       multiple                  0         
                                                                 
 dense_33 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_33 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_33 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_35"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_36 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_34 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_34 (ReLU)             (None, 632)               0         
                                                                 
 dense_34 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_35 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_35 (ReLU)             (None, 2528)              0         
                                                                 
 dense_35 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 64, 100, 0.001, 0.5, 632, 0.00016137877537403256, 0.00014066616131458431, 0.0016088487121953723, 0.001273248112970058, 0.0066616167314350605, 0.011860273778438568, 0.9908749393871613, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_36"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_37 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_36 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_36 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_36 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_37 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_37 (ReLU)             (None, 632)               0         
                                                                 
 dense_37 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_38 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_38 (ReLU)             (None, 2528)              0         
                                                                 
 dense_38 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/100
1493/1493 - 5s - loss: 0.0116 - val_loss: 0.0027 - 5s/epoch - 4ms/step
Epoch 2/100
1493/1493 - 4s - loss: 0.0029 - val_loss: 0.0022 - 4s/epoch - 3ms/step
Epoch 3/100
1493/1493 - 4s - loss: 0.0021 - val_loss: 0.0018 - 4s/epoch - 3ms/step
Epoch 4/100
1493/1493 - 5s - loss: 0.0017 - val_loss: 0.0016 - 5s/epoch - 3ms/step
Epoch 5/100
1493/1493 - 4s - loss: 0.0013 - val_loss: 0.0023 - 4s/epoch - 3ms/step
Epoch 6/100
1493/1493 - 4s - loss: 0.0012 - val_loss: 9.4083e-04 - 4s/epoch - 3ms/step
Epoch 7/100
1493/1493 - 4s - loss: 8.9493e-04 - val_loss: 7.7056e-04 - 4s/epoch - 3ms/step
Epoch 8/100
1493/1493 - 4s - loss: 7.6996e-04 - val_loss: 7.5440e-04 - 4s/epoch - 3ms/step
Epoch 9/100
1493/1493 - 4s - loss: 7.0072e-04 - val_loss: 0.0012 - 4s/epoch - 3ms/step
Epoch 10/100
1493/1493 - 4s - loss: 7.2233e-04 - val_loss: 5.6093e-04 - 4s/epoch - 3ms/step
Epoch 11/100
1493/1493 - 4s - loss: 5.6052e-04 - val_loss: 5.2727e-04 - 4s/epoch - 3ms/step
Epoch 12/100
1493/1493 - 4s - loss: 5.0662e-04 - val_loss: 9.9546e-04 - 4s/epoch - 3ms/step
Epoch 13/100
1493/1493 - 4s - loss: 5.9325e-04 - val_loss: 0.0025 - 4s/epoch - 3ms/step
Epoch 14/100
1493/1493 - 4s - loss: 8.7594e-04 - val_loss: 5.3151e-04 - 4s/epoch - 3ms/step
Epoch 15/100
1493/1493 - 4s - loss: 5.3416e-04 - val_loss: 4.5334e-04 - 4s/epoch - 3ms/step
Epoch 16/100
1493/1493 - 4s - loss: 4.7424e-04 - val_loss: 4.3472e-04 - 4s/epoch - 3ms/step
Epoch 17/100
1493/1493 - 4s - loss: 4.5190e-04 - val_loss: 5.7715e-04 - 4s/epoch - 3ms/step
Epoch 18/100
1493/1493 - 4s - loss: 4.7964e-04 - val_loss: 0.0011 - 4s/epoch - 3ms/step
Epoch 19/100
1493/1493 - 4s - loss: 5.1607e-04 - val_loss: 4.0291e-04 - 4s/epoch - 3ms/step
Epoch 20/100
1493/1493 - 4s - loss: 4.1418e-04 - val_loss: 4.0667e-04 - 4s/epoch - 3ms/step
Epoch 21/100
1493/1493 - 4s - loss: 3.9814e-04 - val_loss: 3.6404e-04 - 4s/epoch - 3ms/step
Epoch 22/100
1493/1493 - 4s - loss: 3.8151e-04 - val_loss: 5.4996e-04 - 4s/epoch - 3ms/step
Epoch 23/100
1493/1493 - 4s - loss: 4.0874e-04 - val_loss: 5.9386e-04 - 4s/epoch - 3ms/step
Epoch 24/100
1493/1493 - 4s - loss: 4.3153e-04 - val_loss: 4.1595e-04 - 4s/epoch - 3ms/step
Epoch 25/100
1493/1493 - 4s - loss: 3.7221e-04 - val_loss: 3.2609e-04 - 4s/epoch - 3ms/step
Epoch 26/100
1493/1493 - 4s - loss: 3.4722e-04 - val_loss: 3.4004e-04 - 4s/epoch - 3ms/step
Epoch 27/100
1493/1493 - 4s - loss: 3.3534e-04 - val_loss: 2.9757e-04 - 4s/epoch - 3ms/step
Epoch 28/100
1493/1493 - 4s - loss: 3.2582e-04 - val_loss: 3.0842e-04 - 4s/epoch - 3ms/step
Epoch 29/100
1493/1493 - 4s - loss: 3.1953e-04 - val_loss: 5.2778e-04 - 4s/epoch - 3ms/step
Epoch 30/100
1493/1493 - 4s - loss: 3.7267e-04 - val_loss: 3.0687e-04 - 4s/epoch - 3ms/step
Epoch 31/100
1493/1493 - 4s - loss: 3.1815e-04 - val_loss: 3.1164e-04 - 4s/epoch - 3ms/step
Epoch 32/100
1493/1493 - 4s - loss: 3.0618e-04 - val_loss: 2.9601e-04 - 4s/epoch - 3ms/step
Epoch 33/100
1493/1493 - 4s - loss: 3.1151e-04 - val_loss: 2.6757e-04 - 4s/epoch - 3ms/step
Epoch 34/100
1493/1493 - 4s - loss: 2.9879e-04 - val_loss: 2.6260e-04 - 4s/epoch - 3ms/step
Epoch 35/100
1493/1493 - 4s - loss: 2.9979e-04 - val_loss: 3.4547e-04 - 4s/epoch - 3ms/step
Epoch 36/100
1493/1493 - 4s - loss: 3.0207e-04 - val_loss: 3.5216e-04 - 4s/epoch - 3ms/step
Epoch 37/100
1493/1493 - 5s - loss: 3.1962e-04 - val_loss: 2.8229e-04 - 5s/epoch - 3ms/step
Epoch 38/100
1493/1493 - 5s - loss: 2.9430e-04 - val_loss: 2.6179e-04 - 5s/epoch - 3ms/step
Epoch 39/100
1493/1493 - 4s - loss: 2.7928e-04 - val_loss: 2.5385e-04 - 4s/epoch - 3ms/step
Epoch 40/100
1493/1493 - 4s - loss: 2.7738e-04 - val_loss: 7.6223e-04 - 4s/epoch - 3ms/step
Epoch 41/100
1493/1493 - 4s - loss: 4.6035e-04 - val_loss: 2.5607e-04 - 4s/epoch - 3ms/step
Epoch 42/100
1493/1493 - 4s - loss: 3.0660e-04 - val_loss: 2.4910e-04 - 4s/epoch - 3ms/step
Epoch 43/100
1493/1493 - 4s - loss: 2.8604e-04 - val_loss: 2.6088e-04 - 4s/epoch - 3ms/step
Epoch 44/100
1493/1493 - 4s - loss: 2.8110e-04 - val_loss: 2.6594e-04 - 4s/epoch - 3ms/step
Epoch 45/100
1493/1493 - 4s - loss: 2.8143e-04 - val_loss: 2.3026e-04 - 4s/epoch - 3ms/step
Epoch 46/100
1493/1493 - 4s - loss: 2.6749e-04 - val_loss: 2.2955e-04 - 4s/epoch - 3ms/step
Epoch 47/100
1493/1493 - 4s - loss: 2.6273e-04 - val_loss: 2.7369e-04 - 4s/epoch - 3ms/step
Epoch 48/100
1493/1493 - 4s - loss: 2.6590e-04 - val_loss: 4.1775e-04 - 4s/epoch - 3ms/step
Epoch 49/100
1493/1493 - 4s - loss: 2.9705e-04 - val_loss: 5.2485e-04 - 4s/epoch - 3ms/step
Epoch 50/100
1493/1493 - 4s - loss: 3.5793e-04 - val_loss: 2.4167e-04 - 4s/epoch - 3ms/step
Epoch 51/100
1493/1493 - 4s - loss: 2.8298e-04 - val_loss: 2.2343e-04 - 4s/epoch - 3ms/step
Epoch 52/100
1493/1493 - 4s - loss: 2.6526e-04 - val_loss: 2.3088e-04 - 4s/epoch - 3ms/step
Epoch 53/100
1493/1493 - 4s - loss: 2.5923e-04 - val_loss: 2.3017e-04 - 4s/epoch - 3ms/step
Epoch 54/100
1493/1493 - 5s - loss: 2.5604e-04 - val_loss: 2.7802e-04 - 5s/epoch - 3ms/step
Epoch 55/100
1493/1493 - 5s - loss: 2.5198e-04 - val_loss: 2.2731e-04 - 5s/epoch - 3ms/step
Epoch 56/100
1493/1493 - 4s - loss: 2.4835e-04 - val_loss: 2.4696e-04 - 4s/epoch - 3ms/step
Epoch 57/100
1493/1493 - 4s - loss: 2.4415e-04 - val_loss: 2.3169e-04 - 4s/epoch - 3ms/step
Epoch 58/100
1493/1493 - 4s - loss: 2.5837e-04 - val_loss: 2.1559e-04 - 4s/epoch - 3ms/step
Epoch 59/100
1493/1493 - 4s - loss: 2.4377e-04 - val_loss: 2.3832e-04 - 4s/epoch - 3ms/step
Epoch 60/100
1493/1493 - 4s - loss: 2.3851e-04 - val_loss: 2.3668e-04 - 4s/epoch - 3ms/step
Epoch 61/100
1493/1493 - 4s - loss: 2.3677e-04 - val_loss: 2.2267e-04 - 4s/epoch - 3ms/step
Epoch 62/100
1493/1493 - 4s - loss: 2.3611e-04 - val_loss: 2.5315e-04 - 4s/epoch - 3ms/step
Epoch 63/100
1493/1493 - 4s - loss: 2.3419e-04 - val_loss: 2.8845e-04 - 4s/epoch - 3ms/step
Epoch 64/100
1493/1493 - 4s - loss: 2.5546e-04 - val_loss: 8.1411e-04 - 4s/epoch - 3ms/step
Epoch 65/100
1493/1493 - 4s - loss: 3.6808e-04 - val_loss: 0.0016 - 4s/epoch - 3ms/step
Epoch 66/100
1493/1493 - 4s - loss: 4.2457e-04 - val_loss: 2.6318e-04 - 4s/epoch - 3ms/step
Epoch 67/100
1493/1493 - 4s - loss: 2.8751e-04 - val_loss: 2.3743e-04 - 4s/epoch - 3ms/step
Epoch 68/100
1493/1493 - 4s - loss: 2.6983e-04 - val_loss: 2.8899e-04 - 4s/epoch - 3ms/step
Epoch 69/100
1493/1493 - 4s - loss: 2.7968e-04 - val_loss: 2.4626e-04 - 4s/epoch - 3ms/step
Epoch 70/100
1493/1493 - 4s - loss: 2.6085e-04 - val_loss: 4.3356e-04 - 4s/epoch - 3ms/step
Epoch 71/100
1493/1493 - 5s - loss: 2.9557e-04 - val_loss: 2.2554e-04 - 5s/epoch - 3ms/step
Epoch 72/100
1493/1493 - 5s - loss: 2.5615e-04 - val_loss: 4.4908e-04 - 5s/epoch - 3ms/step
Epoch 73/100
1493/1493 - 5s - loss: 3.0898e-04 - val_loss: 2.4781e-04 - 5s/epoch - 3ms/step
Epoch 74/100
1493/1493 - 5s - loss: 2.6183e-04 - val_loss: 2.4537e-04 - 5s/epoch - 3ms/step
Epoch 75/100
1493/1493 - 4s - loss: 2.5039e-04 - val_loss: 2.0867e-04 - 4s/epoch - 3ms/step
Epoch 76/100
1493/1493 - 4s - loss: 2.4734e-04 - val_loss: 3.0161e-04 - 4s/epoch - 3ms/step
Epoch 77/100
1493/1493 - 5s - loss: 2.5828e-04 - val_loss: 5.5116e-04 - 5s/epoch - 3ms/step
Epoch 78/100
1493/1493 - 4s - loss: 3.2208e-04 - val_loss: 2.2241e-04 - 4s/epoch - 3ms/step
Epoch 79/100
1493/1493 - 4s - loss: 2.5435e-04 - val_loss: 3.2760e-04 - 4s/epoch - 3ms/step
Epoch 80/100
1493/1493 - 4s - loss: 2.6591e-04 - val_loss: 2.2032e-04 - 4s/epoch - 3ms/step
Epoch 81/100
1493/1493 - 4s - loss: 2.4295e-04 - val_loss: 3.1801e-04 - 4s/epoch - 3ms/step
Epoch 82/100
1493/1493 - 4s - loss: 2.4219e-04 - val_loss: 2.2225e-04 - 4s/epoch - 3ms/step
Epoch 83/100
1493/1493 - 4s - loss: 2.3569e-04 - val_loss: 2.0820e-04 - 4s/epoch - 3ms/step
Epoch 84/100
1493/1493 - 4s - loss: 2.3240e-04 - val_loss: 2.4393e-04 - 4s/epoch - 3ms/step
Epoch 85/100
1493/1493 - 4s - loss: 2.3874e-04 - val_loss: 4.5366e-04 - 4s/epoch - 3ms/step
Epoch 86/100
1493/1493 - 4s - loss: 2.7828e-04 - val_loss: 2.0971e-04 - 4s/epoch - 3ms/step
Epoch 87/100
1493/1493 - 4s - loss: 2.3382e-04 - val_loss: 2.2139e-04 - 4s/epoch - 3ms/step
Epoch 88/100
1493/1493 - 4s - loss: 2.3180e-04 - val_loss: 4.4837e-04 - 4s/epoch - 3ms/step
Epoch 89/100
1493/1493 - 4s - loss: 2.7075e-04 - val_loss: 2.0925e-04 - 4s/epoch - 3ms/step
Epoch 90/100
1493/1493 - 4s - loss: 2.2984e-04 - val_loss: 2.1289e-04 - 4s/epoch - 3ms/step
Epoch 91/100
1493/1493 - 4s - loss: 2.2610e-04 - val_loss: 2.0794e-04 - 4s/epoch - 3ms/step
Epoch 92/100
1493/1493 - 5s - loss: 2.2690e-04 - val_loss: 3.4808e-04 - 5s/epoch - 3ms/step
Epoch 93/100
1493/1493 - 5s - loss: 2.5398e-04 - val_loss: 3.2819e-04 - 5s/epoch - 3ms/step
Epoch 94/100
1493/1493 - 5s - loss: 2.6022e-04 - val_loss: 1.9340e-04 - 5s/epoch - 3ms/step
Epoch 95/100
1493/1493 - 5s - loss: 2.2856e-04 - val_loss: 2.3220e-04 - 5s/epoch - 3ms/step
Epoch 96/100
1493/1493 - 5s - loss: 2.3183e-04 - val_loss: 2.8612e-04 - 5s/epoch - 3ms/step
Epoch 97/100
1493/1493 - 4s - loss: 2.2844e-04 - val_loss: 2.4162e-04 - 4s/epoch - 3ms/step
Epoch 98/100
1493/1493 - 4s - loss: 2.2118e-04 - val_loss: 2.7790e-04 - 4s/epoch - 3ms/step
Epoch 99/100
1493/1493 - 4s - loss: 2.3345e-04 - val_loss: 2.4203e-04 - 4s/epoch - 3ms/step
Epoch 100/100
1493/1493 - 4s - loss: 2.2604e-04 - val_loss: 2.5235e-04 - 4s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00025234519853256643
  1/332 [..............................] - ETA: 35s 49/332 [===>..........................] - ETA: 0s  97/332 [=======>......................] - ETA: 0s145/332 [============>.................] - ETA: 0s193/332 [================>.............] - ETA: 0s241/332 [====================>.........] - ETA: 0s290/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0028226272625335425
cosine 0.0022353791163815134
MAE: 0.009058895
RMSE: 0.01588537
r2: 0.9836303900239342
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_36"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_37 (InputLayer)       multiple                  0         
                                                                 
 dense_36 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_36 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_36 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_37 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_37 (ReLU)             (None, 632)               0         
                                                                 
 dense_37 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_38 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_38 (ReLU)             (None, 2528)              0         
                                                                 
 dense_38 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_37"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_38 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_37 (InputLayer)       multiple                  0         
                                                                 
 dense_36 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_36 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_36 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_38"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_39 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_37 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_37 (ReLU)             (None, 632)               0         
                                                                 
 dense_37 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_38 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_38 (ReLU)             (None, 2528)              0         
                                                                 
 dense_38 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 64, 100, 0.002, 0.5, 632, 0.0002260422770632431, 0.00025234519853256643, 0.0028226272625335425, 0.0022353791163815134, 0.009058894589543343, 0.01588536985218525, 0.9836303900239342, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_39"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_40 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_39 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_39 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_39 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_40 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_40 (ReLU)             (None, 632)               0         
                                                                 
 dense_40 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_41 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_41 (ReLU)             (None, 2528)              0         
                                                                 
 dense_41 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0050 - 5s/epoch - 4ms/step
Epoch 2/200
1493/1493 - 4s - loss: 0.0030 - val_loss: 0.0032 - 4s/epoch - 3ms/step
Epoch 3/200
1493/1493 - 4s - loss: 0.0021 - val_loss: 0.0018 - 4s/epoch - 3ms/step
Epoch 4/200
1493/1493 - 4s - loss: 0.0017 - val_loss: 0.0032 - 4s/epoch - 3ms/step
Epoch 5/200
1493/1493 - 4s - loss: 0.0016 - val_loss: 0.0013 - 4s/epoch - 3ms/step
Epoch 6/200
1493/1493 - 4s - loss: 0.0014 - val_loss: 0.0014 - 4s/epoch - 3ms/step
Epoch 7/200
1493/1493 - 4s - loss: 0.0013 - val_loss: 0.0028 - 4s/epoch - 3ms/step
Epoch 8/200
1493/1493 - 4s - loss: 0.0012 - val_loss: 9.7488e-04 - 4s/epoch - 3ms/step
Epoch 9/200
1493/1493 - 4s - loss: 0.0011 - val_loss: 9.8137e-04 - 4s/epoch - 3ms/step
Epoch 10/200
1493/1493 - 4s - loss: 9.9119e-04 - val_loss: 8.0776e-04 - 4s/epoch - 3ms/step
Epoch 11/200
1493/1493 - 4s - loss: 8.9792e-04 - val_loss: 7.8222e-04 - 4s/epoch - 3ms/step
Epoch 12/200
1493/1493 - 4s - loss: 7.9245e-04 - val_loss: 0.0023 - 4s/epoch - 3ms/step
Epoch 13/200
1493/1493 - 4s - loss: 8.1251e-04 - val_loss: 0.0023 - 4s/epoch - 3ms/step
Epoch 14/200
1493/1493 - 4s - loss: 8.1851e-04 - val_loss: 6.0849e-04 - 4s/epoch - 3ms/step
Epoch 15/200
1493/1493 - 4s - loss: 6.6176e-04 - val_loss: 9.1758e-04 - 4s/epoch - 3ms/step
Epoch 16/200
1493/1493 - 4s - loss: 6.4819e-04 - val_loss: 5.1405e-04 - 4s/epoch - 3ms/step
Epoch 17/200
1493/1493 - 5s - loss: 5.5649e-04 - val_loss: 5.8356e-04 - 5s/epoch - 3ms/step
Epoch 18/200
1493/1493 - 4s - loss: 5.2704e-04 - val_loss: 0.0011 - 4s/epoch - 3ms/step
Epoch 19/200
1493/1493 - 4s - loss: 5.6107e-04 - val_loss: 6.2014e-04 - 4s/epoch - 3ms/step
Epoch 20/200
1493/1493 - 4s - loss: 4.8759e-04 - val_loss: 4.4474e-04 - 4s/epoch - 3ms/step
Epoch 21/200
1493/1493 - 4s - loss: 4.5014e-04 - val_loss: 5.7204e-04 - 4s/epoch - 3ms/step
Epoch 22/200
1493/1493 - 4s - loss: 4.3690e-04 - val_loss: 6.2418e-04 - 4s/epoch - 3ms/step
Epoch 23/200
1493/1493 - 4s - loss: 4.5384e-04 - val_loss: 6.9821e-04 - 4s/epoch - 3ms/step
Epoch 24/200
1493/1493 - 4s - loss: 4.4943e-04 - val_loss: 3.7602e-04 - 4s/epoch - 3ms/step
Epoch 25/200
1493/1493 - 4s - loss: 3.7728e-04 - val_loss: 3.5233e-04 - 4s/epoch - 3ms/step
Epoch 26/200
1493/1493 - 4s - loss: 3.5697e-04 - val_loss: 4.3845e-04 - 4s/epoch - 3ms/step
Epoch 27/200
1493/1493 - 4s - loss: 3.4587e-04 - val_loss: 3.5352e-04 - 4s/epoch - 3ms/step
Epoch 28/200
1493/1493 - 4s - loss: 3.3743e-04 - val_loss: 4.0855e-04 - 4s/epoch - 3ms/step
Epoch 29/200
1493/1493 - 4s - loss: 3.1788e-04 - val_loss: 3.9550e-04 - 4s/epoch - 3ms/step
Epoch 30/200
1493/1493 - 4s - loss: 3.2005e-04 - val_loss: 4.4404e-04 - 4s/epoch - 3ms/step
Epoch 31/200
1493/1493 - 4s - loss: 3.0522e-04 - val_loss: 6.2526e-04 - 4s/epoch - 3ms/step
Epoch 32/200
1493/1493 - 4s - loss: 3.1387e-04 - val_loss: 3.1133e-04 - 4s/epoch - 3ms/step
Epoch 33/200
1493/1493 - 5s - loss: 2.9536e-04 - val_loss: 2.8710e-04 - 5s/epoch - 3ms/step
Epoch 34/200
1493/1493 - 4s - loss: 2.8102e-04 - val_loss: 2.5857e-04 - 4s/epoch - 3ms/step
Epoch 35/200
1493/1493 - 4s - loss: 2.7098e-04 - val_loss: 7.6217e-04 - 4s/epoch - 3ms/step
Epoch 36/200
1493/1493 - 4s - loss: 3.0705e-04 - val_loss: 3.1493e-04 - 4s/epoch - 3ms/step
Epoch 37/200
1493/1493 - 4s - loss: 2.7172e-04 - val_loss: 3.2568e-04 - 4s/epoch - 3ms/step
Epoch 38/200
1493/1493 - 4s - loss: 2.7348e-04 - val_loss: 2.6777e-04 - 4s/epoch - 3ms/step
Epoch 39/200
1493/1493 - 4s - loss: 2.5370e-04 - val_loss: 2.5013e-04 - 4s/epoch - 3ms/step
Epoch 40/200
1493/1493 - 4s - loss: 2.4143e-04 - val_loss: 3.0278e-04 - 4s/epoch - 3ms/step
Epoch 41/200
1493/1493 - 4s - loss: 2.4300e-04 - val_loss: 2.3029e-04 - 4s/epoch - 3ms/step
Epoch 42/200
1493/1493 - 4s - loss: 2.3934e-04 - val_loss: 2.3712e-04 - 4s/epoch - 3ms/step
Epoch 43/200
1493/1493 - 4s - loss: 2.3680e-04 - val_loss: 4.0412e-04 - 4s/epoch - 3ms/step
Epoch 44/200
1493/1493 - 4s - loss: 2.4809e-04 - val_loss: 9.8278e-04 - 4s/epoch - 3ms/step
Epoch 45/200
1493/1493 - 4s - loss: 3.0491e-04 - val_loss: 1.9973e-04 - 4s/epoch - 3ms/step
Epoch 46/200
1493/1493 - 4s - loss: 2.2360e-04 - val_loss: 2.0196e-04 - 4s/epoch - 3ms/step
Epoch 47/200
1493/1493 - 4s - loss: 2.1986e-04 - val_loss: 2.8821e-04 - 4s/epoch - 3ms/step
Epoch 48/200
1493/1493 - 4s - loss: 2.2138e-04 - val_loss: 2.6467e-04 - 4s/epoch - 3ms/step
Epoch 49/200
1493/1493 - 5s - loss: 2.1487e-04 - val_loss: 3.5964e-04 - 5s/epoch - 3ms/step
Epoch 50/200
1493/1493 - 5s - loss: 2.2096e-04 - val_loss: 2.8500e-04 - 5s/epoch - 3ms/step
Epoch 51/200
1493/1493 - 4s - loss: 2.2141e-04 - val_loss: 2.3309e-04 - 4s/epoch - 3ms/step
Epoch 52/200
1493/1493 - 4s - loss: 2.0200e-04 - val_loss: 2.2179e-04 - 4s/epoch - 3ms/step
Epoch 53/200
1493/1493 - 4s - loss: 2.0289e-04 - val_loss: 2.0810e-04 - 4s/epoch - 3ms/step
Epoch 54/200
1493/1493 - 4s - loss: 1.9598e-04 - val_loss: 2.2972e-04 - 4s/epoch - 3ms/step
Epoch 55/200
1493/1493 - 4s - loss: 1.9291e-04 - val_loss: 1.8199e-04 - 4s/epoch - 3ms/step
Epoch 56/200
1493/1493 - 4s - loss: 1.9234e-04 - val_loss: 2.4331e-04 - 4s/epoch - 3ms/step
Epoch 57/200
1493/1493 - 4s - loss: 1.8839e-04 - val_loss: 2.2048e-04 - 4s/epoch - 3ms/step
Epoch 58/200
1493/1493 - 4s - loss: 1.8682e-04 - val_loss: 1.6467e-04 - 4s/epoch - 3ms/step
Epoch 59/200
1493/1493 - 4s - loss: 1.8267e-04 - val_loss: 2.0011e-04 - 4s/epoch - 3ms/step
Epoch 60/200
1493/1493 - 4s - loss: 1.8339e-04 - val_loss: 6.0240e-04 - 4s/epoch - 3ms/step
Epoch 61/200
1493/1493 - 4s - loss: 3.2369e-04 - val_loss: 1.7221e-04 - 4s/epoch - 3ms/step
Epoch 62/200
1493/1493 - 4s - loss: 1.9540e-04 - val_loss: 1.9292e-04 - 4s/epoch - 3ms/step
Epoch 63/200
1493/1493 - 4s - loss: 1.8621e-04 - val_loss: 2.4168e-04 - 4s/epoch - 3ms/step
Epoch 64/200
1493/1493 - 4s - loss: 1.8356e-04 - val_loss: 0.0019 - 4s/epoch - 3ms/step
Epoch 65/200
1493/1493 - 4s - loss: 3.5410e-04 - val_loss: 2.3966e-04 - 4s/epoch - 3ms/step
Epoch 66/200
1493/1493 - 4s - loss: 1.9282e-04 - val_loss: 3.2064e-04 - 4s/epoch - 3ms/step
Epoch 67/200
1493/1493 - 4s - loss: 1.8454e-04 - val_loss: 1.6554e-04 - 4s/epoch - 3ms/step
Epoch 68/200
1493/1493 - 5s - loss: 1.7477e-04 - val_loss: 4.4196e-04 - 5s/epoch - 3ms/step
Epoch 69/200
1493/1493 - 5s - loss: 2.0632e-04 - val_loss: 1.9579e-04 - 5s/epoch - 3ms/step
Epoch 70/200
1493/1493 - 4s - loss: 1.7082e-04 - val_loss: 2.6863e-04 - 4s/epoch - 3ms/step
Epoch 71/200
1493/1493 - 4s - loss: 1.7535e-04 - val_loss: 1.6540e-04 - 4s/epoch - 3ms/step
Epoch 72/200
1493/1493 - 4s - loss: 1.7029e-04 - val_loss: 2.2479e-04 - 4s/epoch - 3ms/step
Epoch 73/200
1493/1493 - 4s - loss: 1.7215e-04 - val_loss: 1.8307e-04 - 4s/epoch - 3ms/step
Epoch 74/200
1493/1493 - 4s - loss: 1.6637e-04 - val_loss: 1.8138e-04 - 4s/epoch - 3ms/step
Epoch 75/200
1493/1493 - 4s - loss: 1.6018e-04 - val_loss: 1.7314e-04 - 4s/epoch - 3ms/step
Epoch 76/200
1493/1493 - 4s - loss: 1.5877e-04 - val_loss: 1.5614e-04 - 4s/epoch - 3ms/step
Epoch 77/200
1493/1493 - 4s - loss: 1.5443e-04 - val_loss: 4.2984e-04 - 4s/epoch - 3ms/step
Epoch 78/200
1493/1493 - 4s - loss: 1.7579e-04 - val_loss: 1.8522e-04 - 4s/epoch - 3ms/step
Epoch 79/200
1493/1493 - 4s - loss: 1.5794e-04 - val_loss: 2.7584e-04 - 4s/epoch - 3ms/step
Epoch 80/200
1493/1493 - 4s - loss: 1.6738e-04 - val_loss: 1.6625e-04 - 4s/epoch - 3ms/step
Epoch 81/200
1493/1493 - 4s - loss: 1.5510e-04 - val_loss: 3.9520e-04 - 4s/epoch - 3ms/step
Epoch 82/200
1493/1493 - 4s - loss: 1.5546e-04 - val_loss: 1.5235e-04 - 4s/epoch - 3ms/step
Epoch 83/200
1493/1493 - 4s - loss: 1.4885e-04 - val_loss: 1.6002e-04 - 4s/epoch - 3ms/step
Epoch 84/200
1493/1493 - 4s - loss: 1.4800e-04 - val_loss: 1.6948e-04 - 4s/epoch - 3ms/step
Epoch 85/200
1493/1493 - 4s - loss: 1.4695e-04 - val_loss: 1.6931e-04 - 4s/epoch - 3ms/step
Epoch 86/200
1493/1493 - 4s - loss: 1.4538e-04 - val_loss: 1.7962e-04 - 4s/epoch - 3ms/step
Epoch 87/200
1493/1493 - 4s - loss: 1.4199e-04 - val_loss: 1.6160e-04 - 4s/epoch - 3ms/step
Epoch 88/200
1493/1493 - 4s - loss: 1.4177e-04 - val_loss: 2.0092e-04 - 4s/epoch - 3ms/step
Epoch 89/200
1493/1493 - 4s - loss: 1.4900e-04 - val_loss: 1.7309e-04 - 4s/epoch - 3ms/step
Epoch 90/200
1493/1493 - 4s - loss: 1.4172e-04 - val_loss: 1.6457e-04 - 4s/epoch - 3ms/step
Epoch 91/200
1493/1493 - 4s - loss: 1.3934e-04 - val_loss: 1.5759e-04 - 4s/epoch - 3ms/step
Epoch 92/200
1493/1493 - 4s - loss: 1.4524e-04 - val_loss: 6.2609e-04 - 4s/epoch - 3ms/step
Epoch 93/200
1493/1493 - 4s - loss: 1.9910e-04 - val_loss: 6.2667e-04 - 4s/epoch - 3ms/step
Epoch 94/200
1493/1493 - 5s - loss: 1.8844e-04 - val_loss: 1.2188e-04 - 5s/epoch - 3ms/step
Epoch 95/200
1493/1493 - 4s - loss: 1.4349e-04 - val_loss: 2.9600e-04 - 4s/epoch - 3ms/step
Epoch 96/200
1493/1493 - 4s - loss: 1.5408e-04 - val_loss: 1.6772e-04 - 4s/epoch - 3ms/step
Epoch 97/200
1493/1493 - 4s - loss: 1.4074e-04 - val_loss: 1.3568e-04 - 4s/epoch - 3ms/step
Epoch 98/200
1493/1493 - 4s - loss: 1.4206e-04 - val_loss: 6.2454e-04 - 4s/epoch - 3ms/step
Epoch 99/200
1493/1493 - 4s - loss: 2.1875e-04 - val_loss: 1.3665e-04 - 4s/epoch - 3ms/step
Epoch 100/200
1493/1493 - 4s - loss: 1.5135e-04 - val_loss: 1.3388e-04 - 4s/epoch - 3ms/step
Epoch 101/200
1493/1493 - 4s - loss: 1.3757e-04 - val_loss: 2.5546e-04 - 4s/epoch - 3ms/step
Epoch 102/200
1493/1493 - 4s - loss: 1.4710e-04 - val_loss: 2.1868e-04 - 4s/epoch - 3ms/step
Epoch 103/200
1493/1493 - 4s - loss: 1.4339e-04 - val_loss: 1.4311e-04 - 4s/epoch - 3ms/step
Epoch 104/200
1493/1493 - 4s - loss: 1.3523e-04 - val_loss: 7.0626e-04 - 4s/epoch - 3ms/step
Epoch 105/200
1493/1493 - 4s - loss: 1.8411e-04 - val_loss: 1.1385e-04 - 4s/epoch - 3ms/step
Epoch 106/200
1493/1493 - 4s - loss: 1.3647e-04 - val_loss: 1.3825e-04 - 4s/epoch - 3ms/step
Epoch 107/200
1493/1493 - 4s - loss: 1.3355e-04 - val_loss: 1.3801e-04 - 4s/epoch - 3ms/step
Epoch 108/200
1493/1493 - 4s - loss: 1.3345e-04 - val_loss: 1.5689e-04 - 4s/epoch - 3ms/step
Epoch 109/200
1493/1493 - 4s - loss: 1.3107e-04 - val_loss: 1.5174e-04 - 4s/epoch - 3ms/step
Epoch 110/200
1493/1493 - 4s - loss: 1.2943e-04 - val_loss: 1.5790e-04 - 4s/epoch - 3ms/step
Epoch 111/200
1493/1493 - 4s - loss: 1.2748e-04 - val_loss: 1.3599e-04 - 4s/epoch - 3ms/step
Epoch 112/200
1493/1493 - 4s - loss: 1.2901e-04 - val_loss: 1.2922e-04 - 4s/epoch - 3ms/step
Epoch 113/200
1493/1493 - 4s - loss: 1.2441e-04 - val_loss: 2.9380e-04 - 4s/epoch - 3ms/step
Epoch 114/200
1493/1493 - 4s - loss: 1.4554e-04 - val_loss: 1.1851e-04 - 4s/epoch - 3ms/step
Epoch 115/200
1493/1493 - 4s - loss: 1.2898e-04 - val_loss: 2.0995e-04 - 4s/epoch - 3ms/step
Epoch 116/200
1493/1493 - 4s - loss: 1.2638e-04 - val_loss: 1.6053e-04 - 4s/epoch - 3ms/step
Epoch 117/200
1493/1493 - 4s - loss: 1.2351e-04 - val_loss: 2.3295e-04 - 4s/epoch - 3ms/step
Epoch 118/200
1493/1493 - 4s - loss: 1.3687e-04 - val_loss: 1.3548e-04 - 4s/epoch - 3ms/step
Epoch 119/200
1493/1493 - 4s - loss: 1.2438e-04 - val_loss: 1.2743e-04 - 4s/epoch - 3ms/step
Epoch 120/200
1493/1493 - 4s - loss: 1.2174e-04 - val_loss: 1.3105e-04 - 4s/epoch - 3ms/step
Epoch 121/200
1493/1493 - 4s - loss: 1.2160e-04 - val_loss: 2.3457e-04 - 4s/epoch - 3ms/step
Epoch 122/200
1493/1493 - 4s - loss: 1.2629e-04 - val_loss: 1.1278e-04 - 4s/epoch - 3ms/step
Epoch 123/200
1493/1493 - 4s - loss: 1.1876e-04 - val_loss: 1.4365e-04 - 4s/epoch - 3ms/step
Epoch 124/200
1493/1493 - 4s - loss: 1.2011e-04 - val_loss: 3.9499e-04 - 4s/epoch - 3ms/step
Epoch 125/200
1493/1493 - 4s - loss: 1.5702e-04 - val_loss: 1.5006e-04 - 4s/epoch - 3ms/step
Epoch 126/200
1493/1493 - 5s - loss: 1.2368e-04 - val_loss: 1.8302e-04 - 5s/epoch - 3ms/step
Epoch 127/200
1493/1493 - 4s - loss: 1.2729e-04 - val_loss: 7.6320e-04 - 4s/epoch - 3ms/step
Epoch 128/200
1493/1493 - 4s - loss: 1.8936e-04 - val_loss: 1.0449e-04 - 4s/epoch - 3ms/step
Epoch 129/200
1493/1493 - 4s - loss: 1.2422e-04 - val_loss: 1.1854e-04 - 4s/epoch - 3ms/step
Epoch 130/200
1493/1493 - 4s - loss: 1.2032e-04 - val_loss: 1.2669e-04 - 4s/epoch - 3ms/step
Epoch 131/200
1493/1493 - 4s - loss: 1.2198e-04 - val_loss: 3.4190e-04 - 4s/epoch - 3ms/step
Epoch 132/200
1493/1493 - 4s - loss: 1.5511e-04 - val_loss: 1.1471e-04 - 4s/epoch - 3ms/step
Epoch 133/200
1493/1493 - 4s - loss: 1.1923e-04 - val_loss: 1.1640e-04 - 4s/epoch - 3ms/step
Epoch 134/200
1493/1493 - 4s - loss: 1.1644e-04 - val_loss: 1.1862e-04 - 4s/epoch - 3ms/step
Epoch 135/200
1493/1493 - 4s - loss: 1.1549e-04 - val_loss: 1.1336e-04 - 4s/epoch - 3ms/step
Epoch 136/200
1493/1493 - 4s - loss: 1.1638e-04 - val_loss: 2.3689e-04 - 4s/epoch - 3ms/step
Epoch 137/200
1493/1493 - 4s - loss: 1.3999e-04 - val_loss: 1.7953e-04 - 4s/epoch - 3ms/step
Epoch 138/200
1493/1493 - 4s - loss: 1.3074e-04 - val_loss: 2.6362e-04 - 4s/epoch - 3ms/step
Epoch 139/200
1493/1493 - 4s - loss: 1.5493e-04 - val_loss: 4.6302e-04 - 4s/epoch - 3ms/step
Epoch 140/200
1493/1493 - 4s - loss: 1.6856e-04 - val_loss: 1.0948e-04 - 4s/epoch - 3ms/step
Epoch 141/200
1493/1493 - 4s - loss: 1.2069e-04 - val_loss: 1.4315e-04 - 4s/epoch - 3ms/step
Epoch 142/200
1493/1493 - 4s - loss: 1.2202e-04 - val_loss: 1.2769e-04 - 4s/epoch - 3ms/step
Epoch 143/200
1493/1493 - 4s - loss: 1.1718e-04 - val_loss: 1.4632e-04 - 4s/epoch - 3ms/step
Epoch 144/200
1493/1493 - 4s - loss: 1.1928e-04 - val_loss: 1.2543e-04 - 4s/epoch - 3ms/step
Epoch 145/200
1493/1493 - 4s - loss: 1.1418e-04 - val_loss: 1.1270e-04 - 4s/epoch - 3ms/step
Epoch 146/200
1493/1493 - 4s - loss: 1.1242e-04 - val_loss: 1.1016e-04 - 4s/epoch - 3ms/step
Epoch 147/200
1493/1493 - 4s - loss: 1.1051e-04 - val_loss: 1.2908e-04 - 4s/epoch - 3ms/step
Epoch 148/200
1493/1493 - 4s - loss: 1.1027e-04 - val_loss: 2.3919e-04 - 4s/epoch - 3ms/step
Epoch 149/200
1493/1493 - 4s - loss: 1.1045e-04 - val_loss: 1.2138e-04 - 4s/epoch - 3ms/step
Epoch 150/200
1493/1493 - 4s - loss: 1.0843e-04 - val_loss: 1.0559e-04 - 4s/epoch - 3ms/step
Epoch 151/200
1493/1493 - 4s - loss: 1.0964e-04 - val_loss: 1.4155e-04 - 4s/epoch - 3ms/step
Epoch 152/200
1493/1493 - 4s - loss: 1.0956e-04 - val_loss: 1.7952e-04 - 4s/epoch - 3ms/step
Epoch 153/200
1493/1493 - 4s - loss: 1.1325e-04 - val_loss: 1.2740e-04 - 4s/epoch - 3ms/step
Epoch 154/200
1493/1493 - 4s - loss: 1.0678e-04 - val_loss: 1.1670e-04 - 4s/epoch - 3ms/step
Epoch 155/200
1493/1493 - 4s - loss: 1.0677e-04 - val_loss: 1.0962e-04 - 4s/epoch - 3ms/step
Epoch 156/200
1493/1493 - 4s - loss: 1.0846e-04 - val_loss: 1.2740e-04 - 4s/epoch - 3ms/step
Epoch 157/200
1493/1493 - 4s - loss: 1.0467e-04 - val_loss: 1.2716e-04 - 4s/epoch - 3ms/step
Epoch 158/200
1493/1493 - 4s - loss: 1.0713e-04 - val_loss: 1.2885e-04 - 4s/epoch - 3ms/step
Epoch 159/200
1493/1493 - 4s - loss: 1.0467e-04 - val_loss: 1.1286e-04 - 4s/epoch - 3ms/step
Epoch 160/200
1493/1493 - 4s - loss: 1.0498e-04 - val_loss: 1.4567e-04 - 4s/epoch - 3ms/step
Epoch 161/200
1493/1493 - 4s - loss: 1.2091e-04 - val_loss: 9.9600e-05 - 4s/epoch - 3ms/step
Epoch 162/200
1493/1493 - 4s - loss: 1.0555e-04 - val_loss: 1.3104e-04 - 4s/epoch - 3ms/step
Epoch 163/200
1493/1493 - 4s - loss: 1.0661e-04 - val_loss: 2.3675e-04 - 4s/epoch - 3ms/step
Epoch 164/200
1493/1493 - 4s - loss: 1.3067e-04 - val_loss: 1.0626e-04 - 4s/epoch - 3ms/step
Epoch 165/200
1493/1493 - 4s - loss: 1.1200e-04 - val_loss: 1.0286e-04 - 4s/epoch - 3ms/step
Epoch 166/200
1493/1493 - 5s - loss: 1.0513e-04 - val_loss: 1.1385e-04 - 5s/epoch - 3ms/step
Epoch 167/200
1493/1493 - 4s - loss: 1.0433e-04 - val_loss: 9.5089e-05 - 4s/epoch - 3ms/step
Epoch 168/200
1493/1493 - 4s - loss: 1.0333e-04 - val_loss: 1.0970e-04 - 4s/epoch - 3ms/step
Epoch 169/200
1493/1493 - 4s - loss: 1.0366e-04 - val_loss: 1.3616e-04 - 4s/epoch - 3ms/step
Epoch 170/200
1493/1493 - 4s - loss: 1.0559e-04 - val_loss: 3.7996e-04 - 4s/epoch - 3ms/step
Epoch 171/200
1493/1493 - 4s - loss: 1.6398e-04 - val_loss: 2.8517e-04 - 4s/epoch - 3ms/step
Epoch 172/200
1493/1493 - 4s - loss: 1.2919e-04 - val_loss: 9.7801e-05 - 4s/epoch - 3ms/step
Epoch 173/200
1493/1493 - 4s - loss: 1.0654e-04 - val_loss: 9.4666e-05 - 4s/epoch - 3ms/step
Epoch 174/200
1493/1493 - 4s - loss: 1.0358e-04 - val_loss: 1.1546e-04 - 4s/epoch - 3ms/step
Epoch 175/200
1493/1493 - 4s - loss: 1.0420e-04 - val_loss: 1.0616e-04 - 4s/epoch - 3ms/step
Epoch 176/200
1493/1493 - 4s - loss: 1.0424e-04 - val_loss: 1.2029e-04 - 4s/epoch - 3ms/step
Epoch 177/200
1493/1493 - 4s - loss: 1.0479e-04 - val_loss: 1.0235e-04 - 4s/epoch - 3ms/step
Epoch 178/200
1493/1493 - 4s - loss: 1.0182e-04 - val_loss: 1.0481e-04 - 4s/epoch - 3ms/step
Epoch 179/200
1493/1493 - 4s - loss: 9.9673e-05 - val_loss: 1.1466e-04 - 4s/epoch - 3ms/step
Epoch 180/200
1493/1493 - 4s - loss: 9.9717e-05 - val_loss: 1.0996e-04 - 4s/epoch - 3ms/step
Epoch 181/200
1493/1493 - 4s - loss: 1.0152e-04 - val_loss: 1.0209e-04 - 4s/epoch - 3ms/step
Epoch 182/200
1493/1493 - 4s - loss: 1.0024e-04 - val_loss: 1.4036e-04 - 4s/epoch - 3ms/step
Epoch 183/200
1493/1493 - 4s - loss: 1.0176e-04 - val_loss: 1.1062e-04 - 4s/epoch - 3ms/step
Epoch 184/200
1493/1493 - 4s - loss: 1.0003e-04 - val_loss: 1.0462e-04 - 4s/epoch - 3ms/step
Epoch 185/200
1493/1493 - 4s - loss: 9.7144e-05 - val_loss: 1.1837e-04 - 4s/epoch - 3ms/step
Epoch 186/200
1493/1493 - 4s - loss: 9.9361e-05 - val_loss: 1.0362e-04 - 4s/epoch - 3ms/step
Epoch 187/200
1493/1493 - 4s - loss: 9.7014e-05 - val_loss: 1.1432e-04 - 4s/epoch - 3ms/step
Epoch 188/200
1493/1493 - 4s - loss: 9.9919e-05 - val_loss: 1.3542e-04 - 4s/epoch - 3ms/step
Epoch 189/200
1493/1493 - 4s - loss: 1.0177e-04 - val_loss: 1.1392e-04 - 4s/epoch - 3ms/step
Epoch 190/200
1493/1493 - 4s - loss: 9.8278e-05 - val_loss: 1.3086e-04 - 4s/epoch - 3ms/step
Epoch 191/200
1493/1493 - 4s - loss: 1.0177e-04 - val_loss: 1.4656e-04 - 4s/epoch - 3ms/step
Epoch 192/200
1493/1493 - 4s - loss: 9.9281e-05 - val_loss: 1.2703e-04 - 4s/epoch - 3ms/step
Epoch 193/200
1493/1493 - 4s - loss: 1.0523e-04 - val_loss: 3.4629e-04 - 4s/epoch - 3ms/step
Epoch 194/200
1493/1493 - 4s - loss: 1.6553e-04 - val_loss: 0.0012 - 4s/epoch - 3ms/step
Epoch 195/200
1493/1493 - 4s - loss: 2.0114e-04 - val_loss: 2.7585e-04 - 4s/epoch - 3ms/step
Epoch 196/200
1493/1493 - 4s - loss: 1.4160e-04 - val_loss: 9.3255e-05 - 4s/epoch - 3ms/step
Epoch 197/200
1493/1493 - 4s - loss: 1.0831e-04 - val_loss: 1.3328e-04 - 4s/epoch - 3ms/step
Epoch 198/200
1493/1493 - 4s - loss: 1.0958e-04 - val_loss: 3.2094e-04 - 4s/epoch - 3ms/step
Epoch 199/200
1493/1493 - 4s - loss: 1.1711e-04 - val_loss: 8.3853e-05 - 4s/epoch - 3ms/step
Epoch 200/200
1493/1493 - 4s - loss: 1.0094e-04 - val_loss: 1.0873e-04 - 4s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00010872851271415129
  1/332 [..............................] - ETA: 32s 49/332 [===>..........................] - ETA: 0s  97/332 [=======>......................] - ETA: 0s145/332 [============>.................] - ETA: 0s193/332 [================>.............] - ETA: 0s242/332 [====================>.........] - ETA: 0s291/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0012333607936507568
cosine 0.0009716243349377991
MAE: 0.005852453
RMSE: 0.010427292
r2: 0.9929473735386589
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_39"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_40 (InputLayer)       multiple                  0         
                                                                 
 dense_39 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_39 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_39 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_40 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_40 (ReLU)             (None, 632)               0         
                                                                 
 dense_40 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_41 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_41 (ReLU)             (None, 2528)              0         
                                                                 
 dense_41 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_40"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_41 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_40 (InputLayer)       multiple                  0         
                                                                 
 dense_39 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_39 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_39 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_41"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_42 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_40 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_40 (ReLU)             (None, 632)               0         
                                                                 
 dense_40 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_41 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_41 (ReLU)             (None, 2528)              0         
                                                                 
 dense_41 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 64, 200, 0.0005, 0.5, 632, 0.00010094046592712402, 0.00010872851271415129, 0.0012333607936507568, 0.0009716243349377991, 0.0058524529449641705, 0.010427292436361313, 0.9929473735386589, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_42"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_43 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_42 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_42 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_42 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_43 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_43 (ReLU)             (None, 632)               0         
                                                                 
 dense_43 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_44 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_44 (ReLU)             (None, 2528)              0         
                                                                 
 dense_44 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
1493/1493 - 5s - loss: 0.0086 - val_loss: 0.0033 - 5s/epoch - 4ms/step
Epoch 2/200
1493/1493 - 4s - loss: 0.0026 - val_loss: 0.0020 - 4s/epoch - 3ms/step
Epoch 3/200
1493/1493 - 4s - loss: 0.0022 - val_loss: 0.0017 - 4s/epoch - 3ms/step
Epoch 4/200
1493/1493 - 4s - loss: 0.0017 - val_loss: 0.0047 - 4s/epoch - 3ms/step
Epoch 5/200
1493/1493 - 4s - loss: 0.0019 - val_loss: 0.0012 - 4s/epoch - 3ms/step
Epoch 6/200
1493/1493 - 4s - loss: 0.0012 - val_loss: 0.0010 - 4s/epoch - 3ms/step
Epoch 7/200
1493/1493 - 4s - loss: 0.0011 - val_loss: 8.8477e-04 - 4s/epoch - 3ms/step
Epoch 8/200
1493/1493 - 4s - loss: 9.0383e-04 - val_loss: 8.9657e-04 - 4s/epoch - 3ms/step
Epoch 9/200
1493/1493 - 4s - loss: 7.7909e-04 - val_loss: 7.2784e-04 - 4s/epoch - 3ms/step
Epoch 10/200
1493/1493 - 4s - loss: 6.8301e-04 - val_loss: 6.0162e-04 - 4s/epoch - 3ms/step
Epoch 11/200
1493/1493 - 4s - loss: 6.0230e-04 - val_loss: 5.8944e-04 - 4s/epoch - 3ms/step
Epoch 12/200
1493/1493 - 4s - loss: 5.3748e-04 - val_loss: 0.0023 - 4s/epoch - 3ms/step
Epoch 13/200
1493/1493 - 4s - loss: 7.8330e-04 - val_loss: 0.0022 - 4s/epoch - 3ms/step
Epoch 14/200
1493/1493 - 4s - loss: 7.2891e-04 - val_loss: 0.0035 - 4s/epoch - 3ms/step
Epoch 15/200
1493/1493 - 4s - loss: 8.3811e-04 - val_loss: 0.0020 - 4s/epoch - 3ms/step
Epoch 16/200
1493/1493 - 4s - loss: 6.4367e-04 - val_loss: 4.5571e-04 - 4s/epoch - 3ms/step
Epoch 17/200
1493/1493 - 4s - loss: 4.9079e-04 - val_loss: 4.3411e-04 - 4s/epoch - 3ms/step
Epoch 18/200
1493/1493 - 4s - loss: 4.4959e-04 - val_loss: 0.0014 - 4s/epoch - 3ms/step
Epoch 19/200
1493/1493 - 4s - loss: 5.1998e-04 - val_loss: 4.8346e-04 - 4s/epoch - 3ms/step
Epoch 20/200
1493/1493 - 4s - loss: 4.1215e-04 - val_loss: 3.9639e-04 - 4s/epoch - 3ms/step
Epoch 21/200
1493/1493 - 4s - loss: 3.8514e-04 - val_loss: 4.8768e-04 - 4s/epoch - 3ms/step
Epoch 22/200
1493/1493 - 4s - loss: 3.8187e-04 - val_loss: 0.0024 - 4s/epoch - 3ms/step
Epoch 23/200
1493/1493 - 4s - loss: 5.9142e-04 - val_loss: 4.1613e-04 - 4s/epoch - 3ms/step
Epoch 24/200
1493/1493 - 4s - loss: 3.8693e-04 - val_loss: 0.0011 - 4s/epoch - 3ms/step
Epoch 25/200
1493/1493 - 4s - loss: 3.9502e-04 - val_loss: 3.1992e-04 - 4s/epoch - 3ms/step
Epoch 26/200
1493/1493 - 4s - loss: 3.4012e-04 - val_loss: 4.0694e-04 - 4s/epoch - 3ms/step
Epoch 27/200
1493/1493 - 4s - loss: 3.2430e-04 - val_loss: 2.8077e-04 - 4s/epoch - 3ms/step
Epoch 28/200
1493/1493 - 4s - loss: 3.0987e-04 - val_loss: 6.1294e-04 - 4s/epoch - 3ms/step
Epoch 29/200
1493/1493 - 4s - loss: 3.4485e-04 - val_loss: 2.8508e-04 - 4s/epoch - 3ms/step
Epoch 30/200
1493/1493 - 4s - loss: 3.0790e-04 - val_loss: 2.7876e-04 - 4s/epoch - 3ms/step
Epoch 31/200
1493/1493 - 4s - loss: 3.0283e-04 - val_loss: 3.2040e-04 - 4s/epoch - 3ms/step
Epoch 32/200
1493/1493 - 4s - loss: 2.8518e-04 - val_loss: 3.0785e-04 - 4s/epoch - 3ms/step
Epoch 33/200
1493/1493 - 4s - loss: 2.8582e-04 - val_loss: 2.5156e-04 - 4s/epoch - 3ms/step
Epoch 34/200
1493/1493 - 4s - loss: 2.6763e-04 - val_loss: 2.4516e-04 - 4s/epoch - 3ms/step
Epoch 35/200
1493/1493 - 4s - loss: 2.6013e-04 - val_loss: 2.6424e-04 - 4s/epoch - 3ms/step
Epoch 36/200
1493/1493 - 4s - loss: 2.6282e-04 - val_loss: 2.6792e-04 - 4s/epoch - 3ms/step
Epoch 37/200
1493/1493 - 4s - loss: 2.5578e-04 - val_loss: 3.4442e-04 - 4s/epoch - 3ms/step
Epoch 38/200
1493/1493 - 4s - loss: 2.7551e-04 - val_loss: 2.1274e-04 - 4s/epoch - 3ms/step
Epoch 39/200
1493/1493 - 4s - loss: 2.4493e-04 - val_loss: 2.2280e-04 - 4s/epoch - 3ms/step
Epoch 40/200
1493/1493 - 4s - loss: 2.3782e-04 - val_loss: 2.5485e-04 - 4s/epoch - 3ms/step
Epoch 41/200
1493/1493 - 4s - loss: 2.4810e-04 - val_loss: 2.1091e-04 - 4s/epoch - 3ms/step
Epoch 42/200
1493/1493 - 4s - loss: 2.3221e-04 - val_loss: 2.2030e-04 - 4s/epoch - 3ms/step
Epoch 43/200
1493/1493 - 4s - loss: 2.3565e-04 - val_loss: 7.4884e-04 - 4s/epoch - 3ms/step
Epoch 44/200
1493/1493 - 4s - loss: 3.1701e-04 - val_loss: 2.3224e-04 - 4s/epoch - 3ms/step
Epoch 45/200
1493/1493 - 4s - loss: 2.3969e-04 - val_loss: 1.9109e-04 - 4s/epoch - 3ms/step
Epoch 46/200
1493/1493 - 4s - loss: 2.2605e-04 - val_loss: 2.0264e-04 - 4s/epoch - 3ms/step
Epoch 47/200
1493/1493 - 4s - loss: 2.2402e-04 - val_loss: 2.2795e-04 - 4s/epoch - 3ms/step
Epoch 48/200
1493/1493 - 4s - loss: 2.2073e-04 - val_loss: 2.3292e-04 - 4s/epoch - 3ms/step
Epoch 49/200
1493/1493 - 4s - loss: 2.2746e-04 - val_loss: 9.0689e-04 - 4s/epoch - 3ms/step
Epoch 50/200
1493/1493 - 4s - loss: 3.7123e-04 - val_loss: 0.0020 - 4s/epoch - 3ms/step
Epoch 51/200
1493/1493 - 4s - loss: 4.3969e-04 - val_loss: 2.1132e-04 - 4s/epoch - 3ms/step
Epoch 52/200
1493/1493 - 4s - loss: 2.4828e-04 - val_loss: 1.9925e-04 - 4s/epoch - 3ms/step
Epoch 53/200
1493/1493 - 5s - loss: 2.3397e-04 - val_loss: 1.8756e-04 - 5s/epoch - 3ms/step
Epoch 54/200
1493/1493 - 4s - loss: 2.2660e-04 - val_loss: 2.0724e-04 - 4s/epoch - 3ms/step
Epoch 55/200
1493/1493 - 4s - loss: 2.1653e-04 - val_loss: 2.0077e-04 - 4s/epoch - 3ms/step
Epoch 56/200
1493/1493 - 4s - loss: 2.1208e-04 - val_loss: 2.1141e-04 - 4s/epoch - 3ms/step
Epoch 57/200
1493/1493 - 4s - loss: 2.0659e-04 - val_loss: 1.9026e-04 - 4s/epoch - 3ms/step
Epoch 58/200
1493/1493 - 4s - loss: 2.0478e-04 - val_loss: 1.8642e-04 - 4s/epoch - 3ms/step
Epoch 59/200
1493/1493 - 4s - loss: 2.1633e-04 - val_loss: 1.9380e-04 - 4s/epoch - 3ms/step
Epoch 60/200
1493/1493 - 4s - loss: 1.9983e-04 - val_loss: 3.3412e-04 - 4s/epoch - 3ms/step
Epoch 61/200
1493/1493 - 4s - loss: 2.2177e-04 - val_loss: 1.7450e-04 - 4s/epoch - 3ms/step
Epoch 62/200
1493/1493 - 4s - loss: 1.9963e-04 - val_loss: 2.1311e-04 - 4s/epoch - 3ms/step
Epoch 63/200
1493/1493 - 4s - loss: 1.9278e-04 - val_loss: 2.0768e-04 - 4s/epoch - 3ms/step
Epoch 64/200
1493/1493 - 4s - loss: 1.9490e-04 - val_loss: 6.3757e-04 - 4s/epoch - 3ms/step
Epoch 65/200
1493/1493 - 4s - loss: 2.4095e-04 - val_loss: 1.8929e-04 - 4s/epoch - 3ms/step
Epoch 66/200
1493/1493 - 4s - loss: 1.9982e-04 - val_loss: 2.0175e-04 - 4s/epoch - 3ms/step
Epoch 67/200
1493/1493 - 4s - loss: 1.9159e-04 - val_loss: 1.6656e-04 - 4s/epoch - 3ms/step
Epoch 68/200
1493/1493 - 4s - loss: 1.8752e-04 - val_loss: 2.9775e-04 - 4s/epoch - 3ms/step
Epoch 69/200
1493/1493 - 4s - loss: 2.3046e-04 - val_loss: 1.7896e-04 - 4s/epoch - 3ms/step
Epoch 70/200
1493/1493 - 4s - loss: 1.9255e-04 - val_loss: 3.3392e-04 - 4s/epoch - 3ms/step
Epoch 71/200
1493/1493 - 5s - loss: 2.2424e-04 - val_loss: 1.7127e-04 - 5s/epoch - 3ms/step
Epoch 72/200
1493/1493 - 5s - loss: 1.8972e-04 - val_loss: 1.8097e-04 - 5s/epoch - 3ms/step
Epoch 73/200
1493/1493 - 4s - loss: 1.9376e-04 - val_loss: 2.7896e-04 - 4s/epoch - 3ms/step
Epoch 74/200
1493/1493 - 4s - loss: 2.1370e-04 - val_loss: 1.8120e-04 - 4s/epoch - 3ms/step
Epoch 75/200
1493/1493 - 4s - loss: 1.8364e-04 - val_loss: 1.6129e-04 - 4s/epoch - 3ms/step
Epoch 76/200
1493/1493 - 4s - loss: 1.8015e-04 - val_loss: 1.6462e-04 - 4s/epoch - 3ms/step
Epoch 77/200
1493/1493 - 4s - loss: 1.7911e-04 - val_loss: 4.0058e-04 - 4s/epoch - 3ms/step
Epoch 78/200
1493/1493 - 4s - loss: 2.3339e-04 - val_loss: 1.6054e-04 - 4s/epoch - 3ms/step
Epoch 79/200
1493/1493 - 4s - loss: 1.8435e-04 - val_loss: 2.0095e-04 - 4s/epoch - 3ms/step
Epoch 80/200
1493/1493 - 4s - loss: 1.9056e-04 - val_loss: 2.0598e-04 - 4s/epoch - 3ms/step
Epoch 81/200
1493/1493 - 4s - loss: 1.8376e-04 - val_loss: 2.3878e-04 - 4s/epoch - 3ms/step
Epoch 82/200
1493/1493 - 4s - loss: 1.7502e-04 - val_loss: 1.6825e-04 - 4s/epoch - 3ms/step
Epoch 83/200
1493/1493 - 4s - loss: 1.7369e-04 - val_loss: 1.5798e-04 - 4s/epoch - 3ms/step
Epoch 84/200
1493/1493 - 4s - loss: 1.7142e-04 - val_loss: 1.9269e-04 - 4s/epoch - 3ms/step
Epoch 85/200
1493/1493 - 4s - loss: 1.7177e-04 - val_loss: 1.8728e-04 - 4s/epoch - 3ms/step
Epoch 86/200
1493/1493 - 4s - loss: 1.7854e-04 - val_loss: 1.6107e-04 - 4s/epoch - 3ms/step
Epoch 87/200
1493/1493 - 4s - loss: 1.6653e-04 - val_loss: 1.6208e-04 - 4s/epoch - 3ms/step
Epoch 88/200
1493/1493 - 4s - loss: 1.6662e-04 - val_loss: 2.6738e-04 - 4s/epoch - 3ms/step
Epoch 89/200
1493/1493 - 4s - loss: 1.9006e-04 - val_loss: 1.6378e-04 - 4s/epoch - 3ms/step
Epoch 90/200
1493/1493 - 4s - loss: 1.6668e-04 - val_loss: 1.5566e-04 - 4s/epoch - 3ms/step
Epoch 91/200
1493/1493 - 5s - loss: 1.6470e-04 - val_loss: 1.6868e-04 - 5s/epoch - 3ms/step
Epoch 92/200
1493/1493 - 5s - loss: 1.6433e-04 - val_loss: 2.9438e-04 - 5s/epoch - 3ms/step
Epoch 93/200
1493/1493 - 4s - loss: 1.9997e-04 - val_loss: 1.6142e-04 - 4s/epoch - 3ms/step
Epoch 94/200
1493/1493 - 4s - loss: 1.6683e-04 - val_loss: 1.3654e-04 - 4s/epoch - 3ms/step
Epoch 95/200
1493/1493 - 4s - loss: 1.6309e-04 - val_loss: 1.6313e-04 - 4s/epoch - 3ms/step
Epoch 96/200
1493/1493 - 4s - loss: 1.6289e-04 - val_loss: 1.9369e-04 - 4s/epoch - 3ms/step
Epoch 97/200
1493/1493 - 4s - loss: 1.6068e-04 - val_loss: 1.5452e-04 - 4s/epoch - 3ms/step
Epoch 98/200
1493/1493 - 4s - loss: 1.6154e-04 - val_loss: 2.6236e-04 - 4s/epoch - 3ms/step
Epoch 99/200
1493/1493 - 4s - loss: 1.8467e-04 - val_loss: 1.4447e-04 - 4s/epoch - 3ms/step
Epoch 100/200
1493/1493 - 4s - loss: 1.6185e-04 - val_loss: 1.5158e-04 - 4s/epoch - 3ms/step
Epoch 101/200
1493/1493 - 4s - loss: 1.6190e-04 - val_loss: 3.9691e-04 - 4s/epoch - 3ms/step
Epoch 102/200
1493/1493 - 4s - loss: 1.9890e-04 - val_loss: 2.3280e-04 - 4s/epoch - 3ms/step
Epoch 103/200
1493/1493 - 4s - loss: 1.8732e-04 - val_loss: 1.6059e-04 - 4s/epoch - 3ms/step
Epoch 104/200
1493/1493 - 4s - loss: 1.6301e-04 - val_loss: 3.3858e-04 - 4s/epoch - 3ms/step
Epoch 105/200
1493/1493 - 4s - loss: 1.9118e-04 - val_loss: 1.6323e-04 - 4s/epoch - 3ms/step
Epoch 106/200
1493/1493 - 4s - loss: 1.6227e-04 - val_loss: 1.6891e-04 - 4s/epoch - 3ms/step
Epoch 107/200
1493/1493 - 4s - loss: 1.6064e-04 - val_loss: 1.6202e-04 - 4s/epoch - 3ms/step
Epoch 108/200
1493/1493 - 4s - loss: 1.5606e-04 - val_loss: 1.6301e-04 - 4s/epoch - 3ms/step
Epoch 109/200
1493/1493 - 4s - loss: 1.5643e-04 - val_loss: 1.7056e-04 - 4s/epoch - 3ms/step
Epoch 110/200
1493/1493 - 4s - loss: 1.5376e-04 - val_loss: 1.7326e-04 - 4s/epoch - 3ms/step
Epoch 111/200
1493/1493 - 4s - loss: 1.5456e-04 - val_loss: 1.5893e-04 - 4s/epoch - 3ms/step
Epoch 112/200
1493/1493 - 4s - loss: 1.5345e-04 - val_loss: 1.6423e-04 - 4s/epoch - 3ms/step
Epoch 113/200
1493/1493 - 4s - loss: 1.5028e-04 - val_loss: 2.2937e-04 - 4s/epoch - 3ms/step
Epoch 114/200
1493/1493 - 4s - loss: 1.8297e-04 - val_loss: 2.1171e-04 - 4s/epoch - 3ms/step
Epoch 115/200
1493/1493 - 4s - loss: 1.9589e-04 - val_loss: 2.2359e-04 - 4s/epoch - 3ms/step
Epoch 116/200
1493/1493 - 5s - loss: 1.6174e-04 - val_loss: 3.0486e-04 - 5s/epoch - 3ms/step
Epoch 117/200
1493/1493 - 4s - loss: 1.7122e-04 - val_loss: 2.3859e-04 - 4s/epoch - 3ms/step
Epoch 118/200
1493/1493 - 4s - loss: 1.6542e-04 - val_loss: 1.5009e-04 - 4s/epoch - 3ms/step
Epoch 119/200
1493/1493 - 4s - loss: 1.5226e-04 - val_loss: 1.4242e-04 - 4s/epoch - 3ms/step
Epoch 120/200
1493/1493 - 4s - loss: 1.4986e-04 - val_loss: 1.4788e-04 - 4s/epoch - 3ms/step
Epoch 121/200
1493/1493 - 4s - loss: 1.5004e-04 - val_loss: 2.1283e-04 - 4s/epoch - 3ms/step
Epoch 122/200
1493/1493 - 4s - loss: 1.5541e-04 - val_loss: 1.4990e-04 - 4s/epoch - 3ms/step
Epoch 123/200
1493/1493 - 4s - loss: 1.4718e-04 - val_loss: 1.5163e-04 - 4s/epoch - 3ms/step
Epoch 124/200
1493/1493 - 4s - loss: 1.4622e-04 - val_loss: 2.2735e-04 - 4s/epoch - 3ms/step
Epoch 125/200
1493/1493 - 4s - loss: 1.5561e-04 - val_loss: 1.4194e-04 - 4s/epoch - 3ms/step
Epoch 126/200
1493/1493 - 4s - loss: 1.4576e-04 - val_loss: 2.3304e-04 - 4s/epoch - 3ms/step
Epoch 127/200
1493/1493 - 4s - loss: 1.5507e-04 - val_loss: 1.6053e-04 - 4s/epoch - 3ms/step
Epoch 128/200
1493/1493 - 4s - loss: 1.4440e-04 - val_loss: 1.6109e-04 - 4s/epoch - 3ms/step
Epoch 129/200
1493/1493 - 4s - loss: 1.4491e-04 - val_loss: 1.3741e-04 - 4s/epoch - 3ms/step
Epoch 130/200
1493/1493 - 4s - loss: 1.4241e-04 - val_loss: 1.3920e-04 - 4s/epoch - 3ms/step
Epoch 131/200
1493/1493 - 4s - loss: 1.4762e-04 - val_loss: 1.6012e-04 - 4s/epoch - 3ms/step
Epoch 132/200
1493/1493 - 4s - loss: 1.4333e-04 - val_loss: 1.5133e-04 - 4s/epoch - 3ms/step
Epoch 133/200
1493/1493 - 4s - loss: 1.4276e-04 - val_loss: 1.8267e-04 - 4s/epoch - 3ms/step
Epoch 134/200
1493/1493 - 4s - loss: 1.5046e-04 - val_loss: 1.6433e-04 - 4s/epoch - 3ms/step
Epoch 135/200
1493/1493 - 4s - loss: 1.4403e-04 - val_loss: 1.4544e-04 - 4s/epoch - 3ms/step
Epoch 136/200
1493/1493 - 4s - loss: 1.3992e-04 - val_loss: 1.6403e-04 - 4s/epoch - 3ms/step
Epoch 137/200
1493/1493 - 4s - loss: 1.4386e-04 - val_loss: 1.7084e-04 - 4s/epoch - 3ms/step
Epoch 138/200
1493/1493 - 4s - loss: 1.4560e-04 - val_loss: 1.7409e-04 - 4s/epoch - 3ms/step
Epoch 139/200
1493/1493 - 4s - loss: 1.4741e-04 - val_loss: 1.5893e-04 - 4s/epoch - 3ms/step
Epoch 140/200
1493/1493 - 4s - loss: 1.4148e-04 - val_loss: 1.4516e-04 - 4s/epoch - 3ms/step
Epoch 141/200
1493/1493 - 4s - loss: 1.3936e-04 - val_loss: 2.1618e-04 - 4s/epoch - 3ms/step
Epoch 142/200
1493/1493 - 4s - loss: 1.6863e-04 - val_loss: 1.4712e-04 - 4s/epoch - 3ms/step
Epoch 143/200
1493/1493 - 4s - loss: 1.4051e-04 - val_loss: 1.7617e-04 - 4s/epoch - 3ms/step
Epoch 144/200
1493/1493 - 4s - loss: 1.4673e-04 - val_loss: 1.7442e-04 - 4s/epoch - 3ms/step
Epoch 145/200
1493/1493 - 5s - loss: 1.4724e-04 - val_loss: 1.4414e-04 - 5s/epoch - 3ms/step
Epoch 146/200
1493/1493 - 4s - loss: 1.3819e-04 - val_loss: 1.5182e-04 - 4s/epoch - 3ms/step
Epoch 147/200
1493/1493 - 4s - loss: 1.3754e-04 - val_loss: 1.7490e-04 - 4s/epoch - 3ms/step
Epoch 148/200
1493/1493 - 4s - loss: 1.4033e-04 - val_loss: 2.2529e-04 - 4s/epoch - 3ms/step
Epoch 149/200
1493/1493 - 4s - loss: 1.3551e-04 - val_loss: 1.4221e-04 - 4s/epoch - 3ms/step
Epoch 150/200
1493/1493 - 4s - loss: 1.3452e-04 - val_loss: 1.3303e-04 - 4s/epoch - 3ms/step
Epoch 151/200
1493/1493 - 4s - loss: 1.3906e-04 - val_loss: 1.6333e-04 - 4s/epoch - 3ms/step
Epoch 152/200
1493/1493 - 4s - loss: 1.3764e-04 - val_loss: 1.9399e-04 - 4s/epoch - 3ms/step
Epoch 153/200
1493/1493 - 4s - loss: 1.3968e-04 - val_loss: 1.6015e-04 - 4s/epoch - 3ms/step
Epoch 154/200
1493/1493 - 4s - loss: 1.3320e-04 - val_loss: 1.7422e-04 - 4s/epoch - 3ms/step
Epoch 155/200
1493/1493 - 4s - loss: 1.3375e-04 - val_loss: 1.5061e-04 - 4s/epoch - 3ms/step
Epoch 156/200
1493/1493 - 4s - loss: 1.3387e-04 - val_loss: 1.5260e-04 - 4s/epoch - 3ms/step
Epoch 157/200
1493/1493 - 4s - loss: 1.3250e-04 - val_loss: 2.9223e-04 - 4s/epoch - 3ms/step
Epoch 158/200
1493/1493 - 4s - loss: 1.6239e-04 - val_loss: 1.4199e-04 - 4s/epoch - 3ms/step
Epoch 159/200
1493/1493 - 4s - loss: 1.4308e-04 - val_loss: 1.5005e-04 - 4s/epoch - 3ms/step
Epoch 160/200
1493/1493 - 4s - loss: 1.3383e-04 - val_loss: 1.3328e-04 - 4s/epoch - 3ms/step
Epoch 161/200
1493/1493 - 4s - loss: 1.3195e-04 - val_loss: 1.4398e-04 - 4s/epoch - 3ms/step
Epoch 162/200
1493/1493 - 4s - loss: 1.3320e-04 - val_loss: 1.3542e-04 - 4s/epoch - 3ms/step
Epoch 163/200
1493/1493 - 4s - loss: 1.3193e-04 - val_loss: 1.6874e-04 - 4s/epoch - 3ms/step
Epoch 164/200
1493/1493 - 4s - loss: 1.3569e-04 - val_loss: 2.1425e-04 - 4s/epoch - 3ms/step
Epoch 165/200
1493/1493 - 4s - loss: 1.5121e-04 - val_loss: 1.3947e-04 - 4s/epoch - 3ms/step
Epoch 166/200
1493/1493 - 4s - loss: 1.3175e-04 - val_loss: 1.4375e-04 - 4s/epoch - 3ms/step
Epoch 167/200
1493/1493 - 4s - loss: 1.3188e-04 - val_loss: 1.4555e-04 - 4s/epoch - 3ms/step
Epoch 168/200
1493/1493 - 4s - loss: 1.2987e-04 - val_loss: 1.3895e-04 - 4s/epoch - 3ms/step
Epoch 169/200
1493/1493 - 4s - loss: 1.3127e-04 - val_loss: 2.9419e-04 - 4s/epoch - 3ms/step
Epoch 170/200
1493/1493 - 4s - loss: 1.3286e-04 - val_loss: 2.8469e-04 - 4s/epoch - 3ms/step
Epoch 171/200
1493/1493 - 4s - loss: 1.7503e-04 - val_loss: 2.2654e-04 - 4s/epoch - 3ms/step
Epoch 172/200
1493/1493 - 4s - loss: 1.4276e-04 - val_loss: 1.4349e-04 - 4s/epoch - 3ms/step
Epoch 173/200
1493/1493 - 4s - loss: 1.3393e-04 - val_loss: 1.4114e-04 - 4s/epoch - 3ms/step
Epoch 174/200
1493/1493 - 4s - loss: 1.3004e-04 - val_loss: 1.5226e-04 - 4s/epoch - 3ms/step
Epoch 175/200
1493/1493 - 4s - loss: 1.3095e-04 - val_loss: 1.3654e-04 - 4s/epoch - 3ms/step
Epoch 176/200
1493/1493 - 4s - loss: 1.3106e-04 - val_loss: 2.0603e-04 - 4s/epoch - 3ms/step
Epoch 177/200
1493/1493 - 4s - loss: 1.3579e-04 - val_loss: 1.3541e-04 - 4s/epoch - 3ms/step
Epoch 178/200
1493/1493 - 4s - loss: 1.2883e-04 - val_loss: 1.3826e-04 - 4s/epoch - 3ms/step
Epoch 179/200
1493/1493 - 4s - loss: 1.2689e-04 - val_loss: 1.4963e-04 - 4s/epoch - 3ms/step
Epoch 180/200
1493/1493 - 4s - loss: 1.2721e-04 - val_loss: 1.7103e-04 - 4s/epoch - 3ms/step
Epoch 181/200
1493/1493 - 4s - loss: 1.3281e-04 - val_loss: 1.7089e-04 - 4s/epoch - 3ms/step
Epoch 182/200
1493/1493 - 5s - loss: 1.3232e-04 - val_loss: 1.5183e-04 - 5s/epoch - 3ms/step
Epoch 183/200
1493/1493 - 5s - loss: 1.2722e-04 - val_loss: 1.4837e-04 - 5s/epoch - 3ms/step
Epoch 184/200
1493/1493 - 4s - loss: 1.3240e-04 - val_loss: 1.3894e-04 - 4s/epoch - 3ms/step
Epoch 185/200
1493/1493 - 4s - loss: 1.2613e-04 - val_loss: 1.4961e-04 - 4s/epoch - 3ms/step
Epoch 186/200
1493/1493 - 4s - loss: 1.2722e-04 - val_loss: 1.4894e-04 - 4s/epoch - 3ms/step
Epoch 187/200
1493/1493 - 4s - loss: 1.2679e-04 - val_loss: 1.6002e-04 - 4s/epoch - 3ms/step
Epoch 188/200
1493/1493 - 4s - loss: 1.3279e-04 - val_loss: 1.5960e-04 - 4s/epoch - 3ms/step
Epoch 189/200
1493/1493 - 4s - loss: 1.2722e-04 - val_loss: 1.5668e-04 - 4s/epoch - 3ms/step
Epoch 190/200
1493/1493 - 4s - loss: 1.2588e-04 - val_loss: 1.8871e-04 - 4s/epoch - 3ms/step
Epoch 191/200
1493/1493 - 4s - loss: 1.3234e-04 - val_loss: 4.7633e-04 - 4s/epoch - 3ms/step
Epoch 192/200
1493/1493 - 4s - loss: 1.6769e-04 - val_loss: 1.3622e-04 - 4s/epoch - 3ms/step
Epoch 193/200
1493/1493 - 4s - loss: 1.3222e-04 - val_loss: 2.1528e-04 - 4s/epoch - 3ms/step
Epoch 194/200
1493/1493 - 4s - loss: 1.3897e-04 - val_loss: 3.4683e-04 - 4s/epoch - 3ms/step
Epoch 195/200
1493/1493 - 4s - loss: 1.6872e-04 - val_loss: 1.3248e-04 - 4s/epoch - 3ms/step
Epoch 196/200
1493/1493 - 4s - loss: 1.3207e-04 - val_loss: 1.3324e-04 - 4s/epoch - 3ms/step
Epoch 197/200
1493/1493 - 4s - loss: 1.2929e-04 - val_loss: 2.6455e-04 - 4s/epoch - 3ms/step
Epoch 198/200
1493/1493 - 4s - loss: 1.4413e-04 - val_loss: 1.6693e-04 - 4s/epoch - 3ms/step
Epoch 199/200
1493/1493 - 4s - loss: 1.3047e-04 - val_loss: 1.2928e-04 - 4s/epoch - 3ms/step
Epoch 200/200
1493/1493 - 4s - loss: 1.2563e-04 - val_loss: 1.3607e-04 - 4s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00013606896391138434
  1/332 [..............................] - ETA: 33s 49/332 [===>..........................] - ETA: 0s  97/332 [=======>......................] - ETA: 0s145/332 [============>.................] - ETA: 0s193/332 [================>.............] - ETA: 0s242/332 [====================>.........] - ETA: 0s291/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0015314646287095622
cosine 0.0012056254246438894
MAE: 0.0063998904
RMSE: 0.011664856
r2: 0.9911731535010034
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_42"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_43 (InputLayer)       multiple                  0         
                                                                 
 dense_42 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_42 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_42 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_43 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_43 (ReLU)             (None, 632)               0         
                                                                 
 dense_43 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_44 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_44 (ReLU)             (None, 2528)              0         
                                                                 
 dense_44 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_43 (InputLayer)       multiple                  0         
                                                                 
 dense_42 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_42 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_42 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_44"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_45 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_43 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_43 (ReLU)             (None, 632)               0         
                                                                 
 dense_43 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_44 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_44 (ReLU)             (None, 2528)              0         
                                                                 
 dense_44 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 64, 200, 0.001, 0.5, 632, 0.0001256306131836027, 0.00013606896391138434, 0.0015314646287095622, 0.0012056254246438894, 0.006399890407919884, 0.011664856225252151, 0.9911731535010034, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_45"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_46 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_45 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_45 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_45 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_46 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_46 (ReLU)             (None, 632)               0         
                                                                 
 dense_46 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_47 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_47 (ReLU)             (None, 2528)              0         
                                                                 
 dense_47 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/200
1493/1493 - 5s - loss: 0.0118 - val_loss: 0.0030 - 5s/epoch - 4ms/step
Epoch 2/200
1493/1493 - 4s - loss: 0.0029 - val_loss: 0.0029 - 4s/epoch - 3ms/step
Epoch 3/200
1493/1493 - 4s - loss: 0.0022 - val_loss: 0.0030 - 4s/epoch - 3ms/step
Epoch 4/200
1493/1493 - 4s - loss: 0.0020 - val_loss: 0.0015 - 4s/epoch - 3ms/step
Epoch 5/200
1493/1493 - 4s - loss: 0.0013 - val_loss: 0.0011 - 4s/epoch - 3ms/step
Epoch 6/200
1493/1493 - 4s - loss: 0.0011 - val_loss: 0.0023 - 4s/epoch - 3ms/step
Epoch 7/200
1493/1493 - 4s - loss: 0.0012 - val_loss: 8.3407e-04 - 4s/epoch - 3ms/step
Epoch 8/200
1493/1493 - 4s - loss: 8.6159e-04 - val_loss: 8.3082e-04 - 4s/epoch - 3ms/step
Epoch 9/200
1493/1493 - 4s - loss: 7.1861e-04 - val_loss: 6.8218e-04 - 4s/epoch - 3ms/step
Epoch 10/200
1493/1493 - 4s - loss: 6.3336e-04 - val_loss: 5.5763e-04 - 4s/epoch - 3ms/step
Epoch 11/200
1493/1493 - 4s - loss: 6.2524e-04 - val_loss: 8.6382e-04 - 4s/epoch - 3ms/step
Epoch 12/200
1493/1493 - 4s - loss: 5.5080e-04 - val_loss: 6.3547e-04 - 4s/epoch - 3ms/step
Epoch 13/200
1493/1493 - 4s - loss: 5.2614e-04 - val_loss: 0.0025 - 4s/epoch - 3ms/step
Epoch 14/200
1493/1493 - 4s - loss: 8.1041e-04 - val_loss: 6.2592e-04 - 4s/epoch - 3ms/step
Epoch 15/200
1493/1493 - 4s - loss: 5.4920e-04 - val_loss: 5.0672e-04 - 4s/epoch - 3ms/step
Epoch 16/200
1493/1493 - 4s - loss: 4.8379e-04 - val_loss: 4.4479e-04 - 4s/epoch - 3ms/step
Epoch 17/200
1493/1493 - 4s - loss: 4.4818e-04 - val_loss: 5.3886e-04 - 4s/epoch - 3ms/step
Epoch 18/200
1493/1493 - 4s - loss: 5.1176e-04 - val_loss: 0.0050 - 4s/epoch - 3ms/step
Epoch 19/200
1493/1493 - 4s - loss: 0.0016 - val_loss: 8.8669e-04 - 4s/epoch - 3ms/step
Epoch 20/200
1493/1493 - 4s - loss: 7.7693e-04 - val_loss: 9.9276e-04 - 4s/epoch - 3ms/step
Epoch 21/200
1493/1493 - 4s - loss: 7.1715e-04 - val_loss: 7.3175e-04 - 4s/epoch - 3ms/step
Epoch 22/200
1493/1493 - 4s - loss: 6.0718e-04 - val_loss: 8.1636e-04 - 4s/epoch - 3ms/step
Epoch 23/200
1493/1493 - 4s - loss: 6.2566e-04 - val_loss: 5.1662e-04 - 4s/epoch - 3ms/step
Epoch 24/200
1493/1493 - 4s - loss: 5.2413e-04 - val_loss: 8.3197e-04 - 4s/epoch - 3ms/step
Epoch 25/200
1493/1493 - 4s - loss: 5.3295e-04 - val_loss: 4.5584e-04 - 4s/epoch - 3ms/step
Epoch 26/200
1493/1493 - 4s - loss: 4.7373e-04 - val_loss: 4.3754e-04 - 4s/epoch - 3ms/step
Epoch 27/200
1493/1493 - 4s - loss: 4.5063e-04 - val_loss: 4.1989e-04 - 4s/epoch - 3ms/step
Epoch 28/200
1493/1493 - 4s - loss: 4.3506e-04 - val_loss: 3.9668e-04 - 4s/epoch - 3ms/step
Epoch 29/200
1493/1493 - 4s - loss: 4.2943e-04 - val_loss: 6.2351e-04 - 4s/epoch - 3ms/step
Epoch 30/200
1493/1493 - 4s - loss: 4.4192e-04 - val_loss: 4.0996e-04 - 4s/epoch - 3ms/step
Epoch 31/200
1493/1493 - 4s - loss: 4.0294e-04 - val_loss: 4.2365e-04 - 4s/epoch - 3ms/step
Epoch 32/200
1493/1493 - 4s - loss: 4.0215e-04 - val_loss: 4.3023e-04 - 4s/epoch - 3ms/step
Epoch 33/200
1493/1493 - 4s - loss: 4.1435e-04 - val_loss: 3.4469e-04 - 4s/epoch - 3ms/step
Epoch 34/200
1493/1493 - 4s - loss: 3.8197e-04 - val_loss: 3.2621e-04 - 4s/epoch - 3ms/step
Epoch 35/200
1493/1493 - 4s - loss: 3.7404e-04 - val_loss: 4.4483e-04 - 4s/epoch - 3ms/step
Epoch 36/200
1493/1493 - 4s - loss: 3.9058e-04 - val_loss: 3.5618e-04 - 4s/epoch - 3ms/step
Epoch 37/200
1493/1493 - 4s - loss: 3.7202e-04 - val_loss: 7.1314e-04 - 4s/epoch - 3ms/step
Epoch 38/200
1493/1493 - 4s - loss: 4.3623e-04 - val_loss: 3.1407e-04 - 4s/epoch - 3ms/step
Epoch 39/200
1493/1493 - 4s - loss: 3.6031e-04 - val_loss: 3.1543e-04 - 4s/epoch - 3ms/step
Epoch 40/200
1493/1493 - 4s - loss: 3.5140e-04 - val_loss: 3.4357e-04 - 4s/epoch - 3ms/step
Epoch 41/200
1493/1493 - 4s - loss: 3.6051e-04 - val_loss: 3.0148e-04 - 4s/epoch - 3ms/step
Epoch 42/200
1493/1493 - 4s - loss: 3.4749e-04 - val_loss: 2.9293e-04 - 4s/epoch - 3ms/step
Epoch 43/200
1493/1493 - 4s - loss: 3.4866e-04 - val_loss: 5.2404e-04 - 4s/epoch - 3ms/step
Epoch 44/200
1493/1493 - 4s - loss: 3.5050e-04 - val_loss: 3.8111e-04 - 4s/epoch - 3ms/step
Epoch 45/200
1493/1493 - 4s - loss: 3.4396e-04 - val_loss: 2.7945e-04 - 4s/epoch - 3ms/step
Epoch 46/200
1493/1493 - 4s - loss: 3.2569e-04 - val_loss: 2.9753e-04 - 4s/epoch - 3ms/step
Epoch 47/200
1493/1493 - 4s - loss: 3.3718e-04 - val_loss: 3.3410e-04 - 4s/epoch - 3ms/step
Epoch 48/200
1493/1493 - 4s - loss: 3.2689e-04 - val_loss: 6.3477e-04 - 4s/epoch - 3ms/step
Epoch 49/200
1493/1493 - 4s - loss: 3.8814e-04 - val_loss: 4.7133e-04 - 4s/epoch - 3ms/step
Epoch 50/200
1493/1493 - 4s - loss: 3.5757e-04 - val_loss: 3.2924e-04 - 4s/epoch - 3ms/step
Epoch 51/200
1493/1493 - 4s - loss: 3.4088e-04 - val_loss: 2.5553e-04 - 4s/epoch - 3ms/step
Epoch 52/200
1493/1493 - 4s - loss: 3.1504e-04 - val_loss: 2.8401e-04 - 4s/epoch - 3ms/step
Epoch 53/200
1493/1493 - 4s - loss: 3.1966e-04 - val_loss: 2.9355e-04 - 4s/epoch - 3ms/step
Epoch 54/200
1493/1493 - 4s - loss: 3.0922e-04 - val_loss: 3.0740e-04 - 4s/epoch - 3ms/step
Epoch 55/200
1493/1493 - 4s - loss: 3.0326e-04 - val_loss: 2.7692e-04 - 4s/epoch - 3ms/step
Epoch 56/200
1493/1493 - 4s - loss: 3.0025e-04 - val_loss: 2.8147e-04 - 4s/epoch - 3ms/step
Epoch 57/200
1493/1493 - 4s - loss: 3.0544e-04 - val_loss: 2.7463e-04 - 4s/epoch - 3ms/step
Epoch 58/200
1493/1493 - 4s - loss: 3.0010e-04 - val_loss: 2.5168e-04 - 4s/epoch - 3ms/step
Epoch 59/200
1493/1493 - 4s - loss: 2.9510e-04 - val_loss: 2.7498e-04 - 4s/epoch - 3ms/step
Epoch 60/200
1493/1493 - 4s - loss: 2.9279e-04 - val_loss: 2.9313e-04 - 4s/epoch - 3ms/step
Epoch 61/200
1493/1493 - 4s - loss: 2.9740e-04 - val_loss: 2.6339e-04 - 4s/epoch - 3ms/step
Epoch 62/200
1493/1493 - 4s - loss: 2.8818e-04 - val_loss: 2.8723e-04 - 4s/epoch - 3ms/step
Epoch 63/200
1493/1493 - 4s - loss: 2.9345e-04 - val_loss: 2.7532e-04 - 4s/epoch - 3ms/step
Epoch 64/200
1493/1493 - 4s - loss: 2.8687e-04 - val_loss: 9.6931e-04 - 4s/epoch - 3ms/step
Epoch 65/200
1493/1493 - 4s - loss: 3.7938e-04 - val_loss: 8.1454e-04 - 4s/epoch - 3ms/step
Epoch 66/200
1493/1493 - 4s - loss: 3.4301e-04 - val_loss: 2.9673e-04 - 4s/epoch - 3ms/step
Epoch 67/200
1493/1493 - 4s - loss: 2.9796e-04 - val_loss: 2.3959e-04 - 4s/epoch - 3ms/step
Epoch 68/200
1493/1493 - 4s - loss: 2.9186e-04 - val_loss: 4.8892e-04 - 4s/epoch - 3ms/step
Epoch 69/200
1493/1493 - 4s - loss: 3.4751e-04 - val_loss: 2.6282e-04 - 4s/epoch - 3ms/step
Epoch 70/200
1493/1493 - 4s - loss: 2.9363e-04 - val_loss: 3.1685e-04 - 4s/epoch - 3ms/step
Epoch 71/200
1493/1493 - 4s - loss: 3.0460e-04 - val_loss: 2.6012e-04 - 4s/epoch - 3ms/step
Epoch 72/200
1493/1493 - 4s - loss: 2.8954e-04 - val_loss: 3.1750e-04 - 4s/epoch - 3ms/step
Epoch 73/200
1493/1493 - 4s - loss: 2.8947e-04 - val_loss: 3.1056e-04 - 4s/epoch - 3ms/step
Epoch 74/200
1493/1493 - 4s - loss: 3.0582e-04 - val_loss: 2.8547e-04 - 4s/epoch - 3ms/step
Epoch 75/200
1493/1493 - 4s - loss: 2.8461e-04 - val_loss: 2.5041e-04 - 4s/epoch - 3ms/step
Epoch 76/200
1493/1493 - 4s - loss: 2.8379e-04 - val_loss: 2.7851e-04 - 4s/epoch - 3ms/step
Epoch 77/200
1493/1493 - 4s - loss: 2.8410e-04 - val_loss: 4.4215e-04 - 4s/epoch - 3ms/step
Epoch 78/200
1493/1493 - 4s - loss: 3.2995e-04 - val_loss: 2.5201e-04 - 4s/epoch - 3ms/step
Epoch 79/200
1493/1493 - 4s - loss: 2.8186e-04 - val_loss: 4.1998e-04 - 4s/epoch - 3ms/step
Epoch 80/200
1493/1493 - 4s - loss: 3.1098e-04 - val_loss: 2.3862e-04 - 4s/epoch - 3ms/step
Epoch 81/200
1493/1493 - 4s - loss: 2.7425e-04 - val_loss: 3.6088e-04 - 4s/epoch - 3ms/step
Epoch 82/200
1493/1493 - 4s - loss: 2.7283e-04 - val_loss: 2.4102e-04 - 4s/epoch - 3ms/step
Epoch 83/200
1493/1493 - 4s - loss: 2.6769e-04 - val_loss: 2.3855e-04 - 4s/epoch - 3ms/step
Epoch 84/200
1493/1493 - 4s - loss: 2.6817e-04 - val_loss: 4.5736e-04 - 4s/epoch - 3ms/step
Epoch 85/200
1493/1493 - 4s - loss: 3.2748e-04 - val_loss: 4.0842e-04 - 4s/epoch - 3ms/step
Epoch 86/200
1493/1493 - 4s - loss: 2.9081e-04 - val_loss: 2.3740e-04 - 4s/epoch - 3ms/step
Epoch 87/200
1493/1493 - 4s - loss: 2.6627e-04 - val_loss: 2.7624e-04 - 4s/epoch - 3ms/step
Epoch 88/200
1493/1493 - 4s - loss: 2.6590e-04 - val_loss: 3.6340e-04 - 4s/epoch - 3ms/step
Epoch 89/200
1493/1493 - 4s - loss: 3.0543e-04 - val_loss: 2.3593e-04 - 4s/epoch - 3ms/step
Epoch 90/200
1493/1493 - 4s - loss: 2.6767e-04 - val_loss: 2.4222e-04 - 4s/epoch - 3ms/step
Epoch 91/200
1493/1493 - 4s - loss: 2.5999e-04 - val_loss: 2.4417e-04 - 4s/epoch - 3ms/step
Epoch 92/200
1493/1493 - 4s - loss: 2.5958e-04 - val_loss: 3.8746e-04 - 4s/epoch - 3ms/step
Epoch 93/200
1493/1493 - 4s - loss: 2.9263e-04 - val_loss: 2.3890e-04 - 4s/epoch - 3ms/step
Epoch 94/200
1493/1493 - 4s - loss: 2.5971e-04 - val_loss: 2.1466e-04 - 4s/epoch - 3ms/step
Epoch 95/200
1493/1493 - 4s - loss: 2.6766e-04 - val_loss: 3.0230e-04 - 4s/epoch - 3ms/step
Epoch 96/200
1493/1493 - 4s - loss: 2.6401e-04 - val_loss: 3.3747e-04 - 4s/epoch - 3ms/step
Epoch 97/200
1493/1493 - 4s - loss: 2.6103e-04 - val_loss: 2.8061e-04 - 4s/epoch - 3ms/step
Epoch 98/200
1493/1493 - 4s - loss: 2.5880e-04 - val_loss: 5.1410e-04 - 4s/epoch - 3ms/step
Epoch 99/200
1493/1493 - 4s - loss: 3.1431e-04 - val_loss: 2.8790e-04 - 4s/epoch - 3ms/step
Epoch 100/200
1493/1493 - 4s - loss: 2.7391e-04 - val_loss: 2.2834e-04 - 4s/epoch - 3ms/step
Epoch 101/200
1493/1493 - 4s - loss: 2.5997e-04 - val_loss: 4.6362e-04 - 4s/epoch - 3ms/step
Epoch 102/200
1493/1493 - 4s - loss: 2.9156e-04 - val_loss: 3.8398e-04 - 4s/epoch - 3ms/step
Epoch 103/200
1493/1493 - 4s - loss: 3.0628e-04 - val_loss: 2.4153e-04 - 4s/epoch - 3ms/step
Epoch 104/200
1493/1493 - 4s - loss: 2.6063e-04 - val_loss: 4.5599e-04 - 4s/epoch - 3ms/step
Epoch 105/200
1493/1493 - 4s - loss: 2.9275e-04 - val_loss: 2.2903e-04 - 4s/epoch - 3ms/step
Epoch 106/200
1493/1493 - 4s - loss: 2.5390e-04 - val_loss: 2.9564e-04 - 4s/epoch - 3ms/step
Epoch 107/200
1493/1493 - 4s - loss: 2.6585e-04 - val_loss: 2.6494e-04 - 4s/epoch - 3ms/step
Epoch 108/200
1493/1493 - 4s - loss: 2.5360e-04 - val_loss: 2.3527e-04 - 4s/epoch - 3ms/step
Epoch 109/200
1493/1493 - 4s - loss: 2.4964e-04 - val_loss: 2.4466e-04 - 4s/epoch - 3ms/step
Epoch 110/200
1493/1493 - 4s - loss: 2.4665e-04 - val_loss: 3.6021e-04 - 4s/epoch - 3ms/step
Epoch 111/200
1493/1493 - 4s - loss: 2.8094e-04 - val_loss: 2.1693e-04 - 4s/epoch - 3ms/step
Epoch 112/200
1493/1493 - 4s - loss: 2.4780e-04 - val_loss: 2.1243e-04 - 4s/epoch - 3ms/step
Epoch 113/200
1493/1493 - 4s - loss: 2.4619e-04 - val_loss: 4.5461e-04 - 4s/epoch - 3ms/step
Epoch 114/200
1493/1493 - 4s - loss: 3.1764e-04 - val_loss: 3.3409e-04 - 4s/epoch - 3ms/step
Epoch 115/200
1493/1493 - 4s - loss: 2.8051e-04 - val_loss: 3.4218e-04 - 4s/epoch - 3ms/step
Epoch 116/200
1493/1493 - 4s - loss: 2.6265e-04 - val_loss: 2.6988e-04 - 4s/epoch - 3ms/step
Epoch 117/200
1493/1493 - 4s - loss: 2.5442e-04 - val_loss: 2.4868e-04 - 4s/epoch - 3ms/step
Epoch 118/200
1493/1493 - 4s - loss: 2.5287e-04 - val_loss: 2.0574e-04 - 4s/epoch - 3ms/step
Epoch 119/200
1493/1493 - 4s - loss: 2.4327e-04 - val_loss: 2.1456e-04 - 4s/epoch - 3ms/step
Epoch 120/200
1493/1493 - 4s - loss: 2.4257e-04 - val_loss: 2.0772e-04 - 4s/epoch - 3ms/step
Epoch 121/200
1493/1493 - 4s - loss: 2.4070e-04 - val_loss: 2.8437e-04 - 4s/epoch - 3ms/step
Epoch 122/200
1493/1493 - 4s - loss: 2.4647e-04 - val_loss: 3.2555e-04 - 4s/epoch - 3ms/step
Epoch 123/200
1493/1493 - 4s - loss: 2.6150e-04 - val_loss: 3.2496e-04 - 4s/epoch - 3ms/step
Epoch 124/200
1493/1493 - 4s - loss: 2.5190e-04 - val_loss: 2.2948e-04 - 4s/epoch - 3ms/step
Epoch 125/200
1493/1493 - 4s - loss: 2.3951e-04 - val_loss: 2.2896e-04 - 4s/epoch - 3ms/step
Epoch 126/200
1493/1493 - 4s - loss: 2.3818e-04 - val_loss: 4.1000e-04 - 4s/epoch - 3ms/step
Epoch 127/200
1493/1493 - 4s - loss: 2.7949e-04 - val_loss: 3.4176e-04 - 4s/epoch - 3ms/step
Epoch 128/200
1493/1493 - 4s - loss: 2.4838e-04 - val_loss: 2.2898e-04 - 4s/epoch - 3ms/step
Epoch 129/200
1493/1493 - 4s - loss: 2.4058e-04 - val_loss: 2.0282e-04 - 4s/epoch - 3ms/step
Epoch 130/200
1493/1493 - 4s - loss: 2.3567e-04 - val_loss: 1.9752e-04 - 4s/epoch - 3ms/step
Epoch 131/200
1493/1493 - 4s - loss: 2.3457e-04 - val_loss: 4.2683e-04 - 4s/epoch - 3ms/step
Epoch 132/200
1493/1493 - 4s - loss: 2.6915e-04 - val_loss: 2.2404e-04 - 4s/epoch - 3ms/step
Epoch 133/200
1493/1493 - 4s - loss: 2.4223e-04 - val_loss: 3.4981e-04 - 4s/epoch - 3ms/step
Epoch 134/200
1493/1493 - 4s - loss: 2.5144e-04 - val_loss: 2.5374e-04 - 4s/epoch - 3ms/step
Epoch 135/200
1493/1493 - 4s - loss: 2.3859e-04 - val_loss: 2.1704e-04 - 4s/epoch - 3ms/step
Epoch 136/200
1493/1493 - 4s - loss: 2.3180e-04 - val_loss: 2.2748e-04 - 4s/epoch - 3ms/step
Epoch 137/200
1493/1493 - 4s - loss: 2.3653e-04 - val_loss: 2.3995e-04 - 4s/epoch - 3ms/step
Epoch 138/200
1493/1493 - 4s - loss: 2.3503e-04 - val_loss: 3.6339e-04 - 4s/epoch - 3ms/step
Epoch 139/200
1493/1493 - 4s - loss: 2.6842e-04 - val_loss: 3.6317e-04 - 4s/epoch - 3ms/step
Epoch 140/200
1493/1493 - 4s - loss: 2.4565e-04 - val_loss: 2.1542e-04 - 4s/epoch - 3ms/step
Epoch 141/200
1493/1493 - 4s - loss: 2.3400e-04 - val_loss: 4.7523e-04 - 4s/epoch - 3ms/step
Epoch 142/200
1493/1493 - 4s - loss: 2.8917e-04 - val_loss: 2.1510e-04 - 4s/epoch - 3ms/step
Epoch 143/200
1493/1493 - 4s - loss: 2.3572e-04 - val_loss: 2.2970e-04 - 4s/epoch - 3ms/step
Epoch 144/200
1493/1493 - 4s - loss: 2.4100e-04 - val_loss: 1.9673e-04 - 4s/epoch - 3ms/step
Epoch 145/200
1493/1493 - 4s - loss: 2.2903e-04 - val_loss: 2.1188e-04 - 4s/epoch - 3ms/step
Epoch 146/200
1493/1493 - 4s - loss: 2.2928e-04 - val_loss: 2.1837e-04 - 4s/epoch - 3ms/step
Epoch 147/200
1493/1493 - 4s - loss: 2.6291e-04 - val_loss: 2.1179e-04 - 4s/epoch - 3ms/step
Epoch 148/200
1493/1493 - 4s - loss: 2.2893e-04 - val_loss: 2.5397e-04 - 4s/epoch - 3ms/step
Epoch 149/200
1493/1493 - 4s - loss: 2.2665e-04 - val_loss: 2.2111e-04 - 4s/epoch - 3ms/step
Epoch 150/200
1493/1493 - 4s - loss: 2.2548e-04 - val_loss: 2.1761e-04 - 4s/epoch - 3ms/step
Epoch 151/200
1493/1493 - 4s - loss: 2.2343e-04 - val_loss: 2.5175e-04 - 4s/epoch - 3ms/step
Epoch 152/200
1493/1493 - 4s - loss: 2.2786e-04 - val_loss: 4.0242e-04 - 4s/epoch - 3ms/step
Epoch 153/200
1493/1493 - 4s - loss: 2.4837e-04 - val_loss: 2.1024e-04 - 4s/epoch - 3ms/step
Epoch 154/200
1493/1493 - 4s - loss: 2.2364e-04 - val_loss: 2.3197e-04 - 4s/epoch - 3ms/step
Epoch 155/200
1493/1493 - 4s - loss: 2.1972e-04 - val_loss: 2.3207e-04 - 4s/epoch - 3ms/step
Epoch 156/200
1493/1493 - 4s - loss: 2.2315e-04 - val_loss: 2.1192e-04 - 4s/epoch - 3ms/step
Epoch 157/200
1493/1493 - 4s - loss: 2.1948e-04 - val_loss: 3.4375e-04 - 4s/epoch - 3ms/step
Epoch 158/200
1493/1493 - 4s - loss: 2.5281e-04 - val_loss: 2.1054e-04 - 4s/epoch - 3ms/step
Epoch 159/200
1493/1493 - 4s - loss: 2.2175e-04 - val_loss: 2.0808e-04 - 4s/epoch - 3ms/step
Epoch 160/200
1493/1493 - 4s - loss: 2.2085e-04 - val_loss: 3.0579e-04 - 4s/epoch - 3ms/step
Epoch 161/200
1493/1493 - 4s - loss: 2.5040e-04 - val_loss: 3.1144e-04 - 4s/epoch - 3ms/step
Epoch 162/200
1493/1493 - 4s - loss: 2.3908e-04 - val_loss: 1.9678e-04 - 4s/epoch - 3ms/step
Epoch 163/200
1493/1493 - 4s - loss: 2.2451e-04 - val_loss: 3.8757e-04 - 4s/epoch - 3ms/step
Epoch 164/200
1493/1493 - 4s - loss: 2.4375e-04 - val_loss: 2.3510e-04 - 4s/epoch - 3ms/step
Epoch 165/200
1493/1493 - 4s - loss: 2.2509e-04 - val_loss: 2.0247e-04 - 4s/epoch - 3ms/step
Epoch 166/200
1493/1493 - 4s - loss: 2.1864e-04 - val_loss: 2.2060e-04 - 4s/epoch - 3ms/step
Epoch 167/200
1493/1493 - 4s - loss: 2.1653e-04 - val_loss: 2.0190e-04 - 4s/epoch - 3ms/step
Epoch 168/200
1493/1493 - 4s - loss: 2.1641e-04 - val_loss: 1.9999e-04 - 4s/epoch - 3ms/step
Epoch 169/200
1493/1493 - 4s - loss: 2.1970e-04 - val_loss: 5.5647e-04 - 4s/epoch - 3ms/step
Epoch 170/200
1493/1493 - 4s - loss: 2.7045e-04 - val_loss: 3.9930e-04 - 4s/epoch - 3ms/step
Epoch 171/200
1493/1493 - 4s - loss: 2.6527e-04 - val_loss: 3.2087e-04 - 4s/epoch - 3ms/step
Epoch 172/200
1493/1493 - 4s - loss: 2.3853e-04 - val_loss: 2.1178e-04 - 4s/epoch - 3ms/step
Epoch 173/200
1493/1493 - 4s - loss: 2.2497e-04 - val_loss: 2.4774e-04 - 4s/epoch - 3ms/step
Epoch 174/200
1493/1493 - 4s - loss: 2.3288e-04 - val_loss: 2.1355e-04 - 4s/epoch - 3ms/step
Epoch 175/200
1493/1493 - 4s - loss: 2.2833e-04 - val_loss: 2.4030e-04 - 4s/epoch - 3ms/step
Epoch 176/200
1493/1493 - 4s - loss: 2.2258e-04 - val_loss: 2.0267e-04 - 4s/epoch - 3ms/step
Epoch 177/200
1493/1493 - 4s - loss: 2.1817e-04 - val_loss: 2.0033e-04 - 4s/epoch - 3ms/step
Epoch 178/200
1493/1493 - 4s - loss: 2.1770e-04 - val_loss: 1.9419e-04 - 4s/epoch - 3ms/step
Epoch 179/200
1493/1493 - 4s - loss: 2.1458e-04 - val_loss: 2.9932e-04 - 4s/epoch - 3ms/step
Epoch 180/200
1493/1493 - 4s - loss: 2.3179e-04 - val_loss: 3.5218e-04 - 4s/epoch - 3ms/step
Epoch 181/200
1493/1493 - 4s - loss: 2.6400e-04 - val_loss: 2.3630e-04 - 4s/epoch - 3ms/step
Epoch 182/200
1493/1493 - 4s - loss: 2.2500e-04 - val_loss: 2.1662e-04 - 4s/epoch - 3ms/step
Epoch 183/200
1493/1493 - 4s - loss: 2.1609e-04 - val_loss: 2.0672e-04 - 4s/epoch - 3ms/step
Epoch 184/200
1493/1493 - 4s - loss: 2.1368e-04 - val_loss: 1.9162e-04 - 4s/epoch - 3ms/step
Epoch 185/200
1493/1493 - 4s - loss: 2.1195e-04 - val_loss: 2.5806e-04 - 4s/epoch - 3ms/step
Epoch 186/200
1493/1493 - 4s - loss: 2.1601e-04 - val_loss: 2.1054e-04 - 4s/epoch - 3ms/step
Epoch 187/200
1493/1493 - 4s - loss: 2.1491e-04 - val_loss: 1.9736e-04 - 4s/epoch - 3ms/step
Epoch 188/200
1493/1493 - 4s - loss: 2.0988e-04 - val_loss: 2.2249e-04 - 4s/epoch - 3ms/step
Epoch 189/200
1493/1493 - 4s - loss: 2.2846e-04 - val_loss: 2.2677e-04 - 4s/epoch - 3ms/step
Epoch 190/200
1493/1493 - 4s - loss: 2.1567e-04 - val_loss: 2.1451e-04 - 4s/epoch - 3ms/step
Epoch 191/200
1493/1493 - 4s - loss: 2.1809e-04 - val_loss: 8.7402e-04 - 4s/epoch - 3ms/step
Epoch 192/200
1493/1493 - 4s - loss: 3.0163e-04 - val_loss: 4.2163e-04 - 4s/epoch - 3ms/step
Epoch 193/200
1493/1493 - 4s - loss: 2.6295e-04 - val_loss: 5.1167e-04 - 4s/epoch - 3ms/step
Epoch 194/200
1493/1493 - 4s - loss: 2.5227e-04 - val_loss: 3.3020e-04 - 4s/epoch - 3ms/step
Epoch 195/200
1493/1493 - 4s - loss: 2.3163e-04 - val_loss: 2.3643e-04 - 4s/epoch - 3ms/step
Epoch 196/200
1493/1493 - 4s - loss: 2.1947e-04 - val_loss: 2.0413e-04 - 4s/epoch - 3ms/step
Epoch 197/200
1493/1493 - 4s - loss: 2.1279e-04 - val_loss: 2.7420e-04 - 4s/epoch - 3ms/step
Epoch 198/200
1493/1493 - 4s - loss: 2.1559e-04 - val_loss: 2.6154e-04 - 4s/epoch - 3ms/step
Epoch 199/200
1493/1493 - 4s - loss: 2.1267e-04 - val_loss: 1.9576e-04 - 4s/epoch - 3ms/step
Epoch 200/200
1493/1493 - 4s - loss: 2.0931e-04 - val_loss: 2.4666e-04 - 4s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.0002466598816681653
  1/332 [..............................] - ETA: 33s 49/332 [===>..........................] - ETA: 0s  97/332 [=======>......................] - ETA: 0s145/332 [============>.................] - ETA: 0s193/332 [================>.............] - ETA: 0s241/332 [====================>.........] - ETA: 0s289/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0028267343550684586
cosine 0.0022430490973397465
MAE: 0.009124211
RMSE: 0.015705403
r2: 0.9839994297413562
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_45"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_46 (InputLayer)       multiple                  0         
                                                                 
 dense_45 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_45 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_45 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_46 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_46 (ReLU)             (None, 632)               0         
                                                                 
 dense_46 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_47 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_47 (ReLU)             (None, 2528)              0         
                                                                 
 dense_47 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_46"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_47 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_46 (InputLayer)       multiple                  0         
                                                                 
 dense_45 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_45 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_45 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_47"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_48 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_46 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_46 (ReLU)             (None, 632)               0         
                                                                 
 dense_46 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_47 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_47 (ReLU)             (None, 2528)              0         
                                                                 
 dense_47 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 64, 200, 0.002, 0.5, 632, 0.0002093141811201349, 0.0002466598816681653, 0.0028267343550684586, 0.0022430490973397465, 0.00912421103566885, 0.015705402940511703, 0.9839994297413562, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_48"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_49 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_48 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_48 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_48 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_49 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_49 (ReLU)             (None, 632)               0         
                                                                 
 dense_49 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_50 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_50 (ReLU)             (None, 2528)              0         
                                                                 
 dense_50 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/300
1493/1493 - 5s - loss: 0.0094 - val_loss: 0.0046 - 5s/epoch - 4ms/step
Epoch 2/300
1493/1493 - 4s - loss: 0.0030 - val_loss: 0.0031 - 4s/epoch - 3ms/step
Epoch 3/300
1493/1493 - 4s - loss: 0.0021 - val_loss: 0.0017 - 4s/epoch - 3ms/step
Epoch 4/300
1493/1493 - 4s - loss: 0.0017 - val_loss: 0.0034 - 4s/epoch - 3ms/step
Epoch 5/300
1493/1493 - 4s - loss: 0.0016 - val_loss: 0.0015 - 4s/epoch - 3ms/step
Epoch 6/300
1493/1493 - 4s - loss: 0.0014 - val_loss: 0.0014 - 4s/epoch - 3ms/step
Epoch 7/300
1493/1493 - 4s - loss: 0.0013 - val_loss: 0.0015 - 4s/epoch - 3ms/step
Epoch 8/300
1493/1493 - 4s - loss: 0.0012 - val_loss: 0.0010 - 4s/epoch - 3ms/step
Epoch 9/300
1493/1493 - 4s - loss: 0.0010 - val_loss: 0.0010 - 4s/epoch - 3ms/step
Epoch 10/300
1493/1493 - 4s - loss: 9.8659e-04 - val_loss: 9.4479e-04 - 4s/epoch - 3ms/step
Epoch 11/300
1493/1493 - 4s - loss: 9.7866e-04 - val_loss: 7.5134e-04 - 4s/epoch - 3ms/step
Epoch 12/300
1493/1493 - 4s - loss: 7.6689e-04 - val_loss: 0.0017 - 4s/epoch - 3ms/step
Epoch 13/300
1493/1493 - 4s - loss: 7.9271e-04 - val_loss: 0.0017 - 4s/epoch - 3ms/step
Epoch 14/300
1493/1493 - 4s - loss: 8.2156e-04 - val_loss: 5.8765e-04 - 4s/epoch - 3ms/step
Epoch 15/300
1493/1493 - 4s - loss: 6.6740e-04 - val_loss: 7.4470e-04 - 4s/epoch - 3ms/step
Epoch 16/300
1493/1493 - 4s - loss: 6.1484e-04 - val_loss: 5.3455e-04 - 4s/epoch - 3ms/step
Epoch 17/300
1493/1493 - 4s - loss: 5.5245e-04 - val_loss: 9.7225e-04 - 4s/epoch - 3ms/step
Epoch 18/300
1493/1493 - 4s - loss: 5.7294e-04 - val_loss: 7.0497e-04 - 4s/epoch - 3ms/step
Epoch 19/300
1493/1493 - 4s - loss: 5.1113e-04 - val_loss: 6.6721e-04 - 4s/epoch - 3ms/step
Epoch 20/300
1493/1493 - 4s - loss: 4.9719e-04 - val_loss: 4.2471e-04 - 4s/epoch - 3ms/step
Epoch 21/300
1493/1493 - 4s - loss: 4.4839e-04 - val_loss: 5.8332e-04 - 4s/epoch - 3ms/step
Epoch 22/300
1493/1493 - 4s - loss: 4.3431e-04 - val_loss: 4.7813e-04 - 4s/epoch - 3ms/step
Epoch 23/300
1493/1493 - 4s - loss: 4.1556e-04 - val_loss: 6.6160e-04 - 4s/epoch - 3ms/step
Epoch 24/300
1493/1493 - 4s - loss: 4.0984e-04 - val_loss: 3.9099e-04 - 4s/epoch - 3ms/step
Epoch 25/300
1493/1493 - 4s - loss: 3.7250e-04 - val_loss: 3.5295e-04 - 4s/epoch - 3ms/step
Epoch 26/300
1493/1493 - 4s - loss: 3.6135e-04 - val_loss: 4.0583e-04 - 4s/epoch - 3ms/step
Epoch 27/300
1493/1493 - 4s - loss: 3.4594e-04 - val_loss: 3.2248e-04 - 4s/epoch - 3ms/step
Epoch 28/300
1493/1493 - 4s - loss: 3.2952e-04 - val_loss: 4.8310e-04 - 4s/epoch - 3ms/step
Epoch 29/300
1493/1493 - 4s - loss: 3.1899e-04 - val_loss: 5.5510e-04 - 4s/epoch - 3ms/step
Epoch 30/300
1493/1493 - 4s - loss: 3.3821e-04 - val_loss: 3.5436e-04 - 4s/epoch - 3ms/step
Epoch 31/300
1493/1493 - 4s - loss: 3.0274e-04 - val_loss: 4.9010e-04 - 4s/epoch - 3ms/step
Epoch 32/300
1493/1493 - 4s - loss: 3.1453e-04 - val_loss: 4.1947e-04 - 4s/epoch - 3ms/step
Epoch 33/300
1493/1493 - 4s - loss: 3.0240e-04 - val_loss: 2.7669e-04 - 4s/epoch - 3ms/step
Epoch 34/300
1493/1493 - 4s - loss: 2.8072e-04 - val_loss: 2.3535e-04 - 4s/epoch - 3ms/step
Epoch 35/300
1493/1493 - 4s - loss: 2.7097e-04 - val_loss: 8.8133e-04 - 4s/epoch - 3ms/step
Epoch 36/300
1493/1493 - 4s - loss: 3.0892e-04 - val_loss: 3.1568e-04 - 4s/epoch - 3ms/step
Epoch 37/300
1493/1493 - 4s - loss: 2.7238e-04 - val_loss: 2.4754e-04 - 4s/epoch - 3ms/step
Epoch 38/300
1493/1493 - 4s - loss: 2.6026e-04 - val_loss: 2.5857e-04 - 4s/epoch - 3ms/step
Epoch 39/300
1493/1493 - 4s - loss: 2.4968e-04 - val_loss: 2.5617e-04 - 4s/epoch - 3ms/step
Epoch 40/300
1493/1493 - 4s - loss: 2.4030e-04 - val_loss: 3.1551e-04 - 4s/epoch - 3ms/step
Epoch 41/300
1493/1493 - 4s - loss: 2.4078e-04 - val_loss: 2.1324e-04 - 4s/epoch - 3ms/step
Epoch 42/300
1493/1493 - 4s - loss: 2.3522e-04 - val_loss: 2.4895e-04 - 4s/epoch - 3ms/step
Epoch 43/300
1493/1493 - 4s - loss: 2.3657e-04 - val_loss: 6.1135e-04 - 4s/epoch - 3ms/step
Epoch 44/300
1493/1493 - 4s - loss: 2.5857e-04 - val_loss: 5.0825e-04 - 4s/epoch - 3ms/step
Epoch 45/300
1493/1493 - 4s - loss: 2.4821e-04 - val_loss: 2.0059e-04 - 4s/epoch - 3ms/step
Epoch 46/300
1493/1493 - 4s - loss: 2.1960e-04 - val_loss: 2.0798e-04 - 4s/epoch - 3ms/step
Epoch 47/300
1493/1493 - 4s - loss: 2.2667e-04 - val_loss: 2.8634e-04 - 4s/epoch - 3ms/step
Epoch 48/300
1493/1493 - 4s - loss: 2.1655e-04 - val_loss: 2.4596e-04 - 4s/epoch - 3ms/step
Epoch 49/300
1493/1493 - 4s - loss: 2.1584e-04 - val_loss: 0.0015 - 4s/epoch - 3ms/step
Epoch 50/300
1493/1493 - 4s - loss: 3.2747e-04 - val_loss: 0.0016 - 4s/epoch - 3ms/step
Epoch 51/300
1493/1493 - 4s - loss: 3.0053e-04 - val_loss: 2.0852e-04 - 4s/epoch - 3ms/step
Epoch 52/300
1493/1493 - 4s - loss: 2.1135e-04 - val_loss: 2.2665e-04 - 4s/epoch - 3ms/step
Epoch 53/300
1493/1493 - 4s - loss: 2.0849e-04 - val_loss: 2.0632e-04 - 4s/epoch - 3ms/step
Epoch 54/300
1493/1493 - 4s - loss: 2.0098e-04 - val_loss: 2.3290e-04 - 4s/epoch - 3ms/step
Epoch 55/300
1493/1493 - 4s - loss: 1.9665e-04 - val_loss: 1.9730e-04 - 4s/epoch - 3ms/step
Epoch 56/300
1493/1493 - 4s - loss: 1.9527e-04 - val_loss: 2.4194e-04 - 4s/epoch - 3ms/step
Epoch 57/300
1493/1493 - 4s - loss: 1.9050e-04 - val_loss: 2.0637e-04 - 4s/epoch - 3ms/step
Epoch 58/300
1493/1493 - 4s - loss: 1.8975e-04 - val_loss: 1.6775e-04 - 4s/epoch - 3ms/step
Epoch 59/300
1493/1493 - 4s - loss: 1.8400e-04 - val_loss: 2.0070e-04 - 4s/epoch - 3ms/step
Epoch 60/300
1493/1493 - 4s - loss: 1.8373e-04 - val_loss: 4.2107e-04 - 4s/epoch - 3ms/step
Epoch 61/300
1493/1493 - 4s - loss: 2.5838e-04 - val_loss: 1.6730e-04 - 4s/epoch - 3ms/step
Epoch 62/300
1493/1493 - 4s - loss: 1.8292e-04 - val_loss: 1.8617e-04 - 4s/epoch - 3ms/step
Epoch 63/300
1493/1493 - 4s - loss: 1.7976e-04 - val_loss: 2.2853e-04 - 4s/epoch - 3ms/step
Epoch 64/300
1493/1493 - 4s - loss: 1.8154e-04 - val_loss: 0.0014 - 4s/epoch - 3ms/step
Epoch 65/300
1493/1493 - 4s - loss: 3.2991e-04 - val_loss: 4.5100e-04 - 4s/epoch - 3ms/step
Epoch 66/300
1493/1493 - 4s - loss: 2.1300e-04 - val_loss: 1.8512e-04 - 4s/epoch - 3ms/step
Epoch 67/300
1493/1493 - 4s - loss: 1.8141e-04 - val_loss: 1.7111e-04 - 4s/epoch - 3ms/step
Epoch 68/300
1493/1493 - 4s - loss: 1.7603e-04 - val_loss: 1.7785e-04 - 4s/epoch - 3ms/step
Epoch 69/300
1493/1493 - 4s - loss: 1.8279e-04 - val_loss: 1.9990e-04 - 4s/epoch - 3ms/step
Epoch 70/300
1493/1493 - 4s - loss: 1.7251e-04 - val_loss: 5.8635e-04 - 4s/epoch - 3ms/step
Epoch 71/300
1493/1493 - 4s - loss: 2.1531e-04 - val_loss: 1.6220e-04 - 4s/epoch - 3ms/step
Epoch 72/300
1493/1493 - 4s - loss: 1.7231e-04 - val_loss: 2.5000e-04 - 4s/epoch - 3ms/step
Epoch 73/300
1493/1493 - 4s - loss: 1.7357e-04 - val_loss: 2.5751e-04 - 4s/epoch - 3ms/step
Epoch 74/300
1493/1493 - 4s - loss: 1.7702e-04 - val_loss: 1.5920e-04 - 4s/epoch - 3ms/step
Epoch 75/300
1493/1493 - 4s - loss: 1.6084e-04 - val_loss: 1.6951e-04 - 4s/epoch - 3ms/step
Epoch 76/300
1493/1493 - 4s - loss: 1.6089e-04 - val_loss: 1.5770e-04 - 4s/epoch - 3ms/step
Epoch 77/300
1493/1493 - 4s - loss: 1.5558e-04 - val_loss: 5.5390e-04 - 4s/epoch - 3ms/step
Epoch 78/300
1493/1493 - 4s - loss: 1.7460e-04 - val_loss: 1.4787e-04 - 4s/epoch - 3ms/step
Epoch 79/300
1493/1493 - 4s - loss: 1.5517e-04 - val_loss: 2.0638e-04 - 4s/epoch - 3ms/step
Epoch 80/300
1493/1493 - 4s - loss: 1.6323e-04 - val_loss: 1.7459e-04 - 4s/epoch - 3ms/step
Epoch 81/300
1493/1493 - 4s - loss: 1.5514e-04 - val_loss: 3.8546e-04 - 4s/epoch - 3ms/step
Epoch 82/300
1493/1493 - 4s - loss: 1.5688e-04 - val_loss: 1.5697e-04 - 4s/epoch - 3ms/step
Epoch 83/300
1493/1493 - 4s - loss: 1.4808e-04 - val_loss: 1.5797e-04 - 4s/epoch - 3ms/step
Epoch 84/300
1493/1493 - 4s - loss: 1.4694e-04 - val_loss: 1.6249e-04 - 4s/epoch - 3ms/step
Epoch 85/300
1493/1493 - 4s - loss: 1.4947e-04 - val_loss: 2.1208e-04 - 4s/epoch - 3ms/step
Epoch 86/300
1493/1493 - 4s - loss: 1.5709e-04 - val_loss: 1.7211e-04 - 4s/epoch - 3ms/step
Epoch 87/300
1493/1493 - 4s - loss: 1.4450e-04 - val_loss: 1.4398e-04 - 4s/epoch - 3ms/step
Epoch 88/300
1493/1493 - 4s - loss: 1.4456e-04 - val_loss: 2.0729e-04 - 4s/epoch - 3ms/step
Epoch 89/300
1493/1493 - 4s - loss: 1.4945e-04 - val_loss: 1.6063e-04 - 4s/epoch - 3ms/step
Epoch 90/300
1493/1493 - 4s - loss: 1.4137e-04 - val_loss: 1.7167e-04 - 4s/epoch - 3ms/step
Epoch 91/300
1493/1493 - 4s - loss: 1.3954e-04 - val_loss: 1.6271e-04 - 4s/epoch - 3ms/step
Epoch 92/300
1493/1493 - 4s - loss: 1.4418e-04 - val_loss: 6.7521e-04 - 4s/epoch - 3ms/step
Epoch 93/300
1493/1493 - 4s - loss: 1.9934e-04 - val_loss: 1.8246e-04 - 4s/epoch - 3ms/step
Epoch 94/300
1493/1493 - 4s - loss: 1.4797e-04 - val_loss: 1.1761e-04 - 4s/epoch - 3ms/step
Epoch 95/300
1493/1493 - 4s - loss: 1.4042e-04 - val_loss: 4.7067e-04 - 4s/epoch - 3ms/step
Epoch 96/300
1493/1493 - 4s - loss: 1.6632e-04 - val_loss: 1.6008e-04 - 4s/epoch - 3ms/step
Epoch 97/300
1493/1493 - 4s - loss: 1.4123e-04 - val_loss: 1.3000e-04 - 4s/epoch - 3ms/step
Epoch 98/300
1493/1493 - 4s - loss: 1.4025e-04 - val_loss: 3.2686e-04 - 4s/epoch - 3ms/step
Epoch 99/300
1493/1493 - 4s - loss: 1.6328e-04 - val_loss: 1.5242e-04 - 4s/epoch - 3ms/step
Epoch 100/300
1493/1493 - 4s - loss: 1.4276e-04 - val_loss: 1.3574e-04 - 4s/epoch - 3ms/step
Epoch 101/300
1493/1493 - 4s - loss: 1.3422e-04 - val_loss: 1.6744e-04 - 4s/epoch - 3ms/step
Epoch 102/300
1493/1493 - 4s - loss: 1.3961e-04 - val_loss: 2.3204e-04 - 4s/epoch - 3ms/step
Epoch 103/300
1493/1493 - 4s - loss: 1.4363e-04 - val_loss: 1.5673e-04 - 4s/epoch - 3ms/step
Epoch 104/300
1493/1493 - 4s - loss: 1.3197e-04 - val_loss: 4.9876e-04 - 4s/epoch - 3ms/step
Epoch 105/300
1493/1493 - 4s - loss: 1.5751e-04 - val_loss: 1.1474e-04 - 4s/epoch - 3ms/step
Epoch 106/300
1493/1493 - 4s - loss: 1.3148e-04 - val_loss: 1.4411e-04 - 4s/epoch - 3ms/step
Epoch 107/300
1493/1493 - 4s - loss: 1.3032e-04 - val_loss: 1.4473e-04 - 4s/epoch - 3ms/step
Epoch 108/300
1493/1493 - 4s - loss: 1.3743e-04 - val_loss: 1.3276e-04 - 4s/epoch - 3ms/step
Epoch 109/300
1493/1493 - 4s - loss: 1.3018e-04 - val_loss: 1.5406e-04 - 4s/epoch - 3ms/step
Epoch 110/300
1493/1493 - 4s - loss: 1.2767e-04 - val_loss: 1.4562e-04 - 4s/epoch - 3ms/step
Epoch 111/300
1493/1493 - 4s - loss: 1.2665e-04 - val_loss: 1.4958e-04 - 4s/epoch - 3ms/step
Epoch 112/300
1493/1493 - 4s - loss: 1.2758e-04 - val_loss: 1.2638e-04 - 4s/epoch - 3ms/step
Epoch 113/300
1493/1493 - 4s - loss: 1.2451e-04 - val_loss: 3.6581e-04 - 4s/epoch - 3ms/step
Epoch 114/300
1493/1493 - 4s - loss: 1.5569e-04 - val_loss: 1.3346e-04 - 4s/epoch - 3ms/step
Epoch 115/300
1493/1493 - 4s - loss: 1.3560e-04 - val_loss: 1.7553e-04 - 4s/epoch - 3ms/step
Epoch 116/300
1493/1493 - 4s - loss: 1.2522e-04 - val_loss: 1.4329e-04 - 4s/epoch - 3ms/step
Epoch 117/300
1493/1493 - 4s - loss: 1.2353e-04 - val_loss: 2.0112e-04 - 4s/epoch - 3ms/step
Epoch 118/300
1493/1493 - 4s - loss: 1.2991e-04 - val_loss: 1.2067e-04 - 4s/epoch - 3ms/step
Epoch 119/300
1493/1493 - 4s - loss: 1.2316e-04 - val_loss: 1.2838e-04 - 4s/epoch - 3ms/step
Epoch 120/300
1493/1493 - 4s - loss: 1.2105e-04 - val_loss: 1.2641e-04 - 4s/epoch - 3ms/step
Epoch 121/300
1493/1493 - 4s - loss: 1.2053e-04 - val_loss: 2.3752e-04 - 4s/epoch - 3ms/step
Epoch 122/300
1493/1493 - 4s - loss: 1.2350e-04 - val_loss: 1.1323e-04 - 4s/epoch - 3ms/step
Epoch 123/300
1493/1493 - 4s - loss: 1.1797e-04 - val_loss: 1.3317e-04 - 4s/epoch - 3ms/step
Epoch 124/300
1493/1493 - 4s - loss: 1.1937e-04 - val_loss: 3.8211e-04 - 4s/epoch - 3ms/step
Epoch 125/300
1493/1493 - 4s - loss: 1.6597e-04 - val_loss: 1.6247e-04 - 4s/epoch - 3ms/step
Epoch 126/300
1493/1493 - 4s - loss: 1.2563e-04 - val_loss: 3.5604e-04 - 4s/epoch - 3ms/step
Epoch 127/300
1493/1493 - 4s - loss: 1.4651e-04 - val_loss: 2.0144e-04 - 4s/epoch - 3ms/step
Epoch 128/300
1493/1493 - 4s - loss: 1.2486e-04 - val_loss: 1.0638e-04 - 4s/epoch - 3ms/step
Epoch 129/300
1493/1493 - 4s - loss: 1.1804e-04 - val_loss: 1.3157e-04 - 4s/epoch - 3ms/step
Epoch 130/300
1493/1493 - 4s - loss: 1.1691e-04 - val_loss: 1.1957e-04 - 4s/epoch - 3ms/step
Epoch 131/300
1493/1493 - 4s - loss: 1.1730e-04 - val_loss: 2.1669e-04 - 4s/epoch - 3ms/step
Epoch 132/300
1493/1493 - 4s - loss: 1.2890e-04 - val_loss: 1.1799e-04 - 4s/epoch - 3ms/step
Epoch 133/300
1493/1493 - 4s - loss: 1.1520e-04 - val_loss: 1.2677e-04 - 4s/epoch - 3ms/step
Epoch 134/300
1493/1493 - 4s - loss: 1.1333e-04 - val_loss: 1.2748e-04 - 4s/epoch - 3ms/step
Epoch 135/300
1493/1493 - 4s - loss: 1.1366e-04 - val_loss: 1.1410e-04 - 4s/epoch - 3ms/step
Epoch 136/300
1493/1493 - 4s - loss: 1.1412e-04 - val_loss: 1.9509e-04 - 4s/epoch - 3ms/step
Epoch 137/300
1493/1493 - 4s - loss: 1.3065e-04 - val_loss: 1.2912e-04 - 4s/epoch - 3ms/step
Epoch 138/300
1493/1493 - 4s - loss: 1.1867e-04 - val_loss: 1.2956e-04 - 4s/epoch - 3ms/step
Epoch 139/300
1493/1493 - 4s - loss: 1.2111e-04 - val_loss: 2.1361e-04 - 4s/epoch - 3ms/step
Epoch 140/300
1493/1493 - 4s - loss: 1.3937e-04 - val_loss: 1.0499e-04 - 4s/epoch - 3ms/step
Epoch 141/300
1493/1493 - 4s - loss: 1.1414e-04 - val_loss: 1.4750e-04 - 4s/epoch - 3ms/step
Epoch 142/300
1493/1493 - 4s - loss: 1.1488e-04 - val_loss: 1.2609e-04 - 4s/epoch - 3ms/step
Epoch 143/300
1493/1493 - 4s - loss: 1.1800e-04 - val_loss: 1.4692e-04 - 4s/epoch - 3ms/step
Epoch 144/300
1493/1493 - 4s - loss: 1.2066e-04 - val_loss: 1.1049e-04 - 4s/epoch - 3ms/step
Epoch 145/300
1493/1493 - 4s - loss: 1.1274e-04 - val_loss: 1.0368e-04 - 4s/epoch - 3ms/step
Epoch 146/300
1493/1493 - 4s - loss: 1.1096e-04 - val_loss: 1.1794e-04 - 4s/epoch - 3ms/step
Epoch 147/300
1493/1493 - 4s - loss: 1.0927e-04 - val_loss: 1.3042e-04 - 4s/epoch - 3ms/step
Epoch 148/300
1493/1493 - 4s - loss: 1.0930e-04 - val_loss: 2.2229e-04 - 4s/epoch - 3ms/step
Epoch 149/300
1493/1493 - 4s - loss: 1.0804e-04 - val_loss: 1.1759e-04 - 4s/epoch - 3ms/step
Epoch 150/300
1493/1493 - 4s - loss: 1.0718e-04 - val_loss: 1.1006e-04 - 4s/epoch - 3ms/step
Epoch 151/300
1493/1493 - 4s - loss: 1.0829e-04 - val_loss: 1.3016e-04 - 4s/epoch - 3ms/step
Epoch 152/300
1493/1493 - 4s - loss: 1.0751e-04 - val_loss: 1.6554e-04 - 4s/epoch - 3ms/step
Epoch 153/300
1493/1493 - 4s - loss: 1.0969e-04 - val_loss: 1.3161e-04 - 4s/epoch - 3ms/step
Epoch 154/300
1493/1493 - 4s - loss: 1.0559e-04 - val_loss: 1.9346e-04 - 4s/epoch - 3ms/step
Epoch 155/300
1493/1493 - 4s - loss: 1.1520e-04 - val_loss: 1.1917e-04 - 4s/epoch - 3ms/step
Epoch 156/300
1493/1493 - 4s - loss: 1.0831e-04 - val_loss: 1.1512e-04 - 4s/epoch - 3ms/step
Epoch 157/300
1493/1493 - 4s - loss: 1.0441e-04 - val_loss: 1.2811e-04 - 4s/epoch - 3ms/step
Epoch 158/300
1493/1493 - 4s - loss: 1.0612e-04 - val_loss: 1.1965e-04 - 4s/epoch - 3ms/step
Epoch 159/300
1493/1493 - 4s - loss: 1.0456e-04 - val_loss: 1.0309e-04 - 4s/epoch - 3ms/step
Epoch 160/300
1493/1493 - 4s - loss: 1.0402e-04 - val_loss: 1.2201e-04 - 4s/epoch - 3ms/step
Epoch 161/300
1493/1493 - 4s - loss: 1.0921e-04 - val_loss: 1.0356e-04 - 4s/epoch - 3ms/step
Epoch 162/300
1493/1493 - 4s - loss: 1.0343e-04 - val_loss: 1.0848e-04 - 4s/epoch - 3ms/step
Epoch 163/300
1493/1493 - 4s - loss: 1.0502e-04 - val_loss: 2.7417e-04 - 4s/epoch - 3ms/step
Epoch 164/300
1493/1493 - 4s - loss: 1.4264e-04 - val_loss: 1.0561e-04 - 4s/epoch - 3ms/step
Epoch 165/300
1493/1493 - 4s - loss: 1.2330e-04 - val_loss: 1.0471e-04 - 4s/epoch - 3ms/step
Epoch 166/300
1493/1493 - 4s - loss: 1.0588e-04 - val_loss: 1.0078e-04 - 4s/epoch - 3ms/step
Epoch 167/300
1493/1493 - 4s - loss: 1.0403e-04 - val_loss: 1.0007e-04 - 4s/epoch - 3ms/step
Epoch 168/300
1493/1493 - 4s - loss: 1.0268e-04 - val_loss: 1.1187e-04 - 4s/epoch - 3ms/step
Epoch 169/300
1493/1493 - 4s - loss: 1.0302e-04 - val_loss: 1.5614e-04 - 4s/epoch - 3ms/step
Epoch 170/300
1493/1493 - 4s - loss: 1.0415e-04 - val_loss: 3.2142e-04 - 4s/epoch - 3ms/step
Epoch 171/300
1493/1493 - 4s - loss: 1.3822e-04 - val_loss: 3.1719e-04 - 4s/epoch - 3ms/step
Epoch 172/300
1493/1493 - 4s - loss: 1.3665e-04 - val_loss: 1.0902e-04 - 4s/epoch - 3ms/step
Epoch 173/300
1493/1493 - 4s - loss: 1.0672e-04 - val_loss: 1.0159e-04 - 4s/epoch - 3ms/step
Epoch 174/300
1493/1493 - 4s - loss: 1.0328e-04 - val_loss: 1.0803e-04 - 4s/epoch - 3ms/step
Epoch 175/300
1493/1493 - 4s - loss: 1.0370e-04 - val_loss: 1.3582e-04 - 4s/epoch - 3ms/step
Epoch 176/300
1493/1493 - 4s - loss: 1.0384e-04 - val_loss: 1.1714e-04 - 4s/epoch - 3ms/step
Epoch 177/300
1493/1493 - 4s - loss: 1.0336e-04 - val_loss: 9.7254e-05 - 4s/epoch - 3ms/step
Epoch 178/300
1493/1493 - 4s - loss: 1.0092e-04 - val_loss: 1.1403e-04 - 4s/epoch - 3ms/step
Epoch 179/300
1493/1493 - 4s - loss: 9.9290e-05 - val_loss: 1.0502e-04 - 4s/epoch - 3ms/step
Epoch 180/300
1493/1493 - 4s - loss: 9.9786e-05 - val_loss: 1.5687e-04 - 4s/epoch - 3ms/step
Epoch 181/300
1493/1493 - 4s - loss: 1.0859e-04 - val_loss: 1.1284e-04 - 4s/epoch - 3ms/step
Epoch 182/300
1493/1493 - 4s - loss: 1.0154e-04 - val_loss: 1.4226e-04 - 4s/epoch - 3ms/step
Epoch 183/300
1493/1493 - 4s - loss: 1.0121e-04 - val_loss: 1.2161e-04 - 4s/epoch - 3ms/step
Epoch 184/300
1493/1493 - 4s - loss: 1.0042e-04 - val_loss: 9.9241e-05 - 4s/epoch - 3ms/step
Epoch 185/300
1493/1493 - 4s - loss: 9.7306e-05 - val_loss: 1.4502e-04 - 4s/epoch - 3ms/step
Epoch 186/300
1493/1493 - 4s - loss: 1.0098e-04 - val_loss: 1.1091e-04 - 4s/epoch - 3ms/step
Epoch 187/300
1493/1493 - 4s - loss: 9.7223e-05 - val_loss: 1.2074e-04 - 4s/epoch - 3ms/step
Epoch 188/300
1493/1493 - 4s - loss: 1.0418e-04 - val_loss: 1.1450e-04 - 4s/epoch - 3ms/step
Epoch 189/300
1493/1493 - 4s - loss: 9.8187e-05 - val_loss: 1.2719e-04 - 4s/epoch - 3ms/step
Epoch 190/300
1493/1493 - 4s - loss: 9.8331e-05 - val_loss: 1.2037e-04 - 4s/epoch - 3ms/step
Epoch 191/300
1493/1493 - 4s - loss: 1.0033e-04 - val_loss: 1.7610e-04 - 4s/epoch - 3ms/step
Epoch 192/300
1493/1493 - 4s - loss: 1.0128e-04 - val_loss: 1.1783e-04 - 4s/epoch - 3ms/step
Epoch 193/300
1493/1493 - 4s - loss: 1.0009e-04 - val_loss: 3.0836e-04 - 4s/epoch - 3ms/step
Epoch 194/300
1493/1493 - 4s - loss: 1.2570e-04 - val_loss: 6.9030e-04 - 4s/epoch - 3ms/step
Epoch 195/300
1493/1493 - 4s - loss: 1.5503e-04 - val_loss: 1.6084e-04 - 4s/epoch - 3ms/step
Epoch 196/300
1493/1493 - 4s - loss: 1.1378e-04 - val_loss: 8.9412e-05 - 4s/epoch - 3ms/step
Epoch 197/300
1493/1493 - 4s - loss: 1.0186e-04 - val_loss: 1.2302e-04 - 4s/epoch - 3ms/step
Epoch 198/300
1493/1493 - 4s - loss: 1.0423e-04 - val_loss: 1.4519e-04 - 4s/epoch - 3ms/step
Epoch 199/300
1493/1493 - 4s - loss: 1.0287e-04 - val_loss: 8.7541e-05 - 4s/epoch - 3ms/step
Epoch 200/300
1493/1493 - 4s - loss: 9.7320e-05 - val_loss: 1.0153e-04 - 4s/epoch - 3ms/step
Epoch 201/300
1493/1493 - 4s - loss: 9.6727e-05 - val_loss: 9.5299e-05 - 4s/epoch - 3ms/step
Epoch 202/300
1493/1493 - 4s - loss: 9.9144e-05 - val_loss: 1.1321e-04 - 4s/epoch - 3ms/step
Epoch 203/300
1493/1493 - 4s - loss: 9.8815e-05 - val_loss: 4.3184e-04 - 4s/epoch - 3ms/step
Epoch 204/300
1493/1493 - 4s - loss: 1.3603e-04 - val_loss: 9.0683e-05 - 4s/epoch - 3ms/step
Epoch 205/300
1493/1493 - 4s - loss: 9.9638e-05 - val_loss: 1.1639e-04 - 4s/epoch - 3ms/step
Epoch 206/300
1493/1493 - 4s - loss: 9.7290e-05 - val_loss: 1.5041e-04 - 4s/epoch - 3ms/step
Epoch 207/300
1493/1493 - 4s - loss: 9.9825e-05 - val_loss: 1.7899e-04 - 4s/epoch - 3ms/step
Epoch 208/300
1493/1493 - 4s - loss: 1.0040e-04 - val_loss: 9.8180e-05 - 4s/epoch - 3ms/step
Epoch 209/300
1493/1493 - 4s - loss: 9.5832e-05 - val_loss: 1.8504e-04 - 4s/epoch - 3ms/step
Epoch 210/300
1493/1493 - 4s - loss: 1.0454e-04 - val_loss: 1.1968e-04 - 4s/epoch - 3ms/step
Epoch 211/300
1493/1493 - 4s - loss: 9.5116e-05 - val_loss: 1.2939e-04 - 4s/epoch - 3ms/step
Epoch 212/300
1493/1493 - 4s - loss: 9.7106e-05 - val_loss: 9.9896e-05 - 4s/epoch - 3ms/step
Epoch 213/300
1493/1493 - 4s - loss: 9.8641e-05 - val_loss: 8.7906e-05 - 4s/epoch - 3ms/step
Epoch 214/300
1493/1493 - 4s - loss: 9.3502e-05 - val_loss: 1.0577e-04 - 4s/epoch - 3ms/step
Epoch 215/300
1493/1493 - 4s - loss: 9.3061e-05 - val_loss: 1.5093e-04 - 4s/epoch - 3ms/step
Epoch 216/300
1493/1493 - 4s - loss: 9.5700e-05 - val_loss: 9.0435e-05 - 4s/epoch - 3ms/step
Epoch 217/300
1493/1493 - 4s - loss: 9.1266e-05 - val_loss: 8.9683e-05 - 4s/epoch - 3ms/step
Epoch 218/300
1493/1493 - 4s - loss: 9.0572e-05 - val_loss: 1.0518e-04 - 4s/epoch - 3ms/step
Epoch 219/300
1493/1493 - 4s - loss: 9.1913e-05 - val_loss: 1.0830e-04 - 4s/epoch - 3ms/step
Epoch 220/300
1493/1493 - 4s - loss: 9.6804e-05 - val_loss: 9.9190e-05 - 4s/epoch - 3ms/step
Epoch 221/300
1493/1493 - 4s - loss: 9.2877e-05 - val_loss: 1.3372e-04 - 4s/epoch - 3ms/step
Epoch 222/300
1493/1493 - 4s - loss: 9.8842e-05 - val_loss: 1.0406e-04 - 4s/epoch - 3ms/step
Epoch 223/300
1493/1493 - 4s - loss: 9.2956e-05 - val_loss: 8.5262e-05 - 4s/epoch - 3ms/step
Epoch 224/300
1493/1493 - 4s - loss: 9.2034e-05 - val_loss: 2.2921e-04 - 4s/epoch - 3ms/step
Epoch 225/300
1493/1493 - 4s - loss: 1.1264e-04 - val_loss: 1.2083e-04 - 4s/epoch - 3ms/step
Epoch 226/300
1493/1493 - 4s - loss: 9.9798e-05 - val_loss: 2.6860e-04 - 4s/epoch - 3ms/step
Epoch 227/300
1493/1493 - 4s - loss: 1.1235e-04 - val_loss: 1.0422e-04 - 4s/epoch - 3ms/step
Epoch 228/300
1493/1493 - 4s - loss: 9.4352e-05 - val_loss: 1.7049e-04 - 4s/epoch - 3ms/step
Epoch 229/300
1493/1493 - 4s - loss: 1.0059e-04 - val_loss: 9.8236e-05 - 4s/epoch - 3ms/step
Epoch 230/300
1493/1493 - 4s - loss: 9.2089e-05 - val_loss: 8.5717e-05 - 4s/epoch - 3ms/step
Epoch 231/300
1493/1493 - 4s - loss: 9.3579e-05 - val_loss: 9.5739e-05 - 4s/epoch - 3ms/step
Epoch 232/300
1493/1493 - 4s - loss: 9.1543e-05 - val_loss: 1.4674e-04 - 4s/epoch - 3ms/step
Epoch 233/300
1493/1493 - 4s - loss: 9.6975e-05 - val_loss: 8.7172e-05 - 4s/epoch - 3ms/step
Epoch 234/300
1493/1493 - 4s - loss: 8.9897e-05 - val_loss: 9.3658e-05 - 4s/epoch - 3ms/step
Epoch 235/300
1493/1493 - 4s - loss: 9.3332e-05 - val_loss: 1.0247e-04 - 4s/epoch - 3ms/step
Epoch 236/300
1493/1493 - 4s - loss: 8.9040e-05 - val_loss: 9.7754e-05 - 4s/epoch - 3ms/step
Epoch 237/300
1493/1493 - 4s - loss: 8.9723e-05 - val_loss: 1.2796e-04 - 4s/epoch - 3ms/step
Epoch 238/300
1493/1493 - 4s - loss: 9.3396e-05 - val_loss: 8.6229e-05 - 4s/epoch - 3ms/step
Epoch 239/300
1493/1493 - 4s - loss: 9.2105e-05 - val_loss: 1.7826e-04 - 4s/epoch - 3ms/step
Epoch 240/300
1493/1493 - 4s - loss: 1.1445e-04 - val_loss: 9.1828e-05 - 4s/epoch - 3ms/step
Epoch 241/300
1493/1493 - 4s - loss: 8.9223e-05 - val_loss: 9.6205e-05 - 4s/epoch - 3ms/step
Epoch 242/300
1493/1493 - 4s - loss: 8.9498e-05 - val_loss: 2.1269e-04 - 4s/epoch - 3ms/step
Epoch 243/300
1493/1493 - 4s - loss: 1.0230e-04 - val_loss: 1.0742e-04 - 4s/epoch - 3ms/step
Epoch 244/300
1493/1493 - 4s - loss: 9.2580e-05 - val_loss: 1.1004e-04 - 4s/epoch - 3ms/step
Epoch 245/300
1493/1493 - 4s - loss: 8.9738e-05 - val_loss: 1.0735e-04 - 4s/epoch - 3ms/step
Epoch 246/300
1493/1493 - 4s - loss: 9.2788e-05 - val_loss: 2.7414e-04 - 4s/epoch - 3ms/step
Epoch 247/300
1493/1493 - 4s - loss: 1.1695e-04 - val_loss: 1.4424e-04 - 4s/epoch - 3ms/step
Epoch 248/300
1493/1493 - 4s - loss: 9.4240e-05 - val_loss: 2.9071e-04 - 4s/epoch - 3ms/step
Epoch 249/300
1493/1493 - 4s - loss: 1.0442e-04 - val_loss: 9.0831e-05 - 4s/epoch - 3ms/step
Epoch 250/300
1493/1493 - 4s - loss: 9.2901e-05 - val_loss: 3.3030e-04 - 4s/epoch - 3ms/step
Epoch 251/300
1493/1493 - 4s - loss: 1.1541e-04 - val_loss: 2.2012e-04 - 4s/epoch - 3ms/step
Epoch 252/300
1493/1493 - 4s - loss: 1.0373e-04 - val_loss: 1.2245e-04 - 4s/epoch - 3ms/step
Epoch 253/300
1493/1493 - 4s - loss: 9.1886e-05 - val_loss: 1.1722e-04 - 4s/epoch - 3ms/step
Epoch 254/300
1493/1493 - 4s - loss: 9.1972e-05 - val_loss: 1.4598e-04 - 4s/epoch - 3ms/step
Epoch 255/300
1493/1493 - 4s - loss: 1.0095e-04 - val_loss: 1.0757e-04 - 4s/epoch - 3ms/step
Epoch 256/300
1493/1493 - 4s - loss: 9.1014e-05 - val_loss: 8.4959e-05 - 4s/epoch - 3ms/step
Epoch 257/300
1493/1493 - 4s - loss: 8.9043e-05 - val_loss: 1.0025e-04 - 4s/epoch - 3ms/step
Epoch 258/300
1493/1493 - 4s - loss: 9.0540e-05 - val_loss: 3.3845e-04 - 4s/epoch - 3ms/step
Epoch 259/300
1493/1493 - 4s - loss: 1.1096e-04 - val_loss: 5.8575e-04 - 4s/epoch - 3ms/step
Epoch 260/300
1493/1493 - 4s - loss: 1.5314e-04 - val_loss: 1.0154e-04 - 4s/epoch - 3ms/step
Epoch 261/300
1493/1493 - 4s - loss: 9.9993e-05 - val_loss: 1.5915e-04 - 4s/epoch - 3ms/step
Epoch 262/300
1493/1493 - 4s - loss: 1.0550e-04 - val_loss: 8.2245e-05 - 4s/epoch - 3ms/step
Epoch 263/300
1493/1493 - 4s - loss: 9.0849e-05 - val_loss: 9.5816e-05 - 4s/epoch - 3ms/step
Epoch 264/300
1493/1493 - 4s - loss: 1.0122e-04 - val_loss: 8.1803e-05 - 4s/epoch - 3ms/step
Epoch 265/300
1493/1493 - 4s - loss: 8.9180e-05 - val_loss: 1.2377e-04 - 4s/epoch - 3ms/step
Epoch 266/300
1493/1493 - 4s - loss: 9.1897e-05 - val_loss: 9.2740e-05 - 4s/epoch - 3ms/step
Epoch 267/300
1493/1493 - 4s - loss: 8.8066e-05 - val_loss: 1.0516e-04 - 4s/epoch - 3ms/step
Epoch 268/300
1493/1493 - 4s - loss: 8.6284e-05 - val_loss: 8.7473e-05 - 4s/epoch - 3ms/step
Epoch 269/300
1493/1493 - 4s - loss: 8.8459e-05 - val_loss: 2.8598e-04 - 4s/epoch - 3ms/step
Epoch 270/300
1493/1493 - 4s - loss: 1.0176e-04 - val_loss: 1.0968e-04 - 4s/epoch - 3ms/step
Epoch 271/300
1493/1493 - 4s - loss: 8.7591e-05 - val_loss: 8.1942e-05 - 4s/epoch - 3ms/step
Epoch 272/300
1493/1493 - 4s - loss: 8.9192e-05 - val_loss: 1.1635e-04 - 4s/epoch - 3ms/step
Epoch 273/300
1493/1493 - 4s - loss: 8.9906e-05 - val_loss: 2.3886e-04 - 4s/epoch - 3ms/step
Epoch 274/300
1493/1493 - 4s - loss: 9.9800e-05 - val_loss: 1.8080e-04 - 4s/epoch - 3ms/step
Epoch 275/300
1493/1493 - 4s - loss: 1.0766e-04 - val_loss: 1.0536e-04 - 4s/epoch - 3ms/step
Epoch 276/300
1493/1493 - 4s - loss: 8.7011e-05 - val_loss: 8.9562e-05 - 4s/epoch - 3ms/step
Epoch 277/300
1493/1493 - 4s - loss: 8.7290e-05 - val_loss: 1.0427e-04 - 4s/epoch - 3ms/step
Epoch 278/300
1493/1493 - 4s - loss: 8.6277e-05 - val_loss: 1.0382e-04 - 4s/epoch - 3ms/step
Epoch 279/300
1493/1493 - 4s - loss: 8.5960e-05 - val_loss: 9.7828e-05 - 4s/epoch - 3ms/step
Epoch 280/300
1493/1493 - 4s - loss: 8.8271e-05 - val_loss: 9.4559e-05 - 4s/epoch - 3ms/step
Epoch 281/300
1493/1493 - 4s - loss: 8.9980e-05 - val_loss: 1.5661e-04 - 4s/epoch - 3ms/step
Epoch 282/300
1493/1493 - 4s - loss: 9.0595e-05 - val_loss: 8.3791e-05 - 4s/epoch - 3ms/step
Epoch 283/300
1493/1493 - 4s - loss: 8.4249e-05 - val_loss: 1.0311e-04 - 4s/epoch - 3ms/step
Epoch 284/300
1493/1493 - 4s - loss: 8.5244e-05 - val_loss: 9.0136e-05 - 4s/epoch - 3ms/step
Epoch 285/300
1493/1493 - 4s - loss: 8.6623e-05 - val_loss: 1.5407e-04 - 4s/epoch - 3ms/step
Epoch 286/300
1493/1493 - 4s - loss: 8.4297e-05 - val_loss: 9.0215e-05 - 4s/epoch - 3ms/step
Epoch 287/300
1493/1493 - 4s - loss: 8.4831e-05 - val_loss: 2.0831e-04 - 4s/epoch - 3ms/step
Epoch 288/300
1493/1493 - 4s - loss: 1.0358e-04 - val_loss: 1.2176e-04 - 4s/epoch - 3ms/step
Epoch 289/300
1493/1493 - 4s - loss: 8.8144e-05 - val_loss: 1.1558e-04 - 4s/epoch - 3ms/step
Epoch 290/300
1493/1493 - 4s - loss: 8.7140e-05 - val_loss: 1.8635e-04 - 4s/epoch - 3ms/step
Epoch 291/300
1493/1493 - 4s - loss: 9.2318e-05 - val_loss: 1.0027e-04 - 4s/epoch - 3ms/step
Epoch 292/300
1493/1493 - 4s - loss: 8.3799e-05 - val_loss: 8.2945e-05 - 4s/epoch - 3ms/step
Epoch 293/300
1493/1493 - 4s - loss: 8.6659e-05 - val_loss: 1.0465e-04 - 4s/epoch - 3ms/step
Epoch 294/300
1493/1493 - 4s - loss: 8.6083e-05 - val_loss: 8.7158e-05 - 4s/epoch - 3ms/step
Epoch 295/300
1493/1493 - 4s - loss: 8.5179e-05 - val_loss: 1.0330e-04 - 4s/epoch - 3ms/step
Epoch 296/300
1493/1493 - 4s - loss: 8.3900e-05 - val_loss: 1.5117e-04 - 4s/epoch - 3ms/step
Epoch 297/300
1493/1493 - 4s - loss: 8.2161e-05 - val_loss: 8.5296e-05 - 4s/epoch - 3ms/step
Epoch 298/300
1493/1493 - 4s - loss: 8.5113e-05 - val_loss: 2.4233e-04 - 4s/epoch - 3ms/step
Epoch 299/300
1493/1493 - 4s - loss: 1.0838e-04 - val_loss: 8.7152e-05 - 4s/epoch - 3ms/step
Epoch 300/300
1493/1493 - 4s - loss: 8.7138e-05 - val_loss: 1.4842e-04 - 4s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00014842151722405106
  1/332 [..............................] - ETA: 35s 49/332 [===>..........................] - ETA: 0s  97/332 [=======>......................] - ETA: 0s145/332 [============>.................] - ETA: 0s193/332 [================>.............] - ETA: 0s242/332 [====================>.........] - ETA: 0s291/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0016894816813089691
cosine 0.0013291558543556464
MAE: 0.0081395805
RMSE: 0.012182831
r2: 0.9903741415006413
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_48"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_49 (InputLayer)       multiple                  0         
                                                                 
 dense_48 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_48 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_48 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_49 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_49 (ReLU)             (None, 632)               0         
                                                                 
 dense_49 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_50 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_50 (ReLU)             (None, 2528)              0         
                                                                 
 dense_50 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_49"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_50 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_49 (InputLayer)       multiple                  0         
                                                                 
 dense_48 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_48 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_48 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_50"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_51 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_49 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_49 (ReLU)             (None, 632)               0         
                                                                 
 dense_49 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_50 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_50 (ReLU)             (None, 2528)              0         
                                                                 
 dense_50 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 64, 300, 0.0005, 0.5, 632, 8.713785791769624e-05, 0.00014842151722405106, 0.0016894816813089691, 0.0013291558543556464, 0.008139580488204956, 0.012182830832898617, 0.9903741415006413, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_51"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_52 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_51 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_51 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_51 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_52 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_52 (ReLU)             (None, 632)               0         
                                                                 
 dense_52 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_53 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_53 (ReLU)             (None, 2528)              0         
                                                                 
 dense_53 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/300
1493/1493 - 5s - loss: 0.0088 - val_loss: 0.0033 - 5s/epoch - 4ms/step
Epoch 2/300
1493/1493 - 4s - loss: 0.0027 - val_loss: 0.0026 - 4s/epoch - 3ms/step
Epoch 3/300
1493/1493 - 4s - loss: 0.0021 - val_loss: 0.0019 - 4s/epoch - 3ms/step
Epoch 4/300
1493/1493 - 4s - loss: 0.0019 - val_loss: 0.0017 - 4s/epoch - 3ms/step
Epoch 5/300
1493/1493 - 4s - loss: 0.0015 - val_loss: 0.0013 - 4s/epoch - 3ms/step
Epoch 6/300
1493/1493 - 4s - loss: 0.0013 - val_loss: 0.0013 - 4s/epoch - 3ms/step
Epoch 7/300
1493/1493 - 4s - loss: 0.0011 - val_loss: 8.7797e-04 - 4s/epoch - 3ms/step
Epoch 8/300
1493/1493 - 4s - loss: 8.9136e-04 - val_loss: 0.0011 - 4s/epoch - 3ms/step
Epoch 9/300
1493/1493 - 4s - loss: 7.8584e-04 - val_loss: 7.1950e-04 - 4s/epoch - 3ms/step
Epoch 10/300
1493/1493 - 4s - loss: 6.7775e-04 - val_loss: 6.3954e-04 - 4s/epoch - 3ms/step
Epoch 11/300
1493/1493 - 4s - loss: 6.1414e-04 - val_loss: 5.7995e-04 - 4s/epoch - 3ms/step
Epoch 12/300
1493/1493 - 4s - loss: 5.4176e-04 - val_loss: 0.0022 - 4s/epoch - 3ms/step
Epoch 13/300
1493/1493 - 4s - loss: 7.3948e-04 - val_loss: 0.0011 - 4s/epoch - 3ms/step
Epoch 14/300
1493/1493 - 4s - loss: 6.3187e-04 - val_loss: 4.4293e-04 - 4s/epoch - 3ms/step
Epoch 15/300
1493/1493 - 4s - loss: 4.7774e-04 - val_loss: 5.8532e-04 - 4s/epoch - 3ms/step
Epoch 16/300
1493/1493 - 4s - loss: 4.7510e-04 - val_loss: 4.3020e-04 - 4s/epoch - 3ms/step
Epoch 17/300
1493/1493 - 4s - loss: 4.1479e-04 - val_loss: 5.2331e-04 - 4s/epoch - 3ms/step
Epoch 18/300
1493/1493 - 4s - loss: 4.0662e-04 - val_loss: 6.2254e-04 - 4s/epoch - 3ms/step
Epoch 19/300
1493/1493 - 4s - loss: 3.9556e-04 - val_loss: 4.7962e-04 - 4s/epoch - 3ms/step
Epoch 20/300
1493/1493 - 4s - loss: 3.7421e-04 - val_loss: 3.4047e-04 - 4s/epoch - 3ms/step
Epoch 21/300
1493/1493 - 4s - loss: 3.4631e-04 - val_loss: 4.0863e-04 - 4s/epoch - 3ms/step
Epoch 22/300
1493/1493 - 4s - loss: 3.4072e-04 - val_loss: 9.0014e-04 - 4s/epoch - 3ms/step
Epoch 23/300
1493/1493 - 4s - loss: 4.0004e-04 - val_loss: 3.6142e-04 - 4s/epoch - 3ms/step
Epoch 24/300
1493/1493 - 4s - loss: 3.4652e-04 - val_loss: 0.0011 - 4s/epoch - 3ms/step
Epoch 25/300
1493/1493 - 4s - loss: 3.5778e-04 - val_loss: 3.0145e-04 - 4s/epoch - 3ms/step
Epoch 26/300
1493/1493 - 4s - loss: 3.0190e-04 - val_loss: 8.7266e-04 - 4s/epoch - 3ms/step
Epoch 27/300
1493/1493 - 4s - loss: 2.9681e-04 - val_loss: 2.6585e-04 - 4s/epoch - 3ms/step
Epoch 28/300
1493/1493 - 4s - loss: 2.7679e-04 - val_loss: 3.3162e-04 - 4s/epoch - 3ms/step
Epoch 29/300
1493/1493 - 4s - loss: 2.7307e-04 - val_loss: 7.4846e-04 - 4s/epoch - 3ms/step
Epoch 30/300
1493/1493 - 4s - loss: 3.3162e-04 - val_loss: 2.7270e-04 - 4s/epoch - 3ms/step
Epoch 31/300
1493/1493 - 4s - loss: 2.7462e-04 - val_loss: 2.9157e-04 - 4s/epoch - 3ms/step
Epoch 32/300
1493/1493 - 4s - loss: 2.5664e-04 - val_loss: 2.6796e-04 - 4s/epoch - 3ms/step
Epoch 33/300
1493/1493 - 4s - loss: 2.5792e-04 - val_loss: 2.4122e-04 - 4s/epoch - 3ms/step
Epoch 34/300
1493/1493 - 4s - loss: 2.4540e-04 - val_loss: 2.2048e-04 - 4s/epoch - 3ms/step
Epoch 35/300
1493/1493 - 4s - loss: 2.3492e-04 - val_loss: 2.6813e-04 - 4s/epoch - 3ms/step
Epoch 36/300
1493/1493 - 4s - loss: 2.4963e-04 - val_loss: 3.3815e-04 - 4s/epoch - 3ms/step
Epoch 37/300
1493/1493 - 4s - loss: 2.4824e-04 - val_loss: 3.2164e-04 - 4s/epoch - 3ms/step
Epoch 38/300
1493/1493 - 4s - loss: 2.4239e-04 - val_loss: 2.1006e-04 - 4s/epoch - 3ms/step
Epoch 39/300
1493/1493 - 4s - loss: 2.2663e-04 - val_loss: 2.1935e-04 - 4s/epoch - 3ms/step
Epoch 40/300
1493/1493 - 4s - loss: 2.1815e-04 - val_loss: 2.6244e-04 - 4s/epoch - 3ms/step
Epoch 41/300
1493/1493 - 4s - loss: 2.2152e-04 - val_loss: 1.9555e-04 - 4s/epoch - 3ms/step
Epoch 42/300
1493/1493 - 4s - loss: 2.1200e-04 - val_loss: 2.0645e-04 - 4s/epoch - 3ms/step
Epoch 43/300
1493/1493 - 4s - loss: 2.1054e-04 - val_loss: 4.4506e-04 - 4s/epoch - 3ms/step
Epoch 44/300
1493/1493 - 4s - loss: 2.4944e-04 - val_loss: 3.5265e-04 - 4s/epoch - 3ms/step
Epoch 45/300
1493/1493 - 4s - loss: 2.4836e-04 - val_loss: 1.8510e-04 - 4s/epoch - 3ms/step
Epoch 46/300
1493/1493 - 4s - loss: 2.0787e-04 - val_loss: 1.9765e-04 - 4s/epoch - 3ms/step
Epoch 47/300
1493/1493 - 4s - loss: 2.0293e-04 - val_loss: 2.3930e-04 - 4s/epoch - 3ms/step
Epoch 48/300
1493/1493 - 4s - loss: 2.0057e-04 - val_loss: 2.1532e-04 - 4s/epoch - 3ms/step
Epoch 49/300
1493/1493 - 4s - loss: 2.0480e-04 - val_loss: 5.8657e-04 - 4s/epoch - 3ms/step
Epoch 50/300
1493/1493 - 4s - loss: 2.7073e-04 - val_loss: 4.9648e-04 - 4s/epoch - 3ms/step
Epoch 51/300
1493/1493 - 4s - loss: 2.4466e-04 - val_loss: 1.8047e-04 - 4s/epoch - 3ms/step
Epoch 52/300
1493/1493 - 4s - loss: 2.0032e-04 - val_loss: 1.8796e-04 - 4s/epoch - 3ms/step
Epoch 53/300
1493/1493 - 4s - loss: 1.9678e-04 - val_loss: 1.8643e-04 - 4s/epoch - 3ms/step
Epoch 54/300
1493/1493 - 4s - loss: 1.9197e-04 - val_loss: 2.0613e-04 - 4s/epoch - 3ms/step
Epoch 55/300
1493/1493 - 4s - loss: 1.8725e-04 - val_loss: 1.7928e-04 - 4s/epoch - 3ms/step
Epoch 56/300
1493/1493 - 4s - loss: 1.8495e-04 - val_loss: 2.0827e-04 - 4s/epoch - 3ms/step
Epoch 57/300
1493/1493 - 4s - loss: 1.8590e-04 - val_loss: 1.8250e-04 - 4s/epoch - 3ms/step
Epoch 58/300
1493/1493 - 4s - loss: 1.8038e-04 - val_loss: 1.7640e-04 - 4s/epoch - 3ms/step
Epoch 59/300
1493/1493 - 4s - loss: 1.7773e-04 - val_loss: 2.0327e-04 - 4s/epoch - 3ms/step
Epoch 60/300
1493/1493 - 4s - loss: 1.7880e-04 - val_loss: 1.8137e-04 - 4s/epoch - 3ms/step
Epoch 61/300
1493/1493 - 4s - loss: 1.7496e-04 - val_loss: 1.7316e-04 - 4s/epoch - 3ms/step
Epoch 62/300
1493/1493 - 4s - loss: 2.1753e-04 - val_loss: 2.1256e-04 - 4s/epoch - 3ms/step
Epoch 63/300
1493/1493 - 4s - loss: 1.8945e-04 - val_loss: 2.1796e-04 - 4s/epoch - 3ms/step
Epoch 64/300
1493/1493 - 4s - loss: 1.8153e-04 - val_loss: 1.9973e-04 - 4s/epoch - 3ms/step
Epoch 65/300
1493/1493 - 4s - loss: 1.7612e-04 - val_loss: 3.4007e-04 - 4s/epoch - 3ms/step
Epoch 66/300
1493/1493 - 4s - loss: 2.0499e-04 - val_loss: 1.7768e-04 - 4s/epoch - 3ms/step
Epoch 67/300
1493/1493 - 4s - loss: 1.7268e-04 - val_loss: 1.6168e-04 - 4s/epoch - 3ms/step
Epoch 68/300
1493/1493 - 4s - loss: 1.7173e-04 - val_loss: 3.3380e-04 - 4s/epoch - 3ms/step
Epoch 69/300
1493/1493 - 4s - loss: 2.1978e-04 - val_loss: 3.2266e-04 - 4s/epoch - 3ms/step
Epoch 70/300
1493/1493 - 4s - loss: 1.8847e-04 - val_loss: 2.6612e-04 - 4s/epoch - 3ms/step
Epoch 71/300
1493/1493 - 4s - loss: 2.0943e-04 - val_loss: 1.6350e-04 - 4s/epoch - 3ms/step
Epoch 72/300
1493/1493 - 4s - loss: 1.7404e-04 - val_loss: 1.6620e-04 - 4s/epoch - 3ms/step
Epoch 73/300
1493/1493 - 4s - loss: 1.7037e-04 - val_loss: 1.7395e-04 - 4s/epoch - 3ms/step
Epoch 74/300
1493/1493 - 4s - loss: 1.7283e-04 - val_loss: 1.6235e-04 - 4s/epoch - 3ms/step
Epoch 75/300
1493/1493 - 4s - loss: 1.6470e-04 - val_loss: 1.5600e-04 - 4s/epoch - 3ms/step
Epoch 76/300
1493/1493 - 4s - loss: 1.6270e-04 - val_loss: 1.7314e-04 - 4s/epoch - 3ms/step
Epoch 77/300
1493/1493 - 4s - loss: 1.6640e-04 - val_loss: 8.2118e-04 - 4s/epoch - 3ms/step
Epoch 78/300
1493/1493 - 4s - loss: 3.1577e-04 - val_loss: 2.0810e-04 - 4s/epoch - 3ms/step
Epoch 79/300
1493/1493 - 4s - loss: 2.0016e-04 - val_loss: 1.7057e-04 - 4s/epoch - 3ms/step
Epoch 80/300
1493/1493 - 4s - loss: 1.7936e-04 - val_loss: 1.8219e-04 - 4s/epoch - 3ms/step
Epoch 81/300
1493/1493 - 4s - loss: 1.7195e-04 - val_loss: 2.4245e-04 - 4s/epoch - 3ms/step
Epoch 82/300
1493/1493 - 4s - loss: 1.6681e-04 - val_loss: 1.6871e-04 - 4s/epoch - 3ms/step
Epoch 83/300
1493/1493 - 4s - loss: 1.6137e-04 - val_loss: 1.4509e-04 - 4s/epoch - 3ms/step
Epoch 84/300
1493/1493 - 4s - loss: 1.6052e-04 - val_loss: 2.2862e-04 - 4s/epoch - 3ms/step
Epoch 85/300
1493/1493 - 4s - loss: 1.6492e-04 - val_loss: 1.7753e-04 - 4s/epoch - 3ms/step
Epoch 86/300
1493/1493 - 4s - loss: 1.6848e-04 - val_loss: 1.5478e-04 - 4s/epoch - 3ms/step
Epoch 87/300
1493/1493 - 4s - loss: 1.5465e-04 - val_loss: 1.5364e-04 - 4s/epoch - 3ms/step
Epoch 88/300
1493/1493 - 4s - loss: 1.5442e-04 - val_loss: 3.9601e-04 - 4s/epoch - 3ms/step
Epoch 89/300
1493/1493 - 4s - loss: 1.9207e-04 - val_loss: 1.4677e-04 - 4s/epoch - 3ms/step
Epoch 90/300
1493/1493 - 4s - loss: 1.5623e-04 - val_loss: 1.5758e-04 - 4s/epoch - 3ms/step
Epoch 91/300
1493/1493 - 4s - loss: 1.5297e-04 - val_loss: 1.6152e-04 - 4s/epoch - 3ms/step
Epoch 92/300
1493/1493 - 4s - loss: 1.5292e-04 - val_loss: 2.9060e-04 - 4s/epoch - 3ms/step
Epoch 93/300
1493/1493 - 4s - loss: 1.8264e-04 - val_loss: 1.4554e-04 - 4s/epoch - 3ms/step
Epoch 94/300
1493/1493 - 4s - loss: 1.5233e-04 - val_loss: 1.5047e-04 - 4s/epoch - 3ms/step
Epoch 95/300
1493/1493 - 4s - loss: 1.5020e-04 - val_loss: 1.4852e-04 - 4s/epoch - 3ms/step
Epoch 96/300
1493/1493 - 4s - loss: 1.5444e-04 - val_loss: 2.0420e-04 - 4s/epoch - 3ms/step
Epoch 97/300
1493/1493 - 4s - loss: 1.5033e-04 - val_loss: 1.3945e-04 - 4s/epoch - 3ms/step
Epoch 98/300
1493/1493 - 4s - loss: 1.5076e-04 - val_loss: 3.5195e-04 - 4s/epoch - 3ms/step
Epoch 99/300
1493/1493 - 4s - loss: 2.0247e-04 - val_loss: 1.3742e-04 - 4s/epoch - 3ms/step
Epoch 100/300
1493/1493 - 4s - loss: 1.5505e-04 - val_loss: 1.4091e-04 - 4s/epoch - 3ms/step
Epoch 101/300
1493/1493 - 4s - loss: 1.5441e-04 - val_loss: 5.1461e-04 - 4s/epoch - 3ms/step
Epoch 102/300
1493/1493 - 4s - loss: 2.0973e-04 - val_loss: 1.8671e-04 - 4s/epoch - 3ms/step
Epoch 103/300
1493/1493 - 4s - loss: 1.6385e-04 - val_loss: 1.4520e-04 - 4s/epoch - 3ms/step
Epoch 104/300
1493/1493 - 4s - loss: 1.5413e-04 - val_loss: 3.4872e-04 - 4s/epoch - 3ms/step
Epoch 105/300
1493/1493 - 4s - loss: 2.0500e-04 - val_loss: 1.6150e-04 - 4s/epoch - 3ms/step
Epoch 106/300
1493/1493 - 4s - loss: 1.5662e-04 - val_loss: 2.0167e-04 - 4s/epoch - 3ms/step
Epoch 107/300
1493/1493 - 4s - loss: 1.6009e-04 - val_loss: 1.5742e-04 - 4s/epoch - 3ms/step
Epoch 108/300
1493/1493 - 4s - loss: 1.4847e-04 - val_loss: 1.6440e-04 - 4s/epoch - 3ms/step
Epoch 109/300
1493/1493 - 4s - loss: 1.5310e-04 - val_loss: 1.5531e-04 - 4s/epoch - 3ms/step
Epoch 110/300
1493/1493 - 4s - loss: 1.4604e-04 - val_loss: 1.6596e-04 - 4s/epoch - 3ms/step
Epoch 111/300
1493/1493 - 4s - loss: 1.4479e-04 - val_loss: 1.5792e-04 - 4s/epoch - 3ms/step
Epoch 112/300
1493/1493 - 4s - loss: 1.4233e-04 - val_loss: 1.3650e-04 - 4s/epoch - 3ms/step
Epoch 113/300
1493/1493 - 4s - loss: 1.4235e-04 - val_loss: 2.8979e-04 - 4s/epoch - 3ms/step
Epoch 114/300
1493/1493 - 4s - loss: 1.8395e-04 - val_loss: 1.8013e-04 - 4s/epoch - 3ms/step
Epoch 115/300
1493/1493 - 4s - loss: 1.5578e-04 - val_loss: 2.4755e-04 - 4s/epoch - 3ms/step
Epoch 116/300
1493/1493 - 4s - loss: 1.5144e-04 - val_loss: 2.3682e-04 - 4s/epoch - 3ms/step
Epoch 117/300
1493/1493 - 4s - loss: 1.5464e-04 - val_loss: 1.6990e-04 - 4s/epoch - 3ms/step
Epoch 118/300
1493/1493 - 4s - loss: 1.4696e-04 - val_loss: 1.3708e-04 - 4s/epoch - 3ms/step
Epoch 119/300
1493/1493 - 4s - loss: 1.4096e-04 - val_loss: 1.3645e-04 - 4s/epoch - 3ms/step
Epoch 120/300
1493/1493 - 4s - loss: 1.3910e-04 - val_loss: 1.4150e-04 - 4s/epoch - 3ms/step
Epoch 121/300
1493/1493 - 4s - loss: 1.4028e-04 - val_loss: 1.9375e-04 - 4s/epoch - 3ms/step
Epoch 122/300
1493/1493 - 4s - loss: 1.4162e-04 - val_loss: 1.5538e-04 - 4s/epoch - 3ms/step
Epoch 123/300
1493/1493 - 4s - loss: 1.4062e-04 - val_loss: 1.4150e-04 - 4s/epoch - 3ms/step
Epoch 124/300
1493/1493 - 4s - loss: 1.3607e-04 - val_loss: 1.8908e-04 - 4s/epoch - 3ms/step
Epoch 125/300
1493/1493 - 4s - loss: 1.4165e-04 - val_loss: 1.3799e-04 - 4s/epoch - 3ms/step
Epoch 126/300
1493/1493 - 4s - loss: 1.3541e-04 - val_loss: 2.2963e-04 - 4s/epoch - 3ms/step
Epoch 127/300
1493/1493 - 4s - loss: 1.5269e-04 - val_loss: 3.4653e-04 - 4s/epoch - 3ms/step
Epoch 128/300
1493/1493 - 4s - loss: 1.4975e-04 - val_loss: 1.5708e-04 - 4s/epoch - 3ms/step
Epoch 129/300
1493/1493 - 4s - loss: 1.4090e-04 - val_loss: 1.3401e-04 - 4s/epoch - 3ms/step
Epoch 130/300
1493/1493 - 4s - loss: 1.3462e-04 - val_loss: 1.2679e-04 - 4s/epoch - 3ms/step
Epoch 131/300
1493/1493 - 4s - loss: 1.3837e-04 - val_loss: 2.1500e-04 - 4s/epoch - 3ms/step
Epoch 132/300
1493/1493 - 4s - loss: 1.5037e-04 - val_loss: 1.3152e-04 - 4s/epoch - 3ms/step
Epoch 133/300
1493/1493 - 4s - loss: 1.3617e-04 - val_loss: 1.4330e-04 - 4s/epoch - 3ms/step
Epoch 134/300
1493/1493 - 4s - loss: 1.3528e-04 - val_loss: 1.4761e-04 - 4s/epoch - 3ms/step
Epoch 135/300
1493/1493 - 4s - loss: 1.3581e-04 - val_loss: 1.4010e-04 - 4s/epoch - 3ms/step
Epoch 136/300
1493/1493 - 4s - loss: 1.3158e-04 - val_loss: 1.4523e-04 - 4s/epoch - 3ms/step
Epoch 137/300
1493/1493 - 4s - loss: 1.3560e-04 - val_loss: 2.3741e-04 - 4s/epoch - 3ms/step
Epoch 138/300
1493/1493 - 4s - loss: 1.4749e-04 - val_loss: 1.2987e-04 - 4s/epoch - 3ms/step
Epoch 139/300
1493/1493 - 4s - loss: 1.3484e-04 - val_loss: 2.8428e-04 - 4s/epoch - 3ms/step
Epoch 140/300
1493/1493 - 4s - loss: 1.7323e-04 - val_loss: 1.2918e-04 - 4s/epoch - 3ms/step
Epoch 141/300
1493/1493 - 4s - loss: 1.3703e-04 - val_loss: 2.1816e-04 - 4s/epoch - 3ms/step
Epoch 142/300
1493/1493 - 4s - loss: 1.5372e-04 - val_loss: 1.4023e-04 - 4s/epoch - 3ms/step
Epoch 143/300
1493/1493 - 4s - loss: 1.3272e-04 - val_loss: 2.1212e-04 - 4s/epoch - 3ms/step
Epoch 144/300
1493/1493 - 4s - loss: 1.3853e-04 - val_loss: 1.5031e-04 - 4s/epoch - 3ms/step
Epoch 145/300
1493/1493 - 4s - loss: 1.3246e-04 - val_loss: 1.3855e-04 - 4s/epoch - 3ms/step
Epoch 146/300
1493/1493 - 4s - loss: 1.3028e-04 - val_loss: 1.3215e-04 - 4s/epoch - 3ms/step
Epoch 147/300
1493/1493 - 4s - loss: 1.2838e-04 - val_loss: 1.5423e-04 - 4s/epoch - 3ms/step
Epoch 148/300
1493/1493 - 4s - loss: 1.3091e-04 - val_loss: 1.7171e-04 - 4s/epoch - 3ms/step
Epoch 149/300
1493/1493 - 4s - loss: 1.2752e-04 - val_loss: 1.3606e-04 - 4s/epoch - 3ms/step
Epoch 150/300
1493/1493 - 4s - loss: 1.2687e-04 - val_loss: 1.3087e-04 - 4s/epoch - 3ms/step
Epoch 151/300
1493/1493 - 4s - loss: 1.2656e-04 - val_loss: 1.5327e-04 - 4s/epoch - 3ms/step
Epoch 152/300
1493/1493 - 4s - loss: 1.2652e-04 - val_loss: 1.8211e-04 - 4s/epoch - 3ms/step
Epoch 153/300
1493/1493 - 4s - loss: 1.3572e-04 - val_loss: 1.4926e-04 - 4s/epoch - 3ms/step
Epoch 154/300
1493/1493 - 4s - loss: 1.2721e-04 - val_loss: 2.3402e-04 - 4s/epoch - 3ms/step
Epoch 155/300
1493/1493 - 4s - loss: 1.5460e-04 - val_loss: 1.3590e-04 - 4s/epoch - 3ms/step
Epoch 156/300
1493/1493 - 4s - loss: 1.2949e-04 - val_loss: 1.3904e-04 - 4s/epoch - 3ms/step
Epoch 157/300
1493/1493 - 4s - loss: 1.2633e-04 - val_loss: 1.8274e-04 - 4s/epoch - 3ms/step
Epoch 158/300
1493/1493 - 4s - loss: 1.4022e-04 - val_loss: 1.3732e-04 - 4s/epoch - 3ms/step
Epoch 159/300
1493/1493 - 4s - loss: 1.2636e-04 - val_loss: 1.1840e-04 - 4s/epoch - 3ms/step
Epoch 160/300
1493/1493 - 4s - loss: 1.2504e-04 - val_loss: 1.6450e-04 - 4s/epoch - 3ms/step
Epoch 161/300
1493/1493 - 4s - loss: 1.3634e-04 - val_loss: 1.3142e-04 - 4s/epoch - 3ms/step
Epoch 162/300
1493/1493 - 4s - loss: 1.2575e-04 - val_loss: 1.2827e-04 - 4s/epoch - 3ms/step
Epoch 163/300
1493/1493 - 4s - loss: 1.2493e-04 - val_loss: 1.5674e-04 - 4s/epoch - 3ms/step
Epoch 164/300
1493/1493 - 4s - loss: 1.2833e-04 - val_loss: 1.6202e-04 - 4s/epoch - 3ms/step
Epoch 165/300
1493/1493 - 4s - loss: 1.4112e-04 - val_loss: 1.3084e-04 - 4s/epoch - 3ms/step
Epoch 166/300
1493/1493 - 4s - loss: 1.2496e-04 - val_loss: 1.4097e-04 - 4s/epoch - 3ms/step
Epoch 167/300
1493/1493 - 4s - loss: 1.2457e-04 - val_loss: 1.3700e-04 - 4s/epoch - 3ms/step
Epoch 168/300
1493/1493 - 4s - loss: 1.2811e-04 - val_loss: 1.3846e-04 - 4s/epoch - 3ms/step
Epoch 169/300
1493/1493 - 4s - loss: 1.2494e-04 - val_loss: 1.5003e-04 - 4s/epoch - 3ms/step
Epoch 170/300
1493/1493 - 4s - loss: 1.2518e-04 - val_loss: 2.6105e-04 - 4s/epoch - 3ms/step
Epoch 171/300
1493/1493 - 4s - loss: 1.5828e-04 - val_loss: 2.0602e-04 - 4s/epoch - 3ms/step
Epoch 172/300
1493/1493 - 4s - loss: 1.3277e-04 - val_loss: 1.2608e-04 - 4s/epoch - 3ms/step
Epoch 173/300
1493/1493 - 4s - loss: 1.2505e-04 - val_loss: 1.4029e-04 - 4s/epoch - 3ms/step
Epoch 174/300
1493/1493 - 4s - loss: 1.2312e-04 - val_loss: 1.3304e-04 - 4s/epoch - 3ms/step
Epoch 175/300
1493/1493 - 4s - loss: 1.2404e-04 - val_loss: 1.5017e-04 - 4s/epoch - 3ms/step
Epoch 176/300
1493/1493 - 4s - loss: 1.2479e-04 - val_loss: 1.5590e-04 - 4s/epoch - 3ms/step
Epoch 177/300
1493/1493 - 4s - loss: 1.2348e-04 - val_loss: 1.5205e-04 - 4s/epoch - 3ms/step
Epoch 178/300
1493/1493 - 4s - loss: 1.2172e-04 - val_loss: 1.2553e-04 - 4s/epoch - 3ms/step
Epoch 179/300
1493/1493 - 4s - loss: 1.2002e-04 - val_loss: 1.4158e-04 - 4s/epoch - 3ms/step
Epoch 180/300
1493/1493 - 4s - loss: 1.2125e-04 - val_loss: 2.7784e-04 - 4s/epoch - 3ms/step
Epoch 181/300
1493/1493 - 4s - loss: 1.4462e-04 - val_loss: 1.5175e-04 - 4s/epoch - 3ms/step
Epoch 182/300
1493/1493 - 4s - loss: 1.2837e-04 - val_loss: 1.3351e-04 - 4s/epoch - 3ms/step
Epoch 183/300
1493/1493 - 4s - loss: 1.2080e-04 - val_loss: 1.3823e-04 - 4s/epoch - 3ms/step
Epoch 184/300
1493/1493 - 4s - loss: 1.2357e-04 - val_loss: 1.2372e-04 - 4s/epoch - 3ms/step
Epoch 185/300
1493/1493 - 4s - loss: 1.1884e-04 - val_loss: 1.5519e-04 - 4s/epoch - 3ms/step
Epoch 186/300
1493/1493 - 4s - loss: 1.2168e-04 - val_loss: 1.3838e-04 - 4s/epoch - 3ms/step
Epoch 187/300
1493/1493 - 4s - loss: 1.1932e-04 - val_loss: 1.2845e-04 - 4s/epoch - 3ms/step
Epoch 188/300
1493/1493 - 4s - loss: 1.1873e-04 - val_loss: 1.5381e-04 - 4s/epoch - 3ms/step
Epoch 189/300
1493/1493 - 4s - loss: 1.1961e-04 - val_loss: 1.4414e-04 - 4s/epoch - 3ms/step
Epoch 190/300
1493/1493 - 4s - loss: 1.2057e-04 - val_loss: 1.8243e-04 - 4s/epoch - 3ms/step
Epoch 191/300
1493/1493 - 4s - loss: 1.2937e-04 - val_loss: 5.6617e-04 - 4s/epoch - 3ms/step
Epoch 192/300
1493/1493 - 4s - loss: 1.8591e-04 - val_loss: 1.4933e-04 - 4s/epoch - 3ms/step
Epoch 193/300
1493/1493 - 4s - loss: 1.3783e-04 - val_loss: 2.4963e-04 - 4s/epoch - 3ms/step
Epoch 194/300
1493/1493 - 4s - loss: 1.3951e-04 - val_loss: 1.9337e-04 - 4s/epoch - 3ms/step
Epoch 195/300
1493/1493 - 4s - loss: 1.4586e-04 - val_loss: 3.8382e-04 - 4s/epoch - 3ms/step
Epoch 196/300
1493/1493 - 4s - loss: 1.6218e-04 - val_loss: 1.3877e-04 - 4s/epoch - 3ms/step
Epoch 197/300
1493/1493 - 4s - loss: 1.2703e-04 - val_loss: 2.0031e-04 - 4s/epoch - 3ms/step
Epoch 198/300
1493/1493 - 4s - loss: 1.3402e-04 - val_loss: 1.4177e-04 - 4s/epoch - 3ms/step
Epoch 199/300
1493/1493 - 4s - loss: 1.2274e-04 - val_loss: 1.1788e-04 - 4s/epoch - 3ms/step
Epoch 200/300
1493/1493 - 4s - loss: 1.2028e-04 - val_loss: 1.3097e-04 - 4s/epoch - 3ms/step
Epoch 201/300
1493/1493 - 4s - loss: 1.1980e-04 - val_loss: 1.3174e-04 - 4s/epoch - 3ms/step
Epoch 202/300
1493/1493 - 4s - loss: 1.1951e-04 - val_loss: 1.4198e-04 - 4s/epoch - 3ms/step
Epoch 203/300
1493/1493 - 4s - loss: 1.2041e-04 - val_loss: 1.4063e-04 - 4s/epoch - 3ms/step
Epoch 204/300
1493/1493 - 4s - loss: 1.2411e-04 - val_loss: 1.1765e-04 - 4s/epoch - 3ms/step
Epoch 205/300
1493/1493 - 4s - loss: 1.1648e-04 - val_loss: 1.4474e-04 - 4s/epoch - 3ms/step
Epoch 206/300
1493/1493 - 4s - loss: 1.1644e-04 - val_loss: 1.7297e-04 - 4s/epoch - 3ms/step
Epoch 207/300
1493/1493 - 4s - loss: 1.1767e-04 - val_loss: 1.2174e-04 - 4s/epoch - 3ms/step
Epoch 208/300
1493/1493 - 4s - loss: 1.1617e-04 - val_loss: 1.4898e-04 - 4s/epoch - 3ms/step
Epoch 209/300
1493/1493 - 4s - loss: 1.1665e-04 - val_loss: 2.2500e-04 - 4s/epoch - 3ms/step
Epoch 210/300
1493/1493 - 4s - loss: 1.3168e-04 - val_loss: 3.1528e-04 - 4s/epoch - 3ms/step
Epoch 211/300
1493/1493 - 4s - loss: 1.3676e-04 - val_loss: 2.1395e-04 - 4s/epoch - 3ms/step
Epoch 212/300
1493/1493 - 4s - loss: 1.2728e-04 - val_loss: 1.2536e-04 - 4s/epoch - 3ms/step
Epoch 213/300
1493/1493 - 4s - loss: 1.1905e-04 - val_loss: 1.2830e-04 - 4s/epoch - 3ms/step
Epoch 214/300
1493/1493 - 4s - loss: 1.1627e-04 - val_loss: 1.4210e-04 - 4s/epoch - 3ms/step
Epoch 215/300
1493/1493 - 4s - loss: 1.1587e-04 - val_loss: 1.4983e-04 - 4s/epoch - 3ms/step
Epoch 216/300
1493/1493 - 4s - loss: 1.1672e-04 - val_loss: 1.2832e-04 - 4s/epoch - 3ms/step
Epoch 217/300
1493/1493 - 4s - loss: 1.1275e-04 - val_loss: 1.1998e-04 - 4s/epoch - 3ms/step
Epoch 218/300
1493/1493 - 4s - loss: 1.1254e-04 - val_loss: 1.2288e-04 - 4s/epoch - 3ms/step
Epoch 219/300
1493/1493 - 4s - loss: 1.1485e-04 - val_loss: 3.0582e-04 - 4s/epoch - 3ms/step
Epoch 220/300
1493/1493 - 4s - loss: 1.3997e-04 - val_loss: 1.1560e-04 - 4s/epoch - 3ms/step
Epoch 221/300
1493/1493 - 4s - loss: 1.1833e-04 - val_loss: 1.8262e-04 - 4s/epoch - 3ms/step
Epoch 222/300
1493/1493 - 4s - loss: 1.2422e-04 - val_loss: 1.5753e-04 - 4s/epoch - 3ms/step
Epoch 223/300
1493/1493 - 4s - loss: 1.1776e-04 - val_loss: 1.2228e-04 - 4s/epoch - 3ms/step
Epoch 224/300
1493/1493 - 4s - loss: 1.1458e-04 - val_loss: 1.5030e-04 - 4s/epoch - 3ms/step
Epoch 225/300
1493/1493 - 4s - loss: 1.1456e-04 - val_loss: 1.5065e-04 - 4s/epoch - 3ms/step
Epoch 226/300
1493/1493 - 4s - loss: 1.1781e-04 - val_loss: 1.2626e-04 - 4s/epoch - 3ms/step
Epoch 227/300
1493/1493 - 4s - loss: 1.1769e-04 - val_loss: 1.5018e-04 - 4s/epoch - 3ms/step
Epoch 228/300
1493/1493 - 4s - loss: 1.1282e-04 - val_loss: 1.3848e-04 - 4s/epoch - 3ms/step
Epoch 229/300
1493/1493 - 4s - loss: 1.1468e-04 - val_loss: 1.3073e-04 - 4s/epoch - 3ms/step
Epoch 230/300
1493/1493 - 4s - loss: 1.1280e-04 - val_loss: 1.3177e-04 - 4s/epoch - 3ms/step
Epoch 231/300
1493/1493 - 4s - loss: 1.1297e-04 - val_loss: 1.2389e-04 - 4s/epoch - 3ms/step
Epoch 232/300
1493/1493 - 4s - loss: 1.1516e-04 - val_loss: 3.2813e-04 - 4s/epoch - 3ms/step
Epoch 233/300
1493/1493 - 4s - loss: 1.6323e-04 - val_loss: 1.3029e-04 - 4s/epoch - 3ms/step
Epoch 234/300
1493/1493 - 4s - loss: 1.1993e-04 - val_loss: 1.4553e-04 - 4s/epoch - 3ms/step
Epoch 235/300
1493/1493 - 4s - loss: 1.2919e-04 - val_loss: 1.2710e-04 - 4s/epoch - 3ms/step
Epoch 236/300
1493/1493 - 4s - loss: 1.1378e-04 - val_loss: 1.1938e-04 - 4s/epoch - 3ms/step
Epoch 237/300
1493/1493 - 4s - loss: 1.1430e-04 - val_loss: 1.9998e-04 - 4s/epoch - 3ms/step
Epoch 238/300
1493/1493 - 4s - loss: 1.4231e-04 - val_loss: 1.1859e-04 - 4s/epoch - 3ms/step
Epoch 239/300
1493/1493 - 4s - loss: 1.1627e-04 - val_loss: 2.0436e-04 - 4s/epoch - 3ms/step
Epoch 240/300
1493/1493 - 4s - loss: 1.2246e-04 - val_loss: 1.3075e-04 - 4s/epoch - 3ms/step
Epoch 241/300
1493/1493 - 4s - loss: 1.1299e-04 - val_loss: 1.1152e-04 - 4s/epoch - 3ms/step
Epoch 242/300
1493/1493 - 4s - loss: 1.1178e-04 - val_loss: 1.4143e-04 - 4s/epoch - 3ms/step
Epoch 243/300
1493/1493 - 4s - loss: 1.1859e-04 - val_loss: 2.4568e-04 - 4s/epoch - 3ms/step
Epoch 244/300
1493/1493 - 4s - loss: 1.3846e-04 - val_loss: 1.7409e-04 - 4s/epoch - 3ms/step
Epoch 245/300
1493/1493 - 4s - loss: 1.1601e-04 - val_loss: 1.3599e-04 - 4s/epoch - 3ms/step
Epoch 246/300
1493/1493 - 4s - loss: 1.1410e-04 - val_loss: 2.0346e-04 - 4s/epoch - 3ms/step
Epoch 247/300
1493/1493 - 4s - loss: 1.3207e-04 - val_loss: 2.6062e-04 - 4s/epoch - 3ms/step
Epoch 248/300
1493/1493 - 4s - loss: 1.3071e-04 - val_loss: 2.8083e-04 - 4s/epoch - 3ms/step
Epoch 249/300
1493/1493 - 4s - loss: 1.3214e-04 - val_loss: 1.1820e-04 - 4s/epoch - 3ms/step
Epoch 250/300
1493/1493 - 4s - loss: 1.1503e-04 - val_loss: 1.7217e-04 - 4s/epoch - 3ms/step
Epoch 251/300
1493/1493 - 4s - loss: 1.1888e-04 - val_loss: 1.9585e-04 - 4s/epoch - 3ms/step
Epoch 252/300
1493/1493 - 4s - loss: 1.2641e-04 - val_loss: 1.7367e-04 - 4s/epoch - 3ms/step
Epoch 253/300
1493/1493 - 4s - loss: 1.1484e-04 - val_loss: 1.3126e-04 - 4s/epoch - 3ms/step
Epoch 254/300
1493/1493 - 4s - loss: 1.1284e-04 - val_loss: 1.8020e-04 - 4s/epoch - 3ms/step
Epoch 255/300
1493/1493 - 4s - loss: 1.2371e-04 - val_loss: 1.1901e-04 - 4s/epoch - 3ms/step
Epoch 256/300
1493/1493 - 4s - loss: 1.1281e-04 - val_loss: 1.3804e-04 - 4s/epoch - 3ms/step
Epoch 257/300
1493/1493 - 4s - loss: 1.1448e-04 - val_loss: 3.7030e-04 - 4s/epoch - 3ms/step
Epoch 258/300
1493/1493 - 4s - loss: 1.7743e-04 - val_loss: 1.3341e-04 - 4s/epoch - 3ms/step
Epoch 259/300
1493/1493 - 4s - loss: 1.2200e-04 - val_loss: 5.2531e-04 - 4s/epoch - 3ms/step
Epoch 260/300
1493/1493 - 4s - loss: 1.5864e-04 - val_loss: 1.4584e-04 - 4s/epoch - 3ms/step
Epoch 261/300
1493/1493 - 4s - loss: 1.2002e-04 - val_loss: 1.8909e-04 - 4s/epoch - 3ms/step
Epoch 262/300
1493/1493 - 4s - loss: 1.1513e-04 - val_loss: 1.3400e-04 - 4s/epoch - 3ms/step
Epoch 263/300
1493/1493 - 4s - loss: 1.1227e-04 - val_loss: 1.5734e-04 - 4s/epoch - 3ms/step
Epoch 264/300
1493/1493 - 4s - loss: 1.2870e-04 - val_loss: 1.2833e-04 - 4s/epoch - 3ms/step
Epoch 265/300
1493/1493 - 4s - loss: 1.1171e-04 - val_loss: 1.2449e-04 - 4s/epoch - 3ms/step
Epoch 266/300
1493/1493 - 4s - loss: 1.1214e-04 - val_loss: 1.2875e-04 - 4s/epoch - 3ms/step
Epoch 267/300
1493/1493 - 4s - loss: 1.1026e-04 - val_loss: 1.5087e-04 - 4s/epoch - 3ms/step
Epoch 268/300
1493/1493 - 4s - loss: 1.1026e-04 - val_loss: 1.2232e-04 - 4s/epoch - 3ms/step
Epoch 269/300
1493/1493 - 4s - loss: 1.1342e-04 - val_loss: 3.8134e-04 - 4s/epoch - 3ms/step
Epoch 270/300
1493/1493 - 4s - loss: 1.5657e-04 - val_loss: 3.4522e-04 - 4s/epoch - 3ms/step
Epoch 271/300
1493/1493 - 4s - loss: 1.2955e-04 - val_loss: 1.4111e-04 - 4s/epoch - 3ms/step
Epoch 272/300
1493/1493 - 4s - loss: 1.1527e-04 - val_loss: 2.2141e-04 - 4s/epoch - 3ms/step
Epoch 273/300
1493/1493 - 4s - loss: 1.3200e-04 - val_loss: 4.3149e-04 - 4s/epoch - 3ms/step
Epoch 274/300
1493/1493 - 4s - loss: 1.4032e-04 - val_loss: 1.9317e-04 - 4s/epoch - 3ms/step
Epoch 275/300
1493/1493 - 4s - loss: 1.1904e-04 - val_loss: 1.2336e-04 - 4s/epoch - 3ms/step
Epoch 276/300
1493/1493 - 4s - loss: 1.1175e-04 - val_loss: 1.2778e-04 - 4s/epoch - 3ms/step
Epoch 277/300
1493/1493 - 4s - loss: 1.1087e-04 - val_loss: 1.5385e-04 - 4s/epoch - 3ms/step
Epoch 278/300
1493/1493 - 4s - loss: 1.1207e-04 - val_loss: 1.4639e-04 - 4s/epoch - 3ms/step
Epoch 279/300
1493/1493 - 4s - loss: 1.0943e-04 - val_loss: 1.3707e-04 - 4s/epoch - 3ms/step
Epoch 280/300
1493/1493 - 4s - loss: 1.1420e-04 - val_loss: 1.6575e-04 - 4s/epoch - 3ms/step
Epoch 281/300
1493/1493 - 4s - loss: 1.1445e-04 - val_loss: 1.4985e-04 - 4s/epoch - 3ms/step
Epoch 282/300
1493/1493 - 4s - loss: 1.0964e-04 - val_loss: 1.2948e-04 - 4s/epoch - 3ms/step
Epoch 283/300
1493/1493 - 4s - loss: 1.0721e-04 - val_loss: 1.3362e-04 - 4s/epoch - 3ms/step
Epoch 284/300
1493/1493 - 4s - loss: 1.0839e-04 - val_loss: 1.2528e-04 - 4s/epoch - 3ms/step
Epoch 285/300
1493/1493 - 4s - loss: 1.0855e-04 - val_loss: 2.1595e-04 - 4s/epoch - 3ms/step
Epoch 286/300
1493/1493 - 4s - loss: 1.0662e-04 - val_loss: 1.1925e-04 - 4s/epoch - 3ms/step
Epoch 287/300
1493/1493 - 4s - loss: 1.0644e-04 - val_loss: 2.2022e-04 - 4s/epoch - 3ms/step
Epoch 288/300
1493/1493 - 4s - loss: 1.1754e-04 - val_loss: 1.4539e-04 - 4s/epoch - 3ms/step
Epoch 289/300
1493/1493 - 4s - loss: 1.0893e-04 - val_loss: 2.7550e-04 - 4s/epoch - 3ms/step
Epoch 290/300
1493/1493 - 4s - loss: 1.2212e-04 - val_loss: 2.3904e-04 - 4s/epoch - 3ms/step
Epoch 291/300
1493/1493 - 4s - loss: 1.2599e-04 - val_loss: 1.3695e-04 - 4s/epoch - 3ms/step
Epoch 292/300
1493/1493 - 4s - loss: 1.0809e-04 - val_loss: 1.3062e-04 - 4s/epoch - 3ms/step
Epoch 293/300
1493/1493 - 4s - loss: 1.0821e-04 - val_loss: 1.3087e-04 - 4s/epoch - 3ms/step
Epoch 294/300
1493/1493 - 4s - loss: 1.0877e-04 - val_loss: 1.4997e-04 - 4s/epoch - 3ms/step
Epoch 295/300
1493/1493 - 4s - loss: 1.1026e-04 - val_loss: 1.2332e-04 - 4s/epoch - 3ms/step
Epoch 296/300
1493/1493 - 4s - loss: 1.0651e-04 - val_loss: 1.6139e-04 - 4s/epoch - 3ms/step
Epoch 297/300
1493/1493 - 4s - loss: 1.0601e-04 - val_loss: 1.3780e-04 - 4s/epoch - 3ms/step
Epoch 298/300
1493/1493 - 4s - loss: 1.0952e-04 - val_loss: 2.2036e-04 - 4s/epoch - 3ms/step
Epoch 299/300
1493/1493 - 4s - loss: 1.3055e-04 - val_loss: 2.0571e-04 - 4s/epoch - 3ms/step
Epoch 300/300
1493/1493 - 4s - loss: 1.2824e-04 - val_loss: 1.9437e-04 - 4s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00019436515867710114
  1/332 [..............................] - ETA: 30s 50/332 [===>..........................] - ETA: 0s  98/332 [=======>......................] - ETA: 0s147/332 [============>.................] - ETA: 0s196/332 [================>.............] - ETA: 0s244/332 [=====================>........] - ETA: 0s293/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.002200322323280418
cosine 0.0017499218976052807
MAE: 0.0094147585
RMSE: 0.013941489
r2: 0.9873982811175523
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_51"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_52 (InputLayer)       multiple                  0         
                                                                 
 dense_51 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_51 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_51 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_52 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_52 (ReLU)             (None, 632)               0         
                                                                 
 dense_52 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_53 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_53 (ReLU)             (None, 2528)              0         
                                                                 
 dense_53 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_52"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_53 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_52 (InputLayer)       multiple                  0         
                                                                 
 dense_51 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_51 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_51 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_53"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_54 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_52 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_52 (ReLU)             (None, 632)               0         
                                                                 
 dense_52 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_53 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_53 (ReLU)             (None, 2528)              0         
                                                                 
 dense_53 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 64, 300, 0.001, 0.5, 632, 0.00012824211444240063, 0.00019436515867710114, 0.002200322323280418, 0.0017499218976052807, 0.009414758533239365, 0.013941489160060883, 0.9873982811175523, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_54 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_54 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_54 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_55 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_55 (ReLU)             (None, 632)               0         
                                                                 
 dense_55 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_56 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_56 (ReLU)             (None, 2528)              0         
                                                                 
 dense_56 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Epoch 1/300
1493/1493 - 6s - loss: 0.0116 - val_loss: 0.0029 - 6s/epoch - 4ms/step
Epoch 2/300
1493/1493 - 5s - loss: 0.0029 - val_loss: 0.0022 - 5s/epoch - 3ms/step
Epoch 3/300
1493/1493 - 5s - loss: 0.0022 - val_loss: 0.0025 - 5s/epoch - 3ms/step
Epoch 4/300
1493/1493 - 5s - loss: 0.0018 - val_loss: 0.0019 - 5s/epoch - 3ms/step
Epoch 5/300
1493/1493 - 5s - loss: 0.0014 - val_loss: 0.0013 - 5s/epoch - 3ms/step
Epoch 6/300
1493/1493 - 5s - loss: 0.0011 - val_loss: 0.0013 - 5s/epoch - 3ms/step
Epoch 7/300
1493/1493 - 5s - loss: 9.5757e-04 - val_loss: 7.7803e-04 - 5s/epoch - 3ms/step
Epoch 8/300
1493/1493 - 5s - loss: 7.8914e-04 - val_loss: 7.3369e-04 - 5s/epoch - 3ms/step
Epoch 9/300
1493/1493 - 5s - loss: 6.7228e-04 - val_loss: 6.2215e-04 - 5s/epoch - 3ms/step
Epoch 10/300
1493/1493 - 5s - loss: 6.0014e-04 - val_loss: 5.3680e-04 - 5s/epoch - 3ms/step
Epoch 11/300
1493/1493 - 5s - loss: 5.4816e-04 - val_loss: 5.7750e-04 - 5s/epoch - 3ms/step
Epoch 12/300
1493/1493 - 5s - loss: 5.0093e-04 - val_loss: 0.0018 - 5s/epoch - 3ms/step
Epoch 13/300
1493/1493 - 5s - loss: 7.8083e-04 - val_loss: 0.0018 - 5s/epoch - 3ms/step
Epoch 14/300
1493/1493 - 5s - loss: 8.5230e-04 - val_loss: 5.4374e-04 - 5s/epoch - 3ms/step
Epoch 15/300
1493/1493 - 5s - loss: 5.4541e-04 - val_loss: 5.2727e-04 - 5s/epoch - 3ms/step
Epoch 16/300
1493/1493 - 5s - loss: 4.8679e-04 - val_loss: 4.3758e-04 - 5s/epoch - 3ms/step
Epoch 17/300
1493/1493 - 5s - loss: 4.4777e-04 - val_loss: 4.4734e-04 - 5s/epoch - 3ms/step
Epoch 18/300
1493/1493 - 5s - loss: 4.3814e-04 - val_loss: 0.0011 - 5s/epoch - 3ms/step
Epoch 19/300
1493/1493 - 5s - loss: 5.2099e-04 - val_loss: 5.1420e-04 - 5s/epoch - 3ms/step
Epoch 20/300
1493/1493 - 5s - loss: 4.1940e-04 - val_loss: 3.6207e-04 - 5s/epoch - 3ms/step
Epoch 21/300
1493/1493 - 5s - loss: 3.9665e-04 - val_loss: 0.0011 - 5s/epoch - 3ms/step
Epoch 22/300
1493/1493 - 5s - loss: 5.4232e-04 - val_loss: 0.0022 - 5s/epoch - 3ms/step
Epoch 23/300
1493/1493 - 5s - loss: 6.2074e-04 - val_loss: 6.2308e-04 - 5s/epoch - 3ms/step
Epoch 24/300
1493/1493 - 5s - loss: 4.2547e-04 - val_loss: 3.5105e-04 - 5s/epoch - 3ms/step
Epoch 25/300
1493/1493 - 5s - loss: 3.9616e-04 - val_loss: 4.0342e-04 - 5s/epoch - 3ms/step
Epoch 26/300
1493/1493 - 5s - loss: 3.7631e-04 - val_loss: 3.4513e-04 - 5s/epoch - 3ms/step
Epoch 27/300
1493/1493 - 5s - loss: 3.6012e-04 - val_loss: 3.2217e-04 - 5s/epoch - 3ms/step
Epoch 28/300
1493/1493 - 5s - loss: 3.5013e-04 - val_loss: 4.7203e-04 - 5s/epoch - 3ms/step
Epoch 29/300
1493/1493 - 5s - loss: 3.6558e-04 - val_loss: 3.1044e-04 - 5s/epoch - 3ms/step
Epoch 30/300
1493/1493 - 5s - loss: 3.3935e-04 - val_loss: 3.1271e-04 - 5s/epoch - 3ms/step
Epoch 31/300
1493/1493 - 5s - loss: 3.3321e-04 - val_loss: 4.0723e-04 - 5s/epoch - 3ms/step
Epoch 32/300
1493/1493 - 5s - loss: 3.3563e-04 - val_loss: 4.1158e-04 - 5s/epoch - 3ms/step
Epoch 33/300
1493/1493 - 5s - loss: 3.7036e-04 - val_loss: 2.8149e-04 - 5s/epoch - 3ms/step
Epoch 34/300
1493/1493 - 5s - loss: 3.2254e-04 - val_loss: 2.7096e-04 - 5s/epoch - 3ms/step
Epoch 35/300
1493/1493 - 5s - loss: 3.1250e-04 - val_loss: 4.5206e-04 - 5s/epoch - 3ms/step
Epoch 36/300
1493/1493 - 5s - loss: 3.4206e-04 - val_loss: 3.7970e-04 - 5s/epoch - 3ms/step
Epoch 37/300
1493/1493 - 5s - loss: 3.4117e-04 - val_loss: 3.9255e-04 - 5s/epoch - 3ms/step
Epoch 38/300
1493/1493 - 5s - loss: 3.3411e-04 - val_loss: 2.8214e-04 - 5s/epoch - 3ms/step
Epoch 39/300
1493/1493 - 5s - loss: 3.0855e-04 - val_loss: 2.7070e-04 - 5s/epoch - 3ms/step
Epoch 40/300
1493/1493 - 5s - loss: 2.9546e-04 - val_loss: 4.1928e-04 - 5s/epoch - 3ms/step
Epoch 41/300
1493/1493 - 5s - loss: 3.3068e-04 - val_loss: 2.4846e-04 - 5s/epoch - 3ms/step
Epoch 42/300
1493/1493 - 5s - loss: 2.9378e-04 - val_loss: 2.5589e-04 - 5s/epoch - 3ms/step
Epoch 43/300
1493/1493 - 5s - loss: 2.8792e-04 - val_loss: 3.5319e-04 - 5s/epoch - 3ms/step
Epoch 44/300
1493/1493 - 5s - loss: 3.1259e-04 - val_loss: 3.1884e-04 - 5s/epoch - 3ms/step
Epoch 45/300
1493/1493 - 5s - loss: 3.0518e-04 - val_loss: 2.5791e-04 - 5s/epoch - 3ms/step
Epoch 46/300
1493/1493 - 5s - loss: 2.8535e-04 - val_loss: 2.5463e-04 - 5s/epoch - 3ms/step
Epoch 47/300
1493/1493 - 5s - loss: 2.8087e-04 - val_loss: 2.4962e-04 - 5s/epoch - 3ms/step
Epoch 48/300
1493/1493 - 5s - loss: 2.8988e-04 - val_loss: 3.4402e-04 - 5s/epoch - 3ms/step
Epoch 49/300
1493/1493 - 5s - loss: 2.8798e-04 - val_loss: 5.7967e-04 - 5s/epoch - 3ms/step
Epoch 50/300
1493/1493 - 5s - loss: 3.4420e-04 - val_loss: 2.4028e-04 - 5s/epoch - 3ms/step
Epoch 51/300
1493/1493 - 5s - loss: 2.7859e-04 - val_loss: 2.2540e-04 - 5s/epoch - 3ms/step
Epoch 52/300
1493/1493 - 5s - loss: 2.7056e-04 - val_loss: 2.2976e-04 - 5s/epoch - 3ms/step
Epoch 53/300
1493/1493 - 5s - loss: 2.6755e-04 - val_loss: 2.5069e-04 - 5s/epoch - 3ms/step
Epoch 54/300
1493/1493 - 5s - loss: 2.6843e-04 - val_loss: 2.8429e-04 - 5s/epoch - 3ms/step
Epoch 55/300
1493/1493 - 5s - loss: 2.6622e-04 - val_loss: 2.3281e-04 - 5s/epoch - 3ms/step
Epoch 56/300
1493/1493 - 5s - loss: 2.6039e-04 - val_loss: 2.5480e-04 - 5s/epoch - 3ms/step
Epoch 57/300
1493/1493 - 5s - loss: 2.5611e-04 - val_loss: 2.5883e-04 - 5s/epoch - 3ms/step
Epoch 58/300
1493/1493 - 5s - loss: 2.5821e-04 - val_loss: 2.3330e-04 - 5s/epoch - 3ms/step
Epoch 59/300
1493/1493 - 5s - loss: 2.5356e-04 - val_loss: 2.3009e-04 - 5s/epoch - 3ms/step
Epoch 60/300
1493/1493 - 5s - loss: 2.5067e-04 - val_loss: 2.4295e-04 - 5s/epoch - 3ms/step
Epoch 61/300
1493/1493 - 5s - loss: 2.5140e-04 - val_loss: 2.2529e-04 - 5s/epoch - 3ms/step
Epoch 62/300
1493/1493 - 5s - loss: 2.8497e-04 - val_loss: 2.5787e-04 - 5s/epoch - 3ms/step
Epoch 63/300
1493/1493 - 5s - loss: 2.5667e-04 - val_loss: 2.8124e-04 - 5s/epoch - 3ms/step
Epoch 64/300
1493/1493 - 5s - loss: 2.5592e-04 - val_loss: 4.8138e-04 - 5s/epoch - 3ms/step
Epoch 65/300
1493/1493 - 5s - loss: 2.8363e-04 - val_loss: 5.2758e-04 - 5s/epoch - 3ms/step
Epoch 66/300
1493/1493 - 5s - loss: 2.9403e-04 - val_loss: 2.3911e-04 - 5s/epoch - 3ms/step
Epoch 67/300
1493/1493 - 5s - loss: 2.5295e-04 - val_loss: 2.1526e-04 - 5s/epoch - 3ms/step
Epoch 68/300
1493/1493 - 5s - loss: 2.5489e-04 - val_loss: 7.1690e-04 - 5s/epoch - 3ms/step
Epoch 69/300
1493/1493 - 5s - loss: 3.7147e-04 - val_loss: 2.2806e-04 - 5s/epoch - 3ms/step
Epoch 70/300
1493/1493 - 5s - loss: 2.6619e-04 - val_loss: 9.4152e-04 - 5s/epoch - 3ms/step
Epoch 71/300
1493/1493 - 5s - loss: 3.3066e-04 - val_loss: 2.3200e-04 - 5s/epoch - 3ms/step
Epoch 72/300
1493/1493 - 5s - loss: 2.6802e-04 - val_loss: 5.6970e-04 - 5s/epoch - 3ms/step
Epoch 73/300
1493/1493 - 5s - loss: 3.4603e-04 - val_loss: 0.0011 - 5s/epoch - 3ms/step
Epoch 74/300
1493/1493 - 5s - loss: 3.7757e-04 - val_loss: 2.7728e-04 - 5s/epoch - 3ms/step
Epoch 75/300
1493/1493 - 5s - loss: 2.8390e-04 - val_loss: 2.3693e-04 - 5s/epoch - 3ms/step
Epoch 76/300
1493/1493 - 5s - loss: 2.7368e-04 - val_loss: 2.2494e-04 - 5s/epoch - 3ms/step
Epoch 77/300
1493/1493 - 5s - loss: 2.6068e-04 - val_loss: 3.0711e-04 - 5s/epoch - 3ms/step
Epoch 78/300
1493/1493 - 5s - loss: 2.6619e-04 - val_loss: 2.7492e-04 - 5s/epoch - 3ms/step
Epoch 79/300
1493/1493 - 5s - loss: 2.5891e-04 - val_loss: 3.9315e-04 - 5s/epoch - 3ms/step
Epoch 80/300
1493/1493 - 5s - loss: 2.7682e-04 - val_loss: 3.2547e-04 - 5s/epoch - 3ms/step
Epoch 81/300
1493/1493 - 5s - loss: 2.7060e-04 - val_loss: 3.1607e-04 - 5s/epoch - 3ms/step
Epoch 82/300
1493/1493 - 5s - loss: 2.5211e-04 - val_loss: 2.1482e-04 - 5s/epoch - 3ms/step
Epoch 83/300
1493/1493 - 5s - loss: 2.4616e-04 - val_loss: 2.1307e-04 - 5s/epoch - 3ms/step
Epoch 84/300
1493/1493 - 5s - loss: 2.4362e-04 - val_loss: 3.0601e-04 - 5s/epoch - 3ms/step
Epoch 85/300
1493/1493 - 5s - loss: 2.5304e-04 - val_loss: 6.1301e-04 - 5s/epoch - 3ms/step
Epoch 86/300
1493/1493 - 5s - loss: 3.2816e-04 - val_loss: 2.1343e-04 - 5s/epoch - 3ms/step
Epoch 87/300
1493/1493 - 5s - loss: 2.4908e-04 - val_loss: 2.1486e-04 - 5s/epoch - 3ms/step
Epoch 88/300
1493/1493 - 5s - loss: 2.4298e-04 - val_loss: 3.2355e-04 - 5s/epoch - 3ms/step
Epoch 89/300
1493/1493 - 5s - loss: 2.5479e-04 - val_loss: 2.0866e-04 - 5s/epoch - 3ms/step
Epoch 90/300
1493/1493 - 5s - loss: 2.3861e-04 - val_loss: 2.1763e-04 - 5s/epoch - 3ms/step
Epoch 91/300
1493/1493 - 5s - loss: 2.3872e-04 - val_loss: 2.2381e-04 - 5s/epoch - 3ms/step
Epoch 92/300
1493/1493 - 5s - loss: 2.3670e-04 - val_loss: 3.9701e-04 - 5s/epoch - 3ms/step
Epoch 93/300
1493/1493 - 5s - loss: 2.8711e-04 - val_loss: 2.2498e-04 - 5s/epoch - 3ms/step
Epoch 94/300
1493/1493 - 5s - loss: 2.4081e-04 - val_loss: 2.0054e-04 - 5s/epoch - 3ms/step
Epoch 95/300
1493/1493 - 5s - loss: 2.3603e-04 - val_loss: 2.3000e-04 - 5s/epoch - 3ms/step
Epoch 96/300
1493/1493 - 5s - loss: 2.3871e-04 - val_loss: 2.9217e-04 - 5s/epoch - 3ms/step
Epoch 97/300
1493/1493 - 5s - loss: 2.3903e-04 - val_loss: 1.9720e-04 - 5s/epoch - 3ms/step
Epoch 98/300
1493/1493 - 5s - loss: 2.3893e-04 - val_loss: 5.4359e-04 - 5s/epoch - 3ms/step
Epoch 99/300
1493/1493 - 5s - loss: 3.0626e-04 - val_loss: 2.1138e-04 - 5s/epoch - 3ms/step
Epoch 100/300
1493/1493 - 5s - loss: 2.4341e-04 - val_loss: 2.1149e-04 - 5s/epoch - 3ms/step
Epoch 101/300
1493/1493 - 5s - loss: 2.4036e-04 - val_loss: 7.7847e-04 - 5s/epoch - 3ms/step
Epoch 102/300
1493/1493 - 5s - loss: 3.1795e-04 - val_loss: 3.6794e-04 - 5s/epoch - 3ms/step
Epoch 103/300
1493/1493 - 5s - loss: 3.0244e-04 - val_loss: 2.1959e-04 - 5s/epoch - 3ms/step
Epoch 104/300
1493/1493 - 5s - loss: 2.4582e-04 - val_loss: 3.9350e-04 - 5s/epoch - 3ms/step
Epoch 105/300
1493/1493 - 5s - loss: 2.7393e-04 - val_loss: 2.2299e-04 - 5s/epoch - 3ms/step
Epoch 106/300
1493/1493 - 5s - loss: 2.3649e-04 - val_loss: 2.6449e-04 - 5s/epoch - 3ms/step
Epoch 107/300
1493/1493 - 5s - loss: 2.3930e-04 - val_loss: 2.9559e-04 - 5s/epoch - 3ms/step
Epoch 108/300
1493/1493 - 5s - loss: 2.3577e-04 - val_loss: 2.1274e-04 - 5s/epoch - 3ms/step
Epoch 109/300
1493/1493 - 5s - loss: 2.2872e-04 - val_loss: 2.3804e-04 - 5s/epoch - 3ms/step
Epoch 110/300
1493/1493 - 5s - loss: 2.2780e-04 - val_loss: 2.3290e-04 - 5s/epoch - 3ms/step
Epoch 111/300
1493/1493 - 5s - loss: 2.2799e-04 - val_loss: 2.0293e-04 - 5s/epoch - 3ms/step
Epoch 112/300
1493/1493 - 5s - loss: 2.2593e-04 - val_loss: 1.8508e-04 - 5s/epoch - 3ms/step
Epoch 113/300
1493/1493 - 5s - loss: 2.2030e-04 - val_loss: 2.9051e-04 - 5s/epoch - 3ms/step
Epoch 114/300
1493/1493 - 5s - loss: 2.4461e-04 - val_loss: 2.6886e-04 - 5s/epoch - 3ms/step
Epoch 115/300
1493/1493 - 5s - loss: 2.3838e-04 - val_loss: 3.9485e-04 - 5s/epoch - 3ms/step
Epoch 116/300
1493/1493 - 5s - loss: 2.4731e-04 - val_loss: 8.0509e-04 - 5s/epoch - 3ms/step
Epoch 117/300
1493/1493 - 5s - loss: 3.1399e-04 - val_loss: 0.0014 - 5s/epoch - 3ms/step
Epoch 118/300
1493/1493 - 5s - loss: 3.4747e-04 - val_loss: 2.1244e-04 - 5s/epoch - 3ms/step
Epoch 119/300
1493/1493 - 5s - loss: 2.4561e-04 - val_loss: 2.0558e-04 - 5s/epoch - 3ms/step
Epoch 120/300
1493/1493 - 5s - loss: 2.3538e-04 - val_loss: 2.2095e-04 - 5s/epoch - 3ms/step
Epoch 121/300
1493/1493 - 5s - loss: 2.3344e-04 - val_loss: 3.1076e-04 - 5s/epoch - 3ms/step
Epoch 122/300
1493/1493 - 5s - loss: 2.3810e-04 - val_loss: 2.1843e-04 - 5s/epoch - 3ms/step
Epoch 123/300
1493/1493 - 5s - loss: 2.2777e-04 - val_loss: 2.1046e-04 - 5s/epoch - 3ms/step
Epoch 124/300
1493/1493 - 5s - loss: 2.2569e-04 - val_loss: 4.9589e-04 - 5s/epoch - 3ms/step
Epoch 125/300
1493/1493 - 5s - loss: 2.6394e-04 - val_loss: 2.0918e-04 - 5s/epoch - 3ms/step
Epoch 126/300
1493/1493 - 5s - loss: 2.2737e-04 - val_loss: 4.6222e-04 - 5s/epoch - 3ms/step
Epoch 127/300
1493/1493 - 5s - loss: 2.4932e-04 - val_loss: 2.7029e-04 - 5s/epoch - 3ms/step
Epoch 128/300
1493/1493 - 5s - loss: 2.3531e-04 - val_loss: 2.0474e-04 - 5s/epoch - 3ms/step
Epoch 129/300
1493/1493 - 5s - loss: 2.2610e-04 - val_loss: 1.8544e-04 - 5s/epoch - 3ms/step
Epoch 130/300
1493/1493 - 5s - loss: 2.2178e-04 - val_loss: 1.8481e-04 - 5s/epoch - 3ms/step
Epoch 131/300
1493/1493 - 5s - loss: 2.1905e-04 - val_loss: 2.7334e-04 - 5s/epoch - 3ms/step
Epoch 132/300
1493/1493 - 5s - loss: 2.2752e-04 - val_loss: 1.8462e-04 - 5s/epoch - 3ms/step
Epoch 133/300
1493/1493 - 5s - loss: 2.2078e-04 - val_loss: 2.1457e-04 - 5s/epoch - 3ms/step
Epoch 134/300
1493/1493 - 5s - loss: 2.1797e-04 - val_loss: 2.7737e-04 - 5s/epoch - 3ms/step
Epoch 135/300
1493/1493 - 5s - loss: 2.3267e-04 - val_loss: 1.9759e-04 - 5s/epoch - 3ms/step
Epoch 136/300
1493/1493 - 5s - loss: 2.1477e-04 - val_loss: 2.1703e-04 - 5s/epoch - 3ms/step
Epoch 137/300
1493/1493 - 5s - loss: 2.2062e-04 - val_loss: 3.9899e-04 - 5s/epoch - 3ms/step
Epoch 138/300
1493/1493 - 5s - loss: 2.4450e-04 - val_loss: 2.8995e-04 - 5s/epoch - 3ms/step
Epoch 139/300
1493/1493 - 5s - loss: 2.3150e-04 - val_loss: 2.1408e-04 - 5s/epoch - 3ms/step
Epoch 140/300
1493/1493 - 5s - loss: 2.1847e-04 - val_loss: 1.8720e-04 - 5s/epoch - 3ms/step
Epoch 141/300
1493/1493 - 5s - loss: 2.2325e-04 - val_loss: 6.4915e-04 - 5s/epoch - 3ms/step
Epoch 142/300
1493/1493 - 5s - loss: 3.2874e-04 - val_loss: 1.9386e-04 - 5s/epoch - 3ms/step
Epoch 143/300
1493/1493 - 5s - loss: 2.3115e-04 - val_loss: 2.1280e-04 - 5s/epoch - 3ms/step
Epoch 144/300
1493/1493 - 5s - loss: 2.4992e-04 - val_loss: 1.8027e-04 - 5s/epoch - 3ms/step
Epoch 145/300
1493/1493 - 5s - loss: 2.1881e-04 - val_loss: 2.0879e-04 - 5s/epoch - 3ms/step
Epoch 146/300
1493/1493 - 5s - loss: 2.1754e-04 - val_loss: 1.9334e-04 - 5s/epoch - 3ms/step
Epoch 147/300
1493/1493 - 5s - loss: 2.1492e-04 - val_loss: 1.8929e-04 - 5s/epoch - 3ms/step
Epoch 148/300
1493/1493 - 5s - loss: 2.1343e-04 - val_loss: 2.3076e-04 - 5s/epoch - 3ms/step
Epoch 149/300
1493/1493 - 5s - loss: 2.1067e-04 - val_loss: 2.1053e-04 - 5s/epoch - 3ms/step
Epoch 150/300
1493/1493 - 5s - loss: 2.0975e-04 - val_loss: 1.9179e-04 - 5s/epoch - 3ms/step
Epoch 151/300
1493/1493 - 5s - loss: 2.1295e-04 - val_loss: 2.1778e-04 - 5s/epoch - 3ms/step
Epoch 152/300
1493/1493 - 5s - loss: 2.1214e-04 - val_loss: 4.7999e-04 - 5s/epoch - 3ms/step
Epoch 153/300
1493/1493 - 5s - loss: 2.5414e-04 - val_loss: 1.8912e-04 - 5s/epoch - 3ms/step
Epoch 154/300
1493/1493 - 5s - loss: 2.1929e-04 - val_loss: 7.4540e-04 - 5s/epoch - 3ms/step
Epoch 155/300
1493/1493 - 5s - loss: 3.1849e-04 - val_loss: 2.1039e-04 - 5s/epoch - 3ms/step
Epoch 156/300
1493/1493 - 5s - loss: 2.4228e-04 - val_loss: 1.9996e-04 - 5s/epoch - 3ms/step
Epoch 157/300
1493/1493 - 5s - loss: 2.1926e-04 - val_loss: 2.2809e-04 - 5s/epoch - 3ms/step
Epoch 158/300
1493/1493 - 5s - loss: 2.2289e-04 - val_loss: 2.0266e-04 - 5s/epoch - 3ms/step
Epoch 159/300
1493/1493 - 5s - loss: 2.1235e-04 - val_loss: 1.9114e-04 - 5s/epoch - 3ms/step
Epoch 160/300
1493/1493 - 5s - loss: 2.1154e-04 - val_loss: 3.3617e-04 - 5s/epoch - 3ms/step
Epoch 161/300
1493/1493 - 5s - loss: 2.5537e-04 - val_loss: 1.9221e-04 - 5s/epoch - 3ms/step
Epoch 162/300
1493/1493 - 5s - loss: 2.1074e-04 - val_loss: 1.9003e-04 - 5s/epoch - 3ms/step
Epoch 163/300
1493/1493 - 5s - loss: 2.1104e-04 - val_loss: 2.9997e-04 - 5s/epoch - 3ms/step
Epoch 164/300
1493/1493 - 5s - loss: 2.3323e-04 - val_loss: 3.8366e-04 - 5s/epoch - 3ms/step
Epoch 165/300
1493/1493 - 5s - loss: 2.5315e-04 - val_loss: 1.7649e-04 - 5s/epoch - 3ms/step
Epoch 166/300
1493/1493 - 5s - loss: 2.1298e-04 - val_loss: 2.1036e-04 - 5s/epoch - 3ms/step
Epoch 167/300
1493/1493 - 5s - loss: 2.1025e-04 - val_loss: 1.8600e-04 - 5s/epoch - 3ms/step
Epoch 168/300
1493/1493 - 5s - loss: 2.0914e-04 - val_loss: 1.9968e-04 - 5s/epoch - 3ms/step
Epoch 169/300
1493/1493 - 5s - loss: 2.0872e-04 - val_loss: 2.8079e-04 - 5s/epoch - 3ms/step
Epoch 170/300
1493/1493 - 5s - loss: 2.0834e-04 - val_loss: 4.1377e-04 - 5s/epoch - 3ms/step
Epoch 171/300
1493/1493 - 5s - loss: 2.6502e-04 - val_loss: 3.3223e-04 - 5s/epoch - 3ms/step
Epoch 172/300
1493/1493 - 5s - loss: 2.2622e-04 - val_loss: 3.8066e-04 - 5s/epoch - 3ms/step
Epoch 173/300
1493/1493 - 5s - loss: 2.3125e-04 - val_loss: 1.7743e-04 - 5s/epoch - 3ms/step
Epoch 174/300
1493/1493 - 5s - loss: 2.0707e-04 - val_loss: 1.9910e-04 - 5s/epoch - 3ms/step
Epoch 175/300
1493/1493 - 5s - loss: 2.0728e-04 - val_loss: 2.2369e-04 - 5s/epoch - 3ms/step
Epoch 176/300
1493/1493 - 5s - loss: 2.0825e-04 - val_loss: 2.3762e-04 - 5s/epoch - 3ms/step
Epoch 177/300
1493/1493 - 5s - loss: 2.1480e-04 - val_loss: 2.6056e-04 - 5s/epoch - 3ms/step
Epoch 178/300
1493/1493 - 5s - loss: 2.1729e-04 - val_loss: 1.8173e-04 - 5s/epoch - 3ms/step
Epoch 179/300
1493/1493 - 5s - loss: 2.0308e-04 - val_loss: 1.8393e-04 - 5s/epoch - 3ms/step
Epoch 180/300
1493/1493 - 5s - loss: 2.0331e-04 - val_loss: 1.9279e-04 - 5s/epoch - 3ms/step
Epoch 181/300
1493/1493 - 5s - loss: 2.0596e-04 - val_loss: 2.1433e-04 - 5s/epoch - 3ms/step
Epoch 182/300
1493/1493 - 5s - loss: 2.0582e-04 - val_loss: 1.9817e-04 - 5s/epoch - 3ms/step
Epoch 183/300
1493/1493 - 5s - loss: 1.9975e-04 - val_loss: 2.1772e-04 - 5s/epoch - 3ms/step
Epoch 184/300
1493/1493 - 5s - loss: 2.0314e-04 - val_loss: 1.7660e-04 - 5s/epoch - 3ms/step
Epoch 185/300
1493/1493 - 5s - loss: 1.9739e-04 - val_loss: 2.2520e-04 - 5s/epoch - 3ms/step
Epoch 186/300
1493/1493 - 5s - loss: 2.0037e-04 - val_loss: 1.7781e-04 - 5s/epoch - 3ms/step
Epoch 187/300
1493/1493 - 5s - loss: 2.0026e-04 - val_loss: 1.8647e-04 - 5s/epoch - 3ms/step
Epoch 188/300
1493/1493 - 5s - loss: 1.9972e-04 - val_loss: 2.2311e-04 - 5s/epoch - 3ms/step
Epoch 189/300
1493/1493 - 5s - loss: 2.0004e-04 - val_loss: 2.1001e-04 - 5s/epoch - 3ms/step
Epoch 190/300
1493/1493 - 5s - loss: 2.0157e-04 - val_loss: 2.3094e-04 - 5s/epoch - 3ms/step
Epoch 191/300
1493/1493 - 5s - loss: 2.1335e-04 - val_loss: 3.1438e-04 - 5s/epoch - 3ms/step
Epoch 192/300
1493/1493 - 5s - loss: 2.0937e-04 - val_loss: 1.9701e-04 - 5s/epoch - 3ms/step
Epoch 193/300
1493/1493 - 5s - loss: 1.9940e-04 - val_loss: 3.0143e-04 - 5s/epoch - 3ms/step
Epoch 194/300
1493/1493 - 5s - loss: 2.2261e-04 - val_loss: 5.5863e-04 - 5s/epoch - 3ms/step
Epoch 195/300
1493/1493 - 5s - loss: 2.8279e-04 - val_loss: 3.1202e-04 - 5s/epoch - 3ms/step
Epoch 196/300
1493/1493 - 5s - loss: 2.2012e-04 - val_loss: 1.8528e-04 - 5s/epoch - 3ms/step
Epoch 197/300
1493/1493 - 5s - loss: 2.0402e-04 - val_loss: 2.6610e-04 - 5s/epoch - 3ms/step
Epoch 198/300
1493/1493 - 5s - loss: 2.0969e-04 - val_loss: 2.1301e-04 - 5s/epoch - 3ms/step
Epoch 199/300
1493/1493 - 5s - loss: 2.0084e-04 - val_loss: 1.9511e-04 - 5s/epoch - 3ms/step
Epoch 200/300
1493/1493 - 5s - loss: 2.0069e-04 - val_loss: 1.7921e-04 - 5s/epoch - 3ms/step
Epoch 201/300
1493/1493 - 5s - loss: 1.9625e-04 - val_loss: 1.7952e-04 - 5s/epoch - 3ms/step
Epoch 202/300
1493/1493 - 5s - loss: 1.9695e-04 - val_loss: 2.4944e-04 - 5s/epoch - 3ms/step
Epoch 203/300
1493/1493 - 5s - loss: 2.0172e-04 - val_loss: 4.5796e-04 - 5s/epoch - 3ms/step
Epoch 204/300
1493/1493 - 5s - loss: 2.4380e-04 - val_loss: 2.1435e-04 - 5s/epoch - 3ms/step
Epoch 205/300
1493/1493 - 5s - loss: 2.0438e-04 - val_loss: 1.9125e-04 - 5s/epoch - 3ms/step
Epoch 206/300
1493/1493 - 5s - loss: 1.9676e-04 - val_loss: 2.5073e-04 - 5s/epoch - 3ms/step
Epoch 207/300
1493/1493 - 5s - loss: 2.0872e-04 - val_loss: 2.2045e-04 - 5s/epoch - 3ms/step
Epoch 208/300
1493/1493 - 5s - loss: 2.0499e-04 - val_loss: 2.0354e-04 - 5s/epoch - 3ms/step
Epoch 209/300
1493/1493 - 5s - loss: 1.9641e-04 - val_loss: 2.9583e-04 - 5s/epoch - 3ms/step
Epoch 210/300
1493/1493 - 5s - loss: 2.1832e-04 - val_loss: 4.8958e-04 - 5s/epoch - 3ms/step
Epoch 211/300
1493/1493 - 5s - loss: 2.5569e-04 - val_loss: 2.7069e-04 - 5s/epoch - 3ms/step
Epoch 212/300
1493/1493 - 5s - loss: 2.0804e-04 - val_loss: 1.7491e-04 - 5s/epoch - 3ms/step
Epoch 213/300
1493/1493 - 5s - loss: 1.9976e-04 - val_loss: 1.7334e-04 - 5s/epoch - 3ms/step
Epoch 214/300
1493/1493 - 5s - loss: 1.9559e-04 - val_loss: 1.8241e-04 - 5s/epoch - 3ms/step
Epoch 215/300
1493/1493 - 5s - loss: 1.9677e-04 - val_loss: 3.0415e-04 - 5s/epoch - 3ms/step
Epoch 216/300
1493/1493 - 5s - loss: 2.0404e-04 - val_loss: 1.7674e-04 - 5s/epoch - 3ms/step
Epoch 217/300
1493/1493 - 5s - loss: 1.9095e-04 - val_loss: 1.7956e-04 - 5s/epoch - 3ms/step
Epoch 218/300
1493/1493 - 5s - loss: 1.9026e-04 - val_loss: 2.1909e-04 - 5s/epoch - 3ms/step
Epoch 219/300
1493/1493 - 5s - loss: 1.9477e-04 - val_loss: 2.3121e-04 - 5s/epoch - 3ms/step
Epoch 220/300
1493/1493 - 5s - loss: 2.0167e-04 - val_loss: 2.1038e-04 - 5s/epoch - 3ms/step
Epoch 221/300
1493/1493 - 5s - loss: 1.9394e-04 - val_loss: 2.7038e-04 - 5s/epoch - 3ms/step
Epoch 222/300
1493/1493 - 5s - loss: 2.0002e-04 - val_loss: 3.3370e-04 - 5s/epoch - 3ms/step
Epoch 223/300
1493/1493 - 5s - loss: 2.1978e-04 - val_loss: 3.4224e-04 - 5s/epoch - 3ms/step
Epoch 224/300
1493/1493 - 5s - loss: 2.2370e-04 - val_loss: 2.3965e-04 - 5s/epoch - 3ms/step
Epoch 225/300
1493/1493 - 5s - loss: 1.9826e-04 - val_loss: 2.3843e-04 - 5s/epoch - 3ms/step
Epoch 226/300
1493/1493 - 5s - loss: 2.0266e-04 - val_loss: 1.8788e-04 - 5s/epoch - 3ms/step
Epoch 227/300
1493/1493 - 5s - loss: 1.9498e-04 - val_loss: 1.9600e-04 - 5s/epoch - 3ms/step
Epoch 228/300
1493/1493 - 5s - loss: 1.9120e-04 - val_loss: 2.2520e-04 - 5s/epoch - 3ms/step
Epoch 229/300
1493/1493 - 5s - loss: 1.9209e-04 - val_loss: 1.8756e-04 - 5s/epoch - 3ms/step
Epoch 230/300
1493/1493 - 5s - loss: 1.8944e-04 - val_loss: 1.7883e-04 - 5s/epoch - 3ms/step
Epoch 231/300
1493/1493 - 5s - loss: 1.9560e-04 - val_loss: 1.7606e-04 - 5s/epoch - 3ms/step
Epoch 232/300
1493/1493 - 5s - loss: 1.8936e-04 - val_loss: 2.6307e-04 - 5s/epoch - 3ms/step
Epoch 233/300
1493/1493 - 5s - loss: 2.0946e-04 - val_loss: 1.8299e-04 - 5s/epoch - 3ms/step
Epoch 234/300
1493/1493 - 5s - loss: 1.8879e-04 - val_loss: 2.4671e-04 - 5s/epoch - 3ms/step
Epoch 235/300
1493/1493 - 5s - loss: 2.0013e-04 - val_loss: 2.6593e-04 - 5s/epoch - 3ms/step
Epoch 236/300
1493/1493 - 5s - loss: 1.9275e-04 - val_loss: 1.8363e-04 - 5s/epoch - 3ms/step
Epoch 237/300
1493/1493 - 5s - loss: 1.8684e-04 - val_loss: 2.1110e-04 - 5s/epoch - 3ms/step
Epoch 238/300
1493/1493 - 5s - loss: 1.8626e-04 - val_loss: 1.6903e-04 - 5s/epoch - 3ms/step
Epoch 239/300
1493/1493 - 5s - loss: 1.8612e-04 - val_loss: 2.5052e-04 - 5s/epoch - 3ms/step
Epoch 240/300
1493/1493 - 5s - loss: 1.8982e-04 - val_loss: 1.7526e-04 - 5s/epoch - 3ms/step
Epoch 241/300
1493/1493 - 5s - loss: 1.8310e-04 - val_loss: 1.6588e-04 - 5s/epoch - 3ms/step
Epoch 242/300
1493/1493 - 5s - loss: 1.8448e-04 - val_loss: 2.7923e-04 - 5s/epoch - 3ms/step
Epoch 243/300
1493/1493 - 5s - loss: 1.9965e-04 - val_loss: 1.9429e-04 - 5s/epoch - 3ms/step
Epoch 244/300
1493/1493 - 5s - loss: 1.8927e-04 - val_loss: 2.5854e-04 - 5s/epoch - 3ms/step
Epoch 245/300
1493/1493 - 5s - loss: 1.8835e-04 - val_loss: 1.9655e-04 - 5s/epoch - 3ms/step
Epoch 246/300
1493/1493 - 5s - loss: 1.9295e-04 - val_loss: 6.8784e-04 - 5s/epoch - 3ms/step
Epoch 247/300
1493/1493 - 5s - loss: 2.8469e-04 - val_loss: 0.0012 - 5s/epoch - 3ms/step
Epoch 248/300
1493/1493 - 5s - loss: 2.9385e-04 - val_loss: 5.1185e-04 - 5s/epoch - 3ms/step
Epoch 249/300
1493/1493 - 5s - loss: 2.2683e-04 - val_loss: 1.7538e-04 - 5s/epoch - 3ms/step
Epoch 250/300
1493/1493 - 5s - loss: 1.9713e-04 - val_loss: 2.0174e-04 - 5s/epoch - 3ms/step
Epoch 251/300
1493/1493 - 5s - loss: 1.9675e-04 - val_loss: 2.0480e-04 - 5s/epoch - 3ms/step
Epoch 252/300
1493/1493 - 5s - loss: 1.9531e-04 - val_loss: 2.2028e-04 - 5s/epoch - 3ms/step
Epoch 253/300
1493/1493 - 5s - loss: 1.9354e-04 - val_loss: 2.5483e-04 - 5s/epoch - 3ms/step
Epoch 254/300
1493/1493 - 5s - loss: 1.9943e-04 - val_loss: 4.5957e-04 - 5s/epoch - 3ms/step
Epoch 255/300
1493/1493 - 5s - loss: 2.3918e-04 - val_loss: 1.7814e-04 - 5s/epoch - 3ms/step
Epoch 256/300
1493/1493 - 5s - loss: 1.9543e-04 - val_loss: 1.7888e-04 - 5s/epoch - 3ms/step
Epoch 257/300
1493/1493 - 5s - loss: 1.8991e-04 - val_loss: 2.5459e-04 - 5s/epoch - 3ms/step
Epoch 258/300
1493/1493 - 5s - loss: 2.0198e-04 - val_loss: 2.3030e-04 - 5s/epoch - 3ms/step
Epoch 259/300
1493/1493 - 5s - loss: 1.9454e-04 - val_loss: 2.9449e-04 - 5s/epoch - 3ms/step
Epoch 260/300
1493/1493 - 5s - loss: 2.0900e-04 - val_loss: 2.9060e-04 - 5s/epoch - 3ms/step
Epoch 261/300
1493/1493 - 5s - loss: 1.9886e-04 - val_loss: 2.2003e-04 - 5s/epoch - 3ms/step
Epoch 262/300
1493/1493 - 5s - loss: 1.8911e-04 - val_loss: 1.8638e-04 - 5s/epoch - 3ms/step
Epoch 263/300
1493/1493 - 5s - loss: 1.8604e-04 - val_loss: 1.9081e-04 - 5s/epoch - 3ms/step
Epoch 264/300
1493/1493 - 5s - loss: 1.8653e-04 - val_loss: 1.9118e-04 - 5s/epoch - 3ms/step
Epoch 265/300
1493/1493 - 5s - loss: 1.8404e-04 - val_loss: 1.9155e-04 - 5s/epoch - 3ms/step
Epoch 266/300
1493/1493 - 5s - loss: 1.9238e-04 - val_loss: 1.8952e-04 - 5s/epoch - 3ms/step
Epoch 267/300
1493/1493 - 5s - loss: 1.8321e-04 - val_loss: 2.0571e-04 - 5s/epoch - 3ms/step
Epoch 268/300
1493/1493 - 5s - loss: 1.8344e-04 - val_loss: 1.9126e-04 - 5s/epoch - 3ms/step
Epoch 269/300
1493/1493 - 5s - loss: 1.8760e-04 - val_loss: 4.8390e-04 - 5s/epoch - 3ms/step
Epoch 270/300
1493/1493 - 5s - loss: 2.4582e-04 - val_loss: 3.1079e-04 - 5s/epoch - 3ms/step
Epoch 271/300
1493/1493 - 5s - loss: 1.9741e-04 - val_loss: 1.8192e-04 - 5s/epoch - 3ms/step
Epoch 272/300
1493/1493 - 5s - loss: 1.9133e-04 - val_loss: 3.7218e-04 - 5s/epoch - 3ms/step
Epoch 273/300
1493/1493 - 5s - loss: 2.2828e-04 - val_loss: 3.1906e-04 - 5s/epoch - 3ms/step
Epoch 274/300
1493/1493 - 5s - loss: 2.0272e-04 - val_loss: 3.7528e-04 - 5s/epoch - 3ms/step
Epoch 275/300
1493/1493 - 5s - loss: 2.0861e-04 - val_loss: 1.7116e-04 - 5s/epoch - 3ms/step
Epoch 276/300
1493/1493 - 5s - loss: 1.8653e-04 - val_loss: 1.8547e-04 - 5s/epoch - 3ms/step
Epoch 277/300
1493/1493 - 5s - loss: 1.8744e-04 - val_loss: 1.8376e-04 - 5s/epoch - 3ms/step
Epoch 278/300
1493/1493 - 5s - loss: 1.8590e-04 - val_loss: 1.9848e-04 - 5s/epoch - 3ms/step
Epoch 279/300
1493/1493 - 5s - loss: 1.8451e-04 - val_loss: 2.0021e-04 - 5s/epoch - 3ms/step
Epoch 280/300
1493/1493 - 5s - loss: 1.8649e-04 - val_loss: 2.3136e-04 - 5s/epoch - 3ms/step
Epoch 281/300
1493/1493 - 5s - loss: 1.9865e-04 - val_loss: 2.1320e-04 - 5s/epoch - 3ms/step
Epoch 282/300
1493/1493 - 5s - loss: 1.8521e-04 - val_loss: 2.1770e-04 - 5s/epoch - 3ms/step
Epoch 283/300
1493/1493 - 5s - loss: 1.8153e-04 - val_loss: 2.9020e-04 - 5s/epoch - 3ms/step
Epoch 284/300
1493/1493 - 5s - loss: 1.9931e-04 - val_loss: 2.4237e-04 - 5s/epoch - 3ms/step
Epoch 285/300
1493/1493 - 5s - loss: 1.9827e-04 - val_loss: 2.3190e-04 - 5s/epoch - 3ms/step
Epoch 286/300
1493/1493 - 5s - loss: 1.8364e-04 - val_loss: 1.8326e-04 - 5s/epoch - 3ms/step
Epoch 287/300
1493/1493 - 5s - loss: 1.8286e-04 - val_loss: 3.3004e-04 - 5s/epoch - 3ms/step
Epoch 288/300
1493/1493 - 5s - loss: 2.0127e-04 - val_loss: 2.1917e-04 - 5s/epoch - 3ms/step
Epoch 289/300
1493/1493 - 5s - loss: 1.8213e-04 - val_loss: 3.3696e-04 - 5s/epoch - 3ms/step
Epoch 290/300
1493/1493 - 5s - loss: 1.9501e-04 - val_loss: 4.1671e-04 - 5s/epoch - 3ms/step
Epoch 291/300
1493/1493 - 5s - loss: 2.1047e-04 - val_loss: 2.1147e-04 - 5s/epoch - 3ms/step
Epoch 292/300
1493/1493 - 5s - loss: 1.8306e-04 - val_loss: 1.7096e-04 - 5s/epoch - 3ms/step
Epoch 293/300
1493/1493 - 5s - loss: 1.8696e-04 - val_loss: 1.8665e-04 - 5s/epoch - 3ms/step
Epoch 294/300
1493/1493 - 5s - loss: 1.8323e-04 - val_loss: 2.0984e-04 - 5s/epoch - 3ms/step
Epoch 295/300
1493/1493 - 5s - loss: 1.8309e-04 - val_loss: 1.8491e-04 - 5s/epoch - 3ms/step
Epoch 296/300
1493/1493 - 5s - loss: 1.8198e-04 - val_loss: 4.5996e-04 - 5s/epoch - 3ms/step
Epoch 297/300
1493/1493 - 5s - loss: 2.2415e-04 - val_loss: 2.1931e-04 - 5s/epoch - 3ms/step
Epoch 298/300
1493/1493 - 5s - loss: 1.8520e-04 - val_loss: 2.3454e-04 - 5s/epoch - 3ms/step
Epoch 299/300
1493/1493 - 5s - loss: 1.8807e-04 - val_loss: 2.0092e-04 - 5s/epoch - 3ms/step
Epoch 300/300
1493/1493 - 5s - loss: 1.8194e-04 - val_loss: 2.7147e-04 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 632
Loss in the autoencoder: 0.00027147220680490136
  1/332 [..............................] - ETA: 28s 49/332 [===>..........................] - ETA: 0s  98/332 [=======>......................] - ETA: 0s148/332 [============>.................] - ETA: 0s197/332 [================>.............] - ETA: 0s246/332 [=====================>........] - ETA: 0s295/332 [=========================>....] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0031259610681681196
cosine 0.002513178729175218
MAE: 0.010736428
RMSE: 0.016476411
r2: 0.9823893437255161
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       multiple                  0         
                                                                 
 dense_54 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_54 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_54 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_55 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_55 (ReLU)             (None, 632)               0         
                                                                 
 dense_55 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_56 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_56 (ReLU)             (None, 2528)              0         
                                                                 
 dense_56 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
Encoder
Model: "model_55"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_56 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_55 (InputLayer)       multiple                  0         
                                                                 
 dense_54 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_54 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_54 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
=================================================================
Total params: 4,806,360
Trainable params: 4,801,304
Non-trainable params: 5,056
_________________________________________________________________
Decoder
Model: "model_56"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_57 (InputLayer)       [(None, 632)]             0         
                                                                 
 batch_normalization_55 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_55 (ReLU)             (None, 632)               0         
                                                                 
 dense_55 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_56 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_56 (ReLU)             (None, 2528)              0         
                                                                 
 dense_56 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 4,809,520
Trainable params: 4,803,200
Non-trainable params: 6,320
_________________________________________________________________
['default', 'mse', 64, 300, 0.002, 0.5, 632, 0.0001819443132262677, 0.00027147220680490136, 0.0031259610681681196, 0.002513178729175218, 0.010736428201198578, 0.016476411372423172, 0.9823893437255161, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_57"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_58 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_57 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_57 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_57 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_58 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_58 (ReLU)             (None, 632)               0         
                                                                 
 dense_58 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_59 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_59 (ReLU)             (None, 2528)              0         
                                                                 
 dense_59 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:08.210558: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:08.210622: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22240833840
MaxInUse:                  22240833840
NumAllocs:                   938154780
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:08.210662: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:08.210666: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 243
2022-12-27 05:12:08.210669: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:08.210672: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:08.210675: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 214
2022-12-27 05:12:08.210682: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 58
2022-12-27 05:12:08.210686: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 428
2022-12-27 05:12:08.210691: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 116
2022-12-27 05:12:08.210695: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 116
2022-12-27 05:12:08.210700: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:08.210704: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 39
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 16, 100, 0.0005, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_58"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_59 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_60 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_60 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_60 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_61 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_61 (ReLU)             (None, 632)               0         
                                                                 
 dense_61 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_62 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_62 (ReLU)             (None, 2528)              0         
                                                                 
 dense_62 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:09.747937: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:09.747982: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21757980784
MaxInUse:                  22240833840
NumAllocs:                   938154822
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:09.748017: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:09.748023: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 243
2022-12-27 05:12:09.748026: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:09.748030: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:09.748033: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 214
2022-12-27 05:12:09.748036: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 58
2022-12-27 05:12:09.748039: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 428
2022-12-27 05:12:09.748042: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 116
2022-12-27 05:12:09.748045: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 116
2022-12-27 05:12:09.748048: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:09.748051: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 16, 100, 0.001, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_59"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_60 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_63 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_63 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_63 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_64 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_64 (ReLU)             (None, 632)               0         
                                                                 
 dense_64 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_65 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_65 (ReLU)             (None, 2528)              0         
                                                                 
 dense_65 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:11.284281: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:11.284333: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21796444304
MaxInUse:                  22240833840
NumAllocs:                   938154864
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:11.284371: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:11.284376: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 243
2022-12-27 05:12:11.284380: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:11.284383: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:11.284386: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 219
2022-12-27 05:12:11.284389: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 59
2022-12-27 05:12:11.284392: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 438
2022-12-27 05:12:11.284395: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 118
2022-12-27 05:12:11.284398: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 118
2022-12-27 05:12:11.284401: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:11.284404: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 16, 100, 0.002, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_60"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_61 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_66 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_66 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_66 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_67 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_67 (ReLU)             (None, 632)               0         
                                                                 
 dense_67 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_68 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_68 (ReLU)             (None, 2528)              0         
                                                                 
 dense_68 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:12.819906: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:12.819961: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21834907832
MaxInUse:                  22240833840
NumAllocs:                   938154906
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:12.819999: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:12.820004: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 245
2022-12-27 05:12:12.820007: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:12.820010: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:12.820013: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 224
2022-12-27 05:12:12.820016: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 60
2022-12-27 05:12:12.820019: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 448
2022-12-27 05:12:12.820022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 120
2022-12-27 05:12:12.820025: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 120
2022-12-27 05:12:12.820028: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:12.820031: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 16, 200, 0.0005, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_61"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_62 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_69 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_69 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_69 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_70 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_70 (ReLU)             (None, 632)               0         
                                                                 
 dense_70 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_71 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_71 (ReLU)             (None, 2528)              0         
                                                                 
 dense_71 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:14.360004: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:14.360061: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21796444304
MaxInUse:                  22240833840
NumAllocs:                   938154948
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:14.360100: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:14.360106: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 243
2022-12-27 05:12:14.360109: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:14.360112: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:14.360115: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 219
2022-12-27 05:12:14.360118: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 59
2022-12-27 05:12:14.360122: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 438
2022-12-27 05:12:14.360133: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 118
2022-12-27 05:12:14.360137: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 118
2022-12-27 05:12:14.360139: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:14.360142: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 16, 200, 0.001, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_62"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_63 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_72 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_72 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_72 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_73 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_73 (ReLU)             (None, 632)               0         
                                                                 
 dense_73 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_74 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_74 (ReLU)             (None, 2528)              0         
                                                                 
 dense_74 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:15.893481: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:15.893535: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21834907832
MaxInUse:                  22240833840
NumAllocs:                   938154990
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:15.893570: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:15.893584: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 245
2022-12-27 05:12:15.893588: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:15.893591: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:15.893593: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 224
2022-12-27 05:12:15.893596: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 60
2022-12-27 05:12:15.893599: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 448
2022-12-27 05:12:15.893601: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 120
2022-12-27 05:12:15.893604: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 120
2022-12-27 05:12:15.893607: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:15.893609: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 16, 200, 0.002, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_63"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_64 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_75 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_75 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_75 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_76 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_76 (ReLU)             (None, 632)               0         
                                                                 
 dense_76 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_77 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_77 (ReLU)             (None, 2528)              0         
                                                                 
 dense_77 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:17.426503: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:17.426564: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21873371352
MaxInUse:                  22240833840
NumAllocs:                   938155032
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:17.426601: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:17.426607: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 245
2022-12-27 05:12:17.426610: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:17.426613: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:17.426616: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 229
2022-12-27 05:12:17.426619: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 61
2022-12-27 05:12:17.426622: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 458
2022-12-27 05:12:17.426625: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 122
2022-12-27 05:12:17.426628: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 122
2022-12-27 05:12:17.426631: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:17.426634: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 16, 300, 0.0005, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_64"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_65 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_78 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_78 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_78 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_79 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_79 (ReLU)             (None, 632)               0         
                                                                 
 dense_79 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_80 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_80 (ReLU)             (None, 2528)              0         
                                                                 
 dense_80 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:18.959655: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:18.959716: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21911834880
MaxInUse:                  22240833840
NumAllocs:                   938155074
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:18.959754: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:18.959759: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 247
2022-12-27 05:12:18.959763: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:18.959766: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:18.959769: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 234
2022-12-27 05:12:18.959772: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 62
2022-12-27 05:12:18.959775: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 468
2022-12-27 05:12:18.959778: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 124
2022-12-27 05:12:18.959781: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 124
2022-12-27 05:12:18.959784: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:18.959787: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 16, 300, 0.001, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_65"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_66 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_81 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_81 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_81 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_82 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_82 (ReLU)             (None, 632)               0         
                                                                 
 dense_82 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_83 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_83 (ReLU)             (None, 2528)              0         
                                                                 
 dense_83 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:20.493610: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:20.493666: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21834907832
MaxInUse:                  22240833840
NumAllocs:                   938155116
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:20.493712: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:20.493718: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 245
2022-12-27 05:12:20.493722: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:20.493725: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:20.493728: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 224
2022-12-27 05:12:20.493731: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 60
2022-12-27 05:12:20.493734: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 448
2022-12-27 05:12:20.493737: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 120
2022-12-27 05:12:20.493740: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 120
2022-12-27 05:12:20.493743: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:20.493746: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 16, 300, 0.002, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_66"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_67 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_84 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_84 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_84 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_85 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_85 (ReLU)             (None, 632)               0         
                                                                 
 dense_85 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_86 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_86 (ReLU)             (None, 2528)              0         
                                                                 
 dense_86 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:22.023690: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:22.023745: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21873371352
MaxInUse:                  22240833840
NumAllocs:                   938155158
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:22.023784: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:22.023790: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 245
2022-12-27 05:12:22.023793: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:22.023796: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:22.023799: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 229
2022-12-27 05:12:22.023802: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 61
2022-12-27 05:12:22.023805: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 458
2022-12-27 05:12:22.023808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 122
2022-12-27 05:12:22.023811: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 122
2022-12-27 05:12:22.023814: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:22.023817: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 32, 100, 0.0005, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_67"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_68 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_87 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_87 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_87 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_88 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_88 (ReLU)             (None, 632)               0         
                                                                 
 dense_88 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_89 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_89 (ReLU)             (None, 2528)              0         
                                                                 
 dense_89 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:23.553565: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:23.553608: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21911834880
MaxInUse:                  22240833840
NumAllocs:                   938155200
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:23.553645: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:23.553650: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 247
2022-12-27 05:12:23.553653: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:23.553656: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:23.553659: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 234
2022-12-27 05:12:23.553662: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 62
2022-12-27 05:12:23.553666: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 468
2022-12-27 05:12:23.553669: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 124
2022-12-27 05:12:23.553686: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 124
2022-12-27 05:12:23.553691: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:23.553696: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 32, 100, 0.001, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_68"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_69 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_90 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_90 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_90 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_91 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_91 (ReLU)             (None, 632)               0         
                                                                 
 dense_91 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_92 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_92 (ReLU)             (None, 2528)              0         
                                                                 
 dense_92 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:25.093514: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:25.093572: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21873371352
MaxInUse:                  22240833840
NumAllocs:                   938155242
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:25.093609: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:25.093614: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 245
2022-12-27 05:12:25.093626: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:25.093629: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:25.093632: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 229
2022-12-27 05:12:25.093635: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 61
2022-12-27 05:12:25.093638: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 458
2022-12-27 05:12:25.093641: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 122
2022-12-27 05:12:25.093644: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 122
2022-12-27 05:12:25.093647: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:25.093650: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 32, 100, 0.002, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_69"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_70 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_93 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_93 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_93 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_94 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_94 (ReLU)             (None, 632)               0         
                                                                 
 dense_94 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_95 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_95 (ReLU)             (None, 2528)              0         
                                                                 
 dense_95 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:26.627528: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:26.627581: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21911834880
MaxInUse:                  22240833840
NumAllocs:                   938155284
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:26.627627: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:26.627633: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 247
2022-12-27 05:12:26.627636: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:26.627639: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:26.627642: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 234
2022-12-27 05:12:26.627645: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 62
2022-12-27 05:12:26.627648: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 468
2022-12-27 05:12:26.627651: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 124
2022-12-27 05:12:26.627654: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 124
2022-12-27 05:12:26.627656: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:26.627659: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 32, 200, 0.0005, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_70"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_71 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_96 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_96 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_96 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_97 (Bat  (None, 632)              2528      
 chNormalization)                                                
                                                                 
 re_lu_97 (ReLU)             (None, 632)               0         
                                                                 
 dense_97 (Dense)            (None, 2528)              1600224   
                                                                 
 batch_normalization_98 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_98 (ReLU)             (None, 2528)              0         
                                                                 
 dense_98 (Dense)            (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:28.158524: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:28.158568: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21950298400
MaxInUse:                  22240833840
NumAllocs:                   938155326
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:28.158604: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:28.158610: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 247
2022-12-27 05:12:28.158613: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:28.158616: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:28.158619: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 239
2022-12-27 05:12:28.158622: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 63
2022-12-27 05:12:28.158625: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 478
2022-12-27 05:12:28.158628: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 126
2022-12-27 05:12:28.158631: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 126
2022-12-27 05:12:28.158634: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:28.158637: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 32, 200, 0.001, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_71"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_72 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_99 (Dense)            (None, 2528)              3197920   
                                                                 
 batch_normalization_99 (Bat  (None, 2528)             10112     
 chNormalization)                                                
                                                                 
 re_lu_99 (ReLU)             (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_100 (Ba  (None, 632)              2528      
 tchNormalization)                                               
                                                                 
 re_lu_100 (ReLU)            (None, 632)               0         
                                                                 
 dense_100 (Dense)           (None, 2528)              1600224   
                                                                 
 batch_normalization_101 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_101 (ReLU)            (None, 2528)              0         
                                                                 
 dense_101 (Dense)           (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:29.693161: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:29.693203: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21988761928
MaxInUse:                  22240833840
NumAllocs:                   938155368
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:29.693245: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:29.693251: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 249
2022-12-27 05:12:29.693254: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:29.693257: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:29.693260: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 244
2022-12-27 05:12:29.693263: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 64
2022-12-27 05:12:29.693267: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 488
2022-12-27 05:12:29.693270: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 128
2022-12-27 05:12:29.693273: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 128
2022-12-27 05:12:29.693276: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:29.693278: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 32, 200, 0.002, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_72"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_73 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_102 (Dense)           (None, 2528)              3197920   
                                                                 
 batch_normalization_102 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_102 (ReLU)            (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_103 (Ba  (None, 632)              2528      
 tchNormalization)                                               
                                                                 
 re_lu_103 (ReLU)            (None, 632)               0         
                                                                 
 dense_103 (Dense)           (None, 2528)              1600224   
                                                                 
 batch_normalization_104 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_104 (ReLU)            (None, 2528)              0         
                                                                 
 dense_104 (Dense)           (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:31.227423: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:31.227474: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21911834880
MaxInUse:                  22240833840
NumAllocs:                   938155410
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:31.227511: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:31.227518: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 247
2022-12-27 05:12:31.227522: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:31.227526: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:31.227529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 234
2022-12-27 05:12:31.227532: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 62
2022-12-27 05:12:31.227535: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 468
2022-12-27 05:12:31.227538: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 124
2022-12-27 05:12:31.227540: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 124
2022-12-27 05:12:31.227543: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:31.227546: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 32, 300, 0.0005, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_73"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_74 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_105 (Dense)           (None, 2528)              3197920   
                                                                 
 batch_normalization_105 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_105 (ReLU)            (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_106 (Ba  (None, 632)              2528      
 tchNormalization)                                               
                                                                 
 re_lu_106 (ReLU)            (None, 632)               0         
                                                                 
 dense_106 (Dense)           (None, 2528)              1600224   
                                                                 
 batch_normalization_107 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_107 (ReLU)            (None, 2528)              0         
                                                                 
 dense_107 (Dense)           (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:32.761584: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:32.761632: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21950298400
MaxInUse:                  22240833840
NumAllocs:                   938155452
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:32.761673: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:32.761683: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 247
2022-12-27 05:12:32.761687: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:32.761690: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:32.761693: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 239
2022-12-27 05:12:32.761696: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 63
2022-12-27 05:12:32.761699: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 478
2022-12-27 05:12:32.761702: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 126
2022-12-27 05:12:32.761705: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 126
2022-12-27 05:12:32.761716: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:32.761719: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 32, 300, 0.001, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_74"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_75 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_108 (Dense)           (None, 2528)              3197920   
                                                                 
 batch_normalization_108 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_108 (ReLU)            (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_109 (Ba  (None, 632)              2528      
 tchNormalization)                                               
                                                                 
 re_lu_109 (ReLU)            (None, 632)               0         
                                                                 
 dense_109 (Dense)           (None, 2528)              1600224   
                                                                 
 batch_normalization_110 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_110 (ReLU)            (None, 2528)              0         
                                                                 
 dense_110 (Dense)           (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:34.295258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:34.295311: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21988761928
MaxInUse:                  22240833840
NumAllocs:                   938155494
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:34.295350: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:34.295356: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 249
2022-12-27 05:12:34.295360: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:34.295373: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:34.295376: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 244
2022-12-27 05:12:34.295379: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 64
2022-12-27 05:12:34.295382: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 488
2022-12-27 05:12:34.295385: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 128
2022-12-27 05:12:34.295388: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 128
2022-12-27 05:12:34.295391: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:34.295394: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 32, 300, 0.002, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_75"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_76 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_111 (Dense)           (None, 2528)              3197920   
                                                                 
 batch_normalization_111 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_111 (ReLU)            (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_112 (Ba  (None, 632)              2528      
 tchNormalization)                                               
                                                                 
 re_lu_112 (ReLU)            (None, 632)               0         
                                                                 
 dense_112 (Dense)           (None, 2528)              1600224   
                                                                 
 batch_normalization_113 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_113 (ReLU)            (None, 2528)              0         
                                                                 
 dense_113 (Dense)           (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:35.829757: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:35.829810: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21950298400
MaxInUse:                  22240833840
NumAllocs:                   938155536
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:35.829854: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:35.829860: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 247
2022-12-27 05:12:35.829864: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:35.829867: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:35.829870: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 239
2022-12-27 05:12:35.829873: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 63
2022-12-27 05:12:35.829876: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 478
2022-12-27 05:12:35.829878: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 126
2022-12-27 05:12:35.829881: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 126
2022-12-27 05:12:35.829884: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:35.829887: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 64, 100, 0.0005, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_76"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_77 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_114 (Dense)           (None, 2528)              3197920   
                                                                 
 batch_normalization_114 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_114 (ReLU)            (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_115 (Ba  (None, 632)              2528      
 tchNormalization)                                               
                                                                 
 re_lu_115 (ReLU)            (None, 632)               0         
                                                                 
 dense_115 (Dense)           (None, 2528)              1600224   
                                                                 
 batch_normalization_116 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_116 (ReLU)            (None, 2528)              0         
                                                                 
 dense_116 (Dense)           (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:37.361538: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:37.361590: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21988761928
MaxInUse:                  22240833840
NumAllocs:                   938155578
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:37.361632: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:37.361638: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 249
2022-12-27 05:12:37.361642: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:37.361645: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:37.361648: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 244
2022-12-27 05:12:37.361651: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 64
2022-12-27 05:12:37.361654: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 488
2022-12-27 05:12:37.361657: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 128
2022-12-27 05:12:37.361660: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 128
2022-12-27 05:12:37.361663: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:37.361666: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 64, 100, 0.001, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_77"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_78 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_117 (Dense)           (None, 2528)              3197920   
                                                                 
 batch_normalization_117 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_117 (ReLU)            (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_118 (Ba  (None, 632)              2528      
 tchNormalization)                                               
                                                                 
 re_lu_118 (ReLU)            (None, 632)               0         
                                                                 
 dense_118 (Dense)           (None, 2528)              1600224   
                                                                 
 batch_normalization_119 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_119 (ReLU)            (None, 2528)              0         
                                                                 
 dense_119 (Dense)           (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:38.898977: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:38.899028: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22027225448
MaxInUse:                  22240833840
NumAllocs:                   938155620
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:38.899066: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:38.899071: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 249
2022-12-27 05:12:38.899075: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:38.899078: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:38.899081: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 249
2022-12-27 05:12:38.899084: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 65
2022-12-27 05:12:38.899087: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 498
2022-12-27 05:12:38.899090: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 130
2022-12-27 05:12:38.899093: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 130
2022-12-27 05:12:38.899095: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:38.899098: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 64, 100, 0.002, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_78"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_79 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_120 (Dense)           (None, 2528)              3197920   
                                                                 
 batch_normalization_120 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_120 (ReLU)            (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_121 (Ba  (None, 632)              2528      
 tchNormalization)                                               
                                                                 
 re_lu_121 (ReLU)            (None, 632)               0         
                                                                 
 dense_121 (Dense)           (None, 2528)              1600224   
                                                                 
 batch_normalization_122 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_122 (ReLU)            (None, 2528)              0         
                                                                 
 dense_122 (Dense)           (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:40.429408: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:40.429459: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22065688976
MaxInUse:                  22240833840
NumAllocs:                   938155662
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:40.429503: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:40.429508: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 251
2022-12-27 05:12:40.429511: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:40.429514: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:40.429517: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 254
2022-12-27 05:12:40.429520: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 66
2022-12-27 05:12:40.429524: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 508
2022-12-27 05:12:40.429526: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 132
2022-12-27 05:12:40.429529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 132
2022-12-27 05:12:40.429532: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:40.429534: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 64, 200, 0.0005, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_79"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_80 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_123 (Dense)           (None, 2528)              3197920   
                                                                 
 batch_normalization_123 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_123 (ReLU)            (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_124 (Ba  (None, 632)              2528      
 tchNormalization)                                               
                                                                 
 re_lu_124 (ReLU)            (None, 632)               0         
                                                                 
 dense_124 (Dense)           (None, 2528)              1600224   
                                                                 
 batch_normalization_125 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_125 (ReLU)            (None, 2528)              0         
                                                                 
 dense_125 (Dense)           (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:41.968883: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:41.968940: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     21988761928
MaxInUse:                  22240833840
NumAllocs:                   938155704
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:41.968978: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:41.968984: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 249
2022-12-27 05:12:41.968988: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:41.968991: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:41.968994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 244
2022-12-27 05:12:41.968997: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 64
2022-12-27 05:12:41.969000: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 488
2022-12-27 05:12:41.969003: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 128
2022-12-27 05:12:41.969006: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 128
2022-12-27 05:12:41.969009: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:41.969022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 64, 200, 0.001, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_80"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_81 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_126 (Dense)           (None, 2528)              3197920   
                                                                 
 batch_normalization_126 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_126 (ReLU)            (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_127 (Ba  (None, 632)              2528      
 tchNormalization)                                               
                                                                 
 re_lu_127 (ReLU)            (None, 632)               0         
                                                                 
 dense_127 (Dense)           (None, 2528)              1600224   
                                                                 
 batch_normalization_128 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_128 (ReLU)            (None, 2528)              0         
                                                                 
 dense_128 (Dense)           (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:43.502002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:43.502057: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22027225448
MaxInUse:                  22240833840
NumAllocs:                   938155746
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:43.502097: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:43.502103: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 249
2022-12-27 05:12:43.502107: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:43.502110: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:43.502122: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 249
2022-12-27 05:12:43.502125: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 65
2022-12-27 05:12:43.502128: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 498
2022-12-27 05:12:43.502131: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 130
2022-12-27 05:12:43.502134: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 130
2022-12-27 05:12:43.502137: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:43.502140: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 64, 200, 0.002, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_81"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_82 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_129 (Dense)           (None, 2528)              3197920   
                                                                 
 batch_normalization_129 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_129 (ReLU)            (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_130 (Ba  (None, 632)              2528      
 tchNormalization)                                               
                                                                 
 re_lu_130 (ReLU)            (None, 632)               0         
                                                                 
 dense_130 (Dense)           (None, 2528)              1600224   
                                                                 
 batch_normalization_131 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_131 (ReLU)            (None, 2528)              0         
                                                                 
 dense_131 (Dense)           (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:45.035162: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:45.035215: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22065688976
MaxInUse:                  22240833840
NumAllocs:                   938155788
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:45.035268: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:45.035273: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 251
2022-12-27 05:12:45.035276: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:45.035279: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:45.035282: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 254
2022-12-27 05:12:45.035284: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 66
2022-12-27 05:12:45.035287: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 508
2022-12-27 05:12:45.035290: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 132
2022-12-27 05:12:45.035293: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 132
2022-12-27 05:12:45.035295: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:45.035298: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 64, 300, 0.0005, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_82"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_83 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_132 (Dense)           (None, 2528)              3197920   
                                                                 
 batch_normalization_132 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_132 (ReLU)            (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_133 (Ba  (None, 632)              2528      
 tchNormalization)                                               
                                                                 
 re_lu_133 (ReLU)            (None, 632)               0         
                                                                 
 dense_133 (Dense)           (None, 2528)              1600224   
                                                                 
 batch_normalization_134 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_134 (ReLU)            (None, 2528)              0         
                                                                 
 dense_134 (Dense)           (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:46.570120: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:46.570172: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22027225448
MaxInUse:                  22240833840
NumAllocs:                   938155830
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:46.570212: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:46.570217: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 249
2022-12-27 05:12:46.570221: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:46.570224: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:46.570227: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 249
2022-12-27 05:12:46.570230: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 65
2022-12-27 05:12:46.570233: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 498
2022-12-27 05:12:46.570236: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 130
2022-12-27 05:12:46.570239: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 130
2022-12-27 05:12:46.570242: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:46.570245: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 64, 300, 0.001, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
./DATAFILES/MP_GapFeats_default already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_83"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_84 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_135 (Dense)           (None, 2528)              3197920   
                                                                 
 batch_normalization_135 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_135 (ReLU)            (None, 2528)              0         
                                                                 
 bottleneck (Dense)          (None, 632)               1598328   
                                                                 
 batch_normalization_136 (Ba  (None, 632)              2528      
 tchNormalization)                                               
                                                                 
 re_lu_136 (ReLU)            (None, 632)               0         
                                                                 
 dense_136 (Dense)           (None, 2528)              1600224   
                                                                 
 batch_normalization_137 (Ba  (None, 2528)             10112     
 tchNormalization)                                               
                                                                 
 re_lu_137 (ReLU)            (None, 2528)              0         
                                                                 
 dense_137 (Dense)           (None, 1264)              3196656   
                                                                 
=================================================================
Total params: 9,615,880
Trainable params: 9,604,504
Non-trainable params: 11,376
_________________________________________________________________
2022-12-27 05:12:48.103490: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 382861312/23676715008
2022-12-27 05:12:48.103536: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22065688976
MaxInUse:                  22240833840
NumAllocs:                   938155872
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-12-27 05:12:48.103574: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-12-27 05:12:48.103581: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 251
2022-12-27 05:12:48.103585: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 177
2022-12-27 05:12:48.103588: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-12-27 05:12:48.103592: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 254
2022-12-27 05:12:48.103595: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 66
2022-12-27 05:12:48.103597: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 508
2022-12-27 05:12:48.103600: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 132
2022-12-27 05:12:48.103603: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 132
2022-12-27 05:12:48.103606: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2022-12-27 05:12:48.103609: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['default', 'log_cosh', 64, 300, 0.002, 0.5, 632, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
Tue Dec 27 05:12:51 CET 2022
done
