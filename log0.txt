start
Fri Dec 30 01:35:55 CET 2022
2022-12-30 01:35:57.064051: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-30 01:35:57.141718: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-12-30 01:36:26,554 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fb86df2e760> object, created with modnet version 0.1.12
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
Shape of dataset to encode: (106113, 1264)
2022-12-30 01:36:28.278778: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense (Dense)               (None, 2780)              3516700   
                                                                 
 batch_normalization (BatchN  (None, 2780)             11120     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1264)              3515184   
                                                                 
 batch_normalization_1 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 1264)              0         
                                                                 
 dense_1 (Dense)             (None, 2780)              3516700   
                                                                 
 batch_normalization_2 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 2780)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 14,091,064
Trainable params: 14,077,416
Non-trainable params: 13,648
_________________________________________________________________
Epoch 1/200
1493/1493 - 105s - loss: 0.0088 - val_loss: 0.0036 - 105s/epoch - 71ms/step
Epoch 2/200
1493/1493 - 105s - loss: 0.0027 - val_loss: 0.0027 - 105s/epoch - 70ms/step
Epoch 3/200
1493/1493 - 105s - loss: 0.0020 - val_loss: 0.0015 - 105s/epoch - 70ms/step
Epoch 4/200
1493/1493 - 105s - loss: 0.0016 - val_loss: 0.0034 - 105s/epoch - 70ms/step
Epoch 5/200
1493/1493 - 105s - loss: 0.0016 - val_loss: 0.0019 - 105s/epoch - 70ms/step
Epoch 6/200
1493/1493 - 105s - loss: 0.0014 - val_loss: 0.0012 - 105s/epoch - 70ms/step
Epoch 7/200
1493/1493 - 105s - loss: 0.0013 - val_loss: 0.0011 - 105s/epoch - 70ms/step
Epoch 8/200
1493/1493 - 105s - loss: 0.0012 - val_loss: 8.7705e-04 - 105s/epoch - 70ms/step
Epoch 9/200
1493/1493 - 105s - loss: 0.0011 - val_loss: 8.8155e-04 - 105s/epoch - 70ms/step
Epoch 10/200
1493/1493 - 105s - loss: 8.8938e-04 - val_loss: 7.0152e-04 - 105s/epoch - 70ms/step
Epoch 11/200
1493/1493 - 105s - loss: 8.1576e-04 - val_loss: 6.1051e-04 - 105s/epoch - 70ms/step
Epoch 12/200
1493/1493 - 105s - loss: 7.2223e-04 - val_loss: 0.0011 - 105s/epoch - 70ms/step
Epoch 13/200
1493/1493 - 105s - loss: 6.6568e-04 - val_loss: 0.0015 - 105s/epoch - 70ms/step
Epoch 14/200
1493/1493 - 105s - loss: 7.3531e-04 - val_loss: 4.7723e-04 - 105s/epoch - 70ms/step
Epoch 15/200
1493/1493 - 105s - loss: 5.8192e-04 - val_loss: 0.0018 - 105s/epoch - 70ms/step
Epoch 16/200
1493/1493 - 105s - loss: 6.3705e-04 - val_loss: 4.4284e-04 - 105s/epoch - 70ms/step
Epoch 17/200
1493/1493 - 105s - loss: 4.8812e-04 - val_loss: 0.0011 - 105s/epoch - 70ms/step
Epoch 18/200
1493/1493 - 105s - loss: 5.0260e-04 - val_loss: 4.9373e-04 - 105s/epoch - 70ms/step
Epoch 19/200
1493/1493 - 105s - loss: 4.3210e-04 - val_loss: 5.1807e-04 - 105s/epoch - 70ms/step
Epoch 20/200
1493/1493 - 105s - loss: 4.2599e-04 - val_loss: 3.8419e-04 - 105s/epoch - 70ms/step
Epoch 21/200
1493/1493 - 105s - loss: 3.9337e-04 - val_loss: 8.5115e-04 - 105s/epoch - 70ms/step
Epoch 22/200
1493/1493 - 105s - loss: 4.5604e-04 - val_loss: 4.0180e-04 - 105s/epoch - 70ms/step
Epoch 23/200
1493/1493 - 105s - loss: 3.7246e-04 - val_loss: 9.1864e-04 - 105s/epoch - 70ms/step
Epoch 24/200
1493/1493 - 105s - loss: 3.6997e-04 - val_loss: 2.9098e-04 - 105s/epoch - 70ms/step
Epoch 25/200
1493/1493 - 105s - loss: 3.2494e-04 - val_loss: 3.1829e-04 - 105s/epoch - 70ms/step
Epoch 26/200
1493/1493 - 105s - loss: 3.2681e-04 - val_loss: 3.7736e-04 - 105s/epoch - 70ms/step
Epoch 27/200
1493/1493 - 105s - loss: 2.9467e-04 - val_loss: 2.7829e-04 - 105s/epoch - 70ms/step
Epoch 28/200
1493/1493 - 105s - loss: 2.8268e-04 - val_loss: 4.6508e-04 - 105s/epoch - 70ms/step
Epoch 29/200
1493/1493 - 105s - loss: 2.7483e-04 - val_loss: 3.9724e-04 - 105s/epoch - 70ms/step
Epoch 30/200
1493/1493 - 105s - loss: 2.8859e-04 - val_loss: 2.6193e-04 - 105s/epoch - 70ms/step
Epoch 31/200
1493/1493 - 105s - loss: 2.5823e-04 - val_loss: 4.9207e-04 - 105s/epoch - 70ms/step
Epoch 32/200
1493/1493 - 105s - loss: 2.9332e-04 - val_loss: 5.7635e-04 - 105s/epoch - 70ms/step
Epoch 33/200
1493/1493 - 105s - loss: 2.7343e-04 - val_loss: 2.4057e-04 - 105s/epoch - 71ms/step
Epoch 34/200
1493/1493 - 105s - loss: 2.4058e-04 - val_loss: 2.0694e-04 - 105s/epoch - 71ms/step
Epoch 35/200
1493/1493 - 105s - loss: 2.3493e-04 - val_loss: 6.8469e-04 - 105s/epoch - 70ms/step
Epoch 36/200
1493/1493 - 105s - loss: 2.6881e-04 - val_loss: 2.7315e-04 - 105s/epoch - 70ms/step
Epoch 37/200
1493/1493 - 105s - loss: 2.2709e-04 - val_loss: 2.8170e-04 - 105s/epoch - 70ms/step
Epoch 38/200
1493/1493 - 105s - loss: 2.3589e-04 - val_loss: 2.0812e-04 - 105s/epoch - 71ms/step
Epoch 39/200
1493/1493 - 105s - loss: 2.1400e-04 - val_loss: 2.0246e-04 - 105s/epoch - 70ms/step
Epoch 40/200
1493/1493 - 105s - loss: 2.0514e-04 - val_loss: 2.9799e-04 - 105s/epoch - 70ms/step
Epoch 41/200
1493/1493 - 105s - loss: 2.0578e-04 - val_loss: 1.7346e-04 - 105s/epoch - 70ms/step
Epoch 42/200
1493/1493 - 105s - loss: 2.0049e-04 - val_loss: 1.9242e-04 - 105s/epoch - 70ms/step
Epoch 43/200
1493/1493 - 105s - loss: 1.9729e-04 - val_loss: 3.2400e-04 - 105s/epoch - 70ms/step
Epoch 44/200
1493/1493 - 105s - loss: 2.1310e-04 - val_loss: 6.8373e-04 - 105s/epoch - 70ms/step
Epoch 45/200
1493/1493 - 105s - loss: 2.3619e-04 - val_loss: 1.6729e-04 - 105s/epoch - 70ms/step
Epoch 46/200
1493/1493 - 105s - loss: 1.8848e-04 - val_loss: 1.7345e-04 - 105s/epoch - 70ms/step
Epoch 47/200
1493/1493 - 105s - loss: 1.9525e-04 - val_loss: 2.6005e-04 - 105s/epoch - 70ms/step
Epoch 48/200
1493/1493 - 105s - loss: 1.8610e-04 - val_loss: 2.3400e-04 - 105s/epoch - 70ms/step
Epoch 49/200
1493/1493 - 105s - loss: 1.8889e-04 - val_loss: 6.4161e-04 - 105s/epoch - 70ms/step
Epoch 50/200
1493/1493 - 105s - loss: 2.4926e-04 - val_loss: 2.1867e-04 - 105s/epoch - 70ms/step
Epoch 51/200
1493/1493 - 105s - loss: 1.8490e-04 - val_loss: 1.8301e-04 - 105s/epoch - 70ms/step
Epoch 52/200
1493/1493 - 105s - loss: 1.7770e-04 - val_loss: 1.9854e-04 - 105s/epoch - 71ms/step
Epoch 53/200
1493/1493 - 105s - loss: 1.7682e-04 - val_loss: 1.5285e-04 - 105s/epoch - 70ms/step
Epoch 54/200
1493/1493 - 105s - loss: 1.6831e-04 - val_loss: 1.8415e-04 - 105s/epoch - 70ms/step
Epoch 55/200
1493/1493 - 105s - loss: 1.6349e-04 - val_loss: 1.6299e-04 - 105s/epoch - 70ms/step
Epoch 56/200
1493/1493 - 105s - loss: 1.6194e-04 - val_loss: 1.9340e-04 - 105s/epoch - 70ms/step
Epoch 57/200
1493/1493 - 105s - loss: 1.6492e-04 - val_loss: 1.5635e-04 - 105s/epoch - 70ms/step
Epoch 58/200
1493/1493 - 105s - loss: 1.5712e-04 - val_loss: 1.3891e-04 - 105s/epoch - 70ms/step
Epoch 59/200
1493/1493 - 105s - loss: 1.5503e-04 - val_loss: 1.6277e-04 - 105s/epoch - 70ms/step
Epoch 60/200
1493/1493 - 105s - loss: 1.5411e-04 - val_loss: 1.9895e-04 - 105s/epoch - 70ms/step
Epoch 61/200
1493/1493 - 105s - loss: 1.5878e-04 - val_loss: 1.5853e-04 - 105s/epoch - 70ms/step
Epoch 62/200
1493/1493 - 104s - loss: 1.4973e-04 - val_loss: 1.8708e-04 - 104s/epoch - 70ms/step
Epoch 63/200
1493/1493 - 104s - loss: 1.5091e-04 - val_loss: 2.0435e-04 - 104s/epoch - 70ms/step
Epoch 64/200
1493/1493 - 104s - loss: 1.4860e-04 - val_loss: 3.8185e-04 - 104s/epoch - 70ms/step
Epoch 65/200
1493/1493 - 104s - loss: 1.6686e-04 - val_loss: 1.5887e-04 - 104s/epoch - 70ms/step
Epoch 66/200
1493/1493 - 104s - loss: 1.4602e-04 - val_loss: 1.6184e-04 - 104s/epoch - 70ms/step
Epoch 67/200
1493/1493 - 104s - loss: 1.4158e-04 - val_loss: 1.3401e-04 - 104s/epoch - 70ms/step
Epoch 68/200
1493/1493 - 105s - loss: 1.3976e-04 - val_loss: 4.2336e-04 - 105s/epoch - 70ms/step
Epoch 69/200
1493/1493 - 104s - loss: 1.8766e-04 - val_loss: 1.4983e-04 - 104s/epoch - 70ms/step
Epoch 70/200
1493/1493 - 104s - loss: 1.4053e-04 - val_loss: 1.9362e-04 - 104s/epoch - 70ms/step
Epoch 71/200
1493/1493 - 104s - loss: 1.4006e-04 - val_loss: 1.3463e-04 - 104s/epoch - 70ms/step
Epoch 72/200
1493/1493 - 104s - loss: 1.3907e-04 - val_loss: 1.4138e-04 - 104s/epoch - 70ms/step
Epoch 73/200
1493/1493 - 104s - loss: 1.3822e-04 - val_loss: 1.5711e-04 - 104s/epoch - 70ms/step
Epoch 74/200
1493/1493 - 105s - loss: 1.3960e-04 - val_loss: 1.6153e-04 - 105s/epoch - 70ms/step
Epoch 75/200
1493/1493 - 104s - loss: 1.3216e-04 - val_loss: 1.1750e-04 - 104s/epoch - 70ms/step
Epoch 76/200
1493/1493 - 104s - loss: 1.2976e-04 - val_loss: 1.2656e-04 - 104s/epoch - 70ms/step
Epoch 77/200
1493/1493 - 104s - loss: 1.2948e-04 - val_loss: 0.0011 - 104s/epoch - 70ms/step
Epoch 78/200
1493/1493 - 104s - loss: 2.1862e-04 - val_loss: 1.4211e-04 - 104s/epoch - 70ms/step
Epoch 79/200
1493/1493 - 104s - loss: 1.3682e-04 - val_loss: 1.3078e-04 - 104s/epoch - 70ms/step
Epoch 80/200
1493/1493 - 104s - loss: 1.3373e-04 - val_loss: 1.5057e-04 - 104s/epoch - 70ms/step
Epoch 81/200
1493/1493 - 104s - loss: 1.3060e-04 - val_loss: 4.3938e-04 - 104s/epoch - 70ms/step
Epoch 82/200
1493/1493 - 104s - loss: 1.3551e-04 - val_loss: 1.2428e-04 - 104s/epoch - 70ms/step
Epoch 83/200
1493/1493 - 105s - loss: 1.2382e-04 - val_loss: 1.3810e-04 - 105s/epoch - 70ms/step
Epoch 84/200
1493/1493 - 105s - loss: 1.2364e-04 - val_loss: 1.3449e-04 - 105s/epoch - 70ms/step
Epoch 85/200
1493/1493 - 105s - loss: 1.2300e-04 - val_loss: 1.8230e-04 - 105s/epoch - 70ms/step
Epoch 86/200
1493/1493 - 104s - loss: 1.3084e-04 - val_loss: 1.4244e-04 - 104s/epoch - 70ms/step
Epoch 87/200
1493/1493 - 105s - loss: 1.1994e-04 - val_loss: 1.1463e-04 - 105s/epoch - 70ms/step
Epoch 88/200
1493/1493 - 104s - loss: 1.1849e-04 - val_loss: 1.3600e-04 - 104s/epoch - 70ms/step
Epoch 89/200
1493/1493 - 104s - loss: 1.2138e-04 - val_loss: 1.1792e-04 - 104s/epoch - 70ms/step
Epoch 90/200
1493/1493 - 104s - loss: 1.1699e-04 - val_loss: 1.2436e-04 - 104s/epoch - 70ms/step
Epoch 91/200
1493/1493 - 104s - loss: 1.1580e-04 - val_loss: 1.3149e-04 - 104s/epoch - 70ms/step
Epoch 92/200
1493/1493 - 104s - loss: 1.2192e-04 - val_loss: 5.2345e-04 - 104s/epoch - 70ms/step
Epoch 93/200
1493/1493 - 104s - loss: 1.8410e-04 - val_loss: 1.3554e-04 - 104s/epoch - 70ms/step
Epoch 94/200
1493/1493 - 104s - loss: 1.2713e-04 - val_loss: 1.0214e-04 - 104s/epoch - 70ms/step
Epoch 95/200
1493/1493 - 104s - loss: 1.1822e-04 - val_loss: 2.4381e-04 - 104s/epoch - 70ms/step
Epoch 96/200
1493/1493 - 104s - loss: 1.2405e-04 - val_loss: 2.3815e-04 - 104s/epoch - 70ms/step
Epoch 97/200
1493/1493 - 104s - loss: 1.3772e-04 - val_loss: 1.2508e-04 - 104s/epoch - 70ms/step
Epoch 98/200
1493/1493 - 105s - loss: 1.1604e-04 - val_loss: 2.4531e-04 - 105s/epoch - 70ms/step
Epoch 99/200
1493/1493 - 105s - loss: 1.3599e-04 - val_loss: 9.4953e-05 - 105s/epoch - 70ms/step
Epoch 100/200
1493/1493 - 105s - loss: 1.1899e-04 - val_loss: 1.1269e-04 - 105s/epoch - 70ms/step
Epoch 101/200
1493/1493 - 104s - loss: 1.1236e-04 - val_loss: 1.5894e-04 - 104s/epoch - 70ms/step
Epoch 102/200
1493/1493 - 104s - loss: 1.1984e-04 - val_loss: 3.4796e-04 - 104s/epoch - 70ms/step
Epoch 103/200
1493/1493 - 105s - loss: 1.2913e-04 - val_loss: 1.1381e-04 - 105s/epoch - 70ms/step
Epoch 104/200
1493/1493 - 104s - loss: 1.1240e-04 - val_loss: 6.3946e-04 - 104s/epoch - 70ms/step
Epoch 105/200
1493/1493 - 105s - loss: 1.6764e-04 - val_loss: 9.2893e-05 - 105s/epoch - 70ms/step
Epoch 106/200
1493/1493 - 104s - loss: 1.1395e-04 - val_loss: 1.0878e-04 - 104s/epoch - 70ms/step
Epoch 107/200
1493/1493 - 105s - loss: 1.1117e-04 - val_loss: 1.0955e-04 - 105s/epoch - 70ms/step
Epoch 108/200
1493/1493 - 104s - loss: 1.1050e-04 - val_loss: 1.2184e-04 - 104s/epoch - 70ms/step
Epoch 109/200
1493/1493 - 105s - loss: 1.0911e-04 - val_loss: 1.2177e-04 - 105s/epoch - 70ms/step
Epoch 110/200
1493/1493 - 104s - loss: 1.0715e-04 - val_loss: 1.2370e-04 - 104s/epoch - 70ms/step
Epoch 111/200
1493/1493 - 104s - loss: 1.0613e-04 - val_loss: 9.7212e-05 - 104s/epoch - 70ms/step
Epoch 112/200
1493/1493 - 104s - loss: 1.0385e-04 - val_loss: 1.0649e-04 - 104s/epoch - 70ms/step
Epoch 113/200
1493/1493 - 105s - loss: 1.0279e-04 - val_loss: 2.3582e-04 - 105s/epoch - 70ms/step
Epoch 114/200
1493/1493 - 104s - loss: 1.2489e-04 - val_loss: 9.2405e-05 - 104s/epoch - 70ms/step
Epoch 115/200
1493/1493 - 104s - loss: 1.0481e-04 - val_loss: 1.4797e-04 - 104s/epoch - 70ms/step
Epoch 116/200
1493/1493 - 104s - loss: 1.0451e-04 - val_loss: 1.1221e-04 - 104s/epoch - 70ms/step
Epoch 117/200
1493/1493 - 104s - loss: 1.0174e-04 - val_loss: 1.6539e-04 - 104s/epoch - 70ms/step
Epoch 118/200
1493/1493 - 104s - loss: 1.0875e-04 - val_loss: 1.2976e-04 - 104s/epoch - 70ms/step
Epoch 119/200
1493/1493 - 104s - loss: 1.0605e-04 - val_loss: 9.7845e-05 - 104s/epoch - 70ms/step
Epoch 120/200
1493/1493 - 104s - loss: 1.0062e-04 - val_loss: 1.0259e-04 - 104s/epoch - 70ms/step
Epoch 121/200
1493/1493 - 104s - loss: 9.9932e-05 - val_loss: 1.2491e-04 - 104s/epoch - 70ms/step
Epoch 122/200
1493/1493 - 104s - loss: 1.0073e-04 - val_loss: 9.3632e-05 - 104s/epoch - 70ms/step
Epoch 123/200
1493/1493 - 104s - loss: 9.8752e-05 - val_loss: 1.0506e-04 - 104s/epoch - 70ms/step
Epoch 124/200
1493/1493 - 104s - loss: 1.0015e-04 - val_loss: 4.3439e-04 - 104s/epoch - 70ms/step
Epoch 125/200
1493/1493 - 105s - loss: 1.5306e-04 - val_loss: 3.0572e-04 - 105s/epoch - 70ms/step
Epoch 126/200
1493/1493 - 104s - loss: 1.1965e-04 - val_loss: 1.1418e-04 - 104s/epoch - 70ms/step
Epoch 127/200
1493/1493 - 104s - loss: 1.0199e-04 - val_loss: 1.6735e-04 - 104s/epoch - 70ms/step
Epoch 128/200
1493/1493 - 105s - loss: 1.0299e-04 - val_loss: 9.1949e-05 - 105s/epoch - 70ms/step
Epoch 129/200
1493/1493 - 104s - loss: 9.7764e-05 - val_loss: 1.0197e-04 - 104s/epoch - 70ms/step
Epoch 130/200
1493/1493 - 105s - loss: 9.6444e-05 - val_loss: 9.2267e-05 - 105s/epoch - 70ms/step
Epoch 131/200
1493/1493 - 105s - loss: 9.9399e-05 - val_loss: 2.8645e-04 - 105s/epoch - 70ms/step
Epoch 132/200
1493/1493 - 105s - loss: 1.1986e-04 - val_loss: 9.0289e-05 - 105s/epoch - 70ms/step
Epoch 133/200
1493/1493 - 105s - loss: 9.9627e-05 - val_loss: 9.6929e-05 - 105s/epoch - 70ms/step
Epoch 134/200
1493/1493 - 105s - loss: 9.5694e-05 - val_loss: 9.6687e-05 - 105s/epoch - 70ms/step
Epoch 135/200
1493/1493 - 105s - loss: 9.4191e-05 - val_loss: 8.8929e-05 - 105s/epoch - 70ms/step
Epoch 136/200
1493/1493 - 105s - loss: 9.4403e-05 - val_loss: 1.7511e-04 - 105s/epoch - 70ms/step
Epoch 137/200
1493/1493 - 105s - loss: 1.1642e-04 - val_loss: 1.0042e-04 - 105s/epoch - 70ms/step
Epoch 138/200
1493/1493 - 105s - loss: 9.4389e-05 - val_loss: 8.7748e-05 - 105s/epoch - 70ms/step
Epoch 139/200
1493/1493 - 105s - loss: 9.6408e-05 - val_loss: 2.3798e-04 - 105s/epoch - 70ms/step
Epoch 140/200
1493/1493 - 104s - loss: 1.4733e-04 - val_loss: 8.8128e-05 - 104s/epoch - 70ms/step
Epoch 141/200
1493/1493 - 105s - loss: 1.0064e-04 - val_loss: 1.7066e-04 - 105s/epoch - 70ms/step
Epoch 142/200
1493/1493 - 104s - loss: 1.1325e-04 - val_loss: 1.0080e-04 - 104s/epoch - 70ms/step
Epoch 143/200
1493/1493 - 105s - loss: 9.5311e-05 - val_loss: 1.2430e-04 - 105s/epoch - 70ms/step
Epoch 144/200
1493/1493 - 105s - loss: 9.6244e-05 - val_loss: 9.8105e-05 - 105s/epoch - 70ms/step
Epoch 145/200
1493/1493 - 105s - loss: 9.2522e-05 - val_loss: 8.9597e-05 - 105s/epoch - 70ms/step
Epoch 146/200
1493/1493 - 105s - loss: 9.1864e-05 - val_loss: 8.1417e-05 - 105s/epoch - 70ms/step
Epoch 147/200
1493/1493 - 105s - loss: 8.9550e-05 - val_loss: 9.0184e-05 - 105s/epoch - 70ms/step
Epoch 148/200
1493/1493 - 105s - loss: 8.9433e-05 - val_loss: 1.4054e-04 - 105s/epoch - 70ms/step
Epoch 149/200
1493/1493 - 105s - loss: 8.9527e-05 - val_loss: 9.1849e-05 - 105s/epoch - 70ms/step
Epoch 150/200
1493/1493 - 105s - loss: 8.8046e-05 - val_loss: 8.2935e-05 - 105s/epoch - 70ms/step
Epoch 151/200
1493/1493 - 105s - loss: 8.7708e-05 - val_loss: 1.0692e-04 - 105s/epoch - 70ms/step
Epoch 152/200
1493/1493 - 104s - loss: 8.9306e-05 - val_loss: 2.1569e-04 - 104s/epoch - 70ms/step
Epoch 153/200
1493/1493 - 105s - loss: 1.0724e-04 - val_loss: 9.1378e-05 - 105s/epoch - 70ms/step
Epoch 154/200
1493/1493 - 105s - loss: 8.8326e-05 - val_loss: 8.5666e-05 - 105s/epoch - 70ms/step
Epoch 155/200
1493/1493 - 104s - loss: 8.5894e-05 - val_loss: 8.3811e-05 - 104s/epoch - 70ms/step
Epoch 156/200
1493/1493 - 104s - loss: 8.9219e-05 - val_loss: 9.0229e-05 - 104s/epoch - 70ms/step
Epoch 157/200
1493/1493 - 105s - loss: 8.5403e-05 - val_loss: 1.0653e-04 - 105s/epoch - 70ms/step
Epoch 158/200
1493/1493 - 104s - loss: 8.6823e-05 - val_loss: 9.3776e-05 - 104s/epoch - 70ms/step
Epoch 159/200
1493/1493 - 104s - loss: 8.5165e-05 - val_loss: 7.8690e-05 - 104s/epoch - 70ms/step
Epoch 160/200
1493/1493 - 104s - loss: 8.7317e-05 - val_loss: 1.6482e-04 - 104s/epoch - 70ms/step
Epoch 161/200
1493/1493 - 104s - loss: 1.2806e-04 - val_loss: 8.0925e-05 - 104s/epoch - 70ms/step
Epoch 162/200
1493/1493 - 104s - loss: 8.8605e-05 - val_loss: 7.7506e-05 - 104s/epoch - 70ms/step
Epoch 163/200
1493/1493 - 104s - loss: 8.6877e-05 - val_loss: 1.0662e-04 - 104s/epoch - 70ms/step
Epoch 164/200
1493/1493 - 104s - loss: 9.2117e-05 - val_loss: 8.5864e-05 - 104s/epoch - 70ms/step
Epoch 165/200
1493/1493 - 105s - loss: 9.6283e-05 - val_loss: 8.4856e-05 - 105s/epoch - 70ms/step
Epoch 166/200
1493/1493 - 104s - loss: 8.5076e-05 - val_loss: 8.5066e-05 - 104s/epoch - 70ms/step
Epoch 167/200
1493/1493 - 104s - loss: 8.4645e-05 - val_loss: 7.6978e-05 - 104s/epoch - 70ms/step
Epoch 168/200
1493/1493 - 105s - loss: 8.3987e-05 - val_loss: 8.5514e-05 - 105s/epoch - 70ms/step
Epoch 169/200
1493/1493 - 104s - loss: 8.3685e-05 - val_loss: 9.1499e-05 - 104s/epoch - 70ms/step
Epoch 170/200
1493/1493 - 104s - loss: 8.2516e-05 - val_loss: 1.1868e-04 - 104s/epoch - 70ms/step
Epoch 171/200
1493/1493 - 104s - loss: 9.5518e-05 - val_loss: 2.2717e-04 - 104s/epoch - 70ms/step
Epoch 172/200
1493/1493 - 105s - loss: 1.1180e-04 - val_loss: 1.1754e-04 - 105s/epoch - 70ms/step
Epoch 173/200
1493/1493 - 104s - loss: 9.0515e-05 - val_loss: 7.7292e-05 - 104s/epoch - 70ms/step
Epoch 174/200
1493/1493 - 105s - loss: 8.3149e-05 - val_loss: 8.0802e-05 - 105s/epoch - 70ms/step
Epoch 175/200
1493/1493 - 104s - loss: 8.3515e-05 - val_loss: 1.0608e-04 - 104s/epoch - 70ms/step
Epoch 176/200
1493/1493 - 104s - loss: 8.4057e-05 - val_loss: 8.6794e-05 - 104s/epoch - 70ms/step
Epoch 177/200
1493/1493 - 104s - loss: 8.2585e-05 - val_loss: 7.1869e-05 - 104s/epoch - 70ms/step
Epoch 178/200
1493/1493 - 104s - loss: 8.0602e-05 - val_loss: 8.1366e-05 - 104s/epoch - 70ms/step
Epoch 179/200
1493/1493 - 104s - loss: 7.9964e-05 - val_loss: 8.7853e-05 - 104s/epoch - 70ms/step
Epoch 180/200
1493/1493 - 104s - loss: 8.0911e-05 - val_loss: 8.4404e-05 - 104s/epoch - 70ms/step
Epoch 181/200
1493/1493 - 104s - loss: 7.9539e-05 - val_loss: 7.9877e-05 - 104s/epoch - 70ms/step
Epoch 182/200
1493/1493 - 104s - loss: 8.0498e-05 - val_loss: 1.0802e-04 - 104s/epoch - 70ms/step
Epoch 183/200
1493/1493 - 104s - loss: 8.1490e-05 - val_loss: 8.5381e-05 - 104s/epoch - 70ms/step
Epoch 184/200
1493/1493 - 104s - loss: 8.0559e-05 - val_loss: 7.4052e-05 - 104s/epoch - 70ms/step
Epoch 185/200
1493/1493 - 104s - loss: 7.8041e-05 - val_loss: 1.0984e-04 - 104s/epoch - 70ms/step
Epoch 186/200
1493/1493 - 104s - loss: 8.0758e-05 - val_loss: 8.0305e-05 - 104s/epoch - 70ms/step
Epoch 187/200
1493/1493 - 104s - loss: 8.1194e-05 - val_loss: 1.7914e-04 - 104s/epoch - 70ms/step
Epoch 188/200
1493/1493 - 105s - loss: 1.1555e-04 - val_loss: 8.2368e-05 - 105s/epoch - 70ms/step
Epoch 189/200
1493/1493 - 104s - loss: 8.4860e-05 - val_loss: 9.7626e-05 - 104s/epoch - 70ms/step
Epoch 190/200
1493/1493 - 104s - loss: 8.4431e-05 - val_loss: 2.0244e-04 - 104s/epoch - 70ms/step
Epoch 191/200
1493/1493 - 105s - loss: 1.0698e-04 - val_loss: 1.6073e-04 - 105s/epoch - 70ms/step
Epoch 192/200
1493/1493 - 105s - loss: 8.9211e-05 - val_loss: 8.2356e-05 - 105s/epoch - 70ms/step
Epoch 193/200
1493/1493 - 105s - loss: 8.2655e-05 - val_loss: 2.0369e-04 - 105s/epoch - 70ms/step
Epoch 194/200
1493/1493 - 105s - loss: 1.0193e-04 - val_loss: 5.1904e-04 - 105s/epoch - 70ms/step
Epoch 195/200
1493/1493 - 105s - loss: 1.3612e-04 - val_loss: 8.4406e-05 - 105s/epoch - 70ms/step
Epoch 196/200
1493/1493 - 105s - loss: 8.8405e-05 - val_loss: 6.9765e-05 - 105s/epoch - 70ms/step
Epoch 197/200
1493/1493 - 105s - loss: 8.3125e-05 - val_loss: 9.8349e-05 - 105s/epoch - 70ms/step
Epoch 198/200
1493/1493 - 104s - loss: 8.5455e-05 - val_loss: 8.7571e-05 - 104s/epoch - 70ms/step
Epoch 199/200
1493/1493 - 104s - loss: 8.0742e-05 - val_loss: 7.7978e-05 - 104s/epoch - 70ms/step
Epoch 200/200
1493/1493 - 104s - loss: 7.9097e-05 - val_loss: 7.6937e-05 - 104s/epoch - 70ms/step
COMPRESSED VECTOR SIZE: 1264
Loss in the autoencoder: 7.693725638091564e-05
  1/332 [..............................] - ETA: 30s  8/332 [..............................] - ETA: 2s  15/332 [>.............................] - ETA: 2s 22/332 [>.............................] - ETA: 2s 29/332 [=>............................] - ETA: 2s 36/332 [==>...........................] - ETA: 2s 43/332 [==>...........................] - ETA: 2s 50/332 [===>..........................] - ETA: 2s 57/332 [====>.........................] - ETA: 2s 64/332 [====>.........................] - ETA: 2s 71/332 [=====>........................] - ETA: 2s 78/332 [======>.......................] - ETA: 2s 85/332 [======>.......................] - ETA: 1s 92/332 [=======>......................] - ETA: 1s 99/332 [=======>......................] - ETA: 1s106/332 [========>.....................] - ETA: 1s113/332 [=========>....................] - ETA: 1s120/332 [=========>....................] - ETA: 1s127/332 [==========>...................] - ETA: 1s134/332 [===========>..................] - ETA: 1s141/332 [===========>..................] - ETA: 1s148/332 [============>.................] - ETA: 1s155/332 [=============>................] - ETA: 1s162/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s176/332 [==============>...............] - ETA: 1s183/332 [===============>..............] - ETA: 1s190/332 [================>.............] - ETA: 1s197/332 [================>.............] - ETA: 1s204/332 [=================>............] - ETA: 1s211/332 [==================>...........] - ETA: 0s218/332 [==================>...........] - ETA: 0s225/332 [===================>..........] - ETA: 0s232/332 [===================>..........] - ETA: 0s239/332 [====================>.........] - ETA: 0s246/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s260/332 [======================>.......] - ETA: 0s267/332 [=======================>......] - ETA: 0s274/332 [=======================>......] - ETA: 0s281/332 [========================>.....] - ETA: 0s288/332 [=========================>....] - ETA: 0s295/332 [=========================>....] - ETA: 0s302/332 [==========================>...] - ETA: 0s309/332 [==========================>...] - ETA: 0s316/332 [===========================>..] - ETA: 0s323/332 [============================>.] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 8ms/step
correlation 0.0008727824332980987
cosine 0.0006873934416334179
MAE: 0.0047894814
RMSE: 0.008771385
r2: 0.9950095047984727
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 2780)              3516700   
                                                                 
 batch_normalization (BatchN  (None, 2780)             11120     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1264)              3515184   
                                                                 
 batch_normalization_1 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 1264)              0         
                                                                 
 dense_1 (Dense)             (None, 2780)              3516700   
                                                                 
 batch_normalization_2 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 2780)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 14,091,064
Trainable params: 14,077,416
Non-trainable params: 13,648
_________________________________________________________________
Encoder
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_1 (InputLayer)        multiple                  0         
                                                                 
 dense (Dense)               (None, 2780)              3516700   
                                                                 
 batch_normalization (BatchN  (None, 2780)             11120     
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 7,043,004
Trainable params: 7,037,444
Non-trainable params: 5,560
_________________________________________________________________
Decoder
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 1264)]            0         
                                                                 
 batch_normalization_1 (Batc  (None, 1264)             5056      
 hNormalization)                                                 
                                                                 
 re_lu_1 (ReLU)              (None, 1264)              0         
                                                                 
 dense_1 (Dense)             (None, 2780)              3516700   
                                                                 
 batch_normalization_2 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_2 (ReLU)              (None, 2780)              0         
                                                                 
 dense_2 (Dense)             (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 7,048,060
Trainable params: 7,039,972
Non-trainable params: 8,088
_________________________________________________________________
['2.2custom_n_b', 'mse', 64, 200, 0.0005, 1.0, 1264, 7.909729902166873e-05, 7.693725638091564e-05, 0.0008727824332980987, 0.0006873934416334179, 0.004789481405168772, 0.008771385066211224, 0.9950095047984727, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_2.2final_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_3 (Dense)             (None, 2780)              3516700   
                                                                 
 batch_normalization_3 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1200)              3337200   
                                                                 
 batch_normalization_4 (Batc  (None, 1200)             4800      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 1200)              0         
                                                                 
 dense_4 (Dense)             (None, 2780)              3338780   
                                                                 
 batch_normalization_5 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2780)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 13,734,904
Trainable params: 13,721,384
Non-trainable params: 13,520
_________________________________________________________________
Epoch 1/200
1493/1493 - 102s - loss: 0.0087 - val_loss: 0.0037 - 102s/epoch - 69ms/step
Epoch 2/200
1493/1493 - 102s - loss: 0.0027 - val_loss: 0.0030 - 102s/epoch - 68ms/step
Epoch 3/200
1493/1493 - 102s - loss: 0.0020 - val_loss: 0.0014 - 102s/epoch - 68ms/step
Epoch 4/200
1493/1493 - 102s - loss: 0.0016 - val_loss: 0.0026 - 102s/epoch - 68ms/step
Epoch 5/200
1493/1493 - 102s - loss: 0.0015 - val_loss: 0.0015 - 102s/epoch - 68ms/step
Epoch 6/200
1493/1493 - 102s - loss: 0.0014 - val_loss: 0.0012 - 102s/epoch - 68ms/step
Epoch 7/200
1493/1493 - 102s - loss: 0.0013 - val_loss: 0.0015 - 102s/epoch - 68ms/step
Epoch 8/200
1493/1493 - 102s - loss: 0.0011 - val_loss: 8.9183e-04 - 102s/epoch - 68ms/step
Epoch 9/200
1493/1493 - 102s - loss: 9.6545e-04 - val_loss: 8.4563e-04 - 102s/epoch - 68ms/step
Epoch 10/200
1493/1493 - 102s - loss: 8.7393e-04 - val_loss: 0.0010 - 102s/epoch - 68ms/step
Epoch 11/200
1493/1493 - 102s - loss: 8.3512e-04 - val_loss: 6.4882e-04 - 102s/epoch - 68ms/step
Epoch 12/200
1493/1493 - 102s - loss: 7.4051e-04 - val_loss: 9.0196e-04 - 102s/epoch - 68ms/step
Epoch 13/200
1493/1493 - 102s - loss: 6.8370e-04 - val_loss: 0.0013 - 102s/epoch - 68ms/step
Epoch 14/200
1493/1493 - 102s - loss: 6.8346e-04 - val_loss: 5.1544e-04 - 102s/epoch - 68ms/step
Epoch 15/200
1493/1493 - 102s - loss: 5.7532e-04 - val_loss: 9.5455e-04 - 102s/epoch - 68ms/step
Epoch 16/200
1493/1493 - 102s - loss: 5.7143e-04 - val_loss: 4.9085e-04 - 102s/epoch - 68ms/step
Epoch 17/200
1493/1493 - 102s - loss: 4.8086e-04 - val_loss: 7.4070e-04 - 102s/epoch - 68ms/step
Epoch 18/200
1493/1493 - 102s - loss: 4.7942e-04 - val_loss: 5.3862e-04 - 102s/epoch - 68ms/step
Epoch 19/200
1493/1493 - 102s - loss: 4.4075e-04 - val_loss: 5.2367e-04 - 102s/epoch - 68ms/step
Epoch 20/200
1493/1493 - 102s - loss: 4.2346e-04 - val_loss: 4.2856e-04 - 102s/epoch - 68ms/step
Epoch 21/200
1493/1493 - 102s - loss: 3.8826e-04 - val_loss: 4.4841e-04 - 102s/epoch - 68ms/step
Epoch 22/200
1493/1493 - 102s - loss: 3.7668e-04 - val_loss: 4.3618e-04 - 102s/epoch - 68ms/step
Epoch 23/200
1493/1493 - 102s - loss: 3.6257e-04 - val_loss: 8.1504e-04 - 102s/epoch - 68ms/step
Epoch 24/200
1493/1493 - 102s - loss: 3.6728e-04 - val_loss: 3.4082e-04 - 102s/epoch - 68ms/step
Epoch 25/200
1493/1493 - 102s - loss: 3.1980e-04 - val_loss: 3.2114e-04 - 102s/epoch - 68ms/step
Epoch 26/200
1493/1493 - 102s - loss: 3.3671e-04 - val_loss: 3.3375e-04 - 102s/epoch - 68ms/step
Epoch 27/200
1493/1493 - 102s - loss: 2.9362e-04 - val_loss: 3.0346e-04 - 102s/epoch - 68ms/step
Epoch 28/200
1493/1493 - 102s - loss: 2.8070e-04 - val_loss: 4.6985e-04 - 102s/epoch - 68ms/step
Epoch 29/200
1493/1493 - 102s - loss: 2.7666e-04 - val_loss: 3.0999e-04 - 102s/epoch - 68ms/step
Epoch 30/200
1493/1493 - 102s - loss: 2.7436e-04 - val_loss: 3.2875e-04 - 102s/epoch - 68ms/step
Epoch 31/200
1493/1493 - 102s - loss: 2.5598e-04 - val_loss: 4.1604e-04 - 102s/epoch - 68ms/step
Epoch 32/200
1493/1493 - 102s - loss: 3.2037e-04 - val_loss: 3.6159e-04 - 102s/epoch - 68ms/step
Epoch 33/200
1493/1493 - 102s - loss: 2.7372e-04 - val_loss: 2.2667e-04 - 102s/epoch - 68ms/step
Epoch 34/200
1493/1493 - 102s - loss: 2.4266e-04 - val_loss: 2.0977e-04 - 102s/epoch - 68ms/step
Epoch 35/200
1493/1493 - 102s - loss: 2.3166e-04 - val_loss: 6.4940e-04 - 102s/epoch - 68ms/step
Epoch 36/200
1493/1493 - 102s - loss: 2.7022e-04 - val_loss: 3.1393e-04 - 102s/epoch - 68ms/step
Epoch 37/200
1493/1493 - 102s - loss: 2.3254e-04 - val_loss: 2.2133e-04 - 102s/epoch - 68ms/step
Epoch 38/200
1493/1493 - 102s - loss: 2.2405e-04 - val_loss: 2.0615e-04 - 102s/epoch - 68ms/step
Epoch 39/200
1493/1493 - 102s - loss: 2.1188e-04 - val_loss: 1.9866e-04 - 102s/epoch - 68ms/step
Epoch 40/200
1493/1493 - 102s - loss: 2.0331e-04 - val_loss: 2.8256e-04 - 102s/epoch - 68ms/step
Epoch 41/200
1493/1493 - 102s - loss: 2.0569e-04 - val_loss: 1.9476e-04 - 102s/epoch - 68ms/step
Epoch 42/200
1493/1493 - 102s - loss: 2.0637e-04 - val_loss: 2.0443e-04 - 102s/epoch - 68ms/step
Epoch 43/200
1493/1493 - 102s - loss: 2.0464e-04 - val_loss: 3.5311e-04 - 102s/epoch - 68ms/step
Epoch 44/200
1493/1493 - 102s - loss: 2.0666e-04 - val_loss: 6.0113e-04 - 102s/epoch - 68ms/step
Epoch 45/200
1493/1493 - 102s - loss: 2.2593e-04 - val_loss: 1.6501e-04 - 102s/epoch - 68ms/step
Epoch 46/200
1493/1493 - 102s - loss: 1.8844e-04 - val_loss: 1.7841e-04 - 102s/epoch - 68ms/step
Epoch 47/200
1493/1493 - 104s - loss: 1.8461e-04 - val_loss: 2.5622e-04 - 104s/epoch - 70ms/step
Epoch 48/200
1493/1493 - 102s - loss: 1.8597e-04 - val_loss: 2.0672e-04 - 102s/epoch - 69ms/step
Epoch 49/200
1493/1493 - 102s - loss: 1.8106e-04 - val_loss: 5.8108e-04 - 102s/epoch - 68ms/step
Epoch 50/200
1493/1493 - 102s - loss: 2.2516e-04 - val_loss: 0.0015 - 102s/epoch - 68ms/step
Epoch 51/200
1493/1493 - 102s - loss: 2.8866e-04 - val_loss: 2.2204e-04 - 102s/epoch - 68ms/step
Epoch 52/200
1493/1493 - 102s - loss: 1.8278e-04 - val_loss: 1.9410e-04 - 102s/epoch - 68ms/step
Epoch 53/200
1493/1493 - 102s - loss: 1.7675e-04 - val_loss: 1.6258e-04 - 102s/epoch - 68ms/step
Epoch 54/200
1493/1493 - 102s - loss: 1.7643e-04 - val_loss: 2.0024e-04 - 102s/epoch - 68ms/step
Epoch 55/200
1493/1493 - 102s - loss: 1.6522e-04 - val_loss: 1.6316e-04 - 102s/epoch - 68ms/step
Epoch 56/200
1493/1493 - 102s - loss: 1.6427e-04 - val_loss: 2.0872e-04 - 102s/epoch - 68ms/step
Epoch 57/200
1493/1493 - 102s - loss: 1.6680e-04 - val_loss: 1.6920e-04 - 102s/epoch - 68ms/step
Epoch 58/200
1493/1493 - 102s - loss: 1.5861e-04 - val_loss: 1.4504e-04 - 102s/epoch - 68ms/step
Epoch 59/200
1493/1493 - 102s - loss: 1.5487e-04 - val_loss: 1.5192e-04 - 102s/epoch - 68ms/step
Epoch 60/200
1493/1493 - 102s - loss: 1.5480e-04 - val_loss: 3.0525e-04 - 102s/epoch - 68ms/step
Epoch 61/200
1493/1493 - 102s - loss: 1.9611e-04 - val_loss: 1.5509e-04 - 102s/epoch - 68ms/step
Epoch 62/200
1493/1493 - 102s - loss: 1.5329e-04 - val_loss: 1.6554e-04 - 102s/epoch - 68ms/step
Epoch 63/200
1493/1493 - 102s - loss: 1.5107e-04 - val_loss: 2.2667e-04 - 102s/epoch - 68ms/step
Epoch 64/200
1493/1493 - 102s - loss: 1.4940e-04 - val_loss: 2.5171e-04 - 102s/epoch - 68ms/step
Epoch 65/200
1493/1493 - 102s - loss: 1.5255e-04 - val_loss: 1.6425e-04 - 102s/epoch - 68ms/step
Epoch 66/200
1493/1493 - 102s - loss: 1.4781e-04 - val_loss: 1.8936e-04 - 102s/epoch - 68ms/step
Epoch 67/200
1493/1493 - 102s - loss: 1.4356e-04 - val_loss: 1.4152e-04 - 102s/epoch - 68ms/step
Epoch 68/200
1493/1493 - 102s - loss: 1.3960e-04 - val_loss: 2.6169e-04 - 102s/epoch - 68ms/step
Epoch 69/200
1493/1493 - 102s - loss: 1.6133e-04 - val_loss: 1.5986e-04 - 102s/epoch - 68ms/step
Epoch 70/200
1493/1493 - 102s - loss: 1.3979e-04 - val_loss: 2.0920e-04 - 102s/epoch - 68ms/step
Epoch 71/200
1493/1493 - 102s - loss: 1.4370e-04 - val_loss: 1.3285e-04 - 102s/epoch - 68ms/step
Epoch 72/200
1493/1493 - 102s - loss: 1.3790e-04 - val_loss: 1.6875e-04 - 102s/epoch - 68ms/step
Epoch 73/200
1493/1493 - 102s - loss: 1.4092e-04 - val_loss: 1.4527e-04 - 102s/epoch - 68ms/step
Epoch 74/200
1493/1493 - 102s - loss: 1.3598e-04 - val_loss: 1.4283e-04 - 102s/epoch - 68ms/step
Epoch 75/200
1493/1493 - 102s - loss: 1.3153e-04 - val_loss: 1.2084e-04 - 102s/epoch - 68ms/step
Epoch 76/200
1493/1493 - 102s - loss: 1.3098e-04 - val_loss: 1.2891e-04 - 102s/epoch - 68ms/step
Epoch 77/200
1493/1493 - 102s - loss: 1.2722e-04 - val_loss: 5.9371e-04 - 102s/epoch - 68ms/step
Epoch 78/200
1493/1493 - 102s - loss: 1.5593e-04 - val_loss: 1.4753e-04 - 102s/epoch - 68ms/step
Epoch 79/200
1493/1493 - 102s - loss: 1.3068e-04 - val_loss: 1.4043e-04 - 102s/epoch - 68ms/step
Epoch 80/200
1493/1493 - 102s - loss: 1.2890e-04 - val_loss: 1.5727e-04 - 102s/epoch - 68ms/step
Epoch 81/200
1493/1493 - 102s - loss: 1.2785e-04 - val_loss: 2.7699e-04 - 102s/epoch - 68ms/step
Epoch 82/200
1493/1493 - 102s - loss: 1.2726e-04 - val_loss: 1.2862e-04 - 102s/epoch - 68ms/step
Epoch 83/200
1493/1493 - 102s - loss: 1.2214e-04 - val_loss: 1.3215e-04 - 102s/epoch - 68ms/step
Epoch 84/200
1493/1493 - 102s - loss: 1.2198e-04 - val_loss: 1.3478e-04 - 102s/epoch - 68ms/step
Epoch 85/200
1493/1493 - 102s - loss: 1.2311e-04 - val_loss: 1.4301e-04 - 102s/epoch - 68ms/step
Epoch 86/200
1493/1493 - 102s - loss: 1.2206e-04 - val_loss: 1.3306e-04 - 102s/epoch - 68ms/step
Epoch 87/200
1493/1493 - 102s - loss: 1.1843e-04 - val_loss: 1.2893e-04 - 102s/epoch - 68ms/step
Epoch 88/200
1493/1493 - 102s - loss: 1.1835e-04 - val_loss: 1.3082e-04 - 102s/epoch - 68ms/step
Epoch 89/200
1493/1493 - 102s - loss: 1.1947e-04 - val_loss: 1.2085e-04 - 102s/epoch - 68ms/step
Epoch 90/200
1493/1493 - 102s - loss: 1.1643e-04 - val_loss: 1.4073e-04 - 102s/epoch - 68ms/step
Epoch 91/200
1493/1493 - 102s - loss: 1.1587e-04 - val_loss: 1.3358e-04 - 102s/epoch - 68ms/step
Epoch 92/200
1493/1493 - 102s - loss: 1.2153e-04 - val_loss: 8.0618e-04 - 102s/epoch - 68ms/step
Epoch 93/200
1493/1493 - 102s - loss: 2.0395e-04 - val_loss: 6.9972e-04 - 102s/epoch - 68ms/step
Epoch 94/200
1493/1493 - 102s - loss: 1.8828e-04 - val_loss: 1.0717e-04 - 102s/epoch - 68ms/step
Epoch 95/200
1493/1493 - 102s - loss: 1.2771e-04 - val_loss: 1.7793e-04 - 102s/epoch - 68ms/step
Epoch 96/200
1493/1493 - 102s - loss: 1.2573e-04 - val_loss: 1.4084e-04 - 102s/epoch - 68ms/step
Epoch 97/200
1493/1493 - 102s - loss: 1.1991e-04 - val_loss: 1.1690e-04 - 102s/epoch - 68ms/step
Epoch 98/200
1493/1493 - 102s - loss: 1.2424e-04 - val_loss: 9.7133e-04 - 102s/epoch - 68ms/step
Epoch 99/200
1493/1493 - 102s - loss: 2.5837e-04 - val_loss: 1.0319e-04 - 102s/epoch - 68ms/step
Epoch 100/200
1493/1493 - 102s - loss: 1.4998e-04 - val_loss: 9.9884e-05 - 102s/epoch - 68ms/step
Epoch 101/200
1493/1493 - 102s - loss: 1.2539e-04 - val_loss: 1.5675e-04 - 102s/epoch - 68ms/step
Epoch 102/200
1493/1493 - 102s - loss: 1.3177e-04 - val_loss: 2.0093e-04 - 102s/epoch - 68ms/step
Epoch 103/200
1493/1493 - 102s - loss: 1.2818e-04 - val_loss: 1.1539e-04 - 102s/epoch - 68ms/step
Epoch 104/200
1493/1493 - 102s - loss: 1.1675e-04 - val_loss: 3.1276e-04 - 102s/epoch - 68ms/step
Epoch 105/200
1493/1493 - 102s - loss: 1.3185e-04 - val_loss: 9.1792e-05 - 102s/epoch - 68ms/step
Epoch 106/200
1493/1493 - 102s - loss: 1.1373e-04 - val_loss: 1.1205e-04 - 102s/epoch - 68ms/step
Epoch 107/200
1493/1493 - 102s - loss: 1.1234e-04 - val_loss: 1.2293e-04 - 102s/epoch - 68ms/step
Epoch 108/200
1493/1493 - 102s - loss: 1.1294e-04 - val_loss: 1.1526e-04 - 102s/epoch - 68ms/step
Epoch 109/200
1493/1493 - 102s - loss: 1.1057e-04 - val_loss: 1.3099e-04 - 102s/epoch - 68ms/step
Epoch 110/200
1493/1493 - 102s - loss: 1.0810e-04 - val_loss: 1.1943e-04 - 102s/epoch - 68ms/step
Epoch 111/200
1493/1493 - 102s - loss: 1.0758e-04 - val_loss: 1.0410e-04 - 102s/epoch - 68ms/step
Epoch 112/200
1493/1493 - 102s - loss: 1.0665e-04 - val_loss: 9.8738e-05 - 102s/epoch - 68ms/step
Epoch 113/200
1493/1493 - 102s - loss: 1.0606e-04 - val_loss: 4.0277e-04 - 102s/epoch - 68ms/step
Epoch 114/200
1493/1493 - 102s - loss: 1.5555e-04 - val_loss: 9.9849e-05 - 102s/epoch - 68ms/step
Epoch 115/200
1493/1493 - 102s - loss: 1.1138e-04 - val_loss: 1.8012e-04 - 102s/epoch - 68ms/step
Epoch 116/200
1493/1493 - 102s - loss: 1.0952e-04 - val_loss: 1.1325e-04 - 102s/epoch - 68ms/step
Epoch 117/200
1493/1493 - 102s - loss: 1.0505e-04 - val_loss: 2.9573e-04 - 102s/epoch - 68ms/step
Epoch 118/200
1493/1493 - 102s - loss: 1.2143e-04 - val_loss: 1.0657e-04 - 102s/epoch - 68ms/step
Epoch 119/200
1493/1493 - 102s - loss: 1.0548e-04 - val_loss: 9.7739e-05 - 102s/epoch - 68ms/step
Epoch 120/200
1493/1493 - 102s - loss: 1.0324e-04 - val_loss: 1.0112e-04 - 102s/epoch - 68ms/step
Epoch 121/200
1493/1493 - 102s - loss: 1.0258e-04 - val_loss: 1.4226e-04 - 102s/epoch - 68ms/step
Epoch 122/200
1493/1493 - 102s - loss: 1.0417e-04 - val_loss: 9.4058e-05 - 102s/epoch - 68ms/step
Epoch 123/200
1493/1493 - 102s - loss: 1.0037e-04 - val_loss: 9.7956e-05 - 102s/epoch - 68ms/step
Epoch 124/200
1493/1493 - 102s - loss: 1.0041e-04 - val_loss: 2.8936e-04 - 102s/epoch - 68ms/step
Epoch 125/200
1493/1493 - 102s - loss: 1.3382e-04 - val_loss: 1.6634e-04 - 102s/epoch - 68ms/step
Epoch 126/200
1493/1493 - 102s - loss: 1.0981e-04 - val_loss: 1.2733e-04 - 102s/epoch - 68ms/step
Epoch 127/200
1493/1493 - 102s - loss: 1.0236e-04 - val_loss: 1.1799e-04 - 102s/epoch - 68ms/step
Epoch 128/200
1493/1493 - 102s - loss: 9.9491e-05 - val_loss: 9.8860e-05 - 102s/epoch - 68ms/step
Epoch 129/200
1493/1493 - 102s - loss: 9.7566e-05 - val_loss: 9.5999e-05 - 102s/epoch - 68ms/step
Epoch 130/200
1493/1493 - 102s - loss: 9.6435e-05 - val_loss: 8.5872e-05 - 102s/epoch - 68ms/step
Epoch 131/200
1493/1493 - 102s - loss: 9.7930e-05 - val_loss: 3.1511e-04 - 102s/epoch - 68ms/step
Epoch 132/200
1493/1493 - 102s - loss: 1.2323e-04 - val_loss: 9.3320e-05 - 102s/epoch - 68ms/step
Epoch 133/200
1493/1493 - 102s - loss: 9.6767e-05 - val_loss: 9.1701e-05 - 102s/epoch - 68ms/step
Epoch 134/200
1493/1493 - 102s - loss: 9.5115e-05 - val_loss: 9.4498e-05 - 102s/epoch - 68ms/step
Epoch 135/200
1493/1493 - 102s - loss: 9.5337e-05 - val_loss: 8.3219e-05 - 102s/epoch - 68ms/step
Epoch 136/200
1493/1493 - 102s - loss: 9.4517e-05 - val_loss: 1.1357e-04 - 102s/epoch - 68ms/step
Epoch 137/200
1493/1493 - 102s - loss: 1.0089e-04 - val_loss: 9.5854e-05 - 102s/epoch - 68ms/step
Epoch 138/200
1493/1493 - 102s - loss: 9.4361e-05 - val_loss: 1.0886e-04 - 102s/epoch - 68ms/step
Epoch 139/200
1493/1493 - 102s - loss: 9.7140e-05 - val_loss: 2.3659e-04 - 102s/epoch - 68ms/step
Epoch 140/200
1493/1493 - 102s - loss: 1.2770e-04 - val_loss: 8.3715e-05 - 102s/epoch - 68ms/step
Epoch 141/200
1493/1493 - 102s - loss: 9.5855e-05 - val_loss: 1.1770e-04 - 102s/epoch - 68ms/step
Epoch 142/200
1493/1493 - 102s - loss: 9.8007e-05 - val_loss: 9.4334e-05 - 102s/epoch - 68ms/step
Epoch 143/200
1493/1493 - 102s - loss: 9.2202e-05 - val_loss: 1.2565e-04 - 102s/epoch - 68ms/step
Epoch 144/200
1493/1493 - 102s - loss: 9.4558e-05 - val_loss: 1.0422e-04 - 102s/epoch - 68ms/step
Epoch 145/200
1493/1493 - 102s - loss: 9.2429e-05 - val_loss: 8.6617e-05 - 102s/epoch - 68ms/step
Epoch 146/200
1493/1493 - 102s - loss: 9.0338e-05 - val_loss: 8.2356e-05 - 102s/epoch - 68ms/step
Epoch 147/200
1493/1493 - 102s - loss: 8.9089e-05 - val_loss: 9.7825e-05 - 102s/epoch - 68ms/step
Epoch 148/200
1493/1493 - 102s - loss: 8.9254e-05 - val_loss: 1.3039e-04 - 102s/epoch - 68ms/step
Epoch 149/200
1493/1493 - 102s - loss: 8.9042e-05 - val_loss: 8.6982e-05 - 102s/epoch - 68ms/step
Epoch 150/200
1493/1493 - 102s - loss: 8.8133e-05 - val_loss: 8.4736e-05 - 102s/epoch - 68ms/step
Epoch 151/200
1493/1493 - 102s - loss: 8.7734e-05 - val_loss: 1.3471e-04 - 102s/epoch - 68ms/step
Epoch 152/200
1493/1493 - 102s - loss: 9.0332e-05 - val_loss: 1.1260e-04 - 102s/epoch - 68ms/step
Epoch 153/200
1493/1493 - 102s - loss: 8.8755e-05 - val_loss: 9.2597e-05 - 102s/epoch - 68ms/step
Epoch 154/200
1493/1493 - 102s - loss: 8.6480e-05 - val_loss: 8.5438e-05 - 102s/epoch - 68ms/step
Epoch 155/200
1493/1493 - 102s - loss: 8.4875e-05 - val_loss: 8.4369e-05 - 102s/epoch - 68ms/step
Epoch 156/200
1493/1493 - 102s - loss: 8.8972e-05 - val_loss: 8.9879e-05 - 102s/epoch - 68ms/step
Epoch 157/200
1493/1493 - 102s - loss: 8.5613e-05 - val_loss: 9.7397e-05 - 102s/epoch - 68ms/step
Epoch 158/200
1493/1493 - 102s - loss: 8.5734e-05 - val_loss: 1.0004e-04 - 102s/epoch - 68ms/step
Epoch 159/200
1493/1493 - 102s - loss: 8.4575e-05 - val_loss: 8.4331e-05 - 102s/epoch - 68ms/step
Epoch 160/200
1493/1493 - 102s - loss: 8.7959e-05 - val_loss: 1.9565e-04 - 102s/epoch - 68ms/step
Epoch 161/200
1493/1493 - 102s - loss: 1.4231e-04 - val_loss: 8.1012e-05 - 102s/epoch - 68ms/step
Epoch 162/200
1493/1493 - 102s - loss: 9.0976e-05 - val_loss: 8.0126e-05 - 102s/epoch - 68ms/step
Epoch 163/200
1493/1493 - 102s - loss: 8.7877e-05 - val_loss: 1.2833e-04 - 102s/epoch - 68ms/step
Epoch 164/200
1493/1493 - 102s - loss: 9.4075e-05 - val_loss: 8.2981e-05 - 102s/epoch - 68ms/step
Epoch 165/200
1493/1493 - 102s - loss: 9.9057e-05 - val_loss: 8.5284e-05 - 102s/epoch - 68ms/step
Epoch 166/200
1493/1493 - 102s - loss: 8.7276e-05 - val_loss: 8.5892e-05 - 102s/epoch - 68ms/step
Epoch 167/200
1493/1493 - 102s - loss: 8.6154e-05 - val_loss: 7.7961e-05 - 102s/epoch - 68ms/step
Epoch 168/200
1493/1493 - 102s - loss: 8.4133e-05 - val_loss: 8.0999e-05 - 102s/epoch - 68ms/step
Epoch 169/200
1493/1493 - 102s - loss: 8.4080e-05 - val_loss: 9.8860e-05 - 102s/epoch - 68ms/step
Epoch 170/200
1493/1493 - 102s - loss: 8.7048e-05 - val_loss: 4.9531e-04 - 102s/epoch - 68ms/step
Epoch 171/200
1493/1493 - 102s - loss: 1.4962e-04 - val_loss: 3.1992e-04 - 102s/epoch - 68ms/step
Epoch 172/200
1493/1493 - 102s - loss: 1.2053e-04 - val_loss: 8.6009e-05 - 102s/epoch - 68ms/step
Epoch 173/200
1493/1493 - 102s - loss: 9.0179e-05 - val_loss: 7.8231e-05 - 102s/epoch - 68ms/step
Epoch 174/200
1493/1493 - 102s - loss: 8.6220e-05 - val_loss: 8.0539e-05 - 102s/epoch - 68ms/step
Epoch 175/200
1493/1493 - 102s - loss: 8.6398e-05 - val_loss: 1.1458e-04 - 102s/epoch - 68ms/step
Epoch 176/200
1493/1493 - 102s - loss: 8.6185e-05 - val_loss: 8.6732e-05 - 102s/epoch - 68ms/step
Epoch 177/200
1493/1493 - 102s - loss: 8.4421e-05 - val_loss: 7.2634e-05 - 102s/epoch - 68ms/step
Epoch 178/200
1493/1493 - 102s - loss: 8.2360e-05 - val_loss: 7.9119e-05 - 102s/epoch - 68ms/step
Epoch 179/200
1493/1493 - 102s - loss: 8.1497e-05 - val_loss: 8.6346e-05 - 102s/epoch - 68ms/step
Epoch 180/200
1493/1493 - 102s - loss: 8.3756e-05 - val_loss: 8.6649e-05 - 102s/epoch - 68ms/step
Epoch 181/200
1493/1493 - 102s - loss: 8.1894e-05 - val_loss: 7.8427e-05 - 102s/epoch - 68ms/step
Epoch 182/200
1493/1493 - 102s - loss: 8.1303e-05 - val_loss: 1.0666e-04 - 102s/epoch - 68ms/step
Epoch 183/200
1493/1493 - 102s - loss: 8.2374e-05 - val_loss: 7.8778e-05 - 102s/epoch - 68ms/step
Epoch 184/200
1493/1493 - 102s - loss: 8.1142e-05 - val_loss: 8.3868e-05 - 102s/epoch - 68ms/step
Epoch 185/200
1493/1493 - 102s - loss: 7.9245e-05 - val_loss: 1.4646e-04 - 102s/epoch - 68ms/step
Epoch 186/200
1493/1493 - 102s - loss: 9.2100e-05 - val_loss: 7.4138e-05 - 102s/epoch - 68ms/step
Epoch 187/200
1493/1493 - 102s - loss: 7.9857e-05 - val_loss: 1.0057e-04 - 102s/epoch - 68ms/step
Epoch 188/200
1493/1493 - 102s - loss: 8.6765e-05 - val_loss: 8.5090e-05 - 102s/epoch - 68ms/step
Epoch 189/200
1493/1493 - 102s - loss: 8.0087e-05 - val_loss: 8.9957e-05 - 102s/epoch - 68ms/step
Epoch 190/200
1493/1493 - 102s - loss: 8.0909e-05 - val_loss: 8.7652e-05 - 102s/epoch - 68ms/step
Epoch 191/200
1493/1493 - 102s - loss: 8.0425e-05 - val_loss: 1.5896e-04 - 102s/epoch - 68ms/step
Epoch 192/200
1493/1493 - 102s - loss: 8.8438e-05 - val_loss: 8.3830e-05 - 102s/epoch - 68ms/step
Epoch 193/200
1493/1493 - 102s - loss: 8.2514e-05 - val_loss: 2.5868e-04 - 102s/epoch - 68ms/step
Epoch 194/200
1493/1493 - 102s - loss: 1.1651e-04 - val_loss: 5.8339e-04 - 102s/epoch - 68ms/step
Epoch 195/200
1493/1493 - 102s - loss: 1.2459e-04 - val_loss: 1.6666e-04 - 102s/epoch - 68ms/step
Epoch 196/200
1493/1493 - 102s - loss: 1.0199e-04 - val_loss: 6.8436e-05 - 102s/epoch - 68ms/step
Epoch 197/200
1493/1493 - 102s - loss: 8.3313e-05 - val_loss: 1.4381e-04 - 102s/epoch - 68ms/step
Epoch 198/200
1493/1493 - 102s - loss: 9.0478e-05 - val_loss: 1.0809e-04 - 102s/epoch - 68ms/step
Epoch 199/200
1493/1493 - 102s - loss: 8.3243e-05 - val_loss: 6.7843e-05 - 102s/epoch - 68ms/step
Epoch 200/200
1493/1493 - 102s - loss: 7.9159e-05 - val_loss: 7.7897e-05 - 102s/epoch - 68ms/step
COMPRESSED VECTOR SIZE: 1200
Loss in the autoencoder: 7.78970425017178e-05
  1/332 [..............................] - ETA: 25s  8/332 [..............................] - ETA: 2s  15/332 [>.............................] - ETA: 2s 22/332 [>.............................] - ETA: 2s 29/332 [=>............................] - ETA: 2s 36/332 [==>...........................] - ETA: 2s 43/332 [==>...........................] - ETA: 2s 50/332 [===>..........................] - ETA: 2s 57/332 [====>.........................] - ETA: 2s 64/332 [====>.........................] - ETA: 2s 71/332 [=====>........................] - ETA: 2s 78/332 [======>.......................] - ETA: 2s 85/332 [======>.......................] - ETA: 1s 92/332 [=======>......................] - ETA: 1s 99/332 [=======>......................] - ETA: 1s106/332 [========>.....................] - ETA: 1s113/332 [=========>....................] - ETA: 1s120/332 [=========>....................] - ETA: 1s127/332 [==========>...................] - ETA: 1s134/332 [===========>..................] - ETA: 1s141/332 [===========>..................] - ETA: 1s148/332 [============>.................] - ETA: 1s155/332 [=============>................] - ETA: 1s162/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s176/332 [==============>...............] - ETA: 1s183/332 [===============>..............] - ETA: 1s190/332 [================>.............] - ETA: 1s197/332 [================>.............] - ETA: 1s204/332 [=================>............] - ETA: 1s211/332 [==================>...........] - ETA: 0s218/332 [==================>...........] - ETA: 0s225/332 [===================>..........] - ETA: 0s232/332 [===================>..........] - ETA: 0s239/332 [====================>.........] - ETA: 0s246/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s260/332 [======================>.......] - ETA: 0s267/332 [=======================>......] - ETA: 0s274/332 [=======================>......] - ETA: 0s281/332 [========================>.....] - ETA: 0s288/332 [=========================>....] - ETA: 0s295/332 [=========================>....] - ETA: 0s302/332 [==========================>...] - ETA: 0s309/332 [==========================>...] - ETA: 0s316/332 [===========================>..] - ETA: 0s323/332 [============================>.] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 8ms/step
correlation 0.0008853701793873395
cosine 0.0006976771244587559
MAE: 0.004920613
RMSE: 0.008825927
r2: 0.9949479536348343
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 2780)              3516700   
                                                                 
 batch_normalization_3 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1200)              3337200   
                                                                 
 batch_normalization_4 (Batc  (None, 1200)             4800      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 1200)              0         
                                                                 
 dense_4 (Dense)             (None, 2780)              3338780   
                                                                 
 batch_normalization_5 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2780)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 13,734,904
Trainable params: 13,721,384
Non-trainable params: 13,520
_________________________________________________________________
Encoder
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_4 (InputLayer)        multiple                  0         
                                                                 
 dense_3 (Dense)             (None, 2780)              3516700   
                                                                 
 batch_normalization_3 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_3 (ReLU)              (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1200)              3337200   
                                                                 
=================================================================
Total params: 6,865,020
Trainable params: 6,859,460
Non-trainable params: 5,560
_________________________________________________________________
Decoder
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 1200)]            0         
                                                                 
 batch_normalization_4 (Batc  (None, 1200)             4800      
 hNormalization)                                                 
                                                                 
 re_lu_4 (ReLU)              (None, 1200)              0         
                                                                 
 dense_4 (Dense)             (None, 2780)              3338780   
                                                                 
 batch_normalization_5 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_5 (ReLU)              (None, 2780)              0         
                                                                 
 dense_5 (Dense)             (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 6,869,884
Trainable params: 6,861,924
Non-trainable params: 7,960
_________________________________________________________________
['2.2custom_n_b', 'mse', 64, 200, 0.0005, 0.95, 1200, 7.915879541542381e-05, 7.78970425017178e-05, 0.0008853701793873395, 0.0006976771244587559, 0.004920613020658493, 0.008825927041471004, 0.9949479536348343, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_2.2final_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 1264)]            0         
                                                                 
 dense_6 (Dense)             (None, 2780)              3516700   
                                                                 
 batch_normalization_6 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1137)              3161997   
                                                                 
 batch_normalization_7 (Batc  (None, 1137)             4548      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 1137)              0         
                                                                 
 dense_7 (Dense)             (None, 2780)              3163640   
                                                                 
 batch_normalization_8 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2780)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 13,384,309
Trainable params: 13,370,915
Non-trainable params: 13,394
_________________________________________________________________
Epoch 1/200
1493/1493 - 100s - loss: 0.0089 - val_loss: 0.0036 - 100s/epoch - 67ms/step
Epoch 2/200
1493/1493 - 99s - loss: 0.0027 - val_loss: 0.0032 - 99s/epoch - 66ms/step
Epoch 3/200
1493/1493 - 99s - loss: 0.0020 - val_loss: 0.0016 - 99s/epoch - 67ms/step
Epoch 4/200
1493/1493 - 99s - loss: 0.0016 - val_loss: 0.0029 - 99s/epoch - 66ms/step
Epoch 5/200
1493/1493 - 99s - loss: 0.0015 - val_loss: 0.0020 - 99s/epoch - 66ms/step
Epoch 6/200
1493/1493 - 99s - loss: 0.0014 - val_loss: 0.0013 - 99s/epoch - 66ms/step
Epoch 7/200
1493/1493 - 99s - loss: 0.0013 - val_loss: 9.1617e-04 - 99s/epoch - 66ms/step
Epoch 8/200
1493/1493 - 99s - loss: 0.0013 - val_loss: 0.0013 - 99s/epoch - 66ms/step
Epoch 9/200
1493/1493 - 99s - loss: 0.0010 - val_loss: 9.7348e-04 - 99s/epoch - 66ms/step
Epoch 10/200
1493/1493 - 99s - loss: 8.7419e-04 - val_loss: 7.0726e-04 - 99s/epoch - 66ms/step
Epoch 11/200
1493/1493 - 99s - loss: 8.3206e-04 - val_loss: 6.5863e-04 - 99s/epoch - 66ms/step
Epoch 12/200
1493/1493 - 99s - loss: 6.8810e-04 - val_loss: 0.0013 - 99s/epoch - 66ms/step
Epoch 13/200
1493/1493 - 99s - loss: 7.3292e-04 - val_loss: 0.0020 - 99s/epoch - 66ms/step
Epoch 14/200
1493/1493 - 99s - loss: 8.1517e-04 - val_loss: 4.7197e-04 - 99s/epoch - 66ms/step
Epoch 15/200
1493/1493 - 99s - loss: 5.9541e-04 - val_loss: 0.0015 - 99s/epoch - 66ms/step
Epoch 16/200
1493/1493 - 99s - loss: 6.1114e-04 - val_loss: 4.9667e-04 - 99s/epoch - 67ms/step
Epoch 17/200
1493/1493 - 99s - loss: 4.9615e-04 - val_loss: 6.0208e-04 - 99s/epoch - 66ms/step
Epoch 18/200
1493/1493 - 99s - loss: 4.8019e-04 - val_loss: 0.0020 - 99s/epoch - 66ms/step
Epoch 19/200
1493/1493 - 99s - loss: 5.7055e-04 - val_loss: 0.0012 - 99s/epoch - 66ms/step
Epoch 20/200
1493/1493 - 99s - loss: 4.6821e-04 - val_loss: 3.8458e-04 - 99s/epoch - 66ms/step
Epoch 21/200
1493/1493 - 99s - loss: 3.9058e-04 - val_loss: 5.1905e-04 - 99s/epoch - 66ms/step
Epoch 22/200
1493/1493 - 99s - loss: 3.8591e-04 - val_loss: 4.1132e-04 - 99s/epoch - 66ms/step
Epoch 23/200
1493/1493 - 99s - loss: 3.6138e-04 - val_loss: 7.5445e-04 - 99s/epoch - 67ms/step
Epoch 24/200
1493/1493 - 99s - loss: 3.7094e-04 - val_loss: 3.2323e-04 - 99s/epoch - 66ms/step
Epoch 25/200
1493/1493 - 99s - loss: 3.2787e-04 - val_loss: 3.0042e-04 - 99s/epoch - 66ms/step
Epoch 26/200
1493/1493 - 99s - loss: 3.1533e-04 - val_loss: 3.4340e-04 - 99s/epoch - 66ms/step
Epoch 27/200
1493/1493 - 99s - loss: 3.0000e-04 - val_loss: 3.0202e-04 - 99s/epoch - 66ms/step
Epoch 28/200
1493/1493 - 99s - loss: 2.8621e-04 - val_loss: 5.4523e-04 - 99s/epoch - 66ms/step
Epoch 29/200
1493/1493 - 99s - loss: 2.8486e-04 - val_loss: 3.2611e-04 - 99s/epoch - 66ms/step
Epoch 30/200
1493/1493 - 99s - loss: 2.8194e-04 - val_loss: 3.3324e-04 - 99s/epoch - 66ms/step
Epoch 31/200
1493/1493 - 99s - loss: 2.6091e-04 - val_loss: 4.0059e-04 - 99s/epoch - 66ms/step
Epoch 32/200
1493/1493 - 99s - loss: 2.8802e-04 - val_loss: 3.5952e-04 - 99s/epoch - 66ms/step
Epoch 33/200
1493/1493 - 99s - loss: 2.6324e-04 - val_loss: 2.4946e-04 - 99s/epoch - 66ms/step
Epoch 34/200
1493/1493 - 99s - loss: 2.4322e-04 - val_loss: 2.1791e-04 - 99s/epoch - 66ms/step
Epoch 35/200
1493/1493 - 99s - loss: 2.3499e-04 - val_loss: 4.9355e-04 - 99s/epoch - 66ms/step
Epoch 36/200
1493/1493 - 99s - loss: 2.5435e-04 - val_loss: 2.8485e-04 - 99s/epoch - 66ms/step
Epoch 37/200
1493/1493 - 99s - loss: 2.3132e-04 - val_loss: 3.1836e-04 - 99s/epoch - 66ms/step
Epoch 38/200
1493/1493 - 99s - loss: 2.3691e-04 - val_loss: 2.4943e-04 - 99s/epoch - 66ms/step
Epoch 39/200
1493/1493 - 99s - loss: 2.1758e-04 - val_loss: 2.1488e-04 - 99s/epoch - 66ms/step
Epoch 40/200
1493/1493 - 99s - loss: 2.0762e-04 - val_loss: 2.7389e-04 - 99s/epoch - 66ms/step
Epoch 41/200
1493/1493 - 99s - loss: 2.1014e-04 - val_loss: 1.9172e-04 - 99s/epoch - 66ms/step
Epoch 42/200
1493/1493 - 99s - loss: 2.0256e-04 - val_loss: 2.0184e-04 - 99s/epoch - 66ms/step
Epoch 43/200
1493/1493 - 99s - loss: 1.9994e-04 - val_loss: 2.9991e-04 - 99s/epoch - 66ms/step
Epoch 44/200
1493/1493 - 99s - loss: 2.1007e-04 - val_loss: 9.2112e-04 - 99s/epoch - 66ms/step
Epoch 45/200
1493/1493 - 99s - loss: 2.5525e-04 - val_loss: 1.6451e-04 - 99s/epoch - 66ms/step
Epoch 46/200
1493/1493 - 99s - loss: 1.9151e-04 - val_loss: 1.8037e-04 - 99s/epoch - 66ms/step
Epoch 47/200
1493/1493 - 99s - loss: 1.9741e-04 - val_loss: 2.5527e-04 - 99s/epoch - 66ms/step
Epoch 48/200
1493/1493 - 99s - loss: 1.8911e-04 - val_loss: 2.1932e-04 - 99s/epoch - 66ms/step
Epoch 49/200
1493/1493 - 99s - loss: 1.8632e-04 - val_loss: 4.0277e-04 - 99s/epoch - 66ms/step
Epoch 50/200
1493/1493 - 99s - loss: 2.0060e-04 - val_loss: 4.4067e-04 - 99s/epoch - 66ms/step
Epoch 51/200
1493/1493 - 99s - loss: 1.9906e-04 - val_loss: 1.9434e-04 - 99s/epoch - 66ms/step
Epoch 52/200
1493/1493 - 99s - loss: 1.7655e-04 - val_loss: 1.9688e-04 - 99s/epoch - 66ms/step
Epoch 53/200
1493/1493 - 99s - loss: 1.7478e-04 - val_loss: 1.5980e-04 - 99s/epoch - 66ms/step
Epoch 54/200
1493/1493 - 99s - loss: 1.7029e-04 - val_loss: 2.0093e-04 - 99s/epoch - 66ms/step
Epoch 55/200
1493/1493 - 99s - loss: 1.6471e-04 - val_loss: 1.5660e-04 - 99s/epoch - 66ms/step
Epoch 56/200
1493/1493 - 99s - loss: 1.6476e-04 - val_loss: 1.9984e-04 - 99s/epoch - 66ms/step
Epoch 57/200
1493/1493 - 99s - loss: 1.6521e-04 - val_loss: 1.6182e-04 - 99s/epoch - 66ms/step
Epoch 58/200
1493/1493 - 99s - loss: 1.5895e-04 - val_loss: 1.4838e-04 - 99s/epoch - 66ms/step
Epoch 59/200
1493/1493 - 99s - loss: 1.5774e-04 - val_loss: 1.5878e-04 - 99s/epoch - 66ms/step
Epoch 60/200
1493/1493 - 99s - loss: 1.5753e-04 - val_loss: 3.7948e-04 - 99s/epoch - 66ms/step
Epoch 61/200
1493/1493 - 99s - loss: 1.9799e-04 - val_loss: 1.6131e-04 - 99s/epoch - 66ms/step
Epoch 62/200
1493/1493 - 99s - loss: 1.5376e-04 - val_loss: 1.8248e-04 - 99s/epoch - 66ms/step
Epoch 63/200
1493/1493 - 99s - loss: 1.5267e-04 - val_loss: 2.0033e-04 - 99s/epoch - 66ms/step
Epoch 64/200
1493/1493 - 99s - loss: 1.5155e-04 - val_loss: 9.0832e-04 - 99s/epoch - 66ms/step
Epoch 65/200
1493/1493 - 99s - loss: 2.0818e-04 - val_loss: 4.3081e-04 - 99s/epoch - 66ms/step
Epoch 66/200
1493/1493 - 99s - loss: 1.7747e-04 - val_loss: 1.6473e-04 - 99s/epoch - 66ms/step
Epoch 67/200
1493/1493 - 99s - loss: 1.5010e-04 - val_loss: 1.2966e-04 - 99s/epoch - 66ms/step
Epoch 68/200
1493/1493 - 99s - loss: 1.4786e-04 - val_loss: 5.5207e-04 - 99s/epoch - 66ms/step
Epoch 69/200
1493/1493 - 99s - loss: 2.4206e-04 - val_loss: 1.6453e-04 - 99s/epoch - 66ms/step
Epoch 70/200
1493/1493 - 99s - loss: 1.5285e-04 - val_loss: 2.9169e-04 - 99s/epoch - 66ms/step
Epoch 71/200
1493/1493 - 99s - loss: 1.6981e-04 - val_loss: 1.4117e-04 - 99s/epoch - 66ms/step
Epoch 72/200
1493/1493 - 99s - loss: 1.4800e-04 - val_loss: 1.8979e-04 - 99s/epoch - 66ms/step
Epoch 73/200
1493/1493 - 99s - loss: 1.4931e-04 - val_loss: 1.4367e-04 - 99s/epoch - 66ms/step
Epoch 74/200
1493/1493 - 99s - loss: 1.4398e-04 - val_loss: 1.4686e-04 - 99s/epoch - 66ms/step
Epoch 75/200
1493/1493 - 99s - loss: 1.3769e-04 - val_loss: 1.1900e-04 - 99s/epoch - 66ms/step
Epoch 76/200
1493/1493 - 99s - loss: 1.3622e-04 - val_loss: 1.2755e-04 - 99s/epoch - 66ms/step
Epoch 77/200
1493/1493 - 99s - loss: 1.3194e-04 - val_loss: 4.0182e-04 - 99s/epoch - 66ms/step
Epoch 78/200
1493/1493 - 99s - loss: 1.4960e-04 - val_loss: 1.5152e-04 - 99s/epoch - 66ms/step
Epoch 79/200
1493/1493 - 99s - loss: 1.3343e-04 - val_loss: 1.2885e-04 - 99s/epoch - 66ms/step
Epoch 80/200
1493/1493 - 99s - loss: 1.3036e-04 - val_loss: 1.4126e-04 - 99s/epoch - 66ms/step
Epoch 81/200
1493/1493 - 99s - loss: 1.3019e-04 - val_loss: 3.3403e-04 - 99s/epoch - 67ms/step
Epoch 82/200
1493/1493 - 99s - loss: 1.3199e-04 - val_loss: 1.2250e-04 - 99s/epoch - 67ms/step
Epoch 83/200
1493/1493 - 99s - loss: 1.2501e-04 - val_loss: 1.3936e-04 - 99s/epoch - 66ms/step
Epoch 84/200
1493/1493 - 99s - loss: 1.2506e-04 - val_loss: 1.4536e-04 - 99s/epoch - 67ms/step
Epoch 85/200
1493/1493 - 99s - loss: 1.2469e-04 - val_loss: 1.3171e-04 - 99s/epoch - 66ms/step
Epoch 86/200
1493/1493 - 99s - loss: 1.2335e-04 - val_loss: 1.4032e-04 - 99s/epoch - 66ms/step
Epoch 87/200
1493/1493 - 99s - loss: 1.2227e-04 - val_loss: 1.2798e-04 - 99s/epoch - 66ms/step
Epoch 88/200
1493/1493 - 99s - loss: 1.2232e-04 - val_loss: 1.4687e-04 - 99s/epoch - 66ms/step
Epoch 89/200
1493/1493 - 99s - loss: 1.2499e-04 - val_loss: 1.2424e-04 - 99s/epoch - 66ms/step
Epoch 90/200
1493/1493 - 99s - loss: 1.1904e-04 - val_loss: 1.4106e-04 - 99s/epoch - 66ms/step
Epoch 91/200
1493/1493 - 99s - loss: 1.1808e-04 - val_loss: 1.3487e-04 - 99s/epoch - 66ms/step
Epoch 92/200
1493/1493 - 99s - loss: 1.1997e-04 - val_loss: 3.7608e-04 - 99s/epoch - 66ms/step
Epoch 93/200
1493/1493 - 99s - loss: 1.5771e-04 - val_loss: 2.7762e-04 - 99s/epoch - 67ms/step
Epoch 94/200
1493/1493 - 99s - loss: 1.3935e-04 - val_loss: 1.0373e-04 - 99s/epoch - 66ms/step
Epoch 95/200
1493/1493 - 99s - loss: 1.2023e-04 - val_loss: 2.4319e-04 - 99s/epoch - 66ms/step
Epoch 96/200
1493/1493 - 99s - loss: 1.3749e-04 - val_loss: 1.4248e-04 - 99s/epoch - 66ms/step
Epoch 97/200
1493/1493 - 99s - loss: 1.2069e-04 - val_loss: 1.2174e-04 - 99s/epoch - 67ms/step
Epoch 98/200
1493/1493 - 99s - loss: 1.1724e-04 - val_loss: 3.5136e-04 - 99s/epoch - 66ms/step
Epoch 99/200
1493/1493 - 99s - loss: 1.4608e-04 - val_loss: 1.0703e-04 - 99s/epoch - 67ms/step
Epoch 100/200
1493/1493 - 99s - loss: 1.2064e-04 - val_loss: 1.0787e-04 - 99s/epoch - 67ms/step
Epoch 101/200
1493/1493 - 99s - loss: 1.1440e-04 - val_loss: 1.7739e-04 - 99s/epoch - 67ms/step
Epoch 102/200
1493/1493 - 99s - loss: 1.2617e-04 - val_loss: 2.8390e-04 - 99s/epoch - 67ms/step
Epoch 103/200
1493/1493 - 99s - loss: 1.3152e-04 - val_loss: 1.0885e-04 - 99s/epoch - 66ms/step
Epoch 104/200
1493/1493 - 99s - loss: 1.1289e-04 - val_loss: 6.5239e-04 - 99s/epoch - 66ms/step
Epoch 105/200
1493/1493 - 99s - loss: 1.3952e-04 - val_loss: 1.0037e-04 - 99s/epoch - 66ms/step
Epoch 106/200
1493/1493 - 99s - loss: 1.1204e-04 - val_loss: 1.1488e-04 - 99s/epoch - 66ms/step
Epoch 107/200
1493/1493 - 99s - loss: 1.1095e-04 - val_loss: 1.1171e-04 - 99s/epoch - 66ms/step
Epoch 108/200
1493/1493 - 99s - loss: 1.1356e-04 - val_loss: 1.1555e-04 - 99s/epoch - 66ms/step
Epoch 109/200
1493/1493 - 99s - loss: 1.0903e-04 - val_loss: 1.2725e-04 - 99s/epoch - 66ms/step
Epoch 110/200
1493/1493 - 99s - loss: 1.0802e-04 - val_loss: 1.1598e-04 - 99s/epoch - 66ms/step
Epoch 111/200
1493/1493 - 99s - loss: 1.0698e-04 - val_loss: 1.0193e-04 - 99s/epoch - 66ms/step
Epoch 112/200
1493/1493 - 99s - loss: 1.0562e-04 - val_loss: 1.0654e-04 - 99s/epoch - 66ms/step
Epoch 113/200
1493/1493 - 99s - loss: 1.0510e-04 - val_loss: 3.8419e-04 - 99s/epoch - 66ms/step
Epoch 114/200
1493/1493 - 99s - loss: 1.4275e-04 - val_loss: 9.7080e-05 - 99s/epoch - 66ms/step
Epoch 115/200
1493/1493 - 99s - loss: 1.1202e-04 - val_loss: 1.8651e-04 - 99s/epoch - 66ms/step
Epoch 116/200
1493/1493 - 99s - loss: 1.0859e-04 - val_loss: 1.1548e-04 - 99s/epoch - 66ms/step
Epoch 117/200
1493/1493 - 99s - loss: 1.0407e-04 - val_loss: 2.1098e-04 - 99s/epoch - 66ms/step
Epoch 118/200
1493/1493 - 99s - loss: 1.1621e-04 - val_loss: 1.2305e-04 - 99s/epoch - 66ms/step
Epoch 119/200
1493/1493 - 99s - loss: 1.0725e-04 - val_loss: 1.0327e-04 - 99s/epoch - 66ms/step
Epoch 120/200
1493/1493 - 99s - loss: 1.0301e-04 - val_loss: 9.9643e-05 - 99s/epoch - 66ms/step
Epoch 121/200
1493/1493 - 99s - loss: 1.0256e-04 - val_loss: 1.7022e-04 - 99s/epoch - 66ms/step
Epoch 122/200
1493/1493 - 99s - loss: 1.0986e-04 - val_loss: 8.8769e-05 - 99s/epoch - 66ms/step
Epoch 123/200
1493/1493 - 99s - loss: 1.0012e-04 - val_loss: 1.0633e-04 - 99s/epoch - 66ms/step
Epoch 124/200
1493/1493 - 99s - loss: 1.0290e-04 - val_loss: 4.5545e-04 - 99s/epoch - 66ms/step
Epoch 125/200
1493/1493 - 99s - loss: 1.5646e-04 - val_loss: 5.2389e-04 - 99s/epoch - 66ms/step
Epoch 126/200
1493/1493 - 99s - loss: 1.4533e-04 - val_loss: 1.1118e-04 - 99s/epoch - 66ms/step
Epoch 127/200
1493/1493 - 99s - loss: 1.0795e-04 - val_loss: 1.7594e-04 - 99s/epoch - 66ms/step
Epoch 128/200
1493/1493 - 99s - loss: 1.1130e-04 - val_loss: 9.0903e-05 - 99s/epoch - 66ms/step
Epoch 129/200
1493/1493 - 99s - loss: 1.0158e-04 - val_loss: 1.0039e-04 - 99s/epoch - 66ms/step
Epoch 130/200
1493/1493 - 99s - loss: 9.9996e-05 - val_loss: 8.9705e-05 - 99s/epoch - 66ms/step
Epoch 131/200
1493/1493 - 99s - loss: 1.0091e-04 - val_loss: 3.0985e-04 - 99s/epoch - 66ms/step
Epoch 132/200
1493/1493 - 99s - loss: 1.2851e-04 - val_loss: 9.6417e-05 - 99s/epoch - 66ms/step
Epoch 133/200
1493/1493 - 99s - loss: 1.0010e-04 - val_loss: 1.0674e-04 - 99s/epoch - 66ms/step
Epoch 134/200
1493/1493 - 99s - loss: 9.8754e-05 - val_loss: 1.0410e-04 - 99s/epoch - 66ms/step
Epoch 135/200
1493/1493 - 99s - loss: 9.7285e-05 - val_loss: 9.1889e-05 - 99s/epoch - 66ms/step
Epoch 136/200
1493/1493 - 99s - loss: 9.6514e-05 - val_loss: 9.5349e-05 - 99s/epoch - 66ms/step
Epoch 137/200
1493/1493 - 99s - loss: 9.9825e-05 - val_loss: 1.3295e-04 - 99s/epoch - 66ms/step
Epoch 138/200
1493/1493 - 99s - loss: 9.9567e-05 - val_loss: 1.1208e-04 - 99s/epoch - 66ms/step
Epoch 139/200
1493/1493 - 99s - loss: 9.8557e-05 - val_loss: 1.4323e-04 - 99s/epoch - 66ms/step
Epoch 140/200
1493/1493 - 99s - loss: 1.0742e-04 - val_loss: 8.4574e-05 - 99s/epoch - 66ms/step
Epoch 141/200
1493/1493 - 99s - loss: 9.4594e-05 - val_loss: 1.1634e-04 - 99s/epoch - 66ms/step
Epoch 142/200
1493/1493 - 99s - loss: 9.6999e-05 - val_loss: 1.0145e-04 - 99s/epoch - 66ms/step
Epoch 143/200
1493/1493 - 99s - loss: 9.3186e-05 - val_loss: 1.2676e-04 - 99s/epoch - 66ms/step
Epoch 144/200
1493/1493 - 99s - loss: 9.4803e-05 - val_loss: 9.3629e-05 - 99s/epoch - 66ms/step
Epoch 145/200
1493/1493 - 99s - loss: 9.1944e-05 - val_loss: 8.6626e-05 - 99s/epoch - 66ms/step
Epoch 146/200
1493/1493 - 99s - loss: 9.1373e-05 - val_loss: 8.5029e-05 - 99s/epoch - 66ms/step
Epoch 147/200
1493/1493 - 99s - loss: 9.0275e-05 - val_loss: 1.1997e-04 - 99s/epoch - 66ms/step
Epoch 148/200
1493/1493 - 99s - loss: 9.1987e-05 - val_loss: 1.5788e-04 - 99s/epoch - 66ms/step
Epoch 149/200
1493/1493 - 99s - loss: 9.0268e-05 - val_loss: 9.3500e-05 - 99s/epoch - 66ms/step
Epoch 150/200
1493/1493 - 99s - loss: 8.9563e-05 - val_loss: 8.9353e-05 - 99s/epoch - 66ms/step
Epoch 151/200
1493/1493 - 99s - loss: 8.9915e-05 - val_loss: 1.1030e-04 - 99s/epoch - 66ms/step
Epoch 152/200
1493/1493 - 99s - loss: 8.9327e-05 - val_loss: 1.1316e-04 - 99s/epoch - 66ms/step
Epoch 153/200
1493/1493 - 99s - loss: 8.9728e-05 - val_loss: 1.0646e-04 - 99s/epoch - 66ms/step
Epoch 154/200
1493/1493 - 99s - loss: 8.7748e-05 - val_loss: 9.3889e-05 - 99s/epoch - 66ms/step
Epoch 155/200
1493/1493 - 99s - loss: 8.6363e-05 - val_loss: 9.2046e-05 - 99s/epoch - 66ms/step
Epoch 156/200
1493/1493 - 99s - loss: 9.2836e-05 - val_loss: 9.4924e-05 - 99s/epoch - 66ms/step
Epoch 157/200
1493/1493 - 99s - loss: 8.6757e-05 - val_loss: 1.0164e-04 - 99s/epoch - 66ms/step
Epoch 158/200
1493/1493 - 99s - loss: 8.8036e-05 - val_loss: 9.9463e-05 - 99s/epoch - 66ms/step
Epoch 159/200
1493/1493 - 99s - loss: 8.6412e-05 - val_loss: 8.5962e-05 - 99s/epoch - 66ms/step
Epoch 160/200
1493/1493 - 99s - loss: 8.6962e-05 - val_loss: 1.3251e-04 - 99s/epoch - 66ms/step
Epoch 161/200
1493/1493 - 99s - loss: 1.0354e-04 - val_loss: 8.7056e-05 - 99s/epoch - 66ms/step
Epoch 162/200
1493/1493 - 99s - loss: 8.6780e-05 - val_loss: 8.3500e-05 - 99s/epoch - 67ms/step
Epoch 163/200
1493/1493 - 99s - loss: 8.6449e-05 - val_loss: 1.0384e-04 - 99s/epoch - 66ms/step
Epoch 164/200
1493/1493 - 99s - loss: 8.9553e-05 - val_loss: 8.6185e-05 - 99s/epoch - 66ms/step
Epoch 165/200
1493/1493 - 99s - loss: 9.3066e-05 - val_loss: 9.5721e-05 - 99s/epoch - 66ms/step
Epoch 166/200
1493/1493 - 99s - loss: 8.6225e-05 - val_loss: 9.0538e-05 - 99s/epoch - 66ms/step
Epoch 167/200
1493/1493 - 99s - loss: 8.5715e-05 - val_loss: 7.9211e-05 - 99s/epoch - 66ms/step
Epoch 168/200
1493/1493 - 99s - loss: 8.4453e-05 - val_loss: 8.3471e-05 - 99s/epoch - 66ms/step
Epoch 169/200
1493/1493 - 99s - loss: 8.4764e-05 - val_loss: 1.1268e-04 - 99s/epoch - 66ms/step
Epoch 170/200
1493/1493 - 99s - loss: 8.6537e-05 - val_loss: 3.7494e-04 - 99s/epoch - 66ms/step
Epoch 171/200
1493/1493 - 99s - loss: 1.2691e-04 - val_loss: 4.7737e-04 - 99s/epoch - 66ms/step
Epoch 172/200
1493/1493 - 99s - loss: 1.4306e-04 - val_loss: 1.0112e-04 - 99s/epoch - 66ms/step
Epoch 173/200
1493/1493 - 99s - loss: 9.5071e-05 - val_loss: 8.3405e-05 - 99s/epoch - 67ms/step
Epoch 174/200
1493/1493 - 99s - loss: 8.8533e-05 - val_loss: 9.3984e-05 - 99s/epoch - 67ms/step
Epoch 175/200
1493/1493 - 99s - loss: 8.8484e-05 - val_loss: 1.0532e-04 - 99s/epoch - 67ms/step
Epoch 176/200
1493/1493 - 99s - loss: 8.6886e-05 - val_loss: 8.4049e-05 - 99s/epoch - 66ms/step
Epoch 177/200
1493/1493 - 99s - loss: 8.5817e-05 - val_loss: 7.2795e-05 - 99s/epoch - 66ms/step
Epoch 178/200
1493/1493 - 99s - loss: 8.3703e-05 - val_loss: 8.0869e-05 - 99s/epoch - 66ms/step
Epoch 179/200
1493/1493 - 99s - loss: 8.2587e-05 - val_loss: 9.4005e-05 - 99s/epoch - 66ms/step
Epoch 180/200
1493/1493 - 99s - loss: 8.3308e-05 - val_loss: 8.7188e-05 - 99s/epoch - 66ms/step
Epoch 181/200
1493/1493 - 99s - loss: 8.2498e-05 - val_loss: 8.3306e-05 - 99s/epoch - 66ms/step
Epoch 182/200
1493/1493 - 99s - loss: 8.2380e-05 - val_loss: 1.2002e-04 - 99s/epoch - 66ms/step
Epoch 183/200
1493/1493 - 99s - loss: 8.3625e-05 - val_loss: 8.1382e-05 - 99s/epoch - 66ms/step
Epoch 184/200
1493/1493 - 99s - loss: 8.2192e-05 - val_loss: 8.1206e-05 - 99s/epoch - 66ms/step
Epoch 185/200
1493/1493 - 99s - loss: 8.5147e-05 - val_loss: 3.2457e-04 - 99s/epoch - 66ms/step
Epoch 186/200
1493/1493 - 99s - loss: 1.3907e-04 - val_loss: 8.0795e-05 - 99s/epoch - 66ms/step
Epoch 187/200
1493/1493 - 99s - loss: 8.9157e-05 - val_loss: 9.8719e-05 - 99s/epoch - 66ms/step
Epoch 188/200
1493/1493 - 99s - loss: 9.4249e-05 - val_loss: 9.0638e-05 - 99s/epoch - 66ms/step
Epoch 189/200
1493/1493 - 99s - loss: 8.4834e-05 - val_loss: 9.6745e-05 - 99s/epoch - 66ms/step
Epoch 190/200
1493/1493 - 99s - loss: 8.5504e-05 - val_loss: 1.7985e-04 - 99s/epoch - 66ms/step
Epoch 191/200
1493/1493 - 99s - loss: 9.6744e-05 - val_loss: 1.3026e-04 - 99s/epoch - 66ms/step
Epoch 192/200
1493/1493 - 99s - loss: 8.5036e-05 - val_loss: 9.2856e-05 - 99s/epoch - 66ms/step
Epoch 193/200
1493/1493 - 99s - loss: 8.3835e-05 - val_loss: 2.3681e-04 - 99s/epoch - 66ms/step
Epoch 194/200
1493/1493 - 99s - loss: 1.0867e-04 - val_loss: 6.3475e-04 - 99s/epoch - 66ms/step
Epoch 195/200
1493/1493 - 99s - loss: 1.7505e-04 - val_loss: 4.9006e-04 - 99s/epoch - 66ms/step
Epoch 196/200
1493/1493 - 99s - loss: 1.6862e-04 - val_loss: 7.9331e-05 - 99s/epoch - 66ms/step
Epoch 197/200
1493/1493 - 99s - loss: 9.8324e-05 - val_loss: 1.2060e-04 - 99s/epoch - 66ms/step
Epoch 198/200
1493/1493 - 99s - loss: 1.0193e-04 - val_loss: 1.4743e-04 - 99s/epoch - 66ms/step
Epoch 199/200
1493/1493 - 99s - loss: 9.4510e-05 - val_loss: 7.3384e-05 - 99s/epoch - 66ms/step
Epoch 200/200
1493/1493 - 99s - loss: 8.6619e-05 - val_loss: 8.5202e-05 - 99s/epoch - 66ms/step
COMPRESSED VECTOR SIZE: 1137
Loss in the autoencoder: 8.520190021954477e-05
  1/332 [..............................] - ETA: 24s  7/332 [..............................] - ETA: 2s  14/332 [>.............................] - ETA: 2s 21/332 [>.............................] - ETA: 2s 28/332 [=>............................] - ETA: 2s 35/332 [==>...........................] - ETA: 2s 42/332 [==>...........................] - ETA: 2s 49/332 [===>..........................] - ETA: 2s 56/332 [====>.........................] - ETA: 2s 63/332 [====>.........................] - ETA: 2s 70/332 [=====>........................] - ETA: 2s 77/332 [=====>........................] - ETA: 2s 84/332 [======>.......................] - ETA: 2s 91/332 [=======>......................] - ETA: 1s 98/332 [=======>......................] - ETA: 1s105/332 [========>.....................] - ETA: 1s112/332 [=========>....................] - ETA: 1s119/332 [=========>....................] - ETA: 1s126/332 [==========>...................] - ETA: 1s133/332 [===========>..................] - ETA: 1s140/332 [===========>..................] - ETA: 1s147/332 [============>.................] - ETA: 1s154/332 [============>.................] - ETA: 1s161/332 [=============>................] - ETA: 1s168/332 [==============>...............] - ETA: 1s175/332 [==============>...............] - ETA: 1s182/332 [===============>..............] - ETA: 1s189/332 [================>.............] - ETA: 1s196/332 [================>.............] - ETA: 1s203/332 [=================>............] - ETA: 1s210/332 [=================>............] - ETA: 0s217/332 [==================>...........] - ETA: 0s224/332 [===================>..........] - ETA: 0s231/332 [===================>..........] - ETA: 0s238/332 [====================>.........] - ETA: 0s245/332 [=====================>........] - ETA: 0s252/332 [=====================>........] - ETA: 0s259/332 [======================>.......] - ETA: 0s266/332 [=======================>......] - ETA: 0s273/332 [=======================>......] - ETA: 0s280/332 [========================>.....] - ETA: 0s287/332 [========================>.....] - ETA: 0s294/332 [=========================>....] - ETA: 0s301/332 [==========================>...] - ETA: 0s308/332 [==========================>...] - ETA: 0s315/332 [===========================>..] - ETA: 0s322/332 [============================>.] - ETA: 0s329/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 8ms/step
correlation 0.0009609337375157741
cosine 0.0007572043820502306
MAE: 0.005098861
RMSE: 0.009230484
r2: 0.9944735264702135
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 2780)              3516700   
                                                                 
 batch_normalization_6 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1137)              3161997   
                                                                 
 batch_normalization_7 (Batc  (None, 1137)             4548      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 1137)              0         
                                                                 
 dense_7 (Dense)             (None, 2780)              3163640   
                                                                 
 batch_normalization_8 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2780)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 13,384,309
Trainable params: 13,370,915
Non-trainable params: 13,394
_________________________________________________________________
Encoder
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 1264)]            0         
                                                                 
 input_7 (InputLayer)        multiple                  0         
                                                                 
 dense_6 (Dense)             (None, 2780)              3516700   
                                                                 
 batch_normalization_6 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_6 (ReLU)              (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1137)              3161997   
                                                                 
=================================================================
Total params: 6,689,817
Trainable params: 6,684,257
Non-trainable params: 5,560
_________________________________________________________________
Decoder
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 1137)]            0         
                                                                 
 batch_normalization_7 (Batc  (None, 1137)             4548      
 hNormalization)                                                 
                                                                 
 re_lu_7 (ReLU)              (None, 1137)              0         
                                                                 
 dense_7 (Dense)             (None, 2780)              3163640   
                                                                 
 batch_normalization_8 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_8 (ReLU)              (None, 2780)              0         
                                                                 
 dense_8 (Dense)             (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 6,694,492
Trainable params: 6,686,658
Non-trainable params: 7,834
_________________________________________________________________
['2.2custom_n_b', 'mse', 64, 200, 0.0005, 0.9, 1137, 8.661876199766994e-05, 8.520190021954477e-05, 0.0009609337375157741, 0.0007572043820502306, 0.005098861176520586, 0.009230484254658222, 0.9944735264702135, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_2.2final_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_9 (Dense)             (None, 2780)              3516700   
                                                                 
 batch_normalization_9 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1074)              2986794   
                                                                 
 batch_normalization_10 (Bat  (None, 1074)             4296      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 1074)              0         
                                                                 
 dense_10 (Dense)            (None, 2780)              2988500   
                                                                 
 batch_normalization_11 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2780)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 13,033,714
Trainable params: 13,020,446
Non-trainable params: 13,268
_________________________________________________________________
Epoch 1/200
1493/1493 - 100s - loss: 0.0090 - val_loss: 0.0038 - 100s/epoch - 67ms/step
Epoch 2/200
1493/1493 - 100s - loss: 0.0027 - val_loss: 0.0032 - 100s/epoch - 67ms/step
Epoch 3/200
1493/1493 - 100s - loss: 0.0020 - val_loss: 0.0016 - 100s/epoch - 67ms/step
Epoch 4/200
1493/1493 - 100s - loss: 0.0016 - val_loss: 0.0036 - 100s/epoch - 67ms/step
Epoch 5/200
1493/1493 - 100s - loss: 0.0016 - val_loss: 0.0013 - 100s/epoch - 67ms/step
Epoch 6/200
1493/1493 - 100s - loss: 0.0014 - val_loss: 0.0014 - 100s/epoch - 67ms/step
Epoch 7/200
1493/1493 - 100s - loss: 0.0012 - val_loss: 0.0014 - 100s/epoch - 67ms/step
Epoch 8/200
1493/1493 - 100s - loss: 0.0011 - val_loss: 0.0018 - 100s/epoch - 67ms/step
Epoch 9/200
1493/1493 - 100s - loss: 0.0010 - val_loss: 0.0015 - 100s/epoch - 67ms/step
Epoch 10/200
1493/1493 - 100s - loss: 0.0010 - val_loss: 8.0000e-04 - 100s/epoch - 67ms/step
Epoch 11/200
1493/1493 - 100s - loss: 8.7306e-04 - val_loss: 8.0687e-04 - 100s/epoch - 67ms/step
Epoch 12/200
1493/1493 - 100s - loss: 7.1026e-04 - val_loss: 7.5497e-04 - 100s/epoch - 67ms/step
Epoch 13/200
1493/1493 - 100s - loss: 6.7514e-04 - val_loss: 9.5512e-04 - 100s/epoch - 67ms/step
Epoch 14/200
1493/1493 - 100s - loss: 6.5161e-04 - val_loss: 4.9866e-04 - 100s/epoch - 67ms/step
Epoch 15/200
1493/1493 - 100s - loss: 5.9504e-04 - val_loss: 0.0012 - 100s/epoch - 67ms/step
Epoch 16/200
1493/1493 - 99s - loss: 6.3072e-04 - val_loss: 4.5630e-04 - 99s/epoch - 67ms/step
Epoch 17/200
1493/1493 - 100s - loss: 4.9376e-04 - val_loss: 6.7241e-04 - 100s/epoch - 67ms/step
Epoch 18/200
1493/1493 - 100s - loss: 4.8216e-04 - val_loss: 5.9153e-04 - 100s/epoch - 67ms/step
Epoch 19/200
1493/1493 - 100s - loss: 4.5148e-04 - val_loss: 5.5227e-04 - 100s/epoch - 67ms/step
Epoch 20/200
1493/1493 - 100s - loss: 4.3532e-04 - val_loss: 4.1384e-04 - 100s/epoch - 67ms/step
Epoch 21/200
1493/1493 - 99s - loss: 3.9195e-04 - val_loss: 4.9733e-04 - 99s/epoch - 67ms/step
Epoch 22/200
1493/1493 - 100s - loss: 3.7940e-04 - val_loss: 5.7810e-04 - 100s/epoch - 67ms/step
Epoch 23/200
1493/1493 - 99s - loss: 4.0685e-04 - val_loss: 4.4410e-04 - 99s/epoch - 67ms/step
Epoch 24/200
1493/1493 - 100s - loss: 3.5052e-04 - val_loss: 3.2651e-04 - 100s/epoch - 67ms/step
Epoch 25/200
1493/1493 - 100s - loss: 3.3417e-04 - val_loss: 3.0492e-04 - 100s/epoch - 67ms/step
Epoch 26/200
1493/1493 - 100s - loss: 3.1324e-04 - val_loss: 4.9410e-04 - 100s/epoch - 67ms/step
Epoch 27/200
1493/1493 - 99s - loss: 3.0550e-04 - val_loss: 2.9551e-04 - 99s/epoch - 67ms/step
Epoch 28/200
1493/1493 - 100s - loss: 2.8284e-04 - val_loss: 7.2425e-04 - 100s/epoch - 67ms/step
Epoch 29/200
1493/1493 - 100s - loss: 2.8532e-04 - val_loss: 4.7672e-04 - 100s/epoch - 67ms/step
Epoch 30/200
1493/1493 - 100s - loss: 3.0243e-04 - val_loss: 2.9622e-04 - 100s/epoch - 67ms/step
Epoch 31/200
1493/1493 - 100s - loss: 2.6025e-04 - val_loss: 3.5551e-04 - 100s/epoch - 67ms/step
Epoch 32/200
1493/1493 - 100s - loss: 2.8763e-04 - val_loss: 2.4400e-04 - 100s/epoch - 67ms/step
Epoch 33/200
1493/1493 - 99s - loss: 2.5418e-04 - val_loss: 2.6849e-04 - 99s/epoch - 67ms/step
Epoch 34/200
1493/1493 - 100s - loss: 2.4409e-04 - val_loss: 2.1914e-04 - 100s/epoch - 67ms/step
Epoch 35/200
1493/1493 - 100s - loss: 2.3176e-04 - val_loss: 3.6168e-04 - 100s/epoch - 67ms/step
Epoch 36/200
1493/1493 - 100s - loss: 2.4346e-04 - val_loss: 2.8189e-04 - 100s/epoch - 67ms/step
Epoch 37/200
1493/1493 - 100s - loss: 2.3084e-04 - val_loss: 7.4309e-04 - 100s/epoch - 67ms/step
Epoch 38/200
1493/1493 - 100s - loss: 2.8732e-04 - val_loss: 1.9729e-04 - 100s/epoch - 67ms/step
Epoch 39/200
1493/1493 - 99s - loss: 2.1605e-04 - val_loss: 2.0369e-04 - 99s/epoch - 67ms/step
Epoch 40/200
1493/1493 - 100s - loss: 2.1017e-04 - val_loss: 2.8501e-04 - 100s/epoch - 67ms/step
Epoch 41/200
1493/1493 - 99s - loss: 2.1170e-04 - val_loss: 2.0142e-04 - 99s/epoch - 67ms/step
Epoch 42/200
1493/1493 - 100s - loss: 2.0393e-04 - val_loss: 2.1933e-04 - 100s/epoch - 67ms/step
Epoch 43/200
1493/1493 - 99s - loss: 2.0894e-04 - val_loss: 3.5169e-04 - 99s/epoch - 67ms/step
Epoch 44/200
1493/1493 - 100s - loss: 2.1113e-04 - val_loss: 2.6704e-04 - 100s/epoch - 67ms/step
Epoch 45/200
1493/1493 - 100s - loss: 2.0078e-04 - val_loss: 1.7345e-04 - 100s/epoch - 67ms/step
Epoch 46/200
1493/1493 - 100s - loss: 1.9079e-04 - val_loss: 1.9030e-04 - 100s/epoch - 67ms/step
Epoch 47/200
1493/1493 - 99s - loss: 1.8712e-04 - val_loss: 2.1742e-04 - 99s/epoch - 67ms/step
Epoch 48/200
1493/1493 - 100s - loss: 1.8774e-04 - val_loss: 2.6304e-04 - 100s/epoch - 67ms/step
Epoch 49/200
1493/1493 - 99s - loss: 1.8629e-04 - val_loss: 0.0014 - 99s/epoch - 67ms/step
Epoch 50/200
1493/1493 - 100s - loss: 3.2476e-04 - val_loss: 0.0020 - 100s/epoch - 67ms/step
Epoch 51/200
1493/1493 - 99s - loss: 4.2035e-04 - val_loss: 1.7326e-04 - 99s/epoch - 67ms/step
Epoch 52/200
1493/1493 - 99s - loss: 2.0687e-04 - val_loss: 1.8908e-04 - 99s/epoch - 67ms/step
Epoch 53/200
1493/1493 - 99s - loss: 1.9773e-04 - val_loss: 1.5729e-04 - 99s/epoch - 67ms/step
Epoch 54/200
1493/1493 - 99s - loss: 1.8300e-04 - val_loss: 2.1235e-04 - 99s/epoch - 67ms/step
Epoch 55/200
1493/1493 - 99s - loss: 1.7658e-04 - val_loss: 1.7379e-04 - 99s/epoch - 67ms/step
Epoch 56/200
1493/1493 - 99s - loss: 1.7464e-04 - val_loss: 2.1170e-04 - 99s/epoch - 67ms/step
Epoch 57/200
1493/1493 - 99s - loss: 1.6857e-04 - val_loss: 2.0840e-04 - 99s/epoch - 67ms/step
Epoch 58/200
1493/1493 - 99s - loss: 1.6626e-04 - val_loss: 1.5940e-04 - 99s/epoch - 67ms/step
Epoch 59/200
1493/1493 - 100s - loss: 1.6127e-04 - val_loss: 1.6975e-04 - 100s/epoch - 67ms/step
Epoch 60/200
1493/1493 - 100s - loss: 1.6310e-04 - val_loss: 4.8648e-04 - 100s/epoch - 67ms/step
Epoch 61/200
1493/1493 - 99s - loss: 2.7377e-04 - val_loss: 1.5754e-04 - 99s/epoch - 67ms/step
Epoch 62/200
1493/1493 - 99s - loss: 1.6557e-04 - val_loss: 1.9467e-04 - 99s/epoch - 67ms/step
Epoch 63/200
1493/1493 - 99s - loss: 1.5851e-04 - val_loss: 2.1337e-04 - 99s/epoch - 67ms/step
Epoch 64/200
1493/1493 - 99s - loss: 1.5843e-04 - val_loss: 0.0011 - 99s/epoch - 67ms/step
Epoch 65/200
1493/1493 - 99s - loss: 2.3083e-04 - val_loss: 1.7595e-04 - 99s/epoch - 67ms/step
Epoch 66/200
1493/1493 - 99s - loss: 1.6420e-04 - val_loss: 2.0415e-04 - 99s/epoch - 67ms/step
Epoch 67/200
1493/1493 - 99s - loss: 1.5543e-04 - val_loss: 1.4619e-04 - 99s/epoch - 67ms/step
Epoch 68/200
1493/1493 - 99s - loss: 1.5003e-04 - val_loss: 1.8090e-04 - 99s/epoch - 67ms/step
Epoch 69/200
1493/1493 - 100s - loss: 1.5315e-04 - val_loss: 1.6955e-04 - 100s/epoch - 67ms/step
Epoch 70/200
1493/1493 - 99s - loss: 1.4749e-04 - val_loss: 2.0597e-04 - 99s/epoch - 67ms/step
Epoch 71/200
1493/1493 - 99s - loss: 1.4900e-04 - val_loss: 1.4464e-04 - 99s/epoch - 67ms/step
Epoch 72/200
1493/1493 - 99s - loss: 1.4406e-04 - val_loss: 1.5993e-04 - 99s/epoch - 67ms/step
Epoch 73/200
1493/1493 - 99s - loss: 1.4424e-04 - val_loss: 1.5777e-04 - 99s/epoch - 67ms/step
Epoch 74/200
1493/1493 - 99s - loss: 1.4085e-04 - val_loss: 1.4846e-04 - 99s/epoch - 67ms/step
Epoch 75/200
1493/1493 - 100s - loss: 1.3681e-04 - val_loss: 1.2949e-04 - 100s/epoch - 67ms/step
Epoch 76/200
1493/1493 - 99s - loss: 1.3495e-04 - val_loss: 1.3214e-04 - 99s/epoch - 67ms/step
Epoch 77/200
1493/1493 - 99s - loss: 1.3478e-04 - val_loss: 3.2011e-04 - 99s/epoch - 67ms/step
Epoch 78/200
1493/1493 - 99s - loss: 1.4666e-04 - val_loss: 2.2579e-04 - 99s/epoch - 67ms/step
Epoch 79/200
1493/1493 - 99s - loss: 1.3815e-04 - val_loss: 1.3860e-04 - 99s/epoch - 67ms/step
Epoch 80/200
1493/1493 - 99s - loss: 1.3230e-04 - val_loss: 1.3095e-04 - 99s/epoch - 67ms/step
Epoch 81/200
1493/1493 - 100s - loss: 1.2957e-04 - val_loss: 2.3121e-04 - 100s/epoch - 67ms/step
Epoch 82/200
1493/1493 - 99s - loss: 1.2845e-04 - val_loss: 1.3431e-04 - 99s/epoch - 67ms/step
Epoch 83/200
1493/1493 - 100s - loss: 1.2574e-04 - val_loss: 1.4104e-04 - 100s/epoch - 67ms/step
Epoch 84/200
1493/1493 - 99s - loss: 1.2547e-04 - val_loss: 1.4364e-04 - 99s/epoch - 67ms/step
Epoch 85/200
1493/1493 - 99s - loss: 1.3861e-04 - val_loss: 1.6573e-04 - 99s/epoch - 67ms/step
Epoch 86/200
1493/1493 - 99s - loss: 1.3509e-04 - val_loss: 1.2032e-04 - 99s/epoch - 67ms/step
Epoch 87/200
1493/1493 - 99s - loss: 1.2305e-04 - val_loss: 1.2006e-04 - 99s/epoch - 67ms/step
Epoch 88/200
1493/1493 - 99s - loss: 1.2244e-04 - val_loss: 1.2979e-04 - 99s/epoch - 67ms/step
Epoch 89/200
1493/1493 - 99s - loss: 1.2324e-04 - val_loss: 1.2379e-04 - 99s/epoch - 67ms/step
Epoch 90/200
1493/1493 - 99s - loss: 1.2019e-04 - val_loss: 1.3509e-04 - 99s/epoch - 67ms/step
Epoch 91/200
1493/1493 - 99s - loss: 1.1962e-04 - val_loss: 1.2948e-04 - 99s/epoch - 67ms/step
Epoch 92/200
1493/1493 - 99s - loss: 1.1874e-04 - val_loss: 4.2386e-04 - 99s/epoch - 67ms/step
Epoch 93/200
1493/1493 - 99s - loss: 1.5790e-04 - val_loss: 2.0284e-04 - 99s/epoch - 67ms/step
Epoch 94/200
1493/1493 - 99s - loss: 1.3071e-04 - val_loss: 1.1192e-04 - 99s/epoch - 67ms/step
Epoch 95/200
1493/1493 - 99s - loss: 1.1904e-04 - val_loss: 1.4112e-04 - 99s/epoch - 67ms/step
Epoch 96/200
1493/1493 - 99s - loss: 1.1789e-04 - val_loss: 1.3978e-04 - 99s/epoch - 67ms/step
Epoch 97/200
1493/1493 - 100s - loss: 1.1662e-04 - val_loss: 1.2212e-04 - 100s/epoch - 67ms/step
Epoch 98/200
1493/1493 - 100s - loss: 1.2162e-04 - val_loss: 9.6275e-04 - 100s/epoch - 67ms/step
Epoch 99/200
1493/1493 - 99s - loss: 2.4826e-04 - val_loss: 1.0515e-04 - 99s/epoch - 67ms/step
Epoch 100/200
1493/1493 - 99s - loss: 1.4680e-04 - val_loss: 1.2208e-04 - 99s/epoch - 67ms/step
Epoch 101/200
1493/1493 - 99s - loss: 1.2521e-04 - val_loss: 1.5048e-04 - 99s/epoch - 67ms/step
Epoch 102/200
1493/1493 - 99s - loss: 1.3084e-04 - val_loss: 4.2089e-04 - 99s/epoch - 67ms/step
Epoch 103/200
1493/1493 - 100s - loss: 1.4216e-04 - val_loss: 1.1507e-04 - 100s/epoch - 67ms/step
Epoch 104/200
1493/1493 - 99s - loss: 1.1798e-04 - val_loss: 3.1737e-04 - 99s/epoch - 67ms/step
Epoch 105/200
1493/1493 - 99s - loss: 1.2844e-04 - val_loss: 1.0004e-04 - 99s/epoch - 67ms/step
Epoch 106/200
1493/1493 - 100s - loss: 1.1367e-04 - val_loss: 1.1676e-04 - 100s/epoch - 67ms/step
Epoch 107/200
1493/1493 - 100s - loss: 1.1216e-04 - val_loss: 1.1942e-04 - 100s/epoch - 67ms/step
Epoch 108/200
1493/1493 - 99s - loss: 1.1536e-04 - val_loss: 1.1940e-04 - 99s/epoch - 67ms/step
Epoch 109/200
1493/1493 - 99s - loss: 1.1140e-04 - val_loss: 1.3223e-04 - 99s/epoch - 67ms/step
Epoch 110/200
1493/1493 - 99s - loss: 1.0947e-04 - val_loss: 1.2015e-04 - 99s/epoch - 67ms/step
Epoch 111/200
1493/1493 - 99s - loss: 1.0830e-04 - val_loss: 1.2923e-04 - 99s/epoch - 67ms/step
Epoch 112/200
1493/1493 - 99s - loss: 1.1041e-04 - val_loss: 1.0740e-04 - 99s/epoch - 67ms/step
Epoch 113/200
1493/1493 - 99s - loss: 1.0683e-04 - val_loss: 4.2573e-04 - 99s/epoch - 67ms/step
Epoch 114/200
1493/1493 - 100s - loss: 1.5020e-04 - val_loss: 9.9615e-05 - 100s/epoch - 67ms/step
Epoch 115/200
1493/1493 - 99s - loss: 1.1186e-04 - val_loss: 1.7725e-04 - 99s/epoch - 67ms/step
Epoch 116/200
1493/1493 - 100s - loss: 1.0794e-04 - val_loss: 1.2667e-04 - 100s/epoch - 67ms/step
Epoch 117/200
1493/1493 - 99s - loss: 1.0500e-04 - val_loss: 2.2059e-04 - 99s/epoch - 67ms/step
Epoch 118/200
1493/1493 - 99s - loss: 1.1613e-04 - val_loss: 1.4537e-04 - 99s/epoch - 67ms/step
Epoch 119/200
1493/1493 - 100s - loss: 1.0933e-04 - val_loss: 1.0001e-04 - 100s/epoch - 67ms/step
Epoch 120/200
1493/1493 - 100s - loss: 1.0416e-04 - val_loss: 1.0808e-04 - 100s/epoch - 67ms/step
Epoch 121/200
1493/1493 - 99s - loss: 1.0337e-04 - val_loss: 2.0154e-04 - 99s/epoch - 67ms/step
Epoch 122/200
1493/1493 - 99s - loss: 1.0778e-04 - val_loss: 1.0005e-04 - 99s/epoch - 67ms/step
Epoch 123/200
1493/1493 - 99s - loss: 1.0261e-04 - val_loss: 1.1080e-04 - 99s/epoch - 67ms/step
Epoch 124/200
1493/1493 - 100s - loss: 1.0233e-04 - val_loss: 3.2261e-04 - 100s/epoch - 67ms/step
Epoch 125/200
1493/1493 - 99s - loss: 1.5334e-04 - val_loss: 1.8520e-04 - 99s/epoch - 67ms/step
Epoch 126/200
1493/1493 - 99s - loss: 1.1366e-04 - val_loss: 2.2861e-04 - 99s/epoch - 67ms/step
Epoch 127/200
1493/1493 - 99s - loss: 1.1081e-04 - val_loss: 1.0377e-04 - 99s/epoch - 67ms/step
Epoch 128/200
1493/1493 - 99s - loss: 1.0168e-04 - val_loss: 8.8511e-05 - 99s/epoch - 67ms/step
Epoch 129/200
1493/1493 - 99s - loss: 1.0036e-04 - val_loss: 9.8088e-05 - 99s/epoch - 67ms/step
Epoch 130/200
1493/1493 - 99s - loss: 9.9382e-05 - val_loss: 9.1707e-05 - 99s/epoch - 67ms/step
Epoch 131/200
1493/1493 - 100s - loss: 1.0285e-04 - val_loss: 4.0266e-04 - 100s/epoch - 67ms/step
Epoch 132/200
1493/1493 - 99s - loss: 1.4544e-04 - val_loss: 9.1394e-05 - 99s/epoch - 67ms/step
Epoch 133/200
1493/1493 - 100s - loss: 1.0187e-04 - val_loss: 9.9755e-05 - 100s/epoch - 67ms/step
Epoch 134/200
1493/1493 - 99s - loss: 9.9296e-05 - val_loss: 1.0719e-04 - 99s/epoch - 67ms/step
Epoch 135/200
1493/1493 - 99s - loss: 9.7355e-05 - val_loss: 9.0551e-05 - 99s/epoch - 67ms/step
Epoch 136/200
1493/1493 - 99s - loss: 9.7165e-05 - val_loss: 1.0814e-04 - 99s/epoch - 67ms/step
Epoch 137/200
1493/1493 - 99s - loss: 1.0080e-04 - val_loss: 1.1004e-04 - 99s/epoch - 67ms/step
Epoch 138/200
1493/1493 - 99s - loss: 9.8673e-05 - val_loss: 1.0302e-04 - 99s/epoch - 67ms/step
Epoch 139/200
1493/1493 - 99s - loss: 9.9862e-05 - val_loss: 2.1142e-04 - 99s/epoch - 67ms/step
Epoch 140/200
1493/1493 - 99s - loss: 1.2468e-04 - val_loss: 9.0228e-05 - 99s/epoch - 67ms/step
Epoch 141/200
1493/1493 - 100s - loss: 9.7733e-05 - val_loss: 1.0586e-04 - 100s/epoch - 67ms/step
Epoch 142/200
1493/1493 - 100s - loss: 9.7998e-05 - val_loss: 1.0687e-04 - 100s/epoch - 67ms/step
Epoch 143/200
1493/1493 - 99s - loss: 9.4467e-05 - val_loss: 1.2039e-04 - 99s/epoch - 67ms/step
Epoch 144/200
1493/1493 - 99s - loss: 9.6455e-05 - val_loss: 1.0667e-04 - 99s/epoch - 67ms/step
Epoch 145/200
1493/1493 - 99s - loss: 9.3943e-05 - val_loss: 9.0463e-05 - 99s/epoch - 67ms/step
Epoch 146/200
1493/1493 - 99s - loss: 9.2381e-05 - val_loss: 8.8701e-05 - 99s/epoch - 67ms/step
Epoch 147/200
1493/1493 - 99s - loss: 9.1455e-05 - val_loss: 1.0085e-04 - 99s/epoch - 67ms/step
Epoch 148/200
1493/1493 - 99s - loss: 9.1121e-05 - val_loss: 1.5273e-04 - 99s/epoch - 67ms/step
Epoch 149/200
1493/1493 - 99s - loss: 9.1221e-05 - val_loss: 9.4052e-05 - 99s/epoch - 67ms/step
Epoch 150/200
1493/1493 - 99s - loss: 9.0030e-05 - val_loss: 9.2002e-05 - 99s/epoch - 67ms/step
Epoch 151/200
1493/1493 - 99s - loss: 9.0151e-05 - val_loss: 1.2580e-04 - 99s/epoch - 67ms/step
Epoch 152/200
1493/1493 - 99s - loss: 9.2587e-05 - val_loss: 1.2022e-04 - 99s/epoch - 67ms/step
Epoch 153/200
1493/1493 - 99s - loss: 9.1270e-05 - val_loss: 9.5777e-05 - 99s/epoch - 67ms/step
Epoch 154/200
1493/1493 - 99s - loss: 8.8231e-05 - val_loss: 9.9912e-05 - 99s/epoch - 67ms/step
Epoch 155/200
1493/1493 - 100s - loss: 8.7408e-05 - val_loss: 1.0016e-04 - 100s/epoch - 67ms/step
Epoch 156/200
1493/1493 - 99s - loss: 9.1706e-05 - val_loss: 9.3089e-05 - 99s/epoch - 67ms/step
Epoch 157/200
1493/1493 - 99s - loss: 8.7718e-05 - val_loss: 1.0757e-04 - 99s/epoch - 67ms/step
Epoch 158/200
1493/1493 - 99s - loss: 8.8811e-05 - val_loss: 1.0563e-04 - 99s/epoch - 67ms/step
Epoch 159/200
1493/1493 - 99s - loss: 8.6867e-05 - val_loss: 8.4757e-05 - 99s/epoch - 67ms/step
Epoch 160/200
1493/1493 - 99s - loss: 8.8669e-05 - val_loss: 1.4119e-04 - 99s/epoch - 67ms/step
Epoch 161/200
1493/1493 - 99s - loss: 1.1455e-04 - val_loss: 8.7446e-05 - 99s/epoch - 67ms/step
Epoch 162/200
1493/1493 - 100s - loss: 8.8815e-05 - val_loss: 8.6008e-05 - 100s/epoch - 67ms/step
Epoch 163/200
1493/1493 - 99s - loss: 8.7628e-05 - val_loss: 1.1202e-04 - 99s/epoch - 67ms/step
Epoch 164/200
1493/1493 - 99s - loss: 9.2257e-05 - val_loss: 8.7170e-05 - 99s/epoch - 67ms/step
Epoch 165/200
1493/1493 - 100s - loss: 9.3431e-05 - val_loss: 9.0558e-05 - 100s/epoch - 67ms/step
Epoch 166/200
1493/1493 - 99s - loss: 8.7521e-05 - val_loss: 9.3011e-05 - 99s/epoch - 67ms/step
Epoch 167/200
1493/1493 - 99s - loss: 8.7106e-05 - val_loss: 7.9199e-05 - 99s/epoch - 67ms/step
Epoch 168/200
1493/1493 - 99s - loss: 8.5475e-05 - val_loss: 8.3893e-05 - 99s/epoch - 67ms/step
Epoch 169/200
1493/1493 - 99s - loss: 8.5514e-05 - val_loss: 1.1963e-04 - 99s/epoch - 67ms/step
Epoch 170/200
1493/1493 - 99s - loss: 8.5490e-05 - val_loss: 1.8356e-04 - 99s/epoch - 67ms/step
Epoch 171/200
1493/1493 - 100s - loss: 1.0840e-04 - val_loss: 2.0978e-04 - 100s/epoch - 67ms/step
Epoch 172/200
1493/1493 - 100s - loss: 9.9782e-05 - val_loss: 1.0308e-04 - 100s/epoch - 67ms/step
Epoch 173/200
1493/1493 - 99s - loss: 8.7287e-05 - val_loss: 7.9429e-05 - 99s/epoch - 67ms/step
Epoch 174/200
1493/1493 - 99s - loss: 8.4713e-05 - val_loss: 7.8708e-05 - 99s/epoch - 67ms/step
Epoch 175/200
1493/1493 - 99s - loss: 8.5749e-05 - val_loss: 1.1373e-04 - 99s/epoch - 67ms/step
Epoch 176/200
1493/1493 - 100s - loss: 8.5586e-05 - val_loss: 8.4139e-05 - 100s/epoch - 67ms/step
Epoch 177/200
1493/1493 - 99s - loss: 8.4185e-05 - val_loss: 7.4895e-05 - 99s/epoch - 67ms/step
Epoch 178/200
1493/1493 - 99s - loss: 8.3041e-05 - val_loss: 9.2853e-05 - 99s/epoch - 67ms/step
Epoch 179/200
1493/1493 - 99s - loss: 8.2081e-05 - val_loss: 9.4892e-05 - 99s/epoch - 67ms/step
Epoch 180/200
1493/1493 - 99s - loss: 8.2960e-05 - val_loss: 8.1329e-05 - 99s/epoch - 67ms/step
Epoch 181/200
1493/1493 - 99s - loss: 8.2105e-05 - val_loss: 8.4336e-05 - 99s/epoch - 67ms/step
Epoch 182/200
1493/1493 - 100s - loss: 8.2364e-05 - val_loss: 1.1623e-04 - 100s/epoch - 67ms/step
Epoch 183/200
1493/1493 - 99s - loss: 8.4659e-05 - val_loss: 8.6520e-05 - 99s/epoch - 67ms/step
Epoch 184/200
1493/1493 - 99s - loss: 8.1957e-05 - val_loss: 8.0708e-05 - 99s/epoch - 67ms/step
Epoch 185/200
1493/1493 - 99s - loss: 8.1103e-05 - val_loss: 1.8764e-04 - 99s/epoch - 67ms/step
Epoch 186/200
1493/1493 - 99s - loss: 1.0083e-04 - val_loss: 7.6656e-05 - 99s/epoch - 67ms/step
Epoch 187/200
1493/1493 - 100s - loss: 8.3290e-05 - val_loss: 1.4796e-04 - 100s/epoch - 67ms/step
Epoch 188/200
1493/1493 - 100s - loss: 1.0425e-04 - val_loss: 8.9483e-05 - 100s/epoch - 67ms/step
Epoch 189/200
1493/1493 - 100s - loss: 8.5799e-05 - val_loss: 9.7042e-05 - 100s/epoch - 67ms/step
Epoch 190/200
1493/1493 - 99s - loss: 8.4706e-05 - val_loss: 1.1047e-04 - 99s/epoch - 67ms/step
Epoch 191/200
1493/1493 - 99s - loss: 8.7649e-05 - val_loss: 1.1966e-04 - 99s/epoch - 67ms/step
Epoch 192/200
1493/1493 - 99s - loss: 8.5869e-05 - val_loss: 8.5469e-05 - 99s/epoch - 67ms/step
Epoch 193/200
1493/1493 - 99s - loss: 8.2422e-05 - val_loss: 9.9247e-05 - 99s/epoch - 67ms/step
Epoch 194/200
1493/1493 - 100s - loss: 8.9779e-05 - val_loss: 3.0866e-04 - 100s/epoch - 67ms/step
Epoch 195/200
1493/1493 - 99s - loss: 1.3082e-04 - val_loss: 1.3316e-04 - 99s/epoch - 67ms/step
Epoch 196/200
1493/1493 - 99s - loss: 9.4755e-05 - val_loss: 7.5336e-05 - 99s/epoch - 67ms/step
Epoch 197/200
1493/1493 - 99s - loss: 8.3919e-05 - val_loss: 1.0836e-04 - 99s/epoch - 67ms/step
Epoch 198/200
1493/1493 - 99s - loss: 8.6907e-05 - val_loss: 8.9071e-05 - 99s/epoch - 67ms/step
Epoch 199/200
1493/1493 - 99s - loss: 8.2101e-05 - val_loss: 8.2289e-05 - 99s/epoch - 67ms/step
Epoch 200/200
1493/1493 - 100s - loss: 8.0567e-05 - val_loss: 8.4627e-05 - 100s/epoch - 67ms/step
COMPRESSED VECTOR SIZE: 1074
Loss in the autoencoder: 8.462734695058316e-05
  1/332 [..............................] - ETA: 25s  8/332 [..............................] - ETA: 2s  15/332 [>.............................] - ETA: 2s 22/332 [>.............................] - ETA: 2s 29/332 [=>............................] - ETA: 2s 36/332 [==>...........................] - ETA: 2s 43/332 [==>...........................] - ETA: 2s 50/332 [===>..........................] - ETA: 2s 57/332 [====>.........................] - ETA: 2s 64/332 [====>.........................] - ETA: 2s 71/332 [=====>........................] - ETA: 2s 78/332 [======>.......................] - ETA: 2s 85/332 [======>.......................] - ETA: 1s 92/332 [=======>......................] - ETA: 1s 99/332 [=======>......................] - ETA: 1s106/332 [========>.....................] - ETA: 1s113/332 [=========>....................] - ETA: 1s120/332 [=========>....................] - ETA: 1s127/332 [==========>...................] - ETA: 1s134/332 [===========>..................] - ETA: 1s141/332 [===========>..................] - ETA: 1s148/332 [============>.................] - ETA: 1s155/332 [=============>................] - ETA: 1s162/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s176/332 [==============>...............] - ETA: 1s183/332 [===============>..............] - ETA: 1s190/332 [================>.............] - ETA: 1s197/332 [================>.............] - ETA: 1s204/332 [=================>............] - ETA: 1s211/332 [==================>...........] - ETA: 0s218/332 [==================>...........] - ETA: 0s225/332 [===================>..........] - ETA: 0s232/332 [===================>..........] - ETA: 0s239/332 [====================>.........] - ETA: 0s246/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s260/332 [======================>.......] - ETA: 0s267/332 [=======================>......] - ETA: 0s274/332 [=======================>......] - ETA: 0s281/332 [========================>.....] - ETA: 0s288/332 [=========================>....] - ETA: 0s295/332 [=========================>....] - ETA: 0s302/332 [==========================>...] - ETA: 0s309/332 [==========================>...] - ETA: 0s316/332 [===========================>..] - ETA: 0s323/332 [============================>.] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 8ms/step
correlation 0.0009515074527897207
cosine 0.0007491325190774549
MAE: 0.0050833025
RMSE: 0.009199309
r2: 0.994510679166031
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 2780)              3516700   
                                                                 
 batch_normalization_9 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1074)              2986794   
                                                                 
 batch_normalization_10 (Bat  (None, 1074)             4296      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 1074)              0         
                                                                 
 dense_10 (Dense)            (None, 2780)              2988500   
                                                                 
 batch_normalization_11 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2780)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 13,033,714
Trainable params: 13,020,446
Non-trainable params: 13,268
_________________________________________________________________
Encoder
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_10 (InputLayer)       multiple                  0         
                                                                 
 dense_9 (Dense)             (None, 2780)              3516700   
                                                                 
 batch_normalization_9 (Batc  (None, 2780)             11120     
 hNormalization)                                                 
                                                                 
 re_lu_9 (ReLU)              (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1074)              2986794   
                                                                 
=================================================================
Total params: 6,514,614
Trainable params: 6,509,054
Non-trainable params: 5,560
_________________________________________________________________
Decoder
Model: "model_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 1074)]            0         
                                                                 
 batch_normalization_10 (Bat  (None, 1074)             4296      
 chNormalization)                                                
                                                                 
 re_lu_10 (ReLU)             (None, 1074)              0         
                                                                 
 dense_10 (Dense)            (None, 2780)              2988500   
                                                                 
 batch_normalization_11 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_11 (ReLU)             (None, 2780)              0         
                                                                 
 dense_11 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 6,519,100
Trainable params: 6,511,392
Non-trainable params: 7,708
_________________________________________________________________
['2.2custom_n_b', 'mse', 64, 200, 0.0005, 0.85, 1074, 8.056678052525967e-05, 8.462734695058316e-05, 0.0009515074527897207, 0.0007491325190774549, 0.00508330250158906, 0.009199309162795544, 0.994510679166031, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_2.2final_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_12 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_12 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1011)              2811591   
                                                                 
 batch_normalization_13 (Bat  (None, 1011)             4044      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 1011)              0         
                                                                 
 dense_13 (Dense)            (None, 2780)              2813360   
                                                                 
 batch_normalization_14 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2780)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 12,683,119
Trainable params: 12,669,977
Non-trainable params: 13,142
_________________________________________________________________
Epoch 1/200
1493/1493 - 96s - loss: 0.0090 - val_loss: 0.0039 - 96s/epoch - 64ms/step
Epoch 2/200
1493/1493 - 95s - loss: 0.0028 - val_loss: 0.0034 - 95s/epoch - 64ms/step
Epoch 3/200
1493/1493 - 95s - loss: 0.0021 - val_loss: 0.0015 - 95s/epoch - 64ms/step
Epoch 4/200
1493/1493 - 95s - loss: 0.0016 - val_loss: 0.0025 - 95s/epoch - 64ms/step
Epoch 5/200
1493/1493 - 95s - loss: 0.0016 - val_loss: 0.0014 - 95s/epoch - 64ms/step
Epoch 6/200
1493/1493 - 95s - loss: 0.0014 - val_loss: 0.0012 - 95s/epoch - 64ms/step
Epoch 7/200
1493/1493 - 95s - loss: 0.0013 - val_loss: 0.0014 - 95s/epoch - 64ms/step
Epoch 8/200
1493/1493 - 95s - loss: 0.0011 - val_loss: 9.3919e-04 - 95s/epoch - 64ms/step
Epoch 9/200
1493/1493 - 95s - loss: 0.0010 - val_loss: 9.2517e-04 - 95s/epoch - 64ms/step
Epoch 10/200
1493/1493 - 95s - loss: 9.0779e-04 - val_loss: 8.7089e-04 - 95s/epoch - 64ms/step
Epoch 11/200
1493/1493 - 95s - loss: 8.5401e-04 - val_loss: 7.2870e-04 - 95s/epoch - 64ms/step
Epoch 12/200
1493/1493 - 95s - loss: 7.1694e-04 - val_loss: 0.0021 - 95s/epoch - 64ms/step
Epoch 13/200
1493/1493 - 95s - loss: 7.9190e-04 - val_loss: 0.0016 - 95s/epoch - 64ms/step
Epoch 14/200
1493/1493 - 95s - loss: 7.1094e-04 - val_loss: 5.6945e-04 - 95s/epoch - 64ms/step
Epoch 15/200
1493/1493 - 95s - loss: 6.0028e-04 - val_loss: 0.0015 - 95s/epoch - 64ms/step
Epoch 16/200
1493/1493 - 95s - loss: 6.7508e-04 - val_loss: 4.8868e-04 - 95s/epoch - 64ms/step
Epoch 17/200
1493/1493 - 95s - loss: 5.0245e-04 - val_loss: 6.6948e-04 - 95s/epoch - 64ms/step
Epoch 18/200
1493/1493 - 95s - loss: 4.8482e-04 - val_loss: 8.8276e-04 - 95s/epoch - 64ms/step
Epoch 19/200
1493/1493 - 95s - loss: 4.9358e-04 - val_loss: 4.7984e-04 - 95s/epoch - 64ms/step
Epoch 20/200
1493/1493 - 95s - loss: 4.3047e-04 - val_loss: 3.8838e-04 - 95s/epoch - 64ms/step
Epoch 21/200
1493/1493 - 95s - loss: 4.0629e-04 - val_loss: 5.7405e-04 - 95s/epoch - 64ms/step
Epoch 22/200
1493/1493 - 95s - loss: 3.9076e-04 - val_loss: 4.3621e-04 - 95s/epoch - 64ms/step
Epoch 23/200
1493/1493 - 95s - loss: 3.5964e-04 - val_loss: 5.7967e-04 - 95s/epoch - 64ms/step
Epoch 24/200
1493/1493 - 95s - loss: 3.9796e-04 - val_loss: 3.4197e-04 - 95s/epoch - 64ms/step
Epoch 25/200
1493/1493 - 95s - loss: 3.3362e-04 - val_loss: 3.0265e-04 - 95s/epoch - 64ms/step
Epoch 26/200
1493/1493 - 95s - loss: 3.1383e-04 - val_loss: 3.3529e-04 - 95s/epoch - 64ms/step
Epoch 27/200
1493/1493 - 95s - loss: 3.0589e-04 - val_loss: 2.9644e-04 - 95s/epoch - 64ms/step
Epoch 28/200
1493/1493 - 95s - loss: 2.9269e-04 - val_loss: 3.5081e-04 - 95s/epoch - 64ms/step
Epoch 29/200
1493/1493 - 95s - loss: 2.8080e-04 - val_loss: 4.8343e-04 - 95s/epoch - 64ms/step
Epoch 30/200
1493/1493 - 95s - loss: 2.9611e-04 - val_loss: 2.9712e-04 - 95s/epoch - 64ms/step
Epoch 31/200
1493/1493 - 95s - loss: 2.6425e-04 - val_loss: 5.3519e-04 - 95s/epoch - 64ms/step
Epoch 32/200
1493/1493 - 95s - loss: 2.8576e-04 - val_loss: 2.3885e-04 - 95s/epoch - 64ms/step
Epoch 33/200
1493/1493 - 95s - loss: 2.5645e-04 - val_loss: 2.6219e-04 - 95s/epoch - 64ms/step
Epoch 34/200
1493/1493 - 95s - loss: 2.4879e-04 - val_loss: 2.2454e-04 - 95s/epoch - 64ms/step
Epoch 35/200
1493/1493 - 95s - loss: 2.3798e-04 - val_loss: 5.9870e-04 - 95s/epoch - 64ms/step
Epoch 36/200
1493/1493 - 95s - loss: 2.6777e-04 - val_loss: 3.0552e-04 - 95s/epoch - 64ms/step
Epoch 37/200
1493/1493 - 95s - loss: 2.4052e-04 - val_loss: 3.5101e-04 - 95s/epoch - 64ms/step
Epoch 38/200
1493/1493 - 95s - loss: 2.5752e-04 - val_loss: 2.0363e-04 - 95s/epoch - 64ms/step
Epoch 39/200
1493/1493 - 95s - loss: 2.2163e-04 - val_loss: 2.2172e-04 - 95s/epoch - 64ms/step
Epoch 40/200
1493/1493 - 95s - loss: 2.1348e-04 - val_loss: 3.2073e-04 - 95s/epoch - 64ms/step
Epoch 41/200
1493/1493 - 95s - loss: 2.1407e-04 - val_loss: 1.8868e-04 - 95s/epoch - 64ms/step
Epoch 42/200
1493/1493 - 95s - loss: 2.0834e-04 - val_loss: 2.2495e-04 - 95s/epoch - 64ms/step
Epoch 43/200
1493/1493 - 95s - loss: 2.0699e-04 - val_loss: 2.8293e-04 - 95s/epoch - 64ms/step
Epoch 44/200
1493/1493 - 95s - loss: 2.1117e-04 - val_loss: 6.6366e-04 - 95s/epoch - 64ms/step
Epoch 45/200
1493/1493 - 95s - loss: 2.4775e-04 - val_loss: 1.6851e-04 - 95s/epoch - 64ms/step
Epoch 46/200
1493/1493 - 95s - loss: 1.9520e-04 - val_loss: 1.7955e-04 - 95s/epoch - 64ms/step
Epoch 47/200
1493/1493 - 95s - loss: 1.9218e-04 - val_loss: 2.5471e-04 - 95s/epoch - 64ms/step
Epoch 48/200
1493/1493 - 95s - loss: 1.9146e-04 - val_loss: 2.5547e-04 - 95s/epoch - 64ms/step
Epoch 49/200
1493/1493 - 95s - loss: 1.8896e-04 - val_loss: 0.0011 - 95s/epoch - 64ms/step
Epoch 50/200
1493/1493 - 95s - loss: 2.7125e-04 - val_loss: 0.0011 - 95s/epoch - 64ms/step
Epoch 51/200
1493/1493 - 95s - loss: 2.3850e-04 - val_loss: 1.9036e-04 - 95s/epoch - 64ms/step
Epoch 52/200
1493/1493 - 95s - loss: 1.8326e-04 - val_loss: 1.7323e-04 - 95s/epoch - 64ms/step
Epoch 53/200
1493/1493 - 95s - loss: 1.8096e-04 - val_loss: 1.8912e-04 - 95s/epoch - 64ms/step
Epoch 54/200
1493/1493 - 95s - loss: 1.7811e-04 - val_loss: 1.9563e-04 - 95s/epoch - 64ms/step
Epoch 55/200
1493/1493 - 95s - loss: 1.7178e-04 - val_loss: 1.7301e-04 - 95s/epoch - 64ms/step
Epoch 56/200
1493/1493 - 95s - loss: 1.6973e-04 - val_loss: 2.0359e-04 - 95s/epoch - 64ms/step
Epoch 57/200
1493/1493 - 95s - loss: 1.6598e-04 - val_loss: 1.7894e-04 - 95s/epoch - 64ms/step
Epoch 58/200
1493/1493 - 95s - loss: 1.6518e-04 - val_loss: 1.4371e-04 - 95s/epoch - 64ms/step
Epoch 59/200
1493/1493 - 95s - loss: 1.6022e-04 - val_loss: 1.7685e-04 - 95s/epoch - 64ms/step
Epoch 60/200
1493/1493 - 95s - loss: 1.6140e-04 - val_loss: 4.3768e-04 - 95s/epoch - 64ms/step
Epoch 61/200
1493/1493 - 95s - loss: 2.2156e-04 - val_loss: 1.4511e-04 - 95s/epoch - 64ms/step
Epoch 62/200
1493/1493 - 95s - loss: 1.6020e-04 - val_loss: 1.6641e-04 - 95s/epoch - 64ms/step
Epoch 63/200
1493/1493 - 95s - loss: 1.5883e-04 - val_loss: 2.0046e-04 - 95s/epoch - 64ms/step
Epoch 64/200
1493/1493 - 95s - loss: 1.5875e-04 - val_loss: 0.0026 - 95s/epoch - 64ms/step
Epoch 65/200
1493/1493 - 95s - loss: 3.0643e-04 - val_loss: 2.6387e-04 - 95s/epoch - 64ms/step
Epoch 66/200
1493/1493 - 95s - loss: 1.7979e-04 - val_loss: 1.7954e-04 - 95s/epoch - 64ms/step
Epoch 67/200
1493/1493 - 95s - loss: 1.6136e-04 - val_loss: 1.4388e-04 - 95s/epoch - 64ms/step
Epoch 68/200
1493/1493 - 95s - loss: 1.5713e-04 - val_loss: 4.3293e-04 - 95s/epoch - 64ms/step
Epoch 69/200
1493/1493 - 95s - loss: 2.1124e-04 - val_loss: 1.7782e-04 - 95s/epoch - 64ms/step
Epoch 70/200
1493/1493 - 95s - loss: 1.5461e-04 - val_loss: 1.6740e-04 - 95s/epoch - 64ms/step
Epoch 71/200
1493/1493 - 95s - loss: 1.5250e-04 - val_loss: 1.4393e-04 - 95s/epoch - 64ms/step
Epoch 72/200
1493/1493 - 95s - loss: 1.4998e-04 - val_loss: 2.9239e-04 - 95s/epoch - 64ms/step
Epoch 73/200
1493/1493 - 95s - loss: 1.6043e-04 - val_loss: 1.4962e-04 - 95s/epoch - 64ms/step
Epoch 74/200
1493/1493 - 95s - loss: 1.4860e-04 - val_loss: 1.6762e-04 - 95s/epoch - 64ms/step
Epoch 75/200
1493/1493 - 95s - loss: 1.4229e-04 - val_loss: 1.3460e-04 - 95s/epoch - 64ms/step
Epoch 76/200
1493/1493 - 95s - loss: 1.4064e-04 - val_loss: 1.2972e-04 - 95s/epoch - 64ms/step
Epoch 77/200
1493/1493 - 95s - loss: 1.3761e-04 - val_loss: 0.0013 - 95s/epoch - 64ms/step
Epoch 78/200
1493/1493 - 95s - loss: 2.1655e-04 - val_loss: 1.5563e-04 - 95s/epoch - 64ms/step
Epoch 79/200
1493/1493 - 95s - loss: 1.4836e-04 - val_loss: 2.6976e-04 - 95s/epoch - 64ms/step
Epoch 80/200
1493/1493 - 95s - loss: 1.6035e-04 - val_loss: 1.9155e-04 - 95s/epoch - 64ms/step
Epoch 81/200
1493/1493 - 95s - loss: 1.4368e-04 - val_loss: 4.1923e-04 - 95s/epoch - 64ms/step
Epoch 82/200
1493/1493 - 95s - loss: 1.4145e-04 - val_loss: 1.2876e-04 - 95s/epoch - 64ms/step
Epoch 83/200
1493/1493 - 95s - loss: 1.3256e-04 - val_loss: 1.3286e-04 - 95s/epoch - 64ms/step
Epoch 84/200
1493/1493 - 95s - loss: 1.3123e-04 - val_loss: 1.3590e-04 - 95s/epoch - 64ms/step
Epoch 85/200
1493/1493 - 95s - loss: 1.3135e-04 - val_loss: 1.4504e-04 - 95s/epoch - 64ms/step
Epoch 86/200
1493/1493 - 95s - loss: 1.2981e-04 - val_loss: 1.5733e-04 - 95s/epoch - 64ms/step
Epoch 87/200
1493/1493 - 95s - loss: 1.2573e-04 - val_loss: 1.2711e-04 - 95s/epoch - 64ms/step
Epoch 88/200
1493/1493 - 95s - loss: 1.2492e-04 - val_loss: 1.4021e-04 - 95s/epoch - 64ms/step
Epoch 89/200
1493/1493 - 95s - loss: 1.2776e-04 - val_loss: 1.2737e-04 - 95s/epoch - 64ms/step
Epoch 90/200
1493/1493 - 95s - loss: 1.2383e-04 - val_loss: 1.3278e-04 - 95s/epoch - 64ms/step
Epoch 91/200
1493/1493 - 95s - loss: 1.2238e-04 - val_loss: 1.3241e-04 - 95s/epoch - 64ms/step
Epoch 92/200
1493/1493 - 95s - loss: 1.2659e-04 - val_loss: 7.4006e-04 - 95s/epoch - 64ms/step
Epoch 93/200
1493/1493 - 95s - loss: 1.9740e-04 - val_loss: 2.5261e-04 - 95s/epoch - 64ms/step
Epoch 94/200
1493/1493 - 95s - loss: 1.4134e-04 - val_loss: 1.1293e-04 - 95s/epoch - 64ms/step
Epoch 95/200
1493/1493 - 95s - loss: 1.2843e-04 - val_loss: 0.0012 - 95s/epoch - 64ms/step
Epoch 96/200
1493/1493 - 95s - loss: 2.0439e-04 - val_loss: 1.4032e-04 - 95s/epoch - 64ms/step
Epoch 97/200
1493/1493 - 95s - loss: 1.3382e-04 - val_loss: 1.1738e-04 - 95s/epoch - 64ms/step
Epoch 98/200
1493/1493 - 95s - loss: 1.3115e-04 - val_loss: 6.0768e-04 - 95s/epoch - 64ms/step
Epoch 99/200
1493/1493 - 95s - loss: 2.0866e-04 - val_loss: 1.1058e-04 - 95s/epoch - 64ms/step
Epoch 100/200
1493/1493 - 95s - loss: 1.4081e-04 - val_loss: 1.1608e-04 - 95s/epoch - 64ms/step
Epoch 101/200
1493/1493 - 95s - loss: 1.2778e-04 - val_loss: 1.3754e-04 - 95s/epoch - 64ms/step
Epoch 102/200
1493/1493 - 95s - loss: 1.2842e-04 - val_loss: 5.4020e-04 - 95s/epoch - 64ms/step
Epoch 103/200
1493/1493 - 95s - loss: 1.4959e-04 - val_loss: 1.1369e-04 - 95s/epoch - 64ms/step
Epoch 104/200
1493/1493 - 95s - loss: 1.2167e-04 - val_loss: 5.4916e-04 - 95s/epoch - 64ms/step
Epoch 105/200
1493/1493 - 95s - loss: 1.4744e-04 - val_loss: 9.9429e-05 - 95s/epoch - 64ms/step
Epoch 106/200
1493/1493 - 95s - loss: 1.2017e-04 - val_loss: 1.1849e-04 - 95s/epoch - 64ms/step
Epoch 107/200
1493/1493 - 95s - loss: 1.1766e-04 - val_loss: 1.1976e-04 - 95s/epoch - 64ms/step
Epoch 108/200
1493/1493 - 95s - loss: 1.2085e-04 - val_loss: 1.2064e-04 - 95s/epoch - 64ms/step
Epoch 109/200
1493/1493 - 95s - loss: 1.1550e-04 - val_loss: 1.2560e-04 - 95s/epoch - 64ms/step
Epoch 110/200
1493/1493 - 95s - loss: 1.1368e-04 - val_loss: 1.2048e-04 - 95s/epoch - 64ms/step
Epoch 111/200
1493/1493 - 95s - loss: 1.1244e-04 - val_loss: 1.3009e-04 - 95s/epoch - 64ms/step
Epoch 112/200
1493/1493 - 95s - loss: 1.1449e-04 - val_loss: 1.0562e-04 - 95s/epoch - 64ms/step
Epoch 113/200
1493/1493 - 95s - loss: 1.1110e-04 - val_loss: 6.5049e-04 - 95s/epoch - 64ms/step
Epoch 114/200
1493/1493 - 95s - loss: 1.5518e-04 - val_loss: 1.1750e-04 - 95s/epoch - 64ms/step
Epoch 115/200
1493/1493 - 95s - loss: 1.2062e-04 - val_loss: 1.7152e-04 - 95s/epoch - 64ms/step
Epoch 116/200
1493/1493 - 95s - loss: 1.1271e-04 - val_loss: 1.1001e-04 - 95s/epoch - 64ms/step
Epoch 117/200
1493/1493 - 95s - loss: 1.0981e-04 - val_loss: 2.1230e-04 - 95s/epoch - 64ms/step
Epoch 118/200
1493/1493 - 95s - loss: 1.1862e-04 - val_loss: 1.0792e-04 - 95s/epoch - 64ms/step
Epoch 119/200
1493/1493 - 95s - loss: 1.0952e-04 - val_loss: 1.0691e-04 - 95s/epoch - 64ms/step
Epoch 120/200
1493/1493 - 95s - loss: 1.0696e-04 - val_loss: 1.0154e-04 - 95s/epoch - 64ms/step
Epoch 121/200
1493/1493 - 95s - loss: 1.0638e-04 - val_loss: 1.7004e-04 - 95s/epoch - 64ms/step
Epoch 122/200
1493/1493 - 95s - loss: 1.0824e-04 - val_loss: 9.8210e-05 - 95s/epoch - 64ms/step
Epoch 123/200
1493/1493 - 95s - loss: 1.0468e-04 - val_loss: 1.1282e-04 - 95s/epoch - 64ms/step
Epoch 124/200
1493/1493 - 95s - loss: 1.0553e-04 - val_loss: 3.4713e-04 - 95s/epoch - 64ms/step
Epoch 125/200
1493/1493 - 95s - loss: 1.3710e-04 - val_loss: 1.9871e-04 - 95s/epoch - 64ms/step
Epoch 126/200
1493/1493 - 95s - loss: 1.1336e-04 - val_loss: 1.2899e-04 - 95s/epoch - 64ms/step
Epoch 127/200
1493/1493 - 95s - loss: 1.0710e-04 - val_loss: 1.1730e-04 - 95s/epoch - 64ms/step
Epoch 128/200
1493/1493 - 95s - loss: 1.0311e-04 - val_loss: 9.3821e-05 - 95s/epoch - 64ms/step
Epoch 129/200
1493/1493 - 95s - loss: 1.0218e-04 - val_loss: 1.1462e-04 - 95s/epoch - 64ms/step
Epoch 130/200
1493/1493 - 95s - loss: 1.0122e-04 - val_loss: 9.2370e-05 - 95s/epoch - 64ms/step
Epoch 131/200
1493/1493 - 95s - loss: 1.0434e-04 - val_loss: 4.1062e-04 - 95s/epoch - 64ms/step
Epoch 132/200
1493/1493 - 95s - loss: 1.4278e-04 - val_loss: 9.7914e-05 - 95s/epoch - 64ms/step
Epoch 133/200
1493/1493 - 95s - loss: 1.0473e-04 - val_loss: 9.6666e-05 - 95s/epoch - 64ms/step
Epoch 134/200
1493/1493 - 95s - loss: 1.0103e-04 - val_loss: 9.6400e-05 - 95s/epoch - 64ms/step
Epoch 135/200
1493/1493 - 95s - loss: 1.0099e-04 - val_loss: 9.2860e-05 - 95s/epoch - 64ms/step
Epoch 136/200
1493/1493 - 95s - loss: 9.9720e-05 - val_loss: 1.1135e-04 - 95s/epoch - 64ms/step
Epoch 137/200
1493/1493 - 95s - loss: 1.0410e-04 - val_loss: 1.0839e-04 - 95s/epoch - 64ms/step
Epoch 138/200
1493/1493 - 95s - loss: 9.9805e-05 - val_loss: 1.4176e-04 - 95s/epoch - 64ms/step
Epoch 139/200
1493/1493 - 95s - loss: 1.0555e-04 - val_loss: 1.4674e-04 - 95s/epoch - 64ms/step
Epoch 140/200
1493/1493 - 95s - loss: 1.1418e-04 - val_loss: 8.6930e-05 - 95s/epoch - 64ms/step
Epoch 141/200
1493/1493 - 95s - loss: 9.8917e-05 - val_loss: 1.2232e-04 - 95s/epoch - 64ms/step
Epoch 142/200
1493/1493 - 95s - loss: 1.0110e-04 - val_loss: 1.0821e-04 - 95s/epoch - 64ms/step
Epoch 143/200
1493/1493 - 95s - loss: 9.6376e-05 - val_loss: 1.3752e-04 - 95s/epoch - 64ms/step
Epoch 144/200
1493/1493 - 95s - loss: 9.8318e-05 - val_loss: 1.0120e-04 - 95s/epoch - 64ms/step
Epoch 145/200
1493/1493 - 95s - loss: 9.6117e-05 - val_loss: 9.1598e-05 - 95s/epoch - 64ms/step
Epoch 146/200
1493/1493 - 95s - loss: 9.4812e-05 - val_loss: 8.7189e-05 - 95s/epoch - 64ms/step
Epoch 147/200
1493/1493 - 95s - loss: 9.3318e-05 - val_loss: 9.6596e-05 - 95s/epoch - 64ms/step
Epoch 148/200
1493/1493 - 95s - loss: 9.3459e-05 - val_loss: 1.6064e-04 - 95s/epoch - 64ms/step
Epoch 149/200
1493/1493 - 95s - loss: 9.3559e-05 - val_loss: 9.5337e-05 - 95s/epoch - 64ms/step
Epoch 150/200
1493/1493 - 95s - loss: 9.2704e-05 - val_loss: 8.8819e-05 - 95s/epoch - 64ms/step
Epoch 151/200
1493/1493 - 95s - loss: 9.4923e-05 - val_loss: 1.4206e-04 - 95s/epoch - 64ms/step
Epoch 152/200
1493/1493 - 95s - loss: 9.7630e-05 - val_loss: 1.6673e-04 - 95s/epoch - 64ms/step
Epoch 153/200
1493/1493 - 95s - loss: 9.9535e-05 - val_loss: 9.7447e-05 - 95s/epoch - 64ms/step
Epoch 154/200
1493/1493 - 95s - loss: 9.1753e-05 - val_loss: 9.5971e-05 - 95s/epoch - 64ms/step
Epoch 155/200
1493/1493 - 95s - loss: 8.9972e-05 - val_loss: 9.2758e-05 - 95s/epoch - 64ms/step
Epoch 156/200
1493/1493 - 95s - loss: 9.3184e-05 - val_loss: 9.2589e-05 - 95s/epoch - 64ms/step
Epoch 157/200
1493/1493 - 95s - loss: 8.9607e-05 - val_loss: 1.1133e-04 - 95s/epoch - 64ms/step
Epoch 158/200
1493/1493 - 96s - loss: 9.1347e-05 - val_loss: 1.0464e-04 - 96s/epoch - 64ms/step
Epoch 159/200
1493/1493 - 95s - loss: 8.9433e-05 - val_loss: 8.9634e-05 - 95s/epoch - 64ms/step
Epoch 160/200
1493/1493 - 95s - loss: 9.2865e-05 - val_loss: 1.7265e-04 - 95s/epoch - 64ms/step
Epoch 161/200
1493/1493 - 95s - loss: 1.3670e-04 - val_loss: 8.6208e-05 - 95s/epoch - 64ms/step
Epoch 162/200
1493/1493 - 95s - loss: 9.4289e-05 - val_loss: 8.2187e-05 - 95s/epoch - 64ms/step
Epoch 163/200
1493/1493 - 95s - loss: 9.1322e-05 - val_loss: 1.1804e-04 - 95s/epoch - 64ms/step
Epoch 164/200
1493/1493 - 95s - loss: 9.5306e-05 - val_loss: 9.6457e-05 - 95s/epoch - 64ms/step
Epoch 165/200
1493/1493 - 95s - loss: 1.0700e-04 - val_loss: 8.4321e-05 - 95s/epoch - 64ms/step
Epoch 166/200
1493/1493 - 95s - loss: 9.1380e-05 - val_loss: 9.1064e-05 - 95s/epoch - 64ms/step
Epoch 167/200
1493/1493 - 95s - loss: 8.9888e-05 - val_loss: 7.5082e-05 - 95s/epoch - 64ms/step
Epoch 168/200
1493/1493 - 95s - loss: 8.8835e-05 - val_loss: 9.1110e-05 - 95s/epoch - 64ms/step
Epoch 169/200
1493/1493 - 95s - loss: 8.8444e-05 - val_loss: 9.1252e-05 - 95s/epoch - 64ms/step
Epoch 170/200
1493/1493 - 95s - loss: 8.7520e-05 - val_loss: 1.2758e-04 - 95s/epoch - 64ms/step
Epoch 171/200
1493/1493 - 95s - loss: 1.0078e-04 - val_loss: 4.0392e-04 - 95s/epoch - 64ms/step
Epoch 172/200
1493/1493 - 95s - loss: 1.2817e-04 - val_loss: 1.6568e-04 - 95s/epoch - 64ms/step
Epoch 173/200
1493/1493 - 95s - loss: 9.8734e-05 - val_loss: 8.0199e-05 - 95s/epoch - 64ms/step
Epoch 174/200
1493/1493 - 95s - loss: 8.9098e-05 - val_loss: 8.7098e-05 - 95s/epoch - 64ms/step
Epoch 175/200
1493/1493 - 95s - loss: 8.9029e-05 - val_loss: 1.0162e-04 - 95s/epoch - 64ms/step
Epoch 176/200
1493/1493 - 95s - loss: 8.9514e-05 - val_loss: 9.1160e-05 - 95s/epoch - 64ms/step
Epoch 177/200
1493/1493 - 95s - loss: 9.0052e-05 - val_loss: 7.8125e-05 - 95s/epoch - 64ms/step
Epoch 178/200
1493/1493 - 95s - loss: 8.7430e-05 - val_loss: 7.9762e-05 - 95s/epoch - 64ms/step
Epoch 179/200
1493/1493 - 95s - loss: 8.5577e-05 - val_loss: 9.2814e-05 - 95s/epoch - 64ms/step
Epoch 180/200
1493/1493 - 95s - loss: 8.5551e-05 - val_loss: 8.8515e-05 - 95s/epoch - 64ms/step
Epoch 181/200
1493/1493 - 95s - loss: 8.5901e-05 - val_loss: 8.3715e-05 - 95s/epoch - 64ms/step
Epoch 182/200
1493/1493 - 95s - loss: 8.5224e-05 - val_loss: 1.1192e-04 - 95s/epoch - 64ms/step
Epoch 183/200
1493/1493 - 95s - loss: 8.4796e-05 - val_loss: 9.6764e-05 - 95s/epoch - 64ms/step
Epoch 184/200
1493/1493 - 95s - loss: 8.5223e-05 - val_loss: 8.2589e-05 - 95s/epoch - 64ms/step
Epoch 185/200
1493/1493 - 95s - loss: 8.2564e-05 - val_loss: 1.0878e-04 - 95s/epoch - 64ms/step
Epoch 186/200
1493/1493 - 95s - loss: 8.4368e-05 - val_loss: 8.5133e-05 - 95s/epoch - 64ms/step
Epoch 187/200
1493/1493 - 95s - loss: 8.2890e-05 - val_loss: 9.9874e-05 - 95s/epoch - 64ms/step
Epoch 188/200
1493/1493 - 95s - loss: 9.2531e-05 - val_loss: 9.0768e-05 - 95s/epoch - 64ms/step
Epoch 189/200
1493/1493 - 95s - loss: 8.5113e-05 - val_loss: 9.6067e-05 - 95s/epoch - 64ms/step
Epoch 190/200
1493/1493 - 95s - loss: 8.4202e-05 - val_loss: 9.5211e-05 - 95s/epoch - 64ms/step
Epoch 191/200
1493/1493 - 95s - loss: 8.5294e-05 - val_loss: 1.1702e-04 - 95s/epoch - 64ms/step
Epoch 192/200
1493/1493 - 95s - loss: 8.5122e-05 - val_loss: 1.1558e-04 - 95s/epoch - 64ms/step
Epoch 193/200
1493/1493 - 95s - loss: 8.9293e-05 - val_loss: 2.4090e-04 - 95s/epoch - 64ms/step
Epoch 194/200
1493/1493 - 95s - loss: 1.2060e-04 - val_loss: 5.3507e-04 - 95s/epoch - 64ms/step
Epoch 195/200
1493/1493 - 95s - loss: 1.6243e-04 - val_loss: 3.8058e-04 - 95s/epoch - 64ms/step
Epoch 196/200
1493/1493 - 95s - loss: 1.3599e-04 - val_loss: 7.6919e-05 - 95s/epoch - 64ms/step
Epoch 197/200
1493/1493 - 95s - loss: 9.2779e-05 - val_loss: 1.1802e-04 - 95s/epoch - 64ms/step
Epoch 198/200
1493/1493 - 95s - loss: 9.6228e-05 - val_loss: 1.4219e-04 - 95s/epoch - 64ms/step
Epoch 199/200
1493/1493 - 95s - loss: 9.4103e-05 - val_loss: 7.6245e-05 - 95s/epoch - 64ms/step
Epoch 200/200
1493/1493 - 95s - loss: 8.5870e-05 - val_loss: 8.0367e-05 - 95s/epoch - 64ms/step
COMPRESSED VECTOR SIZE: 1011
Loss in the autoencoder: 8.036670624278486e-05
  1/332 [..............................] - ETA: 26s  8/332 [..............................] - ETA: 2s  15/332 [>.............................] - ETA: 2s 22/332 [>.............................] - ETA: 2s 29/332 [=>............................] - ETA: 2s 36/332 [==>...........................] - ETA: 2s 43/332 [==>...........................] - ETA: 2s 50/332 [===>..........................] - ETA: 2s 57/332 [====>.........................] - ETA: 2s 64/332 [====>.........................] - ETA: 2s 71/332 [=====>........................] - ETA: 2s 78/332 [======>.......................] - ETA: 1s 85/332 [======>.......................] - ETA: 1s 92/332 [=======>......................] - ETA: 1s 99/332 [=======>......................] - ETA: 1s106/332 [========>.....................] - ETA: 1s113/332 [=========>....................] - ETA: 1s120/332 [=========>....................] - ETA: 1s127/332 [==========>...................] - ETA: 1s134/332 [===========>..................] - ETA: 1s141/332 [===========>..................] - ETA: 1s148/332 [============>.................] - ETA: 1s155/332 [=============>................] - ETA: 1s162/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s176/332 [==============>...............] - ETA: 1s183/332 [===============>..............] - ETA: 1s190/332 [================>.............] - ETA: 1s197/332 [================>.............] - ETA: 1s204/332 [=================>............] - ETA: 0s211/332 [==================>...........] - ETA: 0s218/332 [==================>...........] - ETA: 0s225/332 [===================>..........] - ETA: 0s232/332 [===================>..........] - ETA: 0s239/332 [====================>.........] - ETA: 0s246/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s260/332 [======================>.......] - ETA: 0s267/332 [=======================>......] - ETA: 0s274/332 [=======================>......] - ETA: 0s281/332 [========================>.....] - ETA: 0s288/332 [=========================>....] - ETA: 0s295/332 [=========================>....] - ETA: 0s302/332 [==========================>...] - ETA: 0s309/332 [==========================>...] - ETA: 0s316/332 [===========================>..] - ETA: 0s323/332 [============================>.] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 8ms/step
correlation 0.0009083637776035118
cosine 0.0007162429659132053
MAE: 0.0050101033
RMSE: 0.008964744
r2: 0.9947873550666143
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_12 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1011)              2811591   
                                                                 
 batch_normalization_13 (Bat  (None, 1011)             4044      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 1011)              0         
                                                                 
 dense_13 (Dense)            (None, 2780)              2813360   
                                                                 
 batch_normalization_14 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2780)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 12,683,119
Trainable params: 12,669,977
Non-trainable params: 13,142
_________________________________________________________________
Encoder
Model: "model_13"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_14 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_13 (InputLayer)       multiple                  0         
                                                                 
 dense_12 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_12 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_12 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 1011)              2811591   
                                                                 
=================================================================
Total params: 6,339,411
Trainable params: 6,333,851
Non-trainable params: 5,560
_________________________________________________________________
Decoder
Model: "model_14"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_15 (InputLayer)       [(None, 1011)]            0         
                                                                 
 batch_normalization_13 (Bat  (None, 1011)             4044      
 chNormalization)                                                
                                                                 
 re_lu_13 (ReLU)             (None, 1011)              0         
                                                                 
 dense_13 (Dense)            (None, 2780)              2813360   
                                                                 
 batch_normalization_14 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_14 (ReLU)             (None, 2780)              0         
                                                                 
 dense_14 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 6,343,708
Trainable params: 6,336,126
Non-trainable params: 7,582
_________________________________________________________________
['2.2custom_n_b', 'mse', 64, 200, 0.0005, 0.8, 1011, 8.587007323512807e-05, 8.036670624278486e-05, 0.0009083637776035118, 0.0007162429659132053, 0.005010103341192007, 0.00896474439650774, 0.9947873550666143, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_2.2final_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_15 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_15 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 947)               2633607   
                                                                 
 batch_normalization_16 (Bat  (None, 947)              3788      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 947)               0         
                                                                 
 dense_16 (Dense)            (None, 2780)              2635440   
                                                                 
 batch_normalization_17 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2780)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 12,326,959
Trainable params: 12,313,945
Non-trainable params: 13,014
_________________________________________________________________
Epoch 1/200
1493/1493 - 93s - loss: 0.0091 - val_loss: 0.0036 - 93s/epoch - 63ms/step
Epoch 2/200
1493/1493 - 93s - loss: 0.0028 - val_loss: 0.0038 - 93s/epoch - 62ms/step
Epoch 3/200
1493/1493 - 93s - loss: 0.0021 - val_loss: 0.0015 - 93s/epoch - 62ms/step
Epoch 4/200
1493/1493 - 93s - loss: 0.0016 - val_loss: 0.0040 - 93s/epoch - 62ms/step
Epoch 5/200
1493/1493 - 93s - loss: 0.0016 - val_loss: 0.0014 - 93s/epoch - 62ms/step
Epoch 6/200
1493/1493 - 93s - loss: 0.0014 - val_loss: 0.0013 - 93s/epoch - 62ms/step
Epoch 7/200
1493/1493 - 93s - loss: 0.0013 - val_loss: 0.0013 - 93s/epoch - 62ms/step
Epoch 8/200
1493/1493 - 93s - loss: 0.0011 - val_loss: 9.4076e-04 - 93s/epoch - 62ms/step
Epoch 9/200
1493/1493 - 93s - loss: 0.0010 - val_loss: 0.0012 - 93s/epoch - 62ms/step
Epoch 10/200
1493/1493 - 93s - loss: 9.4543e-04 - val_loss: 8.1538e-04 - 93s/epoch - 62ms/step
Epoch 11/200
1493/1493 - 93s - loss: 8.7944e-04 - val_loss: 6.5515e-04 - 93s/epoch - 62ms/step
Epoch 12/200
1493/1493 - 93s - loss: 7.0687e-04 - val_loss: 0.0024 - 93s/epoch - 62ms/step
Epoch 13/200
1493/1493 - 93s - loss: 7.6886e-04 - val_loss: 0.0017 - 93s/epoch - 62ms/step
Epoch 14/200
1493/1493 - 93s - loss: 7.1180e-04 - val_loss: 5.6357e-04 - 93s/epoch - 62ms/step
Epoch 15/200
1493/1493 - 93s - loss: 5.9790e-04 - val_loss: 6.1861e-04 - 93s/epoch - 62ms/step
Epoch 16/200
1493/1493 - 93s - loss: 5.5749e-04 - val_loss: 5.1643e-04 - 93s/epoch - 62ms/step
Epoch 17/200
1493/1493 - 93s - loss: 4.9954e-04 - val_loss: 7.7669e-04 - 93s/epoch - 62ms/step
Epoch 18/200
1493/1493 - 93s - loss: 5.0501e-04 - val_loss: 6.7999e-04 - 93s/epoch - 62ms/step
Epoch 19/200
1493/1493 - 93s - loss: 4.5942e-04 - val_loss: 5.2600e-04 - 93s/epoch - 62ms/step
Epoch 20/200
1493/1493 - 93s - loss: 4.3172e-04 - val_loss: 4.3017e-04 - 93s/epoch - 62ms/step
Epoch 21/200
1493/1493 - 93s - loss: 4.0882e-04 - val_loss: 4.6208e-04 - 93s/epoch - 62ms/step
Epoch 22/200
1493/1493 - 93s - loss: 3.7909e-04 - val_loss: 6.4690e-04 - 93s/epoch - 62ms/step
Epoch 23/200
1493/1493 - 93s - loss: 4.0700e-04 - val_loss: 6.6531e-04 - 93s/epoch - 62ms/step
Epoch 24/200
1493/1493 - 93s - loss: 3.7427e-04 - val_loss: 3.1902e-04 - 93s/epoch - 62ms/step
Epoch 25/200
1493/1493 - 93s - loss: 3.3601e-04 - val_loss: 3.0327e-04 - 93s/epoch - 62ms/step
Epoch 26/200
1493/1493 - 93s - loss: 3.1642e-04 - val_loss: 3.5768e-04 - 93s/epoch - 62ms/step
Epoch 27/200
1493/1493 - 93s - loss: 3.1188e-04 - val_loss: 3.0562e-04 - 93s/epoch - 62ms/step
Epoch 28/200
1493/1493 - 93s - loss: 2.9015e-04 - val_loss: 6.7919e-04 - 93s/epoch - 62ms/step
Epoch 29/200
1493/1493 - 93s - loss: 2.9119e-04 - val_loss: 3.1284e-04 - 93s/epoch - 62ms/step
Epoch 30/200
1493/1493 - 93s - loss: 2.7599e-04 - val_loss: 3.5704e-04 - 93s/epoch - 62ms/step
Epoch 31/200
1493/1493 - 93s - loss: 2.6598e-04 - val_loss: 4.4633e-04 - 93s/epoch - 62ms/step
Epoch 32/200
1493/1493 - 93s - loss: 3.0263e-04 - val_loss: 3.5586e-04 - 93s/epoch - 62ms/step
Epoch 33/200
1493/1493 - 93s - loss: 2.7271e-04 - val_loss: 2.7375e-04 - 93s/epoch - 62ms/step
Epoch 34/200
1493/1493 - 93s - loss: 2.4675e-04 - val_loss: 2.2573e-04 - 93s/epoch - 62ms/step
Epoch 35/200
1493/1493 - 93s - loss: 2.3868e-04 - val_loss: 4.2607e-04 - 93s/epoch - 62ms/step
Epoch 36/200
1493/1493 - 93s - loss: 2.4747e-04 - val_loss: 2.6933e-04 - 93s/epoch - 62ms/step
Epoch 37/200
1493/1493 - 93s - loss: 2.3214e-04 - val_loss: 3.0420e-04 - 93s/epoch - 62ms/step
Epoch 38/200
1493/1493 - 93s - loss: 2.4349e-04 - val_loss: 2.2712e-04 - 93s/epoch - 62ms/step
Epoch 39/200
1493/1493 - 93s - loss: 2.1911e-04 - val_loss: 2.1825e-04 - 93s/epoch - 62ms/step
Epoch 40/200
1493/1493 - 93s - loss: 2.1215e-04 - val_loss: 3.1143e-04 - 93s/epoch - 62ms/step
Epoch 41/200
1493/1493 - 93s - loss: 2.1517e-04 - val_loss: 1.9264e-04 - 93s/epoch - 62ms/step
Epoch 42/200
1493/1493 - 93s - loss: 2.0587e-04 - val_loss: 2.0714e-04 - 93s/epoch - 62ms/step
Epoch 43/200
1493/1493 - 93s - loss: 2.0309e-04 - val_loss: 4.4813e-04 - 93s/epoch - 62ms/step
Epoch 44/200
1493/1493 - 93s - loss: 2.2708e-04 - val_loss: 4.5400e-04 - 93s/epoch - 62ms/step
Epoch 45/200
1493/1493 - 93s - loss: 2.2238e-04 - val_loss: 1.6596e-04 - 93s/epoch - 62ms/step
Epoch 46/200
1493/1493 - 93s - loss: 1.9566e-04 - val_loss: 1.8198e-04 - 93s/epoch - 62ms/step
Epoch 47/200
1493/1493 - 93s - loss: 1.9567e-04 - val_loss: 2.3117e-04 - 93s/epoch - 62ms/step
Epoch 48/200
1493/1493 - 93s - loss: 1.9080e-04 - val_loss: 2.4256e-04 - 93s/epoch - 62ms/step
Epoch 49/200
1493/1493 - 93s - loss: 1.8965e-04 - val_loss: 7.9209e-04 - 93s/epoch - 62ms/step
Epoch 50/200
1493/1493 - 93s - loss: 2.4198e-04 - val_loss: 0.0013 - 93s/epoch - 62ms/step
Epoch 51/200
1493/1493 - 93s - loss: 2.5735e-04 - val_loss: 2.0486e-04 - 93s/epoch - 62ms/step
Epoch 52/200
1493/1493 - 93s - loss: 1.8332e-04 - val_loss: 1.7576e-04 - 93s/epoch - 62ms/step
Epoch 53/200
1493/1493 - 93s - loss: 1.8093e-04 - val_loss: 1.6982e-04 - 93s/epoch - 62ms/step
Epoch 54/200
1493/1493 - 93s - loss: 1.7564e-04 - val_loss: 2.1731e-04 - 93s/epoch - 62ms/step
Epoch 55/200
1493/1493 - 93s - loss: 1.6913e-04 - val_loss: 1.7789e-04 - 93s/epoch - 62ms/step
Epoch 56/200
1493/1493 - 93s - loss: 1.6897e-04 - val_loss: 2.0119e-04 - 93s/epoch - 62ms/step
Epoch 57/200
1493/1493 - 93s - loss: 1.6528e-04 - val_loss: 1.8146e-04 - 93s/epoch - 62ms/step
Epoch 58/200
1493/1493 - 93s - loss: 1.6353e-04 - val_loss: 1.5721e-04 - 93s/epoch - 62ms/step
Epoch 59/200
1493/1493 - 93s - loss: 1.5959e-04 - val_loss: 1.7058e-04 - 93s/epoch - 62ms/step
Epoch 60/200
1493/1493 - 93s - loss: 1.5808e-04 - val_loss: 2.3252e-04 - 93s/epoch - 62ms/step
Epoch 61/200
1493/1493 - 93s - loss: 1.8069e-04 - val_loss: 1.5364e-04 - 93s/epoch - 62ms/step
Epoch 62/200
1493/1493 - 93s - loss: 1.5485e-04 - val_loss: 2.0023e-04 - 93s/epoch - 62ms/step
Epoch 63/200
1493/1493 - 93s - loss: 1.5386e-04 - val_loss: 2.0306e-04 - 93s/epoch - 62ms/step
Epoch 64/200
1493/1493 - 93s - loss: 1.5738e-04 - val_loss: 0.0021 - 93s/epoch - 62ms/step
Epoch 65/200
1493/1493 - 93s - loss: 3.3136e-04 - val_loss: 2.7150e-04 - 93s/epoch - 62ms/step
Epoch 66/200
1493/1493 - 93s - loss: 1.8110e-04 - val_loss: 1.9733e-04 - 93s/epoch - 62ms/step
Epoch 67/200
1493/1493 - 93s - loss: 1.5906e-04 - val_loss: 1.3235e-04 - 93s/epoch - 62ms/step
Epoch 68/200
1493/1493 - 93s - loss: 1.5645e-04 - val_loss: 7.3741e-04 - 93s/epoch - 62ms/step
Epoch 69/200
1493/1493 - 93s - loss: 2.7105e-04 - val_loss: 1.8015e-04 - 93s/epoch - 62ms/step
Epoch 70/200
1493/1493 - 93s - loss: 1.6540e-04 - val_loss: 2.1449e-04 - 93s/epoch - 62ms/step
Epoch 71/200
1493/1493 - 93s - loss: 1.5840e-04 - val_loss: 1.4286e-04 - 93s/epoch - 62ms/step
Epoch 72/200
1493/1493 - 93s - loss: 1.5249e-04 - val_loss: 2.1861e-04 - 93s/epoch - 62ms/step
Epoch 73/200
1493/1493 - 93s - loss: 1.5512e-04 - val_loss: 3.2874e-04 - 93s/epoch - 62ms/step
Epoch 74/200
1493/1493 - 93s - loss: 1.6519e-04 - val_loss: 1.4662e-04 - 93s/epoch - 62ms/step
Epoch 75/200
1493/1493 - 93s - loss: 1.4283e-04 - val_loss: 1.3863e-04 - 93s/epoch - 62ms/step
Epoch 76/200
1493/1493 - 93s - loss: 1.3991e-04 - val_loss: 1.3884e-04 - 93s/epoch - 62ms/step
Epoch 77/200
1493/1493 - 93s - loss: 1.3713e-04 - val_loss: 6.0936e-04 - 93s/epoch - 62ms/step
Epoch 78/200
1493/1493 - 93s - loss: 1.6315e-04 - val_loss: 1.4527e-04 - 93s/epoch - 62ms/step
Epoch 79/200
1493/1493 - 93s - loss: 1.3841e-04 - val_loss: 1.2897e-04 - 93s/epoch - 62ms/step
Epoch 80/200
1493/1493 - 93s - loss: 1.3440e-04 - val_loss: 1.4087e-04 - 93s/epoch - 62ms/step
Epoch 81/200
1493/1493 - 93s - loss: 1.3351e-04 - val_loss: 3.7974e-04 - 93s/epoch - 62ms/step
Epoch 82/200
1493/1493 - 93s - loss: 1.3636e-04 - val_loss: 1.2960e-04 - 93s/epoch - 62ms/step
Epoch 83/200
1493/1493 - 93s - loss: 1.2900e-04 - val_loss: 1.4436e-04 - 93s/epoch - 62ms/step
Epoch 84/200
1493/1493 - 93s - loss: 1.2813e-04 - val_loss: 1.2954e-04 - 93s/epoch - 62ms/step
Epoch 85/200
1493/1493 - 93s - loss: 1.2748e-04 - val_loss: 1.5597e-04 - 93s/epoch - 62ms/step
Epoch 86/200
1493/1493 - 93s - loss: 1.2814e-04 - val_loss: 1.4599e-04 - 93s/epoch - 62ms/step
Epoch 87/200
1493/1493 - 93s - loss: 1.2694e-04 - val_loss: 1.3503e-04 - 93s/epoch - 62ms/step
Epoch 88/200
1493/1493 - 93s - loss: 1.2699e-04 - val_loss: 1.5308e-04 - 93s/epoch - 62ms/step
Epoch 89/200
1493/1493 - 93s - loss: 1.2736e-04 - val_loss: 1.3008e-04 - 93s/epoch - 62ms/step
Epoch 90/200
1493/1493 - 93s - loss: 1.2213e-04 - val_loss: 1.3653e-04 - 93s/epoch - 62ms/step
Epoch 91/200
1493/1493 - 93s - loss: 1.2123e-04 - val_loss: 1.4213e-04 - 93s/epoch - 62ms/step
Epoch 92/200
1493/1493 - 93s - loss: 1.2166e-04 - val_loss: 6.6803e-04 - 93s/epoch - 62ms/step
Epoch 93/200
1493/1493 - 93s - loss: 1.9041e-04 - val_loss: 0.0011 - 93s/epoch - 62ms/step
Epoch 94/200
1493/1493 - 93s - loss: 2.1958e-04 - val_loss: 1.1110e-04 - 93s/epoch - 62ms/step
Epoch 95/200
1493/1493 - 93s - loss: 1.3108e-04 - val_loss: 1.1668e-04 - 93s/epoch - 62ms/step
Epoch 96/200
1493/1493 - 93s - loss: 1.2606e-04 - val_loss: 1.4556e-04 - 93s/epoch - 62ms/step
Epoch 97/200
1493/1493 - 93s - loss: 1.2489e-04 - val_loss: 1.3475e-04 - 93s/epoch - 62ms/step
Epoch 98/200
1493/1493 - 93s - loss: 1.2435e-04 - val_loss: 5.9456e-04 - 93s/epoch - 62ms/step
Epoch 99/200
1493/1493 - 92s - loss: 1.9193e-04 - val_loss: 1.0545e-04 - 92s/epoch - 62ms/step
Epoch 100/200
1493/1493 - 93s - loss: 1.3280e-04 - val_loss: 1.0574e-04 - 93s/epoch - 62ms/step
Epoch 101/200
1493/1493 - 93s - loss: 1.2412e-04 - val_loss: 1.8403e-04 - 93s/epoch - 62ms/step
Epoch 102/200
1493/1493 - 93s - loss: 1.3242e-04 - val_loss: 1.8412e-04 - 93s/epoch - 62ms/step
Epoch 103/200
1493/1493 - 93s - loss: 1.2871e-04 - val_loss: 1.1647e-04 - 93s/epoch - 62ms/step
Epoch 104/200
1493/1493 - 93s - loss: 1.1717e-04 - val_loss: 2.5125e-04 - 93s/epoch - 62ms/step
Epoch 105/200
1493/1493 - 93s - loss: 1.2521e-04 - val_loss: 1.0760e-04 - 93s/epoch - 62ms/step
Epoch 106/200
1493/1493 - 93s - loss: 1.1502e-04 - val_loss: 1.2839e-04 - 93s/epoch - 62ms/step
Epoch 107/200
1493/1493 - 93s - loss: 1.1345e-04 - val_loss: 1.2180e-04 - 93s/epoch - 62ms/step
Epoch 108/200
1493/1493 - 93s - loss: 1.1571e-04 - val_loss: 1.2771e-04 - 93s/epoch - 62ms/step
Epoch 109/200
1493/1493 - 93s - loss: 1.1262e-04 - val_loss: 1.4147e-04 - 93s/epoch - 62ms/step
Epoch 110/200
1493/1493 - 93s - loss: 1.1060e-04 - val_loss: 1.1857e-04 - 93s/epoch - 62ms/step
Epoch 111/200
1493/1493 - 93s - loss: 1.0957e-04 - val_loss: 1.1737e-04 - 93s/epoch - 62ms/step
Epoch 112/200
1493/1493 - 93s - loss: 1.0992e-04 - val_loss: 1.1058e-04 - 93s/epoch - 62ms/step
Epoch 113/200
1493/1493 - 93s - loss: 1.0644e-04 - val_loss: 1.7043e-04 - 93s/epoch - 62ms/step
Epoch 114/200
1493/1493 - 93s - loss: 1.1781e-04 - val_loss: 9.8172e-05 - 93s/epoch - 62ms/step
Epoch 115/200
1493/1493 - 93s - loss: 1.0740e-04 - val_loss: 2.0538e-04 - 93s/epoch - 62ms/step
Epoch 116/200
1493/1493 - 93s - loss: 1.1046e-04 - val_loss: 1.4163e-04 - 93s/epoch - 62ms/step
Epoch 117/200
1493/1493 - 93s - loss: 1.0485e-04 - val_loss: 1.8377e-04 - 93s/epoch - 62ms/step
Epoch 118/200
1493/1493 - 93s - loss: 1.1162e-04 - val_loss: 1.1090e-04 - 93s/epoch - 62ms/step
Epoch 119/200
1493/1493 - 93s - loss: 1.0548e-04 - val_loss: 1.0756e-04 - 93s/epoch - 62ms/step
Epoch 120/200
1493/1493 - 93s - loss: 1.0405e-04 - val_loss: 1.1745e-04 - 93s/epoch - 62ms/step
Epoch 121/200
1493/1493 - 93s - loss: 1.0377e-04 - val_loss: 1.9099e-04 - 93s/epoch - 62ms/step
Epoch 122/200
1493/1493 - 93s - loss: 1.0839e-04 - val_loss: 9.1587e-05 - 93s/epoch - 62ms/step
Epoch 123/200
1493/1493 - 93s - loss: 1.0170e-04 - val_loss: 1.1549e-04 - 93s/epoch - 62ms/step
Epoch 124/200
1493/1493 - 93s - loss: 1.0366e-04 - val_loss: 4.6269e-04 - 93s/epoch - 62ms/step
Epoch 125/200
1493/1493 - 93s - loss: 1.6647e-04 - val_loss: 2.2685e-04 - 93s/epoch - 62ms/step
Epoch 126/200
1493/1493 - 93s - loss: 1.1744e-04 - val_loss: 1.4085e-04 - 93s/epoch - 62ms/step
Epoch 127/200
1493/1493 - 93s - loss: 1.0898e-04 - val_loss: 8.9172e-04 - 93s/epoch - 62ms/step
Epoch 128/200
1493/1493 - 93s - loss: 1.7635e-04 - val_loss: 9.7214e-05 - 93s/epoch - 62ms/step
Epoch 129/200
1493/1493 - 93s - loss: 1.1098e-04 - val_loss: 9.8647e-05 - 93s/epoch - 62ms/step
Epoch 130/200
1493/1493 - 93s - loss: 1.0503e-04 - val_loss: 9.6697e-05 - 93s/epoch - 62ms/step
Epoch 131/200
1493/1493 - 93s - loss: 1.0634e-04 - val_loss: 4.0626e-04 - 93s/epoch - 62ms/step
Epoch 132/200
1493/1493 - 93s - loss: 1.4751e-04 - val_loss: 1.0572e-04 - 93s/epoch - 62ms/step
Epoch 133/200
1493/1493 - 93s - loss: 1.0561e-04 - val_loss: 1.2382e-04 - 93s/epoch - 62ms/step
Epoch 134/200
1493/1493 - 93s - loss: 1.0526e-04 - val_loss: 9.2761e-05 - 93s/epoch - 62ms/step
Epoch 135/200
1493/1493 - 93s - loss: 1.0060e-04 - val_loss: 9.0590e-05 - 93s/epoch - 62ms/step
Epoch 136/200
1493/1493 - 93s - loss: 9.9628e-05 - val_loss: 9.3630e-05 - 93s/epoch - 62ms/step
Epoch 137/200
1493/1493 - 93s - loss: 1.0216e-04 - val_loss: 1.4039e-04 - 93s/epoch - 62ms/step
Epoch 138/200
1493/1493 - 93s - loss: 1.0282e-04 - val_loss: 1.3657e-04 - 93s/epoch - 62ms/step
Epoch 139/200
1493/1493 - 93s - loss: 1.0678e-04 - val_loss: 1.5338e-04 - 93s/epoch - 62ms/step
Epoch 140/200
1493/1493 - 93s - loss: 1.0634e-04 - val_loss: 8.3727e-05 - 93s/epoch - 62ms/step
Epoch 141/200
1493/1493 - 93s - loss: 9.7161e-05 - val_loss: 1.1927e-04 - 93s/epoch - 62ms/step
Epoch 142/200
1493/1493 - 93s - loss: 9.9940e-05 - val_loss: 1.1335e-04 - 93s/epoch - 62ms/step
Epoch 143/200
1493/1493 - 93s - loss: 9.5349e-05 - val_loss: 1.2509e-04 - 93s/epoch - 62ms/step
Epoch 144/200
1493/1493 - 92s - loss: 9.8426e-05 - val_loss: 1.0223e-04 - 92s/epoch - 62ms/step
Epoch 145/200
1493/1493 - 93s - loss: 9.4642e-05 - val_loss: 8.9801e-05 - 93s/epoch - 62ms/step
Epoch 146/200
1493/1493 - 93s - loss: 9.3813e-05 - val_loss: 1.0351e-04 - 93s/epoch - 62ms/step
Epoch 147/200
1493/1493 - 93s - loss: 9.3075e-05 - val_loss: 1.0028e-04 - 93s/epoch - 62ms/step
Epoch 148/200
1493/1493 - 93s - loss: 9.2633e-05 - val_loss: 1.7250e-04 - 93s/epoch - 62ms/step
Epoch 149/200
1493/1493 - 93s - loss: 9.2524e-05 - val_loss: 9.4952e-05 - 93s/epoch - 62ms/step
Epoch 150/200
1493/1493 - 93s - loss: 9.1610e-05 - val_loss: 8.2995e-05 - 93s/epoch - 62ms/step
Epoch 151/200
1493/1493 - 93s - loss: 9.2901e-05 - val_loss: 1.2550e-04 - 93s/epoch - 62ms/step
Epoch 152/200
1493/1493 - 93s - loss: 9.2720e-05 - val_loss: 1.2834e-04 - 93s/epoch - 62ms/step
Epoch 153/200
1493/1493 - 93s - loss: 9.2660e-05 - val_loss: 9.6779e-05 - 93s/epoch - 62ms/step
Epoch 154/200
1493/1493 - 93s - loss: 9.0308e-05 - val_loss: 9.8062e-05 - 93s/epoch - 62ms/step
Epoch 155/200
1493/1493 - 93s - loss: 9.0445e-05 - val_loss: 9.1194e-05 - 93s/epoch - 62ms/step
Epoch 156/200
1493/1493 - 93s - loss: 9.1935e-05 - val_loss: 9.5728e-05 - 93s/epoch - 62ms/step
Epoch 157/200
1493/1493 - 93s - loss: 8.8706e-05 - val_loss: 9.8514e-05 - 93s/epoch - 62ms/step
Epoch 158/200
1493/1493 - 93s - loss: 8.9246e-05 - val_loss: 1.0076e-04 - 93s/epoch - 62ms/step
Epoch 159/200
1493/1493 - 93s - loss: 8.8458e-05 - val_loss: 8.5259e-05 - 93s/epoch - 62ms/step
Epoch 160/200
1493/1493 - 93s - loss: 8.8573e-05 - val_loss: 1.3008e-04 - 93s/epoch - 62ms/step
Epoch 161/200
1493/1493 - 93s - loss: 1.2674e-04 - val_loss: 8.5940e-05 - 93s/epoch - 62ms/step
Epoch 162/200
1493/1493 - 93s - loss: 8.9880e-05 - val_loss: 8.7753e-05 - 93s/epoch - 62ms/step
Epoch 163/200
1493/1493 - 93s - loss: 8.8347e-05 - val_loss: 9.9210e-05 - 93s/epoch - 62ms/step
Epoch 164/200
1493/1493 - 93s - loss: 9.0764e-05 - val_loss: 9.3542e-05 - 93s/epoch - 62ms/step
Epoch 165/200
1493/1493 - 93s - loss: 9.7199e-05 - val_loss: 9.0633e-05 - 93s/epoch - 62ms/step
Epoch 166/200
1493/1493 - 93s - loss: 8.9564e-05 - val_loss: 1.1004e-04 - 93s/epoch - 62ms/step
Epoch 167/200
1493/1493 - 93s - loss: 8.8311e-05 - val_loss: 7.9464e-05 - 93s/epoch - 62ms/step
Epoch 168/200
1493/1493 - 93s - loss: 8.6779e-05 - val_loss: 8.6213e-05 - 93s/epoch - 62ms/step
Epoch 169/200
1493/1493 - 93s - loss: 8.6832e-05 - val_loss: 1.0299e-04 - 93s/epoch - 62ms/step
Epoch 170/200
1493/1493 - 93s - loss: 8.9818e-05 - val_loss: 3.0067e-04 - 93s/epoch - 62ms/step
Epoch 171/200
1493/1493 - 93s - loss: 1.3914e-04 - val_loss: 2.9468e-04 - 93s/epoch - 62ms/step
Epoch 172/200
1493/1493 - 93s - loss: 1.1359e-04 - val_loss: 1.0122e-04 - 93s/epoch - 62ms/step
Epoch 173/200
1493/1493 - 93s - loss: 9.0884e-05 - val_loss: 8.9740e-05 - 93s/epoch - 62ms/step
Epoch 174/200
1493/1493 - 93s - loss: 8.7461e-05 - val_loss: 8.0683e-05 - 93s/epoch - 62ms/step
Epoch 175/200
1493/1493 - 93s - loss: 8.8707e-05 - val_loss: 1.1285e-04 - 93s/epoch - 62ms/step
Epoch 176/200
1493/1493 - 93s - loss: 8.7777e-05 - val_loss: 8.6170e-05 - 93s/epoch - 62ms/step
Epoch 177/200
1493/1493 - 93s - loss: 8.7467e-05 - val_loss: 7.7046e-05 - 93s/epoch - 62ms/step
Epoch 178/200
1493/1493 - 93s - loss: 8.5408e-05 - val_loss: 8.6909e-05 - 93s/epoch - 62ms/step
Epoch 179/200
1493/1493 - 93s - loss: 8.4326e-05 - val_loss: 1.0194e-04 - 93s/epoch - 62ms/step
Epoch 180/200
1493/1493 - 93s - loss: 8.5276e-05 - val_loss: 9.3361e-05 - 93s/epoch - 62ms/step
Epoch 181/200
1493/1493 - 93s - loss: 8.5261e-05 - val_loss: 8.9357e-05 - 93s/epoch - 62ms/step
Epoch 182/200
1493/1493 - 93s - loss: 8.4634e-05 - val_loss: 1.2117e-04 - 93s/epoch - 62ms/step
Epoch 183/200
1493/1493 - 93s - loss: 8.4787e-05 - val_loss: 8.9331e-05 - 93s/epoch - 62ms/step
Epoch 184/200
1493/1493 - 93s - loss: 8.4270e-05 - val_loss: 8.2731e-05 - 93s/epoch - 62ms/step
Epoch 185/200
1493/1493 - 93s - loss: 8.2148e-05 - val_loss: 1.2685e-04 - 93s/epoch - 62ms/step
Epoch 186/200
1493/1493 - 93s - loss: 8.9254e-05 - val_loss: 8.6908e-05 - 93s/epoch - 62ms/step
Epoch 187/200
1493/1493 - 93s - loss: 8.2664e-05 - val_loss: 1.0923e-04 - 93s/epoch - 62ms/step
Epoch 188/200
1493/1493 - 93s - loss: 8.8158e-05 - val_loss: 9.2091e-05 - 93s/epoch - 62ms/step
Epoch 189/200
1493/1493 - 93s - loss: 8.2870e-05 - val_loss: 9.6653e-05 - 93s/epoch - 62ms/step
Epoch 190/200
1493/1493 - 93s - loss: 8.3713e-05 - val_loss: 9.1901e-05 - 93s/epoch - 62ms/step
Epoch 191/200
1493/1493 - 93s - loss: 8.3568e-05 - val_loss: 1.4658e-04 - 93s/epoch - 62ms/step
Epoch 192/200
1493/1493 - 93s - loss: 8.7812e-05 - val_loss: 9.5289e-05 - 93s/epoch - 62ms/step
Epoch 193/200
1493/1493 - 93s - loss: 8.4604e-05 - val_loss: 2.2050e-04 - 93s/epoch - 62ms/step
Epoch 194/200
1493/1493 - 93s - loss: 1.1431e-04 - val_loss: 4.5353e-04 - 93s/epoch - 62ms/step
Epoch 195/200
1493/1493 - 93s - loss: 1.2287e-04 - val_loss: 2.2337e-04 - 93s/epoch - 62ms/step
Epoch 196/200
1493/1493 - 93s - loss: 1.0972e-04 - val_loss: 7.4210e-05 - 93s/epoch - 62ms/step
Epoch 197/200
1493/1493 - 93s - loss: 8.6803e-05 - val_loss: 2.3582e-04 - 93s/epoch - 62ms/step
Epoch 198/200
1493/1493 - 93s - loss: 9.9155e-05 - val_loss: 1.0735e-04 - 93s/epoch - 62ms/step
Epoch 199/200
1493/1493 - 93s - loss: 8.5949e-05 - val_loss: 7.5547e-05 - 93s/epoch - 62ms/step
Epoch 200/200
1493/1493 - 93s - loss: 8.2481e-05 - val_loss: 8.4011e-05 - 93s/epoch - 62ms/step
COMPRESSED VECTOR SIZE: 947
Loss in the autoencoder: 8.401132799917832e-05
  1/332 [..............................] - ETA: 25s  8/332 [..............................] - ETA: 2s  15/332 [>.............................] - ETA: 2s 22/332 [>.............................] - ETA: 2s 29/332 [=>............................] - ETA: 2s 36/332 [==>...........................] - ETA: 2s 43/332 [==>...........................] - ETA: 2s 50/332 [===>..........................] - ETA: 2s 57/332 [====>.........................] - ETA: 2s 64/332 [====>.........................] - ETA: 1s 71/332 [=====>........................] - ETA: 1s 78/332 [======>.......................] - ETA: 1s 85/332 [======>.......................] - ETA: 1s 92/332 [=======>......................] - ETA: 1s 99/332 [=======>......................] - ETA: 1s106/332 [========>.....................] - ETA: 1s113/332 [=========>....................] - ETA: 1s120/332 [=========>....................] - ETA: 1s127/332 [==========>...................] - ETA: 1s134/332 [===========>..................] - ETA: 1s141/332 [===========>..................] - ETA: 1s148/332 [============>.................] - ETA: 1s155/332 [=============>................] - ETA: 1s162/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s176/332 [==============>...............] - ETA: 1s183/332 [===============>..............] - ETA: 1s190/332 [================>.............] - ETA: 1s197/332 [================>.............] - ETA: 0s204/332 [=================>............] - ETA: 0s211/332 [==================>...........] - ETA: 0s218/332 [==================>...........] - ETA: 0s225/332 [===================>..........] - ETA: 0s232/332 [===================>..........] - ETA: 0s239/332 [====================>.........] - ETA: 0s246/332 [=====================>........] - ETA: 0s253/332 [=====================>........] - ETA: 0s260/332 [======================>.......] - ETA: 0s267/332 [=======================>......] - ETA: 0s274/332 [=======================>......] - ETA: 0s281/332 [========================>.....] - ETA: 0s288/332 [=========================>....] - ETA: 0s295/332 [=========================>....] - ETA: 0s302/332 [==========================>...] - ETA: 0s309/332 [==========================>...] - ETA: 0s316/332 [===========================>..] - ETA: 0s323/332 [============================>.] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 3s 7ms/step
correlation 0.0009526293591378187
cosine 0.0007505394178563767
MAE: 0.00506973
RMSE: 0.009165766
r2: 0.9945508671100357
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_15 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 947)               2633607   
                                                                 
 batch_normalization_16 (Bat  (None, 947)              3788      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 947)               0         
                                                                 
 dense_16 (Dense)            (None, 2780)              2635440   
                                                                 
 batch_normalization_17 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2780)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 12,326,959
Trainable params: 12,313,945
Non-trainable params: 13,014
_________________________________________________________________
Encoder
Model: "model_16"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_17 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_16 (InputLayer)       multiple                  0         
                                                                 
 dense_15 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_15 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_15 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 947)               2633607   
                                                                 
=================================================================
Total params: 6,161,427
Trainable params: 6,155,867
Non-trainable params: 5,560
_________________________________________________________________
Decoder
Model: "model_17"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_18 (InputLayer)       [(None, 947)]             0         
                                                                 
 batch_normalization_16 (Bat  (None, 947)              3788      
 chNormalization)                                                
                                                                 
 re_lu_16 (ReLU)             (None, 947)               0         
                                                                 
 dense_16 (Dense)            (None, 2780)              2635440   
                                                                 
 batch_normalization_17 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_17 (ReLU)             (None, 2780)              0         
                                                                 
 dense_17 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 6,165,532
Trainable params: 6,158,078
Non-trainable params: 7,454
_________________________________________________________________
['2.2custom_n_b', 'mse', 64, 200, 0.0005, 0.75, 947, 8.248145604738966e-05, 8.401132799917832e-05, 0.0009526293591378187, 0.0007505394178563767, 0.005069729872047901, 0.009165765717625618, 0.9945508671100357, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_2.2final_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_18 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_18 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 884)               2458404   
                                                                 
 batch_normalization_19 (Bat  (None, 884)              3536      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 884)               0         
                                                                 
 dense_19 (Dense)            (None, 2780)              2460300   
                                                                 
 batch_normalization_20 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2780)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 11,976,364
Trainable params: 11,963,476
Non-trainable params: 12,888
_________________________________________________________________
Epoch 1/200
1493/1493 - 92s - loss: 0.0090 - val_loss: 0.0039 - 92s/epoch - 61ms/step
Epoch 2/200
1493/1493 - 91s - loss: 0.0028 - val_loss: 0.0043 - 91s/epoch - 61ms/step
Epoch 3/200
1493/1493 - 91s - loss: 0.0021 - val_loss: 0.0015 - 91s/epoch - 61ms/step
Epoch 4/200
1493/1493 - 91s - loss: 0.0017 - val_loss: 0.0040 - 91s/epoch - 61ms/step
Epoch 5/200
1493/1493 - 91s - loss: 0.0017 - val_loss: 0.0016 - 91s/epoch - 61ms/step
Epoch 6/200
1493/1493 - 91s - loss: 0.0014 - val_loss: 0.0014 - 91s/epoch - 61ms/step
Epoch 7/200
1493/1493 - 91s - loss: 0.0013 - val_loss: 0.0013 - 91s/epoch - 61ms/step
Epoch 8/200
1493/1493 - 91s - loss: 0.0011 - val_loss: 8.9878e-04 - 91s/epoch - 61ms/step
Epoch 9/200
1493/1493 - 91s - loss: 0.0010 - val_loss: 0.0012 - 91s/epoch - 61ms/step
Epoch 10/200
1493/1493 - 91s - loss: 9.9797e-04 - val_loss: 0.0011 - 91s/epoch - 61ms/step
Epoch 11/200
1493/1493 - 91s - loss: 8.6586e-04 - val_loss: 7.8359e-04 - 91s/epoch - 61ms/step
Epoch 12/200
1493/1493 - 91s - loss: 7.3430e-04 - val_loss: 9.0731e-04 - 91s/epoch - 61ms/step
Epoch 13/200
1493/1493 - 91s - loss: 7.0311e-04 - val_loss: 0.0020 - 91s/epoch - 61ms/step
Epoch 14/200
1493/1493 - 91s - loss: 7.8538e-04 - val_loss: 5.0041e-04 - 91s/epoch - 61ms/step
Epoch 15/200
1493/1493 - 91s - loss: 6.1339e-04 - val_loss: 6.5250e-04 - 91s/epoch - 61ms/step
Epoch 16/200
1493/1493 - 91s - loss: 5.7026e-04 - val_loss: 4.8879e-04 - 91s/epoch - 61ms/step
Epoch 17/200
1493/1493 - 91s - loss: 5.0915e-04 - val_loss: 5.7585e-04 - 91s/epoch - 61ms/step
Epoch 18/200
1493/1493 - 91s - loss: 4.8317e-04 - val_loss: 6.5998e-04 - 91s/epoch - 61ms/step
Epoch 19/200
1493/1493 - 91s - loss: 4.7628e-04 - val_loss: 6.2653e-04 - 91s/epoch - 61ms/step
Epoch 20/200
1493/1493 - 91s - loss: 4.4754e-04 - val_loss: 4.4301e-04 - 91s/epoch - 61ms/step
Epoch 21/200
1493/1493 - 91s - loss: 4.1143e-04 - val_loss: 5.1026e-04 - 91s/epoch - 61ms/step
Epoch 22/200
1493/1493 - 91s - loss: 3.9287e-04 - val_loss: 4.9423e-04 - 91s/epoch - 61ms/step
Epoch 23/200
1493/1493 - 91s - loss: 3.9345e-04 - val_loss: 0.0010 - 91s/epoch - 61ms/step
Epoch 24/200
1493/1493 - 91s - loss: 4.0757e-04 - val_loss: 3.2646e-04 - 91s/epoch - 61ms/step
Epoch 25/200
1493/1493 - 91s - loss: 3.4224e-04 - val_loss: 3.1806e-04 - 91s/epoch - 61ms/step
Epoch 26/200
1493/1493 - 91s - loss: 3.2845e-04 - val_loss: 3.9648e-04 - 91s/epoch - 61ms/step
Epoch 27/200
1493/1493 - 91s - loss: 3.1499e-04 - val_loss: 3.1576e-04 - 91s/epoch - 61ms/step
Epoch 28/200
1493/1493 - 91s - loss: 2.9802e-04 - val_loss: 7.2633e-04 - 91s/epoch - 61ms/step
Epoch 29/200
1493/1493 - 91s - loss: 2.9828e-04 - val_loss: 3.9003e-04 - 91s/epoch - 61ms/step
Epoch 30/200
1493/1493 - 91s - loss: 3.0256e-04 - val_loss: 3.1130e-04 - 91s/epoch - 61ms/step
Epoch 31/200
1493/1493 - 91s - loss: 2.7200e-04 - val_loss: 3.4885e-04 - 91s/epoch - 61ms/step
Epoch 32/200
1493/1493 - 91s - loss: 3.0394e-04 - val_loss: 3.3438e-04 - 91s/epoch - 61ms/step
Epoch 33/200
1493/1493 - 91s - loss: 2.7812e-04 - val_loss: 2.5697e-04 - 91s/epoch - 61ms/step
Epoch 34/200
1493/1493 - 91s - loss: 2.5577e-04 - val_loss: 2.4006e-04 - 91s/epoch - 61ms/step
Epoch 35/200
1493/1493 - 91s - loss: 2.4479e-04 - val_loss: 4.4619e-04 - 91s/epoch - 61ms/step
Epoch 36/200
1493/1493 - 91s - loss: 2.5987e-04 - val_loss: 3.9552e-04 - 91s/epoch - 61ms/step
Epoch 37/200
1493/1493 - 91s - loss: 2.4419e-04 - val_loss: 4.6393e-04 - 91s/epoch - 61ms/step
Epoch 38/200
1493/1493 - 91s - loss: 2.5927e-04 - val_loss: 2.1695e-04 - 91s/epoch - 61ms/step
Epoch 39/200
1493/1493 - 91s - loss: 2.2621e-04 - val_loss: 2.3508e-04 - 91s/epoch - 61ms/step
Epoch 40/200
1493/1493 - 91s - loss: 2.1964e-04 - val_loss: 3.0049e-04 - 91s/epoch - 61ms/step
Epoch 41/200
1493/1493 - 91s - loss: 2.2364e-04 - val_loss: 2.0678e-04 - 91s/epoch - 61ms/step
Epoch 42/200
1493/1493 - 91s - loss: 2.1395e-04 - val_loss: 2.0840e-04 - 91s/epoch - 61ms/step
Epoch 43/200
1493/1493 - 91s - loss: 2.1278e-04 - val_loss: 5.0193e-04 - 91s/epoch - 61ms/step
Epoch 44/200
1493/1493 - 91s - loss: 2.4073e-04 - val_loss: 2.8001e-04 - 91s/epoch - 61ms/step
Epoch 45/200
1493/1493 - 91s - loss: 2.1190e-04 - val_loss: 1.8708e-04 - 91s/epoch - 61ms/step
Epoch 46/200
1493/1493 - 91s - loss: 2.0189e-04 - val_loss: 2.0462e-04 - 91s/epoch - 61ms/step
Epoch 47/200
1493/1493 - 91s - loss: 1.9761e-04 - val_loss: 2.4706e-04 - 91s/epoch - 61ms/step
Epoch 48/200
1493/1493 - 91s - loss: 1.9822e-04 - val_loss: 2.6358e-04 - 91s/epoch - 61ms/step
Epoch 49/200
1493/1493 - 91s - loss: 1.9381e-04 - val_loss: 6.4641e-04 - 91s/epoch - 61ms/step
Epoch 50/200
1493/1493 - 91s - loss: 2.2913e-04 - val_loss: 8.9198e-04 - 91s/epoch - 61ms/step
Epoch 51/200
1493/1493 - 91s - loss: 2.3638e-04 - val_loss: 1.9210e-04 - 91s/epoch - 61ms/step
Epoch 52/200
1493/1493 - 91s - loss: 1.8479e-04 - val_loss: 1.7397e-04 - 91s/epoch - 61ms/step
Epoch 53/200
1493/1493 - 91s - loss: 1.8439e-04 - val_loss: 2.0630e-04 - 91s/epoch - 61ms/step
Epoch 54/200
1493/1493 - 91s - loss: 1.9641e-04 - val_loss: 2.1165e-04 - 91s/epoch - 61ms/step
Epoch 55/200
1493/1493 - 91s - loss: 1.7602e-04 - val_loss: 1.7353e-04 - 91s/epoch - 61ms/step
Epoch 56/200
1493/1493 - 91s - loss: 1.7567e-04 - val_loss: 2.1029e-04 - 91s/epoch - 61ms/step
Epoch 57/200
1493/1493 - 91s - loss: 1.7312e-04 - val_loss: 1.6916e-04 - 91s/epoch - 61ms/step
Epoch 58/200
1493/1493 - 91s - loss: 1.6890e-04 - val_loss: 1.5369e-04 - 91s/epoch - 61ms/step
Epoch 59/200
1493/1493 - 91s - loss: 1.6497e-04 - val_loss: 1.8412e-04 - 91s/epoch - 61ms/step
Epoch 60/200
1493/1493 - 91s - loss: 1.6472e-04 - val_loss: 3.4434e-04 - 91s/epoch - 61ms/step
Epoch 61/200
1493/1493 - 91s - loss: 1.9298e-04 - val_loss: 1.7183e-04 - 91s/epoch - 61ms/step
Epoch 62/200
1493/1493 - 91s - loss: 1.6138e-04 - val_loss: 1.7712e-04 - 91s/epoch - 61ms/step
Epoch 63/200
1493/1493 - 91s - loss: 1.5887e-04 - val_loss: 2.3123e-04 - 91s/epoch - 61ms/step
Epoch 64/200
1493/1493 - 91s - loss: 1.6412e-04 - val_loss: 0.0018 - 91s/epoch - 61ms/step
Epoch 65/200
1493/1493 - 91s - loss: 3.5485e-04 - val_loss: 8.6096e-04 - 91s/epoch - 61ms/step
Epoch 66/200
1493/1493 - 91s - loss: 2.2347e-04 - val_loss: 1.6367e-04 - 91s/epoch - 61ms/step
Epoch 67/200
1493/1493 - 91s - loss: 1.6985e-04 - val_loss: 1.4114e-04 - 91s/epoch - 61ms/step
Epoch 68/200
1493/1493 - 91s - loss: 1.6300e-04 - val_loss: 4.6131e-04 - 91s/epoch - 61ms/step
Epoch 69/200
1493/1493 - 91s - loss: 2.1002e-04 - val_loss: 1.6092e-04 - 91s/epoch - 61ms/step
Epoch 70/200
1493/1493 - 91s - loss: 1.5836e-04 - val_loss: 2.0059e-04 - 91s/epoch - 61ms/step
Epoch 71/200
1493/1493 - 91s - loss: 1.6021e-04 - val_loss: 1.4711e-04 - 91s/epoch - 61ms/step
Epoch 72/200
1493/1493 - 91s - loss: 1.5397e-04 - val_loss: 1.6090e-04 - 91s/epoch - 61ms/step
Epoch 73/200
1493/1493 - 91s - loss: 1.5416e-04 - val_loss: 1.7674e-04 - 91s/epoch - 61ms/step
Epoch 74/200
1493/1493 - 91s - loss: 1.5118e-04 - val_loss: 1.6332e-04 - 91s/epoch - 61ms/step
Epoch 75/200
1493/1493 - 91s - loss: 1.4514e-04 - val_loss: 1.3624e-04 - 91s/epoch - 61ms/step
Epoch 76/200
1493/1493 - 91s - loss: 1.4257e-04 - val_loss: 1.4774e-04 - 91s/epoch - 61ms/step
Epoch 77/200
1493/1493 - 91s - loss: 1.3941e-04 - val_loss: 4.6445e-04 - 91s/epoch - 61ms/step
Epoch 78/200
1493/1493 - 91s - loss: 1.5994e-04 - val_loss: 1.7277e-04 - 91s/epoch - 61ms/step
Epoch 79/200
1493/1493 - 91s - loss: 1.4240e-04 - val_loss: 1.5175e-04 - 91s/epoch - 61ms/step
Epoch 80/200
1493/1493 - 91s - loss: 1.4128e-04 - val_loss: 1.4379e-04 - 91s/epoch - 61ms/step
Epoch 81/200
1493/1493 - 91s - loss: 1.3693e-04 - val_loss: 2.8801e-04 - 91s/epoch - 61ms/step
Epoch 82/200
1493/1493 - 91s - loss: 1.3694e-04 - val_loss: 1.4306e-04 - 91s/epoch - 61ms/step
Epoch 83/200
1493/1493 - 91s - loss: 1.3260e-04 - val_loss: 1.5219e-04 - 91s/epoch - 61ms/step
Epoch 84/200
1493/1493 - 91s - loss: 1.3281e-04 - val_loss: 1.5203e-04 - 91s/epoch - 61ms/step
Epoch 85/200
1493/1493 - 91s - loss: 1.3773e-04 - val_loss: 1.4195e-04 - 91s/epoch - 61ms/step
Epoch 86/200
1493/1493 - 91s - loss: 1.3400e-04 - val_loss: 1.4185e-04 - 91s/epoch - 61ms/step
Epoch 87/200
1493/1493 - 91s - loss: 1.2749e-04 - val_loss: 1.3093e-04 - 91s/epoch - 61ms/step
Epoch 88/200
1493/1493 - 91s - loss: 1.2733e-04 - val_loss: 1.4622e-04 - 91s/epoch - 61ms/step
Epoch 89/200
1493/1493 - 91s - loss: 1.2853e-04 - val_loss: 1.3592e-04 - 91s/epoch - 61ms/step
Epoch 90/200
1493/1493 - 91s - loss: 1.2604e-04 - val_loss: 1.5910e-04 - 91s/epoch - 61ms/step
Epoch 91/200
1493/1493 - 91s - loss: 1.2489e-04 - val_loss: 1.3739e-04 - 91s/epoch - 61ms/step
Epoch 92/200
1493/1493 - 91s - loss: 1.2520e-04 - val_loss: 6.1331e-04 - 91s/epoch - 61ms/step
Epoch 93/200
1493/1493 - 91s - loss: 1.8014e-04 - val_loss: 2.0491e-04 - 91s/epoch - 61ms/step
Epoch 94/200
1493/1493 - 91s - loss: 1.4329e-04 - val_loss: 1.1266e-04 - 91s/epoch - 61ms/step
Epoch 95/200
1493/1493 - 91s - loss: 1.2688e-04 - val_loss: 1.4867e-04 - 91s/epoch - 61ms/step
Epoch 96/200
1493/1493 - 91s - loss: 1.2577e-04 - val_loss: 1.5852e-04 - 91s/epoch - 61ms/step
Epoch 97/200
1493/1493 - 91s - loss: 1.2434e-04 - val_loss: 1.3008e-04 - 91s/epoch - 61ms/step
Epoch 98/200
1493/1493 - 91s - loss: 1.2515e-04 - val_loss: 5.1401e-04 - 91s/epoch - 61ms/step
Epoch 99/200
1493/1493 - 91s - loss: 1.9234e-04 - val_loss: 1.1612e-04 - 91s/epoch - 61ms/step
Epoch 100/200
1493/1493 - 91s - loss: 1.3515e-04 - val_loss: 1.2935e-04 - 91s/epoch - 61ms/step
Epoch 101/200
1493/1493 - 91s - loss: 1.2396e-04 - val_loss: 2.0913e-04 - 91s/epoch - 61ms/step
Epoch 102/200
1493/1493 - 91s - loss: 1.3219e-04 - val_loss: 1.4840e-04 - 91s/epoch - 61ms/step
Epoch 103/200
1493/1493 - 91s - loss: 1.2406e-04 - val_loss: 1.2228e-04 - 91s/epoch - 61ms/step
Epoch 104/200
1493/1493 - 91s - loss: 1.1888e-04 - val_loss: 3.2390e-04 - 91s/epoch - 61ms/step
Epoch 105/200
1493/1493 - 91s - loss: 1.3159e-04 - val_loss: 1.1809e-04 - 91s/epoch - 61ms/step
Epoch 106/200
1493/1493 - 91s - loss: 1.1748e-04 - val_loss: 1.2971e-04 - 91s/epoch - 61ms/step
Epoch 107/200
1493/1493 - 91s - loss: 1.1648e-04 - val_loss: 1.2264e-04 - 91s/epoch - 61ms/step
Epoch 108/200
1493/1493 - 91s - loss: 1.2144e-04 - val_loss: 1.1903e-04 - 91s/epoch - 61ms/step
Epoch 109/200
1493/1493 - 91s - loss: 1.1545e-04 - val_loss: 1.4602e-04 - 91s/epoch - 61ms/step
Epoch 110/200
1493/1493 - 91s - loss: 1.1326e-04 - val_loss: 1.3360e-04 - 91s/epoch - 61ms/step
Epoch 111/200
1493/1493 - 91s - loss: 1.1295e-04 - val_loss: 1.2348e-04 - 91s/epoch - 61ms/step
Epoch 112/200
1493/1493 - 91s - loss: 1.1422e-04 - val_loss: 1.1626e-04 - 91s/epoch - 61ms/step
Epoch 113/200
1493/1493 - 91s - loss: 1.1232e-04 - val_loss: 4.5143e-04 - 91s/epoch - 61ms/step
Epoch 114/200
1493/1493 - 91s - loss: 1.6170e-04 - val_loss: 1.1433e-04 - 91s/epoch - 61ms/step
Epoch 115/200
1493/1493 - 91s - loss: 1.1825e-04 - val_loss: 2.0473e-04 - 91s/epoch - 61ms/step
Epoch 116/200
1493/1493 - 91s - loss: 1.1404e-04 - val_loss: 1.2614e-04 - 91s/epoch - 61ms/step
Epoch 117/200
1493/1493 - 91s - loss: 1.1053e-04 - val_loss: 2.8270e-04 - 91s/epoch - 61ms/step
Epoch 118/200
1493/1493 - 91s - loss: 1.3404e-04 - val_loss: 1.1845e-04 - 91s/epoch - 61ms/step
Epoch 119/200
1493/1493 - 91s - loss: 1.1104e-04 - val_loss: 1.1071e-04 - 91s/epoch - 61ms/step
Epoch 120/200
1493/1493 - 91s - loss: 1.0927e-04 - val_loss: 1.1048e-04 - 91s/epoch - 61ms/step
Epoch 121/200
1493/1493 - 91s - loss: 1.0844e-04 - val_loss: 1.7991e-04 - 91s/epoch - 61ms/step
Epoch 122/200
1493/1493 - 91s - loss: 1.1033e-04 - val_loss: 1.1042e-04 - 91s/epoch - 61ms/step
Epoch 123/200
1493/1493 - 91s - loss: 1.0773e-04 - val_loss: 1.1268e-04 - 91s/epoch - 61ms/step
Epoch 124/200
1493/1493 - 91s - loss: 1.0667e-04 - val_loss: 2.5522e-04 - 91s/epoch - 61ms/step
Epoch 125/200
1493/1493 - 91s - loss: 1.2647e-04 - val_loss: 1.6132e-04 - 91s/epoch - 61ms/step
Epoch 126/200
1493/1493 - 91s - loss: 1.1465e-04 - val_loss: 1.8003e-04 - 91s/epoch - 61ms/step
Epoch 127/200
1493/1493 - 91s - loss: 1.1277e-04 - val_loss: 1.7726e-04 - 91s/epoch - 61ms/step
Epoch 128/200
1493/1493 - 91s - loss: 1.1034e-04 - val_loss: 1.0146e-04 - 91s/epoch - 61ms/step
Epoch 129/200
1493/1493 - 91s - loss: 1.0436e-04 - val_loss: 1.1715e-04 - 91s/epoch - 61ms/step
Epoch 130/200
1493/1493 - 91s - loss: 1.0313e-04 - val_loss: 9.9042e-05 - 91s/epoch - 61ms/step
Epoch 131/200
1493/1493 - 91s - loss: 1.0674e-04 - val_loss: 4.5839e-04 - 91s/epoch - 61ms/step
Epoch 132/200
1493/1493 - 91s - loss: 1.6377e-04 - val_loss: 1.0164e-04 - 91s/epoch - 61ms/step
Epoch 133/200
1493/1493 - 91s - loss: 1.0895e-04 - val_loss: 1.0050e-04 - 91s/epoch - 61ms/step
Epoch 134/200
1493/1493 - 91s - loss: 1.0466e-04 - val_loss: 1.0503e-04 - 91s/epoch - 61ms/step
Epoch 135/200
1493/1493 - 91s - loss: 1.0312e-04 - val_loss: 9.4738e-05 - 91s/epoch - 61ms/step
Epoch 136/200
1493/1493 - 91s - loss: 1.0230e-04 - val_loss: 1.0452e-04 - 91s/epoch - 61ms/step
Epoch 137/200
1493/1493 - 91s - loss: 1.0615e-04 - val_loss: 1.2481e-04 - 91s/epoch - 61ms/step
Epoch 138/200
1493/1493 - 91s - loss: 1.0228e-04 - val_loss: 1.0924e-04 - 91s/epoch - 61ms/step
Epoch 139/200
1493/1493 - 91s - loss: 1.0298e-04 - val_loss: 1.6089e-04 - 91s/epoch - 61ms/step
Epoch 140/200
1493/1493 - 91s - loss: 1.1280e-04 - val_loss: 9.6313e-05 - 91s/epoch - 61ms/step
Epoch 141/200
1493/1493 - 91s - loss: 1.0114e-04 - val_loss: 1.1461e-04 - 91s/epoch - 61ms/step
Epoch 142/200
1493/1493 - 91s - loss: 1.0275e-04 - val_loss: 1.1491e-04 - 91s/epoch - 61ms/step
Epoch 143/200
1493/1493 - 91s - loss: 9.8333e-05 - val_loss: 1.4042e-04 - 91s/epoch - 61ms/step
Epoch 144/200
1493/1493 - 91s - loss: 1.0074e-04 - val_loss: 1.1406e-04 - 91s/epoch - 61ms/step
Epoch 145/200
1493/1493 - 91s - loss: 9.8120e-05 - val_loss: 1.0057e-04 - 91s/epoch - 61ms/step
Epoch 146/200
1493/1493 - 91s - loss: 9.6589e-05 - val_loss: 9.1968e-05 - 91s/epoch - 61ms/step
Epoch 147/200
1493/1493 - 91s - loss: 9.5073e-05 - val_loss: 1.1226e-04 - 91s/epoch - 61ms/step
Epoch 148/200
1493/1493 - 91s - loss: 9.5884e-05 - val_loss: 1.9620e-04 - 91s/epoch - 61ms/step
Epoch 149/200
1493/1493 - 91s - loss: 9.5290e-05 - val_loss: 9.7550e-05 - 91s/epoch - 61ms/step
Epoch 150/200
1493/1493 - 91s - loss: 9.4513e-05 - val_loss: 9.9036e-05 - 91s/epoch - 61ms/step
Epoch 151/200
1493/1493 - 91s - loss: 9.5076e-05 - val_loss: 1.3545e-04 - 91s/epoch - 61ms/step
Epoch 152/200
1493/1493 - 91s - loss: 9.5968e-05 - val_loss: 1.1110e-04 - 91s/epoch - 61ms/step
Epoch 153/200
1493/1493 - 91s - loss: 9.5007e-05 - val_loss: 1.1322e-04 - 91s/epoch - 61ms/step
Epoch 154/200
1493/1493 - 91s - loss: 9.3873e-05 - val_loss: 2.4524e-04 - 91s/epoch - 61ms/step
Epoch 155/200
1493/1493 - 91s - loss: 1.2083e-04 - val_loss: 9.5034e-05 - 91s/epoch - 61ms/step
Epoch 156/200
1493/1493 - 91s - loss: 9.7877e-05 - val_loss: 1.0230e-04 - 91s/epoch - 61ms/step
Epoch 157/200
1493/1493 - 91s - loss: 9.3094e-05 - val_loss: 1.0658e-04 - 91s/epoch - 61ms/step
Epoch 158/200
1493/1493 - 91s - loss: 9.3547e-05 - val_loss: 1.0528e-04 - 91s/epoch - 61ms/step
Epoch 159/200
1493/1493 - 91s - loss: 9.2257e-05 - val_loss: 9.3641e-05 - 91s/epoch - 61ms/step
Epoch 160/200
1493/1493 - 91s - loss: 9.3343e-05 - val_loss: 1.5864e-04 - 91s/epoch - 61ms/step
Epoch 161/200
1493/1493 - 91s - loss: 1.2684e-04 - val_loss: 9.1861e-05 - 91s/epoch - 61ms/step
Epoch 162/200
1493/1493 - 91s - loss: 9.4622e-05 - val_loss: 8.9349e-05 - 91s/epoch - 61ms/step
Epoch 163/200
1493/1493 - 91s - loss: 9.2328e-05 - val_loss: 9.9529e-05 - 91s/epoch - 61ms/step
Epoch 164/200
1493/1493 - 91s - loss: 9.4356e-05 - val_loss: 9.3527e-05 - 91s/epoch - 61ms/step
Epoch 165/200
1493/1493 - 91s - loss: 9.8792e-05 - val_loss: 9.3957e-05 - 91s/epoch - 61ms/step
Epoch 166/200
1493/1493 - 91s - loss: 9.2275e-05 - val_loss: 1.0332e-04 - 91s/epoch - 61ms/step
Epoch 167/200
1493/1493 - 91s - loss: 9.1374e-05 - val_loss: 9.1687e-05 - 91s/epoch - 61ms/step
Epoch 168/200
1493/1493 - 91s - loss: 8.9897e-05 - val_loss: 9.1581e-05 - 91s/epoch - 61ms/step
Epoch 169/200
1493/1493 - 91s - loss: 9.0301e-05 - val_loss: 1.0291e-04 - 91s/epoch - 61ms/step
Epoch 170/200
1493/1493 - 91s - loss: 9.1902e-05 - val_loss: 3.7256e-04 - 91s/epoch - 61ms/step
Epoch 171/200
1493/1493 - 91s - loss: 1.3361e-04 - val_loss: 5.7031e-04 - 91s/epoch - 61ms/step
Epoch 172/200
1493/1493 - 91s - loss: 1.4322e-04 - val_loss: 1.5598e-04 - 91s/epoch - 61ms/step
Epoch 173/200
1493/1493 - 91s - loss: 9.9457e-05 - val_loss: 9.4689e-05 - 91s/epoch - 61ms/step
Epoch 174/200
1493/1493 - 91s - loss: 9.2093e-05 - val_loss: 9.3245e-05 - 91s/epoch - 61ms/step
Epoch 175/200
1493/1493 - 91s - loss: 9.2271e-05 - val_loss: 1.0701e-04 - 91s/epoch - 61ms/step
Epoch 176/200
1493/1493 - 91s - loss: 9.1547e-05 - val_loss: 8.7936e-05 - 91s/epoch - 61ms/step
Epoch 177/200
1493/1493 - 91s - loss: 9.1242e-05 - val_loss: 8.4367e-05 - 91s/epoch - 61ms/step
Epoch 178/200
1493/1493 - 91s - loss: 8.8643e-05 - val_loss: 8.9842e-05 - 91s/epoch - 61ms/step
Epoch 179/200
1493/1493 - 91s - loss: 8.7877e-05 - val_loss: 9.8978e-05 - 91s/epoch - 61ms/step
Epoch 180/200
1493/1493 - 91s - loss: 9.7366e-05 - val_loss: 9.0291e-05 - 91s/epoch - 61ms/step
Epoch 181/200
1493/1493 - 91s - loss: 8.9579e-05 - val_loss: 8.1782e-05 - 91s/epoch - 61ms/step
Epoch 182/200
1493/1493 - 91s - loss: 8.8181e-05 - val_loss: 1.0516e-04 - 91s/epoch - 61ms/step
Epoch 183/200
1493/1493 - 91s - loss: 8.6411e-05 - val_loss: 9.5076e-05 - 91s/epoch - 61ms/step
Epoch 184/200
1493/1493 - 91s - loss: 8.6886e-05 - val_loss: 8.6298e-05 - 91s/epoch - 61ms/step
Epoch 185/200
1493/1493 - 91s - loss: 8.5366e-05 - val_loss: 1.8001e-04 - 91s/epoch - 61ms/step
Epoch 186/200
1493/1493 - 91s - loss: 9.5229e-05 - val_loss: 8.2569e-05 - 91s/epoch - 61ms/step
Epoch 187/200
1493/1493 - 91s - loss: 8.6441e-05 - val_loss: 1.1489e-04 - 91s/epoch - 61ms/step
Epoch 188/200
1493/1493 - 91s - loss: 9.6102e-05 - val_loss: 9.4843e-05 - 91s/epoch - 61ms/step
Epoch 189/200
1493/1493 - 91s - loss: 8.6912e-05 - val_loss: 9.4879e-05 - 91s/epoch - 61ms/step
Epoch 190/200
1493/1493 - 91s - loss: 8.7264e-05 - val_loss: 1.1559e-04 - 91s/epoch - 61ms/step
Epoch 191/200
1493/1493 - 91s - loss: 9.0018e-05 - val_loss: 1.5596e-04 - 91s/epoch - 61ms/step
Epoch 192/200
1493/1493 - 91s - loss: 9.0488e-05 - val_loss: 1.0334e-04 - 91s/epoch - 61ms/step
Epoch 193/200
1493/1493 - 91s - loss: 8.6767e-05 - val_loss: 1.4554e-04 - 91s/epoch - 61ms/step
Epoch 194/200
1493/1493 - 91s - loss: 9.7402e-05 - val_loss: 3.5850e-04 - 91s/epoch - 61ms/step
Epoch 195/200
1493/1493 - 91s - loss: 1.3380e-04 - val_loss: 2.3688e-04 - 91s/epoch - 61ms/step
Epoch 196/200
1493/1493 - 91s - loss: 1.1686e-04 - val_loss: 7.3320e-05 - 91s/epoch - 61ms/step
Epoch 197/200
1493/1493 - 91s - loss: 8.9460e-05 - val_loss: 1.0543e-04 - 91s/epoch - 61ms/step
Epoch 198/200
1493/1493 - 91s - loss: 9.7748e-05 - val_loss: 1.3371e-04 - 91s/epoch - 61ms/step
Epoch 199/200
1493/1493 - 91s - loss: 9.4038e-05 - val_loss: 8.1375e-05 - 91s/epoch - 61ms/step
Epoch 200/200
1493/1493 - 91s - loss: 8.6018e-05 - val_loss: 9.2031e-05 - 91s/epoch - 61ms/step
COMPRESSED VECTOR SIZE: 884
Loss in the autoencoder: 9.203071385854855e-05
  1/332 [..............................] - ETA: 24s  8/332 [..............................] - ETA: 2s  16/332 [>.............................] - ETA: 2s 24/332 [=>............................] - ETA: 2s 32/332 [=>............................] - ETA: 2s 40/332 [==>...........................] - ETA: 2s 48/332 [===>..........................] - ETA: 1s 56/332 [====>.........................] - ETA: 1s 64/332 [====>.........................] - ETA: 1s 72/332 [=====>........................] - ETA: 1s 80/332 [======>.......................] - ETA: 1s 88/332 [======>.......................] - ETA: 1s 96/332 [=======>......................] - ETA: 1s104/332 [========>.....................] - ETA: 1s112/332 [=========>....................] - ETA: 1s120/332 [=========>....................] - ETA: 1s128/332 [==========>...................] - ETA: 1s136/332 [===========>..................] - ETA: 1s144/332 [============>.................] - ETA: 1s152/332 [============>.................] - ETA: 1s160/332 [=============>................] - ETA: 1s168/332 [==============>...............] - ETA: 1s176/332 [==============>...............] - ETA: 1s184/332 [===============>..............] - ETA: 1s192/332 [================>.............] - ETA: 0s200/332 [=================>............] - ETA: 0s208/332 [=================>............] - ETA: 0s216/332 [==================>...........] - ETA: 0s224/332 [===================>..........] - ETA: 0s232/332 [===================>..........] - ETA: 0s240/332 [====================>.........] - ETA: 0s248/332 [=====================>........] - ETA: 0s256/332 [======================>.......] - ETA: 0s264/332 [======================>.......] - ETA: 0s272/332 [=======================>......] - ETA: 0s280/332 [========================>.....] - ETA: 0s288/332 [=========================>....] - ETA: 0s296/332 [=========================>....] - ETA: 0s304/332 [==========================>...] - ETA: 0s312/332 [===========================>..] - ETA: 0s320/332 [===========================>..] - ETA: 0s328/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 7ms/step
correlation 0.001037228699454801
cosine 0.0008174447610249755
MAE: 0.0053092605
RMSE: 0.009593259
r2: 0.9940301040908446
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_18 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 884)               2458404   
                                                                 
 batch_normalization_19 (Bat  (None, 884)              3536      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 884)               0         
                                                                 
 dense_19 (Dense)            (None, 2780)              2460300   
                                                                 
 batch_normalization_20 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2780)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 11,976,364
Trainable params: 11,963,476
Non-trainable params: 12,888
_________________________________________________________________
Encoder
Model: "model_19"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_20 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_19 (InputLayer)       multiple                  0         
                                                                 
 dense_18 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_18 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_18 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 884)               2458404   
                                                                 
=================================================================
Total params: 5,986,224
Trainable params: 5,980,664
Non-trainable params: 5,560
_________________________________________________________________
Decoder
Model: "model_20"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_21 (InputLayer)       [(None, 884)]             0         
                                                                 
 batch_normalization_19 (Bat  (None, 884)              3536      
 chNormalization)                                                
                                                                 
 re_lu_19 (ReLU)             (None, 884)               0         
                                                                 
 dense_19 (Dense)            (None, 2780)              2460300   
                                                                 
 batch_normalization_20 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_20 (ReLU)             (None, 2780)              0         
                                                                 
 dense_20 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 5,990,140
Trainable params: 5,982,812
Non-trainable params: 7,328
_________________________________________________________________
['2.2custom_n_b', 'mse', 64, 200, 0.0005, 0.7, 884, 8.601768786320463e-05, 9.203071385854855e-05, 0.001037228699454801, 0.0008174447610249755, 0.0053092604503035545, 0.009593258611857891, 0.9940301040908446, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_2.2final_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_21 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_21 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 821)               2283201   
                                                                 
 batch_normalization_22 (Bat  (None, 821)              3284      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 821)               0         
                                                                 
 dense_22 (Dense)            (None, 2780)              2285160   
                                                                 
 batch_normalization_23 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2780)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 11,625,769
Trainable params: 11,613,007
Non-trainable params: 12,762
_________________________________________________________________
Epoch 1/200
1493/1493 - 88s - loss: 0.0092 - val_loss: 0.0041 - 88s/epoch - 59ms/step
Epoch 2/200
1493/1493 - 87s - loss: 0.0029 - val_loss: 0.0031 - 87s/epoch - 58ms/step
Epoch 3/200
1493/1493 - 87s - loss: 0.0021 - val_loss: 0.0016 - 87s/epoch - 58ms/step
Epoch 4/200
1493/1493 - 87s - loss: 0.0017 - val_loss: 0.0021 - 87s/epoch - 58ms/step
Epoch 5/200
1493/1493 - 87s - loss: 0.0015 - val_loss: 0.0015 - 87s/epoch - 58ms/step
Epoch 6/200
1493/1493 - 87s - loss: 0.0014 - val_loss: 0.0011 - 87s/epoch - 58ms/step
Epoch 7/200
1493/1493 - 87s - loss: 0.0013 - val_loss: 0.0021 - 87s/epoch - 58ms/step
Epoch 8/200
1493/1493 - 87s - loss: 0.0011 - val_loss: 9.5454e-04 - 87s/epoch - 58ms/step
Epoch 9/200
1493/1493 - 87s - loss: 0.0010 - val_loss: 0.0012 - 87s/epoch - 58ms/step
Epoch 10/200
1493/1493 - 87s - loss: 9.0729e-04 - val_loss: 7.8773e-04 - 87s/epoch - 58ms/step
Epoch 11/200
1493/1493 - 87s - loss: 8.8606e-04 - val_loss: 8.5390e-04 - 87s/epoch - 58ms/step
Epoch 12/200
1493/1493 - 87s - loss: 7.2488e-04 - val_loss: 0.0033 - 87s/epoch - 58ms/step
Epoch 13/200
1493/1493 - 87s - loss: 7.4339e-04 - val_loss: 0.0014 - 87s/epoch - 58ms/step
Epoch 14/200
1493/1493 - 87s - loss: 6.8192e-04 - val_loss: 5.6157e-04 - 87s/epoch - 58ms/step
Epoch 15/200
1493/1493 - 87s - loss: 6.0023e-04 - val_loss: 5.7064e-04 - 87s/epoch - 58ms/step
Epoch 16/200
1493/1493 - 87s - loss: 5.5663e-04 - val_loss: 5.1967e-04 - 87s/epoch - 58ms/step
Epoch 17/200
1493/1493 - 87s - loss: 5.2836e-04 - val_loss: 5.9681e-04 - 87s/epoch - 58ms/step
Epoch 18/200
1493/1493 - 87s - loss: 4.7080e-04 - val_loss: 4.6856e-04 - 87s/epoch - 58ms/step
Epoch 19/200
1493/1493 - 87s - loss: 4.5013e-04 - val_loss: 5.6673e-04 - 87s/epoch - 58ms/step
Epoch 20/200
1493/1493 - 87s - loss: 4.4054e-04 - val_loss: 4.1144e-04 - 87s/epoch - 58ms/step
Epoch 21/200
1493/1493 - 87s - loss: 4.2148e-04 - val_loss: 4.8831e-04 - 87s/epoch - 58ms/step
Epoch 22/200
1493/1493 - 87s - loss: 3.9073e-04 - val_loss: 4.6109e-04 - 87s/epoch - 58ms/step
Epoch 23/200
1493/1493 - 87s - loss: 3.6816e-04 - val_loss: 7.8130e-04 - 87s/epoch - 58ms/step
Epoch 24/200
1493/1493 - 87s - loss: 3.8163e-04 - val_loss: 3.5859e-04 - 87s/epoch - 58ms/step
Epoch 25/200
1493/1493 - 87s - loss: 3.3486e-04 - val_loss: 3.7357e-04 - 87s/epoch - 58ms/step
Epoch 26/200
1493/1493 - 87s - loss: 3.2782e-04 - val_loss: 4.2132e-04 - 87s/epoch - 58ms/step
Epoch 27/200
1493/1493 - 87s - loss: 3.1546e-04 - val_loss: 3.1598e-04 - 87s/epoch - 58ms/step
Epoch 28/200
1493/1493 - 87s - loss: 2.9820e-04 - val_loss: 3.9419e-04 - 87s/epoch - 58ms/step
Epoch 29/200
1493/1493 - 87s - loss: 2.8628e-04 - val_loss: 3.4110e-04 - 87s/epoch - 58ms/step
Epoch 30/200
1493/1493 - 87s - loss: 2.9142e-04 - val_loss: 3.4102e-04 - 87s/epoch - 58ms/step
Epoch 31/200
1493/1493 - 87s - loss: 2.7348e-04 - val_loss: 6.3232e-04 - 87s/epoch - 58ms/step
Epoch 32/200
1493/1493 - 87s - loss: 2.9871e-04 - val_loss: 2.9423e-04 - 87s/epoch - 58ms/step
Epoch 33/200
1493/1493 - 87s - loss: 2.6638e-04 - val_loss: 2.9313e-04 - 87s/epoch - 58ms/step
Epoch 34/200
1493/1493 - 87s - loss: 2.5432e-04 - val_loss: 2.4282e-04 - 87s/epoch - 58ms/step
Epoch 35/200
1493/1493 - 87s - loss: 2.4753e-04 - val_loss: 7.7933e-04 - 87s/epoch - 58ms/step
Epoch 36/200
1493/1493 - 87s - loss: 2.7443e-04 - val_loss: 3.0600e-04 - 87s/epoch - 58ms/step
Epoch 37/200
1493/1493 - 87s - loss: 2.4587e-04 - val_loss: 2.8108e-04 - 87s/epoch - 58ms/step
Epoch 38/200
1493/1493 - 87s - loss: 2.4691e-04 - val_loss: 2.3124e-04 - 87s/epoch - 58ms/step
Epoch 39/200
1493/1493 - 87s - loss: 2.2747e-04 - val_loss: 2.1494e-04 - 87s/epoch - 58ms/step
Epoch 40/200
1493/1493 - 87s - loss: 2.1747e-04 - val_loss: 2.8470e-04 - 87s/epoch - 58ms/step
Epoch 41/200
1493/1493 - 87s - loss: 2.2052e-04 - val_loss: 1.9500e-04 - 87s/epoch - 58ms/step
Epoch 42/200
1493/1493 - 87s - loss: 2.1600e-04 - val_loss: 2.2534e-04 - 87s/epoch - 58ms/step
Epoch 43/200
1493/1493 - 87s - loss: 2.1221e-04 - val_loss: 4.5449e-04 - 87s/epoch - 58ms/step
Epoch 44/200
1493/1493 - 87s - loss: 2.3345e-04 - val_loss: 4.6887e-04 - 87s/epoch - 58ms/step
Epoch 45/200
1493/1493 - 87s - loss: 2.2791e-04 - val_loss: 1.8759e-04 - 87s/epoch - 58ms/step
Epoch 46/200
1493/1493 - 87s - loss: 2.0079e-04 - val_loss: 1.8384e-04 - 87s/epoch - 58ms/step
Epoch 47/200
1493/1493 - 87s - loss: 2.0050e-04 - val_loss: 2.7194e-04 - 87s/epoch - 58ms/step
Epoch 48/200
1493/1493 - 87s - loss: 1.9610e-04 - val_loss: 2.5589e-04 - 87s/epoch - 58ms/step
Epoch 49/200
1493/1493 - 87s - loss: 1.9278e-04 - val_loss: 3.2380e-04 - 87s/epoch - 58ms/step
Epoch 50/200
1493/1493 - 87s - loss: 2.0300e-04 - val_loss: 5.2974e-04 - 87s/epoch - 58ms/step
Epoch 51/200
1493/1493 - 87s - loss: 2.1472e-04 - val_loss: 2.2136e-04 - 87s/epoch - 58ms/step
Epoch 52/200
1493/1493 - 87s - loss: 1.8174e-04 - val_loss: 1.8987e-04 - 87s/epoch - 58ms/step
Epoch 53/200
1493/1493 - 87s - loss: 1.8262e-04 - val_loss: 1.7781e-04 - 87s/epoch - 58ms/step
Epoch 54/200
1493/1493 - 87s - loss: 1.7718e-04 - val_loss: 2.2046e-04 - 87s/epoch - 58ms/step
Epoch 55/200
1493/1493 - 87s - loss: 1.7366e-04 - val_loss: 1.5888e-04 - 87s/epoch - 58ms/step
Epoch 56/200
1493/1493 - 87s - loss: 1.7679e-04 - val_loss: 2.1165e-04 - 87s/epoch - 58ms/step
Epoch 57/200
1493/1493 - 87s - loss: 1.6953e-04 - val_loss: 1.8764e-04 - 87s/epoch - 58ms/step
Epoch 58/200
1493/1493 - 87s - loss: 1.6814e-04 - val_loss: 1.5612e-04 - 87s/epoch - 58ms/step
Epoch 59/200
1493/1493 - 87s - loss: 1.6497e-04 - val_loss: 1.9935e-04 - 87s/epoch - 58ms/step
Epoch 60/200
1493/1493 - 87s - loss: 1.6614e-04 - val_loss: 4.9083e-04 - 87s/epoch - 58ms/step
Epoch 61/200
1493/1493 - 87s - loss: 3.5444e-04 - val_loss: 1.5785e-04 - 87s/epoch - 58ms/step
Epoch 62/200
1493/1493 - 87s - loss: 1.7839e-04 - val_loss: 1.9015e-04 - 87s/epoch - 58ms/step
Epoch 63/200
1493/1493 - 87s - loss: 1.6850e-04 - val_loss: 2.2547e-04 - 87s/epoch - 58ms/step
Epoch 64/200
1493/1493 - 87s - loss: 1.7068e-04 - val_loss: 0.0012 - 87s/epoch - 58ms/step
Epoch 65/200
1493/1493 - 87s - loss: 3.3983e-04 - val_loss: 3.4952e-04 - 87s/epoch - 58ms/step
Epoch 66/200
1493/1493 - 87s - loss: 1.9406e-04 - val_loss: 2.0394e-04 - 87s/epoch - 58ms/step
Epoch 67/200
1493/1493 - 87s - loss: 1.7028e-04 - val_loss: 1.5935e-04 - 87s/epoch - 58ms/step
Epoch 68/200
1493/1493 - 87s - loss: 1.6281e-04 - val_loss: 2.4277e-04 - 87s/epoch - 58ms/step
Epoch 69/200
1493/1493 - 87s - loss: 1.6964e-04 - val_loss: 1.8523e-04 - 87s/epoch - 58ms/step
Epoch 70/200
1493/1493 - 87s - loss: 1.5594e-04 - val_loss: 2.0132e-04 - 87s/epoch - 58ms/step
Epoch 71/200
1493/1493 - 87s - loss: 1.5666e-04 - val_loss: 1.5053e-04 - 87s/epoch - 58ms/step
Epoch 72/200
1493/1493 - 87s - loss: 1.5437e-04 - val_loss: 1.4981e-04 - 87s/epoch - 58ms/step
Epoch 73/200
1493/1493 - 87s - loss: 1.5290e-04 - val_loss: 1.8065e-04 - 87s/epoch - 58ms/step
Epoch 74/200
1493/1493 - 87s - loss: 1.5511e-04 - val_loss: 1.5056e-04 - 87s/epoch - 58ms/step
Epoch 75/200
1493/1493 - 87s - loss: 1.4389e-04 - val_loss: 1.3889e-04 - 87s/epoch - 58ms/step
Epoch 76/200
1493/1493 - 87s - loss: 1.4414e-04 - val_loss: 1.3861e-04 - 87s/epoch - 58ms/step
Epoch 77/200
1493/1493 - 87s - loss: 1.4055e-04 - val_loss: 9.9098e-04 - 87s/epoch - 58ms/step
Epoch 78/200
1493/1493 - 87s - loss: 1.9047e-04 - val_loss: 1.4423e-04 - 87s/epoch - 58ms/step
Epoch 79/200
1493/1493 - 87s - loss: 1.4386e-04 - val_loss: 2.9322e-04 - 87s/epoch - 58ms/step
Epoch 80/200
1493/1493 - 87s - loss: 1.6379e-04 - val_loss: 2.3727e-04 - 87s/epoch - 58ms/step
Epoch 81/200
1493/1493 - 87s - loss: 1.4449e-04 - val_loss: 5.0021e-04 - 87s/epoch - 58ms/step
Epoch 82/200
1493/1493 - 87s - loss: 1.4903e-04 - val_loss: 1.4084e-04 - 87s/epoch - 58ms/step
Epoch 83/200
1493/1493 - 87s - loss: 1.3448e-04 - val_loss: 1.4153e-04 - 87s/epoch - 58ms/step
Epoch 84/200
1493/1493 - 87s - loss: 1.3410e-04 - val_loss: 1.6372e-04 - 87s/epoch - 58ms/step
Epoch 85/200
1493/1493 - 87s - loss: 1.3278e-04 - val_loss: 1.5410e-04 - 87s/epoch - 58ms/step
Epoch 86/200
1493/1493 - 87s - loss: 1.3421e-04 - val_loss: 1.6598e-04 - 87s/epoch - 58ms/step
Epoch 87/200
1493/1493 - 87s - loss: 1.2825e-04 - val_loss: 1.3290e-04 - 87s/epoch - 58ms/step
Epoch 88/200
1493/1493 - 87s - loss: 1.2746e-04 - val_loss: 1.5667e-04 - 87s/epoch - 58ms/step
Epoch 89/200
1493/1493 - 87s - loss: 1.3429e-04 - val_loss: 1.4066e-04 - 87s/epoch - 58ms/step
Epoch 90/200
1493/1493 - 87s - loss: 1.3079e-04 - val_loss: 1.2834e-04 - 87s/epoch - 58ms/step
Epoch 91/200
1493/1493 - 87s - loss: 1.2618e-04 - val_loss: 1.3849e-04 - 87s/epoch - 58ms/step
Epoch 92/200
1493/1493 - 87s - loss: 1.2569e-04 - val_loss: 6.6279e-04 - 87s/epoch - 58ms/step
Epoch 93/200
1493/1493 - 87s - loss: 1.7556e-04 - val_loss: 6.7007e-04 - 87s/epoch - 58ms/step
Epoch 94/200
1493/1493 - 87s - loss: 1.9002e-04 - val_loss: 1.0828e-04 - 87s/epoch - 58ms/step
Epoch 95/200
1493/1493 - 87s - loss: 1.3316e-04 - val_loss: 9.5962e-04 - 87s/epoch - 58ms/step
Epoch 96/200
1493/1493 - 87s - loss: 1.9455e-04 - val_loss: 1.3485e-04 - 87s/epoch - 58ms/step
Epoch 97/200
1493/1493 - 87s - loss: 1.3591e-04 - val_loss: 1.2097e-04 - 87s/epoch - 58ms/step
Epoch 98/200
1493/1493 - 87s - loss: 1.3103e-04 - val_loss: 4.2446e-04 - 87s/epoch - 58ms/step
Epoch 99/200
1493/1493 - 87s - loss: 1.8137e-04 - val_loss: 1.1997e-04 - 87s/epoch - 58ms/step
Epoch 100/200
1493/1493 - 87s - loss: 1.3638e-04 - val_loss: 1.2117e-04 - 87s/epoch - 58ms/step
Epoch 101/200
1493/1493 - 87s - loss: 1.2740e-04 - val_loss: 1.8507e-04 - 87s/epoch - 58ms/step
Epoch 102/200
1493/1493 - 87s - loss: 1.3732e-04 - val_loss: 2.0867e-04 - 87s/epoch - 58ms/step
Epoch 103/200
1493/1493 - 87s - loss: 1.3452e-04 - val_loss: 1.3502e-04 - 87s/epoch - 58ms/step
Epoch 104/200
1493/1493 - 87s - loss: 1.2348e-04 - val_loss: 9.2778e-04 - 87s/epoch - 58ms/step
Epoch 105/200
1493/1493 - 87s - loss: 1.7616e-04 - val_loss: 1.0304e-04 - 87s/epoch - 58ms/step
Epoch 106/200
1493/1493 - 87s - loss: 1.2484e-04 - val_loss: 1.2160e-04 - 87s/epoch - 58ms/step
Epoch 107/200
1493/1493 - 87s - loss: 1.2295e-04 - val_loss: 2.0861e-04 - 87s/epoch - 58ms/step
Epoch 108/200
1493/1493 - 87s - loss: 1.2503e-04 - val_loss: 1.2493e-04 - 87s/epoch - 58ms/step
Epoch 109/200
1493/1493 - 87s - loss: 1.1926e-04 - val_loss: 1.3788e-04 - 87s/epoch - 58ms/step
Epoch 110/200
1493/1493 - 87s - loss: 1.1700e-04 - val_loss: 1.3951e-04 - 87s/epoch - 58ms/step
Epoch 111/200
1493/1493 - 87s - loss: 1.1609e-04 - val_loss: 1.4374e-04 - 87s/epoch - 58ms/step
Epoch 112/200
1493/1493 - 87s - loss: 1.1937e-04 - val_loss: 1.1491e-04 - 87s/epoch - 58ms/step
Epoch 113/200
1493/1493 - 87s - loss: 1.1322e-04 - val_loss: 3.8926e-04 - 87s/epoch - 58ms/step
Epoch 114/200
1493/1493 - 87s - loss: 1.4649e-04 - val_loss: 1.1267e-04 - 87s/epoch - 58ms/step
Epoch 115/200
1493/1493 - 87s - loss: 1.1685e-04 - val_loss: 1.7535e-04 - 87s/epoch - 58ms/step
Epoch 116/200
1493/1493 - 87s - loss: 1.1602e-04 - val_loss: 1.3873e-04 - 87s/epoch - 58ms/step
Epoch 117/200
1493/1493 - 87s - loss: 1.1179e-04 - val_loss: 2.1659e-04 - 87s/epoch - 58ms/step
Epoch 118/200
1493/1493 - 87s - loss: 1.2024e-04 - val_loss: 1.2539e-04 - 87s/epoch - 58ms/step
Epoch 119/200
1493/1493 - 87s - loss: 1.1302e-04 - val_loss: 1.0614e-04 - 87s/epoch - 58ms/step
Epoch 120/200
1493/1493 - 87s - loss: 1.0956e-04 - val_loss: 1.1436e-04 - 87s/epoch - 58ms/step
Epoch 121/200
1493/1493 - 87s - loss: 1.0881e-04 - val_loss: 1.5742e-04 - 87s/epoch - 58ms/step
Epoch 122/200
1493/1493 - 87s - loss: 1.0919e-04 - val_loss: 1.0034e-04 - 87s/epoch - 58ms/step
Epoch 123/200
1493/1493 - 87s - loss: 1.0723e-04 - val_loss: 1.2198e-04 - 87s/epoch - 58ms/step
Epoch 124/200
1493/1493 - 87s - loss: 1.0807e-04 - val_loss: 3.8572e-04 - 87s/epoch - 58ms/step
Epoch 125/200
1493/1493 - 87s - loss: 1.4123e-04 - val_loss: 4.3435e-04 - 87s/epoch - 58ms/step
Epoch 126/200
1493/1493 - 87s - loss: 1.4669e-04 - val_loss: 1.2918e-04 - 87s/epoch - 58ms/step
Epoch 127/200
1493/1493 - 87s - loss: 1.1256e-04 - val_loss: 1.4325e-04 - 87s/epoch - 58ms/step
Epoch 128/200
1493/1493 - 87s - loss: 1.0991e-04 - val_loss: 1.0272e-04 - 87s/epoch - 58ms/step
Epoch 129/200
1493/1493 - 87s - loss: 1.0582e-04 - val_loss: 1.1186e-04 - 87s/epoch - 58ms/step
Epoch 130/200
1493/1493 - 87s - loss: 1.0534e-04 - val_loss: 1.0361e-04 - 87s/epoch - 58ms/step
Epoch 131/200
1493/1493 - 87s - loss: 1.0912e-04 - val_loss: 5.5678e-04 - 87s/epoch - 58ms/step
Epoch 132/200
1493/1493 - 87s - loss: 1.5921e-04 - val_loss: 1.0065e-04 - 87s/epoch - 58ms/step
Epoch 133/200
1493/1493 - 87s - loss: 1.0858e-04 - val_loss: 1.0438e-04 - 87s/epoch - 58ms/step
Epoch 134/200
1493/1493 - 87s - loss: 1.0561e-04 - val_loss: 1.1192e-04 - 87s/epoch - 58ms/step
Epoch 135/200
1493/1493 - 87s - loss: 1.0483e-04 - val_loss: 9.6084e-05 - 87s/epoch - 58ms/step
Epoch 136/200
1493/1493 - 87s - loss: 1.0376e-04 - val_loss: 1.7197e-04 - 87s/epoch - 58ms/step
Epoch 137/200
1493/1493 - 87s - loss: 1.1604e-04 - val_loss: 1.3647e-04 - 87s/epoch - 58ms/step
Epoch 138/200
1493/1493 - 87s - loss: 1.1160e-04 - val_loss: 1.6561e-04 - 87s/epoch - 58ms/step
Epoch 139/200
1493/1493 - 87s - loss: 1.1535e-04 - val_loss: 1.8782e-04 - 87s/epoch - 58ms/step
Epoch 140/200
1493/1493 - 87s - loss: 1.2025e-04 - val_loss: 9.0623e-05 - 87s/epoch - 58ms/step
Epoch 141/200
1493/1493 - 87s - loss: 1.0271e-04 - val_loss: 1.5249e-04 - 87s/epoch - 58ms/step
Epoch 142/200
1493/1493 - 87s - loss: 1.0474e-04 - val_loss: 1.1099e-04 - 87s/epoch - 58ms/step
Epoch 143/200
1493/1493 - 87s - loss: 1.0032e-04 - val_loss: 1.5760e-04 - 87s/epoch - 58ms/step
Epoch 144/200
1493/1493 - 87s - loss: 1.0353e-04 - val_loss: 9.9084e-05 - 87s/epoch - 58ms/step
Epoch 145/200
1493/1493 - 87s - loss: 9.8903e-05 - val_loss: 9.6080e-05 - 87s/epoch - 58ms/step
Epoch 146/200
1493/1493 - 87s - loss: 9.8251e-05 - val_loss: 1.1673e-04 - 87s/epoch - 58ms/step
Epoch 147/200
1493/1493 - 87s - loss: 9.7881e-05 - val_loss: 1.0759e-04 - 87s/epoch - 58ms/step
Epoch 148/200
1493/1493 - 87s - loss: 9.6640e-05 - val_loss: 1.6434e-04 - 87s/epoch - 58ms/step
Epoch 149/200
1493/1493 - 87s - loss: 9.7692e-05 - val_loss: 1.0137e-04 - 87s/epoch - 58ms/step
Epoch 150/200
1493/1493 - 87s - loss: 9.5731e-05 - val_loss: 9.7939e-05 - 87s/epoch - 58ms/step
Epoch 151/200
1493/1493 - 87s - loss: 9.6066e-05 - val_loss: 1.1339e-04 - 87s/epoch - 58ms/step
Epoch 152/200
1493/1493 - 87s - loss: 9.6868e-05 - val_loss: 1.5141e-04 - 87s/epoch - 58ms/step
Epoch 153/200
1493/1493 - 87s - loss: 1.0130e-04 - val_loss: 1.0381e-04 - 87s/epoch - 58ms/step
Epoch 154/200
1493/1493 - 87s - loss: 9.5656e-05 - val_loss: 1.5958e-04 - 87s/epoch - 58ms/step
Epoch 155/200
1493/1493 - 87s - loss: 1.0538e-04 - val_loss: 1.0019e-04 - 87s/epoch - 58ms/step
Epoch 156/200
1493/1493 - 87s - loss: 9.9255e-05 - val_loss: 9.9364e-05 - 87s/epoch - 58ms/step
Epoch 157/200
1493/1493 - 87s - loss: 9.3372e-05 - val_loss: 1.0431e-04 - 87s/epoch - 58ms/step
Epoch 158/200
1493/1493 - 87s - loss: 9.4054e-05 - val_loss: 1.0578e-04 - 87s/epoch - 58ms/step
Epoch 159/200
1493/1493 - 87s - loss: 9.2990e-05 - val_loss: 1.0558e-04 - 87s/epoch - 58ms/step
Epoch 160/200
1493/1493 - 87s - loss: 9.2545e-05 - val_loss: 1.0379e-04 - 87s/epoch - 58ms/step
Epoch 161/200
1493/1493 - 87s - loss: 9.7016e-05 - val_loss: 9.7894e-05 - 87s/epoch - 58ms/step
Epoch 162/200
1493/1493 - 87s - loss: 9.2600e-05 - val_loss: 9.6950e-05 - 87s/epoch - 58ms/step
Epoch 163/200
1493/1493 - 87s - loss: 9.2075e-05 - val_loss: 1.7945e-04 - 87s/epoch - 58ms/step
Epoch 164/200
1493/1493 - 87s - loss: 1.0333e-04 - val_loss: 9.3953e-05 - 87s/epoch - 58ms/step
Epoch 165/200
1493/1493 - 87s - loss: 9.5515e-05 - val_loss: 9.9927e-05 - 87s/epoch - 58ms/step
Epoch 166/200
1493/1493 - 87s - loss: 9.2066e-05 - val_loss: 9.8978e-05 - 87s/epoch - 58ms/step
Epoch 167/200
1493/1493 - 87s - loss: 9.1315e-05 - val_loss: 8.6115e-05 - 87s/epoch - 58ms/step
Epoch 168/200
1493/1493 - 87s - loss: 9.3662e-05 - val_loss: 9.2734e-05 - 87s/epoch - 58ms/step
Epoch 169/200
1493/1493 - 87s - loss: 9.2869e-05 - val_loss: 1.0151e-04 - 87s/epoch - 58ms/step
Epoch 170/200
1493/1493 - 87s - loss: 9.3339e-05 - val_loss: 2.7948e-04 - 87s/epoch - 58ms/step
Epoch 171/200
1493/1493 - 87s - loss: 1.3672e-04 - val_loss: 4.9379e-04 - 87s/epoch - 58ms/step
Epoch 172/200
1493/1493 - 87s - loss: 1.4644e-04 - val_loss: 9.4942e-05 - 87s/epoch - 58ms/step
Epoch 173/200
1493/1493 - 87s - loss: 9.9158e-05 - val_loss: 9.2090e-05 - 87s/epoch - 58ms/step
Epoch 174/200
1493/1493 - 87s - loss: 9.3955e-05 - val_loss: 9.2430e-05 - 87s/epoch - 58ms/step
Epoch 175/200
1493/1493 - 87s - loss: 9.3197e-05 - val_loss: 1.0863e-04 - 87s/epoch - 58ms/step
Epoch 176/200
1493/1493 - 87s - loss: 9.2579e-05 - val_loss: 9.3279e-05 - 87s/epoch - 58ms/step
Epoch 177/200
1493/1493 - 87s - loss: 9.1331e-05 - val_loss: 8.6777e-05 - 87s/epoch - 58ms/step
Epoch 178/200
1493/1493 - 87s - loss: 8.9652e-05 - val_loss: 9.0471e-05 - 87s/epoch - 58ms/step
Epoch 179/200
1493/1493 - 87s - loss: 8.8550e-05 - val_loss: 9.6403e-05 - 87s/epoch - 58ms/step
Epoch 180/200
1493/1493 - 87s - loss: 8.8970e-05 - val_loss: 1.2635e-04 - 87s/epoch - 58ms/step
Epoch 181/200
1493/1493 - 87s - loss: 9.4893e-05 - val_loss: 8.5677e-05 - 87s/epoch - 58ms/step
Epoch 182/200
1493/1493 - 87s - loss: 8.8802e-05 - val_loss: 1.1339e-04 - 87s/epoch - 58ms/step
Epoch 183/200
1493/1493 - 87s - loss: 8.8431e-05 - val_loss: 9.7356e-05 - 87s/epoch - 58ms/step
Epoch 184/200
1493/1493 - 87s - loss: 8.8314e-05 - val_loss: 8.8816e-05 - 87s/epoch - 58ms/step
Epoch 185/200
1493/1493 - 87s - loss: 8.9143e-05 - val_loss: 3.0471e-04 - 87s/epoch - 58ms/step
Epoch 186/200
1493/1493 - 87s - loss: 1.2852e-04 - val_loss: 8.2900e-05 - 87s/epoch - 58ms/step
Epoch 187/200
1493/1493 - 87s - loss: 9.1194e-05 - val_loss: 9.0138e-05 - 87s/epoch - 58ms/step
Epoch 188/200
1493/1493 - 87s - loss: 9.3068e-05 - val_loss: 1.0153e-04 - 87s/epoch - 58ms/step
Epoch 189/200
1493/1493 - 87s - loss: 8.8053e-05 - val_loss: 1.0249e-04 - 87s/epoch - 58ms/step
Epoch 190/200
1493/1493 - 87s - loss: 8.8501e-05 - val_loss: 9.4419e-05 - 87s/epoch - 58ms/step
Epoch 191/200
1493/1493 - 87s - loss: 8.8499e-05 - val_loss: 1.2780e-04 - 87s/epoch - 58ms/step
Epoch 192/200
1493/1493 - 87s - loss: 8.8262e-05 - val_loss: 1.0018e-04 - 87s/epoch - 58ms/step
Epoch 193/200
1493/1493 - 87s - loss: 8.9132e-05 - val_loss: 2.3704e-04 - 87s/epoch - 58ms/step
Epoch 194/200
1493/1493 - 87s - loss: 1.1402e-04 - val_loss: 4.4165e-04 - 87s/epoch - 58ms/step
Epoch 195/200
1493/1493 - 87s - loss: 1.2427e-04 - val_loss: 1.8228e-04 - 87s/epoch - 58ms/step
Epoch 196/200
1493/1493 - 87s - loss: 1.1250e-04 - val_loss: 7.8473e-05 - 87s/epoch - 58ms/step
Epoch 197/200
1493/1493 - 87s - loss: 9.0814e-05 - val_loss: 1.4765e-04 - 87s/epoch - 58ms/step
Epoch 198/200
1493/1493 - 87s - loss: 1.0079e-04 - val_loss: 1.4402e-04 - 87s/epoch - 58ms/step
Epoch 199/200
1493/1493 - 87s - loss: 9.2453e-05 - val_loss: 8.1130e-05 - 87s/epoch - 58ms/step
Epoch 200/200
1493/1493 - 87s - loss: 8.6606e-05 - val_loss: 8.4311e-05 - 87s/epoch - 58ms/step
COMPRESSED VECTOR SIZE: 821
Loss in the autoencoder: 8.431119204033166e-05
  1/332 [..............................] - ETA: 25s  9/332 [..............................] - ETA: 2s  17/332 [>.............................] - ETA: 2s 25/332 [=>............................] - ETA: 2s 33/332 [=>............................] - ETA: 2s 41/332 [==>...........................] - ETA: 2s 49/332 [===>..........................] - ETA: 1s 57/332 [====>.........................] - ETA: 1s 65/332 [====>.........................] - ETA: 1s 73/332 [=====>........................] - ETA: 1s 81/332 [======>.......................] - ETA: 1s 89/332 [=======>......................] - ETA: 1s 97/332 [=======>......................] - ETA: 1s105/332 [========>.....................] - ETA: 1s113/332 [=========>....................] - ETA: 1s121/332 [=========>....................] - ETA: 1s129/332 [==========>...................] - ETA: 1s137/332 [===========>..................] - ETA: 1s145/332 [============>.................] - ETA: 1s153/332 [============>.................] - ETA: 1s161/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s177/332 [==============>...............] - ETA: 1s185/332 [===============>..............] - ETA: 1s193/332 [================>.............] - ETA: 0s201/332 [=================>............] - ETA: 0s209/332 [=================>............] - ETA: 0s217/332 [==================>...........] - ETA: 0s225/332 [===================>..........] - ETA: 0s233/332 [====================>.........] - ETA: 0s241/332 [====================>.........] - ETA: 0s249/332 [=====================>........] - ETA: 0s257/332 [======================>.......] - ETA: 0s265/332 [======================>.......] - ETA: 0s273/332 [=======================>......] - ETA: 0s281/332 [========================>.....] - ETA: 0s289/332 [=========================>....] - ETA: 0s297/332 [=========================>....] - ETA: 0s305/332 [==========================>...] - ETA: 0s313/332 [===========================>..] - ETA: 0s321/332 [============================>.] - ETA: 0s329/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 7ms/step
correlation 0.0009581813700370774
cosine 0.0007561417264934759
MAE: 0.0051078107
RMSE: 0.0091821095
r2: 0.9945315623762949
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       multiple                  0         
                                                                 
 dense_21 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_21 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 821)               2283201   
                                                                 
 batch_normalization_22 (Bat  (None, 821)              3284      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 821)               0         
                                                                 
 dense_22 (Dense)            (None, 2780)              2285160   
                                                                 
 batch_normalization_23 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2780)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 11,625,769
Trainable params: 11,613,007
Non-trainable params: 12,762
_________________________________________________________________
Encoder
Model: "model_22"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_23 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_22 (InputLayer)       multiple                  0         
                                                                 
 dense_21 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_21 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_21 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 821)               2283201   
                                                                 
=================================================================
Total params: 5,811,021
Trainable params: 5,805,461
Non-trainable params: 5,560
_________________________________________________________________
Decoder
Model: "model_23"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_24 (InputLayer)       [(None, 821)]             0         
                                                                 
 batch_normalization_22 (Bat  (None, 821)              3284      
 chNormalization)                                                
                                                                 
 re_lu_22 (ReLU)             (None, 821)               0         
                                                                 
 dense_22 (Dense)            (None, 2780)              2285160   
                                                                 
 batch_normalization_23 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_23 (ReLU)             (None, 2780)              0         
                                                                 
 dense_23 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 5,814,748
Trainable params: 5,807,546
Non-trainable params: 7,202
_________________________________________________________________
['2.2custom_n_b', 'mse', 64, 200, 0.0005, 0.65, 821, 8.660641469759867e-05, 8.431119204033166e-05, 0.0009581813700370774, 0.0007561417264934759, 0.0051078107208013535, 0.009182109497487545, 0.9945315623762949, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_2.2final_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_24"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_25 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_24 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_24 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 758)               2107998   
                                                                 
 batch_normalization_25 (Bat  (None, 758)              3032      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 758)               0         
                                                                 
 dense_25 (Dense)            (None, 2780)              2110020   
                                                                 
 batch_normalization_26 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2780)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 11,275,174
Trainable params: 11,262,538
Non-trainable params: 12,636
_________________________________________________________________
Epoch 1/200
1493/1493 - 86s - loss: 0.0093 - val_loss: 0.0039 - 86s/epoch - 58ms/step
Epoch 2/200
1493/1493 - 86s - loss: 0.0029 - val_loss: 0.0032 - 86s/epoch - 57ms/step
Epoch 3/200
1493/1493 - 86s - loss: 0.0021 - val_loss: 0.0017 - 86s/epoch - 57ms/step
Epoch 4/200
1493/1493 - 86s - loss: 0.0017 - val_loss: 0.0029 - 86s/epoch - 57ms/step
Epoch 5/200
1493/1493 - 86s - loss: 0.0016 - val_loss: 0.0013 - 86s/epoch - 57ms/step
Epoch 6/200
1493/1493 - 86s - loss: 0.0014 - val_loss: 0.0013 - 86s/epoch - 57ms/step
Epoch 7/200
1493/1493 - 86s - loss: 0.0015 - val_loss: 9.9526e-04 - 86s/epoch - 57ms/step
Epoch 8/200
1493/1493 - 86s - loss: 0.0011 - val_loss: 0.0013 - 86s/epoch - 57ms/step
Epoch 9/200
1493/1493 - 86s - loss: 0.0010 - val_loss: 0.0021 - 86s/epoch - 57ms/step
Epoch 10/200
1493/1493 - 86s - loss: 0.0011 - val_loss: 8.7971e-04 - 86s/epoch - 57ms/step
Epoch 11/200
1493/1493 - 85s - loss: 8.8006e-04 - val_loss: 8.8788e-04 - 85s/epoch - 57ms/step
Epoch 12/200
1493/1493 - 86s - loss: 7.4548e-04 - val_loss: 8.9675e-04 - 86s/epoch - 57ms/step
Epoch 13/200
1493/1493 - 85s - loss: 7.0089e-04 - val_loss: 0.0010 - 85s/epoch - 57ms/step
Epoch 14/200
1493/1493 - 86s - loss: 6.7273e-04 - val_loss: 5.7426e-04 - 86s/epoch - 57ms/step
Epoch 15/200
1493/1493 - 85s - loss: 6.2180e-04 - val_loss: 6.3640e-04 - 85s/epoch - 57ms/step
Epoch 16/200
1493/1493 - 85s - loss: 5.6406e-04 - val_loss: 5.2013e-04 - 85s/epoch - 57ms/step
Epoch 17/200
1493/1493 - 86s - loss: 5.1666e-04 - val_loss: 6.0267e-04 - 86s/epoch - 57ms/step
Epoch 18/200
1493/1493 - 86s - loss: 4.8875e-04 - val_loss: 6.5063e-04 - 86s/epoch - 57ms/step
Epoch 19/200
1493/1493 - 86s - loss: 4.7532e-04 - val_loss: 5.7261e-04 - 86s/epoch - 57ms/step
Epoch 20/200
1493/1493 - 86s - loss: 4.4943e-04 - val_loss: 4.1872e-04 - 86s/epoch - 57ms/step
Epoch 21/200
1493/1493 - 86s - loss: 4.1572e-04 - val_loss: 5.7556e-04 - 86s/epoch - 57ms/step
Epoch 22/200
1493/1493 - 86s - loss: 3.9826e-04 - val_loss: 0.0011 - 86s/epoch - 57ms/step
Epoch 23/200
1493/1493 - 85s - loss: 4.6565e-04 - val_loss: 0.0015 - 85s/epoch - 57ms/step
Epoch 24/200
1493/1493 - 86s - loss: 4.5475e-04 - val_loss: 3.5124e-04 - 86s/epoch - 57ms/step
Epoch 25/200
1493/1493 - 86s - loss: 3.5818e-04 - val_loss: 3.1900e-04 - 86s/epoch - 57ms/step
Epoch 26/200
1493/1493 - 86s - loss: 3.3680e-04 - val_loss: 4.3903e-04 - 86s/epoch - 57ms/step
Epoch 27/200
1493/1493 - 86s - loss: 3.2233e-04 - val_loss: 3.2818e-04 - 86s/epoch - 57ms/step
Epoch 28/200
1493/1493 - 86s - loss: 3.0524e-04 - val_loss: 6.5024e-04 - 86s/epoch - 57ms/step
Epoch 29/200
1493/1493 - 86s - loss: 3.0648e-04 - val_loss: 6.6006e-04 - 86s/epoch - 57ms/step
Epoch 30/200
1493/1493 - 86s - loss: 3.2575e-04 - val_loss: 3.1266e-04 - 86s/epoch - 57ms/step
Epoch 31/200
1493/1493 - 86s - loss: 2.8153e-04 - val_loss: 6.0201e-04 - 86s/epoch - 57ms/step
Epoch 32/200
1493/1493 - 86s - loss: 3.1603e-04 - val_loss: 2.7362e-04 - 86s/epoch - 57ms/step
Epoch 33/200
1493/1493 - 86s - loss: 2.7528e-04 - val_loss: 2.8515e-04 - 86s/epoch - 57ms/step
Epoch 34/200
1493/1493 - 86s - loss: 2.5978e-04 - val_loss: 2.7748e-04 - 86s/epoch - 57ms/step
Epoch 35/200
1493/1493 - 86s - loss: 2.4848e-04 - val_loss: 4.0222e-04 - 86s/epoch - 57ms/step
Epoch 36/200
1493/1493 - 86s - loss: 2.6273e-04 - val_loss: 2.8354e-04 - 86s/epoch - 57ms/step
Epoch 37/200
1493/1493 - 86s - loss: 2.4662e-04 - val_loss: 9.1842e-04 - 86s/epoch - 57ms/step
Epoch 38/200
1493/1493 - 86s - loss: 3.1775e-04 - val_loss: 2.0653e-04 - 86s/epoch - 57ms/step
Epoch 39/200
1493/1493 - 86s - loss: 2.3219e-04 - val_loss: 2.2087e-04 - 86s/epoch - 57ms/step
Epoch 40/200
1493/1493 - 86s - loss: 2.2547e-04 - val_loss: 2.8539e-04 - 86s/epoch - 57ms/step
Epoch 41/200
1493/1493 - 86s - loss: 2.2651e-04 - val_loss: 2.0139e-04 - 86s/epoch - 57ms/step
Epoch 42/200
1493/1493 - 86s - loss: 2.1850e-04 - val_loss: 2.1680e-04 - 86s/epoch - 57ms/step
Epoch 43/200
1493/1493 - 86s - loss: 2.1637e-04 - val_loss: 3.2528e-04 - 86s/epoch - 57ms/step
Epoch 44/200
1493/1493 - 86s - loss: 2.1909e-04 - val_loss: 3.3525e-04 - 86s/epoch - 57ms/step
Epoch 45/200
1493/1493 - 86s - loss: 2.2051e-04 - val_loss: 1.7568e-04 - 86s/epoch - 57ms/step
Epoch 46/200
1493/1493 - 86s - loss: 2.0436e-04 - val_loss: 1.9688e-04 - 86s/epoch - 57ms/step
Epoch 47/200
1493/1493 - 86s - loss: 2.0004e-04 - val_loss: 2.3773e-04 - 86s/epoch - 57ms/step
Epoch 48/200
1493/1493 - 86s - loss: 1.9932e-04 - val_loss: 2.3563e-04 - 86s/epoch - 57ms/step
Epoch 49/200
1493/1493 - 86s - loss: 1.9942e-04 - val_loss: 0.0017 - 86s/epoch - 57ms/step
Epoch 50/200
1493/1493 - 86s - loss: 3.6908e-04 - val_loss: 0.0021 - 86s/epoch - 57ms/step
Epoch 51/200
1493/1493 - 86s - loss: 3.6456e-04 - val_loss: 1.8565e-04 - 86s/epoch - 57ms/step
Epoch 52/200
1493/1493 - 86s - loss: 2.0907e-04 - val_loss: 1.9840e-04 - 86s/epoch - 57ms/step
Epoch 53/200
1493/1493 - 86s - loss: 2.0424e-04 - val_loss: 1.7713e-04 - 86s/epoch - 57ms/step
Epoch 54/200
1493/1493 - 85s - loss: 1.9374e-04 - val_loss: 2.1151e-04 - 85s/epoch - 57ms/step
Epoch 55/200
1493/1493 - 86s - loss: 1.8603e-04 - val_loss: 1.8736e-04 - 86s/epoch - 57ms/step
Epoch 56/200
1493/1493 - 86s - loss: 1.8440e-04 - val_loss: 2.1453e-04 - 86s/epoch - 57ms/step
Epoch 57/200
1493/1493 - 85s - loss: 1.7814e-04 - val_loss: 2.0577e-04 - 85s/epoch - 57ms/step
Epoch 58/200
1493/1493 - 86s - loss: 1.7619e-04 - val_loss: 1.6706e-04 - 86s/epoch - 57ms/step
Epoch 59/200
1493/1493 - 86s - loss: 1.7250e-04 - val_loss: 1.7240e-04 - 86s/epoch - 57ms/step
Epoch 60/200
1493/1493 - 86s - loss: 1.7271e-04 - val_loss: 4.3536e-04 - 86s/epoch - 57ms/step
Epoch 61/200
1493/1493 - 86s - loss: 2.6185e-04 - val_loss: 1.5912e-04 - 86s/epoch - 57ms/step
Epoch 62/200
1493/1493 - 86s - loss: 1.7324e-04 - val_loss: 2.1019e-04 - 86s/epoch - 57ms/step
Epoch 63/200
1493/1493 - 86s - loss: 1.6732e-04 - val_loss: 2.4203e-04 - 86s/epoch - 57ms/step
Epoch 64/200
1493/1493 - 86s - loss: 1.6911e-04 - val_loss: 0.0012 - 86s/epoch - 57ms/step
Epoch 65/200
1493/1493 - 86s - loss: 2.5581e-04 - val_loss: 3.3677e-04 - 86s/epoch - 57ms/step
Epoch 66/200
1493/1493 - 86s - loss: 1.8479e-04 - val_loss: 1.9681e-04 - 86s/epoch - 57ms/step
Epoch 67/200
1493/1493 - 86s - loss: 1.6584e-04 - val_loss: 1.5499e-04 - 86s/epoch - 57ms/step
Epoch 68/200
1493/1493 - 85s - loss: 1.6071e-04 - val_loss: 3.4751e-04 - 85s/epoch - 57ms/step
Epoch 69/200
1493/1493 - 86s - loss: 1.8138e-04 - val_loss: 1.8252e-04 - 86s/epoch - 57ms/step
Epoch 70/200
1493/1493 - 86s - loss: 1.5730e-04 - val_loss: 1.9317e-04 - 86s/epoch - 57ms/step
Epoch 71/200
1493/1493 - 85s - loss: 1.5669e-04 - val_loss: 1.4751e-04 - 85s/epoch - 57ms/step
Epoch 72/200
1493/1493 - 86s - loss: 1.5559e-04 - val_loss: 2.1374e-04 - 86s/epoch - 57ms/step
Epoch 73/200
1493/1493 - 86s - loss: 1.5643e-04 - val_loss: 2.0639e-04 - 86s/epoch - 57ms/step
Epoch 74/200
1493/1493 - 86s - loss: 1.5576e-04 - val_loss: 1.6956e-04 - 86s/epoch - 57ms/step
Epoch 75/200
1493/1493 - 86s - loss: 1.4782e-04 - val_loss: 1.5621e-04 - 86s/epoch - 57ms/step
Epoch 76/200
1493/1493 - 85s - loss: 1.4517e-04 - val_loss: 1.5340e-04 - 85s/epoch - 57ms/step
Epoch 77/200
1493/1493 - 85s - loss: 1.4568e-04 - val_loss: 3.4247e-04 - 85s/epoch - 57ms/step
Epoch 78/200
1493/1493 - 85s - loss: 1.5173e-04 - val_loss: 2.7751e-04 - 85s/epoch - 57ms/step
Epoch 79/200
1493/1493 - 85s - loss: 1.5797e-04 - val_loss: 3.4330e-04 - 85s/epoch - 57ms/step
Epoch 80/200
1493/1493 - 86s - loss: 1.9070e-04 - val_loss: 1.3882e-04 - 86s/epoch - 57ms/step
Epoch 81/200
1493/1493 - 85s - loss: 1.4505e-04 - val_loss: 2.8310e-04 - 85s/epoch - 57ms/step
Epoch 82/200
1493/1493 - 86s - loss: 1.4230e-04 - val_loss: 1.4739e-04 - 86s/epoch - 57ms/step
Epoch 83/200
1493/1493 - 86s - loss: 1.3817e-04 - val_loss: 1.5146e-04 - 86s/epoch - 57ms/step
Epoch 84/200
1493/1493 - 86s - loss: 1.3692e-04 - val_loss: 1.5713e-04 - 86s/epoch - 57ms/step
Epoch 85/200
1493/1493 - 86s - loss: 1.5079e-04 - val_loss: 1.5028e-04 - 86s/epoch - 57ms/step
Epoch 86/200
1493/1493 - 86s - loss: 1.4139e-04 - val_loss: 1.4716e-04 - 86s/epoch - 57ms/step
Epoch 87/200
1493/1493 - 86s - loss: 1.3354e-04 - val_loss: 1.4468e-04 - 86s/epoch - 57ms/step
Epoch 88/200
1493/1493 - 86s - loss: 1.3253e-04 - val_loss: 1.7821e-04 - 86s/epoch - 57ms/step
Epoch 89/200
1493/1493 - 85s - loss: 1.3686e-04 - val_loss: 1.4050e-04 - 85s/epoch - 57ms/step
Epoch 90/200
1493/1493 - 86s - loss: 1.3065e-04 - val_loss: 1.5062e-04 - 86s/epoch - 57ms/step
Epoch 91/200
1493/1493 - 86s - loss: 1.2951e-04 - val_loss: 1.4767e-04 - 86s/epoch - 57ms/step
Epoch 92/200
1493/1493 - 86s - loss: 1.3053e-04 - val_loss: 5.6893e-04 - 86s/epoch - 57ms/step
Epoch 93/200
1493/1493 - 86s - loss: 1.8662e-04 - val_loss: 4.1721e-04 - 86s/epoch - 57ms/step
Epoch 94/200
1493/1493 - 86s - loss: 1.5884e-04 - val_loss: 1.1235e-04 - 86s/epoch - 57ms/step
Epoch 95/200
1493/1493 - 86s - loss: 1.3196e-04 - val_loss: 1.3924e-04 - 86s/epoch - 57ms/step
Epoch 96/200
1493/1493 - 86s - loss: 1.2969e-04 - val_loss: 1.4114e-04 - 86s/epoch - 57ms/step
Epoch 97/200
1493/1493 - 86s - loss: 1.2798e-04 - val_loss: 1.2616e-04 - 86s/epoch - 57ms/step
Epoch 98/200
1493/1493 - 86s - loss: 1.3358e-04 - val_loss: 6.4569e-04 - 86s/epoch - 57ms/step
Epoch 99/200
1493/1493 - 85s - loss: 2.3958e-04 - val_loss: 1.1711e-04 - 85s/epoch - 57ms/step
Epoch 100/200
1493/1493 - 86s - loss: 1.4616e-04 - val_loss: 1.2218e-04 - 86s/epoch - 57ms/step
Epoch 101/200
1493/1493 - 86s - loss: 1.3329e-04 - val_loss: 1.2668e-04 - 86s/epoch - 57ms/step
Epoch 102/200
1493/1493 - 86s - loss: 1.3072e-04 - val_loss: 1.6352e-04 - 86s/epoch - 57ms/step
Epoch 103/200
1493/1493 - 86s - loss: 1.3101e-04 - val_loss: 1.2113e-04 - 86s/epoch - 57ms/step
Epoch 104/200
1493/1493 - 86s - loss: 1.2477e-04 - val_loss: 2.3492e-04 - 86s/epoch - 57ms/step
Epoch 105/200
1493/1493 - 86s - loss: 1.3145e-04 - val_loss: 1.2380e-04 - 86s/epoch - 57ms/step
Epoch 106/200
1493/1493 - 86s - loss: 1.2213e-04 - val_loss: 1.3488e-04 - 86s/epoch - 57ms/step
Epoch 107/200
1493/1493 - 86s - loss: 1.2132e-04 - val_loss: 1.3691e-04 - 86s/epoch - 57ms/step
Epoch 108/200
1493/1493 - 86s - loss: 1.2408e-04 - val_loss: 1.3891e-04 - 86s/epoch - 57ms/step
Epoch 109/200
1493/1493 - 86s - loss: 1.2091e-04 - val_loss: 1.4446e-04 - 86s/epoch - 57ms/step
Epoch 110/200
1493/1493 - 85s - loss: 1.1837e-04 - val_loss: 1.2926e-04 - 85s/epoch - 57ms/step
Epoch 111/200
1493/1493 - 86s - loss: 1.1792e-04 - val_loss: 2.1017e-04 - 86s/epoch - 57ms/step
Epoch 112/200
1493/1493 - 86s - loss: 1.3460e-04 - val_loss: 1.1758e-04 - 86s/epoch - 57ms/step
Epoch 113/200
1493/1493 - 85s - loss: 1.1664e-04 - val_loss: 2.8882e-04 - 85s/epoch - 57ms/step
Epoch 114/200
1493/1493 - 85s - loss: 1.4152e-04 - val_loss: 1.2484e-04 - 85s/epoch - 57ms/step
Epoch 115/200
1493/1493 - 86s - loss: 1.1967e-04 - val_loss: 2.3689e-04 - 86s/epoch - 57ms/step
Epoch 116/200
1493/1493 - 86s - loss: 1.1669e-04 - val_loss: 1.4438e-04 - 86s/epoch - 57ms/step
Epoch 117/200
1493/1493 - 86s - loss: 1.1439e-04 - val_loss: 2.2964e-04 - 86s/epoch - 57ms/step
Epoch 118/200
1493/1493 - 86s - loss: 1.2672e-04 - val_loss: 1.1804e-04 - 86s/epoch - 57ms/step
Epoch 119/200
1493/1493 - 86s - loss: 1.1544e-04 - val_loss: 1.1447e-04 - 86s/epoch - 57ms/step
Epoch 120/200
1493/1493 - 86s - loss: 1.1309e-04 - val_loss: 1.1638e-04 - 86s/epoch - 57ms/step
Epoch 121/200
1493/1493 - 86s - loss: 1.1242e-04 - val_loss: 1.6601e-04 - 86s/epoch - 57ms/step
Epoch 122/200
1493/1493 - 86s - loss: 1.1404e-04 - val_loss: 1.0496e-04 - 86s/epoch - 57ms/step
Epoch 123/200
1493/1493 - 86s - loss: 1.1038e-04 - val_loss: 1.2090e-04 - 86s/epoch - 57ms/step
Epoch 124/200
1493/1493 - 86s - loss: 1.0948e-04 - val_loss: 1.9880e-04 - 86s/epoch - 57ms/step
Epoch 125/200
1493/1493 - 86s - loss: 1.2129e-04 - val_loss: 1.3820e-04 - 86s/epoch - 57ms/step
Epoch 126/200
1493/1493 - 86s - loss: 1.2114e-04 - val_loss: 1.7497e-04 - 86s/epoch - 57ms/step
Epoch 127/200
1493/1493 - 86s - loss: 1.1538e-04 - val_loss: 1.4514e-04 - 86s/epoch - 57ms/step
Epoch 128/200
1493/1493 - 86s - loss: 1.1158e-04 - val_loss: 1.0635e-04 - 86s/epoch - 57ms/step
Epoch 129/200
1493/1493 - 86s - loss: 1.0758e-04 - val_loss: 1.0928e-04 - 86s/epoch - 57ms/step
Epoch 130/200
1493/1493 - 86s - loss: 1.0726e-04 - val_loss: 1.0646e-04 - 86s/epoch - 57ms/step
Epoch 131/200
1493/1493 - 86s - loss: 1.1011e-04 - val_loss: 3.9582e-04 - 86s/epoch - 57ms/step
Epoch 132/200
1493/1493 - 86s - loss: 1.5322e-04 - val_loss: 1.0262e-04 - 86s/epoch - 57ms/step
Epoch 133/200
1493/1493 - 86s - loss: 1.1115e-04 - val_loss: 1.0954e-04 - 86s/epoch - 57ms/step
Epoch 134/200
1493/1493 - 86s - loss: 1.0744e-04 - val_loss: 1.1087e-04 - 86s/epoch - 57ms/step
Epoch 135/200
1493/1493 - 86s - loss: 1.0637e-04 - val_loss: 1.0717e-04 - 86s/epoch - 57ms/step
Epoch 136/200
1493/1493 - 85s - loss: 1.0584e-04 - val_loss: 1.1924e-04 - 85s/epoch - 57ms/step
Epoch 137/200
1493/1493 - 86s - loss: 1.1232e-04 - val_loss: 1.2252e-04 - 86s/epoch - 57ms/step
Epoch 138/200
1493/1493 - 86s - loss: 1.0717e-04 - val_loss: 1.0951e-04 - 86s/epoch - 57ms/step
Epoch 139/200
1493/1493 - 86s - loss: 1.0730e-04 - val_loss: 1.4447e-04 - 86s/epoch - 57ms/step
Epoch 140/200
1493/1493 - 86s - loss: 1.1407e-04 - val_loss: 9.4323e-05 - 86s/epoch - 57ms/step
Epoch 141/200
1493/1493 - 86s - loss: 1.0540e-04 - val_loss: 1.2505e-04 - 86s/epoch - 57ms/step
Epoch 142/200
1493/1493 - 86s - loss: 1.0699e-04 - val_loss: 1.1830e-04 - 86s/epoch - 57ms/step
Epoch 143/200
1493/1493 - 86s - loss: 1.0259e-04 - val_loss: 1.2154e-04 - 86s/epoch - 57ms/step
Epoch 144/200
1493/1493 - 86s - loss: 1.0751e-04 - val_loss: 1.1212e-04 - 86s/epoch - 57ms/step
Epoch 145/200
1493/1493 - 86s - loss: 1.0253e-04 - val_loss: 1.0091e-04 - 86s/epoch - 57ms/step
Epoch 146/200
1493/1493 - 86s - loss: 1.0117e-04 - val_loss: 1.0305e-04 - 86s/epoch - 57ms/step
Epoch 147/200
1493/1493 - 85s - loss: 9.9883e-05 - val_loss: 1.2320e-04 - 85s/epoch - 57ms/step
Epoch 148/200
1493/1493 - 86s - loss: 1.0016e-04 - val_loss: 1.9133e-04 - 86s/epoch - 57ms/step
Epoch 149/200
1493/1493 - 86s - loss: 9.9697e-05 - val_loss: 1.0637e-04 - 86s/epoch - 57ms/step
Epoch 150/200
1493/1493 - 86s - loss: 9.9017e-05 - val_loss: 9.8929e-05 - 86s/epoch - 57ms/step
Epoch 151/200
1493/1493 - 86s - loss: 1.0318e-04 - val_loss: 1.1724e-04 - 86s/epoch - 57ms/step
Epoch 152/200
1493/1493 - 86s - loss: 1.0032e-04 - val_loss: 1.5833e-04 - 86s/epoch - 57ms/step
Epoch 153/200
1493/1493 - 86s - loss: 1.0256e-04 - val_loss: 1.0394e-04 - 86s/epoch - 57ms/step
Epoch 154/200
1493/1493 - 86s - loss: 1.0104e-04 - val_loss: 3.0065e-04 - 86s/epoch - 57ms/step
Epoch 155/200
1493/1493 - 86s - loss: 1.4555e-04 - val_loss: 1.0167e-04 - 86s/epoch - 57ms/step
Epoch 156/200
1493/1493 - 86s - loss: 1.1187e-04 - val_loss: 1.0269e-04 - 86s/epoch - 57ms/step
Epoch 157/200
1493/1493 - 86s - loss: 9.9194e-05 - val_loss: 1.0831e-04 - 86s/epoch - 57ms/step
Epoch 158/200
1493/1493 - 86s - loss: 9.9572e-05 - val_loss: 1.1011e-04 - 86s/epoch - 57ms/step
Epoch 159/200
1493/1493 - 86s - loss: 9.7709e-05 - val_loss: 9.5702e-05 - 86s/epoch - 57ms/step
Epoch 160/200
1493/1493 - 86s - loss: 9.7266e-05 - val_loss: 1.1160e-04 - 86s/epoch - 57ms/step
Epoch 161/200
1493/1493 - 86s - loss: 1.0264e-04 - val_loss: 9.8216e-05 - 86s/epoch - 57ms/step
Epoch 162/200
1493/1493 - 86s - loss: 9.6760e-05 - val_loss: 9.9640e-05 - 86s/epoch - 57ms/step
Epoch 163/200
1493/1493 - 86s - loss: 9.6085e-05 - val_loss: 1.1206e-04 - 86s/epoch - 57ms/step
Epoch 164/200
1493/1493 - 86s - loss: 9.7785e-05 - val_loss: 1.0972e-04 - 86s/epoch - 57ms/step
Epoch 165/200
1493/1493 - 86s - loss: 1.2602e-04 - val_loss: 9.7725e-05 - 86s/epoch - 57ms/step
Epoch 166/200
1493/1493 - 86s - loss: 9.8242e-05 - val_loss: 1.0905e-04 - 86s/epoch - 57ms/step
Epoch 167/200
1493/1493 - 86s - loss: 9.6101e-05 - val_loss: 9.1416e-05 - 86s/epoch - 57ms/step
Epoch 168/200
1493/1493 - 86s - loss: 9.4621e-05 - val_loss: 8.6174e-05 - 86s/epoch - 57ms/step
Epoch 169/200
1493/1493 - 86s - loss: 9.4989e-05 - val_loss: 1.2825e-04 - 86s/epoch - 57ms/step
Epoch 170/200
1493/1493 - 86s - loss: 9.6105e-05 - val_loss: 2.7735e-04 - 86s/epoch - 57ms/step
Epoch 171/200
1493/1493 - 86s - loss: 1.3016e-04 - val_loss: 2.7509e-04 - 86s/epoch - 57ms/step
Epoch 172/200
1493/1493 - 86s - loss: 1.1518e-04 - val_loss: 1.1932e-04 - 86s/epoch - 57ms/step
Epoch 173/200
1493/1493 - 86s - loss: 9.7315e-05 - val_loss: 9.3623e-05 - 86s/epoch - 57ms/step
Epoch 174/200
1493/1493 - 86s - loss: 9.4826e-05 - val_loss: 9.5089e-05 - 86s/epoch - 57ms/step
Epoch 175/200
1493/1493 - 86s - loss: 9.5665e-05 - val_loss: 1.2220e-04 - 86s/epoch - 57ms/step
Epoch 176/200
1493/1493 - 86s - loss: 9.5643e-05 - val_loss: 9.6097e-05 - 86s/epoch - 57ms/step
Epoch 177/200
1493/1493 - 85s - loss: 9.5510e-05 - val_loss: 8.4110e-05 - 85s/epoch - 57ms/step
Epoch 178/200
1493/1493 - 86s - loss: 9.2759e-05 - val_loss: 9.7782e-05 - 86s/epoch - 57ms/step
Epoch 179/200
1493/1493 - 86s - loss: 9.1789e-05 - val_loss: 1.0258e-04 - 86s/epoch - 57ms/step
Epoch 180/200
1493/1493 - 85s - loss: 9.1570e-05 - val_loss: 1.1342e-04 - 85s/epoch - 57ms/step
Epoch 181/200
1493/1493 - 86s - loss: 9.2723e-05 - val_loss: 9.4038e-05 - 86s/epoch - 57ms/step
Epoch 182/200
1493/1493 - 86s - loss: 9.1659e-05 - val_loss: 1.0273e-04 - 86s/epoch - 57ms/step
Epoch 183/200
1493/1493 - 86s - loss: 9.0244e-05 - val_loss: 1.0325e-04 - 86s/epoch - 57ms/step
Epoch 184/200
1493/1493 - 86s - loss: 9.1819e-05 - val_loss: 9.3026e-05 - 86s/epoch - 57ms/step
Epoch 185/200
1493/1493 - 86s - loss: 9.1843e-05 - val_loss: 3.1721e-04 - 86s/epoch - 57ms/step
Epoch 186/200
1493/1493 - 86s - loss: 1.3919e-04 - val_loss: 8.6522e-05 - 86s/epoch - 57ms/step
Epoch 187/200
1493/1493 - 86s - loss: 9.8169e-05 - val_loss: 9.5550e-05 - 86s/epoch - 57ms/step
Epoch 188/200
1493/1493 - 86s - loss: 9.6846e-05 - val_loss: 1.1471e-04 - 86s/epoch - 57ms/step
Epoch 189/200
1493/1493 - 86s - loss: 9.2557e-05 - val_loss: 1.0265e-04 - 86s/epoch - 57ms/step
Epoch 190/200
1493/1493 - 85s - loss: 9.2626e-05 - val_loss: 1.0985e-04 - 85s/epoch - 57ms/step
Epoch 191/200
1493/1493 - 86s - loss: 9.5394e-05 - val_loss: 3.4817e-04 - 86s/epoch - 57ms/step
Epoch 192/200
1493/1493 - 86s - loss: 1.2494e-04 - val_loss: 9.7054e-05 - 86s/epoch - 57ms/step
Epoch 193/200
1493/1493 - 86s - loss: 9.9128e-05 - val_loss: 3.8455e-04 - 86s/epoch - 57ms/step
Epoch 194/200
1493/1493 - 86s - loss: 1.2242e-04 - val_loss: 7.9005e-04 - 86s/epoch - 57ms/step
Epoch 195/200
1493/1493 - 85s - loss: 1.7318e-04 - val_loss: 2.1117e-04 - 85s/epoch - 57ms/step
Epoch 196/200
1493/1493 - 86s - loss: 1.1692e-04 - val_loss: 8.6861e-05 - 86s/epoch - 57ms/step
Epoch 197/200
1493/1493 - 85s - loss: 9.9189e-05 - val_loss: 1.5435e-04 - 85s/epoch - 57ms/step
Epoch 198/200
1493/1493 - 85s - loss: 1.0684e-04 - val_loss: 1.3069e-04 - 85s/epoch - 57ms/step
Epoch 199/200
1493/1493 - 85s - loss: 9.8218e-05 - val_loss: 8.4382e-05 - 85s/epoch - 57ms/step
Epoch 200/200
1493/1493 - 86s - loss: 9.2700e-05 - val_loss: 9.4474e-05 - 86s/epoch - 57ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 9.447374031879008e-05
  1/332 [..............................] - ETA: 24s  9/332 [..............................] - ETA: 2s  17/332 [>.............................] - ETA: 2s 25/332 [=>............................] - ETA: 2s 33/332 [=>............................] - ETA: 1s 41/332 [==>...........................] - ETA: 1s 49/332 [===>..........................] - ETA: 1s 57/332 [====>.........................] - ETA: 1s 65/332 [====>.........................] - ETA: 1s 73/332 [=====>........................] - ETA: 1s 81/332 [======>.......................] - ETA: 1s 89/332 [=======>......................] - ETA: 1s 97/332 [=======>......................] - ETA: 1s105/332 [========>.....................] - ETA: 1s113/332 [=========>....................] - ETA: 1s121/332 [=========>....................] - ETA: 1s129/332 [==========>...................] - ETA: 1s137/332 [===========>..................] - ETA: 1s145/332 [============>.................] - ETA: 1s153/332 [============>.................] - ETA: 1s161/332 [=============>................] - ETA: 1s169/332 [==============>...............] - ETA: 1s177/332 [==============>...............] - ETA: 1s185/332 [===============>..............] - ETA: 0s193/332 [================>.............] - ETA: 0s201/332 [=================>............] - ETA: 0s209/332 [=================>............] - ETA: 0s217/332 [==================>...........] - ETA: 0s225/332 [===================>..........] - ETA: 0s233/332 [====================>.........] - ETA: 0s241/332 [====================>.........] - ETA: 0s249/332 [=====================>........] - ETA: 0s257/332 [======================>.......] - ETA: 0s265/332 [======================>.......] - ETA: 0s273/332 [=======================>......] - ETA: 0s281/332 [========================>.....] - ETA: 0s289/332 [=========================>....] - ETA: 0s297/332 [=========================>....] - ETA: 0s305/332 [==========================>...] - ETA: 0s313/332 [===========================>..] - ETA: 0s321/332 [============================>.] - ETA: 0s329/332 [============================>.] - ETA: 0s332/332 [==============================] - 2s 7ms/step
correlation 0.0010643114840988839
cosine 0.0008385015115888893
MAE: 0.005411794
RMSE: 0.009719756
r2: 0.9938721427418928
RMSE zero-vector: 0.23411466903540806
Full AutoEncoder
Model: "model_24"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_25 (InputLayer)       multiple                  0         
                                                                 
 dense_24 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_24 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 758)               2107998   
                                                                 
 batch_normalization_25 (Bat  (None, 758)              3032      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 758)               0         
                                                                 
 dense_25 (Dense)            (None, 2780)              2110020   
                                                                 
 batch_normalization_26 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2780)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 11,275,174
Trainable params: 11,262,538
Non-trainable params: 12,636
_________________________________________________________________
Encoder
Model: "model_25"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_26 (InputLayer)       [(None, 1264)]            0         
                                                                 
 input_25 (InputLayer)       multiple                  0         
                                                                 
 dense_24 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_24 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_24 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 758)               2107998   
                                                                 
=================================================================
Total params: 5,635,818
Trainable params: 5,630,258
Non-trainable params: 5,560
_________________________________________________________________
Decoder
Model: "model_26"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_27 (InputLayer)       [(None, 758)]             0         
                                                                 
 batch_normalization_25 (Bat  (None, 758)              3032      
 chNormalization)                                                
                                                                 
 re_lu_25 (ReLU)             (None, 758)               0         
                                                                 
 dense_25 (Dense)            (None, 2780)              2110020   
                                                                 
 batch_normalization_26 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_26 (ReLU)             (None, 2780)              0         
                                                                 
 dense_26 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 5,639,356
Trainable params: 5,632,280
Non-trainable params: 7,076
_________________________________________________________________
['2.2custom_n_b', 'mse', 64, 200, 0.0005, 0.6, 758, 9.270010923501104e-05, 9.447374031879008e-05, 0.0010643114840988839, 0.0008385015115888893, 0.005411793943494558, 0.009719756431877613, 0.9938721427418928, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
./DATAFILES/MP_GapFeats_2.2final_custom_n_b already created.
Shape of dataset to encode: (106113, 1264)
Model: "model_27"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       [(None, 1264)]            0         
                                                                 
 dense_27 (Dense)            (None, 2780)              3516700   
                                                                 
 batch_normalization_27 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_27 (ReLU)             (None, 2780)              0         
                                                                 
 bottleneck (Dense)          (None, 695)               1932795   
                                                                 
 batch_normalization_28 (Bat  (None, 695)              2780      
 chNormalization)                                                
                                                                 
 re_lu_28 (ReLU)             (None, 695)               0         
                                                                 
 dense_28 (Dense)            (None, 2780)              1934880   
                                                                 
 batch_normalization_29 (Bat  (None, 2780)             11120     
 chNormalization)                                                
                                                                 
 re_lu_29 (ReLU)             (None, 2780)              0         
                                                                 
 dense_29 (Dense)            (None, 1264)              3515184   
                                                                 
=================================================================
Total params: 10,924,579
Trainable params: 10,912,069
Non-trainable params: 12,510
_________________________________________________________________
Epoch 1/200
1493/1493 - 82s - loss: 0.0093 - val_loss: 0.0044 - 82s/epoch - 55ms/step
Epoch 2/200
1493/1493 - 81s - loss: 0.0029 - val_loss: 0.0038 - 81s/epoch - 54ms/step
Epoch 3/200
1493/1493 - 81s - loss: 0.0021 - val_loss: 0.0015 - 81s/epoch - 54ms/step
Epoch 4/200
1493/1493 - 81s - loss: 0.0017 - val_loss: 0.0029 - 81s/epoch - 54ms/step
Epoch 5/200
1493/1493 - 81s - loss: 0.0016 - val_loss: 0.0019 - 81s/epoch - 54ms/step
Epoch 6/200
1493/1493 - 81s - loss: 0.0015 - val_loss: 0.0015 - 81s/epoch - 54ms/step
Epoch 7/200
1493/1493 - 81s - loss: 0.0013 - val_loss: 9.4555e-04 - 81s/epoch - 54ms/step
Epoch 8/200
1493/1493 - 81s - loss: 0.0012 - val_loss: 9.5727e-04 - 81s/epoch - 54ms/step
Epoch 9/200
1493/1493 - 81s - loss: 0.0011 - val_loss: 0.0011 - 81s/epoch - 54ms/step
Epoch 10/200
1493/1493 - 81s - loss: 0.0010 - val_loss: 7.8982e-04 - 81s/epoch - 54ms/step
Epoch 11/200
1493/1493 - 81s - loss: 8.9052e-04 - val_loss: 7.2458e-04 - 81s/epoch - 54ms/step
Epoch 12/200
1493/1493 - 81s - loss: 7.4521e-04 - val_loss: 0.0017 - 81s/epoch - 54ms/step
Epoch 13/200
1493/1493 - 81s - loss: 7.5062e-04 - val_loss: 0.0015 - 81s/epoch - 54ms/step
Epoch 14/200
1493/1493 - 81s - loss: 7.5728e-04 - val_loss: 5.7710e-04 - 81s/epoch - 54ms/step
Epoch 15/200
1493/1493 - 81s - loss: 6.3778e-04 - val_loss: 5.7380e-04 - 81s/epoch - 54ms/step
Epoch 16/200
1493/1493 - 81s - loss: 5.6580e-04 - val_loss: 5.2026e-04 - 81s/epoch - 54ms/step
Epoch 17/200
1493/1493 - 81s - loss: 5.2010e-04 - val_loss: 7.0129e-04 - 81s/epoch - 54ms/step
Epoch 18/200
slurmstepd: error: *** JOB 35401999 ON mb-icg101 CANCELLED AT 2023-01-01T01:36:16 DUE TO TIME LIMIT ***
